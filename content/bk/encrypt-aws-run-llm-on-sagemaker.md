---
title: "AWS: Run LLM on SageMaker"
date: 2024-08-09T21:56:40+07:00
draft: false
authors: ["hoangmnsd"]
#categories: [Tech-Tutorials]
#tags: [LLM,SageMaker,AWS]
comments: true
toc: true
# below props are for the404blog theme
authorImg: "https://raw.githubusercontent.com/hoangmnsd/images-bucket/master/static/images/hoangmsnd-avatar001.jpg"
description: "Run LLM on AWS SageMaker"
---

## Overview

SageMaker l√† 1 AWS service cung c·∫•p kh·∫£ nƒÉng qu·∫£n l√Ω/build/deploy/run c√°c Machine Learning task.

C√≥ 1 v√†i term m√† m√¨nh ch·∫°m ƒë·∫øn:

- Application -> Notebook instance: b·∫°n t·∫°o 1 VM ri√™ng cho vi·ªác ch·∫°y jupyter code notebook, th√≠ch h·ª£p cho vi·ªác d√πng chung c·∫£ team.
- Admin -> Domain - user trong domain: ch·∫Øc l√† ƒë·ªÉ t·∫°o group c√°c user l√†m vi·ªác t·∫≠p trung theo 1 domain n√†o ƒë√≥.
- Inference -> Models: B·∫°n deploy c√°c models l√™n ƒë√¢y, ch∆∞a t√≠nh ti·ªÅn.
- Inference -> Endpoint Configuration: Endpoint config, ch∆∞a t√≠nh ti·ªÅn. B·∫°n c√≥ th·ªÉ d√πng c√°i n√†y ƒë·ªÉ t·∫°o ra Endpoints.
- Inference -> Endpoints: C√°c model b·∫°n deploy s·∫Ω t·∫°o ra c√°c endpoints, `InService` c√†ng l√¢u c√†ng t·ªën nhi·ªÅu ti·ªÅn.
- C√≥ ki·ªÉu Real-time endpoint v√† Serverless endpoint: real-time th√¨ ko d√πng c≈©ng t·ªën ti·ªÅn. Serverless t√≠nh ti·ªÅn ki·ªÉu pay-as-you-go.
- Nh∆∞ng deploy ki·ªÉu Serverless c√≥ nhi·ªÅu h·∫°n ch·∫ø, v√≠ d·ª• support 1GPU th√¥i, RAM ch·ªâ max l√† 6GB th√¥i, Image max 10GB th√¥i. Code sample t√¨m kh√≥, m√¨nh g·∫∑p l·ªói li√™n t·ª•c v√† v·∫´n ch∆∞a deploy ƒëc Model m√¨nh mu·ªën l√™n Serverless v√¨ l·ªói.
- C√≥ th·ªÉ deploy Endpoints, d√πng xong th√¨ x√≥a ƒëi, khi n√†o c·∫ßn th√¨ l√†m deploy l·∫°i t·ª´ c√°i Endpoint configurations.

## 1. Deploy Model to SageMaker from Jupyter Notebook instance

M√¨nh l√†m theo video n√†y: C√≥ v·∫ª d·ªÖ hi·ªÉu nh·∫•t, h∆∞·ªõng d·∫´n d√πng JupyterNoteBook instance, deploy model l√™n SageMaker, r·ªìi c√≤n test connect t·ª´ Lambda, API Gateway lu√¥n: https://www.youtube.com/watch?v=A9Pu4xg-Nas

H∆∞·ªõng l√†m s·∫Ω l√† b·∫°n deploy 1 Jupyter Notebook instance (v√≠ d·ª•: m2.medium).
R·ªìi b·∫°n l√™n ƒë√≥ code, deploy model l√™n 1 instance kh√°c, type c√≥ th·ªÉ ph·∫£i m·∫°nh h∆°n v√¨ b·∫°n s·∫Ω run Model tr√™n ƒë√≥ (v√≠ d·ª•: ml.g4dn.xlarge - c√≥ GPU).
instance ch·ª©a model ƒë√≥ c√≥ endpoint v√† ch·∫°y li√™n t·ª•c `InService` ƒë·ªÉ serve request real-time. (T·ªën ti·ªÅn)

Khi b·∫°n ch·ªçn AWS SageMaker -> QuickSetup n√≥ s·∫Ω t·∫°o 1 Domain b·∫Øt ƒë·∫ßu = "QuickSetupDomain-xxx", t·∫°o 1 User profile trong domain ƒë√≥ "default-xxx", t·∫°o 2 S3 bucket "sagemaker-studio-xxx-6vkb4c8n3v8", "sagemaker-us-east-1-USERACCOUNTID". 
V√† b·∫°n ko thay ƒë·ªïi ƒë∆∞·ª£c t√™n c√°c c√°i setup n√†y.

T·∫°o notebook instance:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-create-notebook-instance1.jpg)

Ch·ªù notebook instance ƒë∆∞·ª£c deploy, th√¨ click "JupyterLab":
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-create-notebook-instance1-in-svc.jpg)

Ch·ªçn c√°i n√†y:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-conda-pytorch-p310.jpg)

D√πng pip install 1 s·ªë lib ƒë·∫ßu ti√™n: 
```python
!pip install transformers einops accelerate bitsandbytes
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-install-pip-1.jpg)

Ch·ªù install xong:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-install-pip-1-ok.jpg)

g√µ ti·∫øp 1 v√†i command:
```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import pipeline
import torch
import base64
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-command-2.jpg)

B·ªüi v√¨ m√¨nh s·∫Ω d√πng HuggingFace model sau: M√¨nh s·∫Ω d√πng Model n√†y: https://huggingface.co/MBZUAI/LaMini-T5-738M/tree/main

N√™n copy c√°i n√†y:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-hf-model-copy.jpg)

v√† g√µ ti·∫øp command:
```python
checkpoint = "MBZUAI/LaMini-T5-738M"
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-command-3.jpg)

g√µ ti·∫øp c√°c command, `device_map="auto"` b·ªã l·ªói n√™n m√¨nh chuy·ªÉn th√†nh `cpu`:
```python
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map="cpu", torch_dtype=torch.float32)
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-command-4.jpg)

Th√™m command sau ƒë·ªÉ install langchain:
```python
!pip install langchain
!pip install langchain-community langchain-core
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-command-5.jpg)

Th√™m command sau ƒë·ªÉ t·∫°o function:
```python
def llm_pipeline():
    pipe = pipeline(
        'text2text-generation',
        model=base_model,
        tokenizer=tokenizer,
        max_length=256,
        do_sample=True,
        temperature=0.3,
        top_p=0.95
    )
    local_llm = HuggingFacePipeline(pipeline=pipe)
    return local_llm
```

```python
input_prompt = "Write an article about Blockchain"
model = llm_pipeline()
generateed_text = model(input_prompt)
generateed_text
```

B·∫°n ph·∫£i ch·ªù 1 l√∫c ƒë·ªÉ command tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi Blockchain:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-command-6.jpg)

```python
pip show sagemaker
```

L√™n huggingface model url, copy ƒëo·∫°n code h·ªç cung c·∫•p:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-hf-copy-code-snipet.jpg)

Paste v√†o JupiterLabs, nh∆∞ng s·∫Ω s·ª≠a l·∫°i 1 ch√∫t:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-sagemaker-paste-edited.jpg)

N·∫øu b·ªã l·ªói n√†y, nghƒ©a l√† b·∫°n n√™n k√©o l√™n, upgrade version c·ªßa sagemaker lib:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-err-sagemaker.jpg)
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-sagemaker.jpg)

N·∫øu v·∫´n c√≥ d·∫•u * nh∆∞ h√¨nh n√†y nghƒ©a l√† code v·∫´n ƒëang Running:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-running.jpg)


V√†o SageMaker dashboard ch·ªó n√†y s·∫Ω th·∫•y Endpoint ƒëang ƒëc t·∫°o:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-deploy-endpoint.jpg)
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-deploy-endpointc.jpg)
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-deploy-models.jpg)

Quay l·∫°i JupyterLab th·∫•y code ƒë√£ ch·∫°y xong:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-deploy-models-result.jpg)

Endpoint ƒë√£ inservice ch√∫ t√™n endpoint l√† Global n√™n s·∫Ω l√† unique:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-deploy-models-insvc.jpg)

Ti·∫øp test th·ª≠ 1 s·ªë payload:

```
prompt = "Write a short article about Donald Trump"
payload = {
    "inputs": prompt,
    "parameters": {
        "do_sample": True
    }
}

response = predictor.predict(payload)
print(response)
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-next-question.jpg)

C√¢u tr·∫£ l·ªùi s·∫Ω c√≥ nhanh h∆°n v√¨ ta ƒë√£ deploy r·ªìi.


Gi·ªù v·ªõi 1 script python ta ch·ªâ c·∫ßn endpoint l√† c√≥ th·ªÉ call ƒë·∫øn SageMaker model v·ª´a deploy xong:

```py
ENDPOINT = "huggingface-pytorch-tgi-inference-2024-08-10-11-33-33-545"
import boto3
runtime = boto3.client('runtime.sagemaker')
response = runtime.invoke_endpoint(EndpointName=ENDPOINT, ContentType="application/json", Body=json.dumps(payload))
prediction = json.loads(response['Body'].read().decode('utf-8'))
prediction
```
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-jupyterlabs-next-question-with-boto3.jpg)

Youtuber `AI Anytime` ƒë√£ th·ª≠ t·∫°o Lambda function v√† d√πng boto3 ƒë·ªÉ call ƒë·∫øn endpoint ƒë√≥:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-call-fromlambda-youtube.jpg)

## 2. Pricing

ƒê√¢y l√† billing t·∫°m t√≠nh sau 1 ng√†y m√¨nh d√πng, du ch·ªâ run model ƒë∆∞·ª£c 1 l·∫ßn v√† g·ª≠i question 2,3 c√¢u.

![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-billing-temporary.jpg)

ƒê·ª•ng ƒë·∫øn Machine Learning l√† ƒë·ª•ng ƒë·∫øn ti·ªÅn nong t·ªën k√©m, h√£y c·∫©n th·∫≠n.

https://stackoverflow.com/questions/76212134/options-for-stopping-sagemaker-endpoint-to-avoid-charges

N·∫øu c·ª© ƒë·ªÉ Endpoint `InService` m√£i, B·∫°n s·∫Ω t·ªën nhi·ªÅu ti·ªÅn cho n√≥ ngay c·∫£ khi ko d√πng.  
B·∫°n c√≥ th·ªÉ delete Endpoint ƒëi, t·∫°o l·∫°i n√≥ khi c·∫ßn b·∫±ng `Endpoint Configurations`.  
C√≥ c·∫£ l·ª±a ch·ªçn Serverless endpoint, ch·ªâ tr·∫£ ti·ªÅn cho s·ªë l·∫ßn invoke_endpoint, nh∆∞ng n√≥ ko support GPU instance nh∆∞ `ml.g4dn.xlarge`:
> Also try serverless inference (https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html) if you're experimenting/for demos, so you're only paying per invocations rather than for the entire time the endpoint is in service.
> @durga_sury's answer is great, however, I should point out that Serverless Inference does not support GPU-enabled instances such as the ml.g4dn.xlarge

https://www.reddit.com/r/aws/comments/xfugrw/comment/iorsmnj/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button


## 3. Deploy Model to SageMaker serverless endpoint from localhost

## 3.1. Story

H∆∞·ªõng d·∫´n deploy t·ª´ NoteBook instance c·ªßa AWS ch·ªâ h·ª£p l√†m team share v·ªõi nhau. C√≤n l√†m c√° nh√¢n th√¨ nh∆∞ v·∫≠y qu√° t·ªën k√©m. 

Ch·ªâ c·∫ßn code ·ªü m√°y local v√† send request l√™n API ƒë·ªÉ deploy model l√™n SageMaker l√† ƒë∆∞·ª£c.

K·∫øt h·ª£p clip n√†y: https://www.youtube.com/watch?v=B0lFMUBnGEw

K·∫øt h·ª£p github n√†y: https://github.com/siddhardhan23/llama2-deploy-aws-sagemaker

K·∫øt h·ª£p b√†i n√†y: https://www.philschmid.de/sagemaker-llama-llm

M√¨nh mu·ªën l√†m ki·ªÉu Serverless Endpoint ƒë·ªÉ ko m·∫•t ti·ªÅn cho th·ªùi gian ko s·ª≠ d·ª•ng. T√†i li·ªáu n√†y: 

https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html

Ch√∫ √Ω v·ªÅ size c·ªßa SageMaker Serverless endpoint default ch·ªâ c√≥ ~3GB RAM, disk 5GB. V√¨ th·ªÉ deploy c√°c model n·∫∑ng r·∫•t kh√≥:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-serverless-memory-size-docs.jpg)

M√¨nh s·∫Ω test b·∫Øt ƒë·∫ßu v·ªõi `https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat/tree/main`

### 3.2. Deploy

C·∫ßn t·∫°o 1 user ƒë·ªÉ sau n√†y c√≥ ACCESS_KEY request ƒë·∫øn AWS API (Sau n√†y d√πng xong s·∫Ω x√≥a key ƒëi cho an to√†n):

![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-iam-user.jpg)

C·∫ßn t·∫°o 1 account Hunggingface ƒë·ªÉ l·∫•y Huggingface Token v√† accept term c·ªßa 1 s·ªë model.

```sh
python3.10 -m pip install boto3 sagemaker
```

Run file `create-sagemaker-serverless-endpoint.py`:

```sh
python create-sagemaker-serverless-endpoint.py
```

### 3.3. Troubleshoot

#### L·ªói IAM getRole

```s
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 565, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 1017, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::USERACCOUNTID:user/sagemaker_deployment is not authorized to perform: iam:GetRole on resource: role sagemaker_execution_role because no identity-based policy allows the iam:GetRole action
```
b·∫£n th√¢n User `sagemaker_deployment` ch∆∞a ƒë∆∞·ª£c g√°n ƒë·ªß quy·ªÅn, v√† c≈©ng ko t·ªìn t·∫°i role `sagemaker_execution_role`.
Minh c·∫ßn ph·∫£i add th√™m policy cho user:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-user-policy.jpg)

Role ARN th√¨ m√¨nh l·∫•y 1 c√°i role ƒë∆∞·ª£c t·∫°o t·ª´ tr∆∞·ªõc khi m√¨nh Quicksetup Domain n√≥ t·ª± t·∫°o cho:
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-role-default.jpg)

File `config.json` c·∫ßn s·ª≠a c√°i `ROLE_NAME` theo c√°i role ƒë√£ c√≥ s·∫µn trong IAM.

#### L·ªói syntax name c·ªßa model ko ƒë∆∞·ª£c c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát (nh∆∞ d·∫•u ch·∫•m)

```s
    return self._make_api_call(operation_name, kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 1017, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value 'codeqwen1.5-7b-chat-gguf-hoangmnsd-model' at 'modelName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([\-a-zA-Z0-9]*[a-zA-Z0-9])?
```

#### L·ªói quotas

```s
    res = self.sagemaker_client.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 565, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 1017, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'Memory size in MB per serverless endpoint' is 3072 MBs, with current utilization of 0 MBs and a request delta of 4096 MBs. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.
```
M·∫∑c ƒë·ªãnh c·ªßa account th√¨ Serverless RAM ch·ªâ c√≥ 3072MB nh∆∞ng b·∫°n l·∫°i set `memory_size_in_mb` l√† 4096 ch·∫≥ng h·∫°n, cao h∆°n quotas. M√¨nh ch∆∞a th·ª≠ request tƒÉng quota v√¨ ko c·∫ßn l·∫Øm, m√¨nh quy·∫øt ƒë·ªãnh s·ª≠a code gi·∫£m `memory_size_in_mb` xu·ªëng 3072


#### L·ªói ko download ƒë∆∞·ª£c .safetensors

```s
Couldn't call 'get_role'' to get Role ARN from role name sagemaker_deployment to get Role path.
----------------------------------------*Traceback (most recent call last):
  File "/opt/devops/LLM-lab/sagemaker-lab/create-sagemaker-serverless-endpoint.py", line 66, in <module>
    # Deploy model to an endpoint
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/huggingface/model.py", line 319, in deploy
    return super(HuggingFaceModel, self).deploy(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/model.py", line 1749, in deploy
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint codeqwen15-7b-chat-gguf-hoangmnsd-model-2024-08-11-07-04-51-412: Failed. Reason: Received server error (0) from model with message "An error occurred while handling request as the model process exited.". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/codeqwen15-7b-chat-gguf-hoangmnsd-model-2024-08-11-07-04-51-412 in account USERACCOUNTID for more information.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html


# Cloudwatch log:

[2m2024-08-11T07:23:37.228062Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "Qwen/CodeQwen1.5-7B-Chat-GGUF", revision: None, validation_workers: 2, sharded: None, num_shard: Some(1), quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T07:23:37.228249Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T07:25:21.732515Z[0m [31mERROR[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Download encountered an error: Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py", line 96, in weight_files
    filenames = weight_hub_files(model_id, revision, extension)
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py", line 37, in weight_hub_files
    raise EntryNotFoundError(
huggingface_hub.utils._errors.EntryNotFoundError: No .safetensors weights found for model Qwen/CodeQwen1.5-7B-Chat-GGUF and revision None.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/opt/conda/bin/text-generation-server", line 8, in <module>
    sys.exit(app())
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py", line 109, in download_weights
    utils.weight_files(model_id, revision, extension)
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py", line 101, in weight_files
    pt_filenames = weight_hub_files(model_id, revision, extension=".bin")
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py", line 37, in weight_hub_files
    raise EntryNotFoundError(
huggingface_hub.utils._errors.EntryNotFoundError: No .bin weights found for model Qwen/CodeQwen1.5-7B-Chat-GGUF and revision None.
Error: DownloadError
```

Nguy√™n nh√¢n l√† SageMaker ko support file GGUF. SageMaker ch·ªâ support ƒëu√¥i GPTQ, AWQ: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/discussions/15#655f1eb76821269b27fd8ce2


#### L·ªói insufficient memory

```s
sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml
Couldn't call 'get_role' to get Role ARN from role name sagemaker_deployment to get Role path.
--------------------------*Traceback (most recent call last):
  File "/opt/devops/LLM-lab/sagemaker-lab/create-sagemaker-serverless-endpoint.py", line 68, in <module>
    llm = llm_model.deploy(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/huggingface/model.py", line 319, in deploy
    return super(HuggingFaceModel, self).deploy(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/model.py", line 1749, in deploy
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint codeqwen15-7b-chat-hoangmnsd-endpoint: Failed. Reason: Ping failed due to insufficient memory.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html'


# Cloudwatch log:

[2m2024-08-11T07:47:14.768273Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "Qwen/CodeQwen1.5-7B-Chat", revision: None, validation_workers: 2, sharded: None, num_shard: Some(1), quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T07:47:14.768456Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T07:48:36.572375Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: model-00001-of-00004.safetensors
[2m2024-08-11T07:49:37.085851Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Downloaded /tmp/models--Qwen--CodeQwen1.5-7B-Chat/snapshots/7b0cc3380fe815e6f08fe2f80c03e05a8b1883d8/model-00001-of-00004.safetensors in 0:01:00.
[2m2024-08-11T07:49:37.085896Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download: [1/4] -- ETA: 0:03:00
[2m2024-08-11T07:49:37.097217Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: model-00002-of-00004.safetensors
```

M√¨nh th·ª≠ chuy·ªÉn sang model `MBZUAI/LaMini-T5-738M` nh·∫π h∆°n ch·ªâ c√≥ kho·∫£ng 2.95GB.

Nh∆∞ng v·∫´n b·ªã l·ªói RAM DownloadError:

```s
  File "/opt/devops/LLM-lab/sagemaker-lab/create-sagemaker-serverless-endpoint.py", line 68, in <module>
    llm = llm_model.deploy(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/huggingface/model.py", line 319, in deploy
    return super(HuggingFaceModel, self).deploy(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/model.py", line 1749, in deploy
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint laminit5-738m-hoangmnsd-endpoint: Failed. Reason: Ping failed due to insufficient memory.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html


# Cloudwatch log:
[2m2024-08-11T08:29:55.863090Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "MBZUAI/LaMini-T5-738M", revision: None, validation_workers: 2, sharded: None, num_shard: Some(1), quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T08:29:55.863282Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T08:30:04.836935Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m No safetensors weights found for model MBZUAI/LaMini-T5-738M at revision None. Downloading PyTorch weights.
[2m2024-08-11T08:30:04.857846Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: pytorch_model.bin
[2m2024-08-11T08:30:41.259152Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Downloaded /tmp/models--MBZUAI--LaMini-T5-738M/snapshots/5fc690b5cf88f188fd7b8fd19aee6c43af183d8b/pytorch_model.bin in 0:00:36.
[2m2024-08-11T08:30:41.259264Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download: [1/1] -- ETA: 0
[2m2024-08-11T08:30:41.259366Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m No safetensors weights found for model MBZUAI/LaMini-T5-738M at revision None. Converting PyTorch weights to safetensors.
[2m2024-08-11T08:31:20.364603Z[0m [31mERROR[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Download process was signaled to shutdown with signal 9: 
Error: DownloadError
```

Check log th·∫•y ƒë√£ down v·ªÅ ƒë∆∞·ª£c, nh∆∞ng ko hi·ªÉu sao b·ªã shutdown with signal 9.

Th·ª≠ chuy·ªÉn sang `ml.g4dn.xlarge` v·∫´n b·ªã l·ªói t∆∞∆°ng t·ª±:
```s
    return super(HuggingFaceModel, self).deploy(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/model.py", line 1749, in deploy
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint laminit5-738m-hoangmnsd-endpoint: Failed. Reason: Ping failed due to insufficient memory.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html

# Cloudwatch log:

[2m2024-08-11T08:58:29.461460Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "MBZUAI/LaMini-T5-738M", revision: None, validation_workers: 2, sharded: None, num_shard: Some(1), quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T08:58:29.461647Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T08:59:51.736949Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m No safetensors weights found for model MBZUAI/LaMini-T5-738M at revision None. Downloading PyTorch weights.
[2m2024-08-11T08:59:51.781071Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: pytorch_model.bin
[2m2024-08-11T09:00:29.564279Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Downloaded /tmp/models--MBZUAI--LaMini-T5-738M/snapshots/5fc690b5cf88f188fd7b8fd19aee6c43af183d8b/pytorch_model.bin in 0:00:37.
[2m2024-08-11T09:00:29.564374Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download: [1/1] -- ETA: 0
[2m2024-08-11T09:00:29.564452Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m No safetensors weights found for model MBZUAI/LaMini-T5-738M at revision None. Converting PyTorch weights to safetensors.
Error: DownloadError
[2m2024-08-11T09:01:08.181942Z[0m [31mERROR[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Download process was signaled to shutdown with signal 9: 
```

ƒê·ªçc 1 comment:
> Your container got sigkill because it didn't have anough RAM to do the weights conversion. (https://github.com/huggingface/text-generation-inference/issues/451#issuecomment-1590887778)

C√≥ v·∫ª nh∆∞ nguy√™n nh√¢n l√† 3GB RAM c·ªßa Serverless endpoint m·∫∑c d√π donwload ƒë∆∞·ª£c model 3GB v·ªÅ nh∆∞ng c√≤n c·∫ßn ph·∫£i weights conversion n·ªØa. V√† nh∆∞ th·∫ø th√¨ 3GB l√† ko ƒë·ªß.
Chuy·ªÉn sang `TheBloke/TinyLlama-1.1B-python-v0.1-AWQ` 700MB th√¨ ko b·ªã l·ªói ƒë√≥ n·ªØa

#### L·ªói Could not import Flash Attention enabled models: CUDA is not available

```s
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint tinyllama1-1b-python-v01-hoangmnsd-endpoint: Failed. Reason: Ping failed due to insufficient memory.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html

# Cloudwatch Log:

[2m2024-08-11T09:34:57.275284Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "TheBloke/TinyLlama-1.1B-python-v0.1-AWQ", revision: None, validation_workers: 2, sharded: None, num_shard: Some(1), quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T09:34:57.275430Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T09:35:02.448288Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: model.safetensors
[2m2024-08-11T09:35:17.274389Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Downloaded /tmp/models--TheBloke--TinyLlama-1.1B-python-v0.1-AWQ/snapshots/a41947ea2b361cc12892f005aaeb8a55151e2015/model.safetensors in 0:00:14.
[2m2024-08-11T09:35:17.274454Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download: [1/1] -- ETA: 0
[2m2024-08-11T09:35:37.348319Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights.
[2m2024-08-11T09:35:37.348556Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard 0 [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:35:47.360285Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:35:57.374189Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:36:07.387239Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:36:17.169618Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Attention enabled models: CUDA is not available
[2m2024-08-11T09:36:17.400726Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:36:27.413939Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:36:31.932515Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard complete standard error output:
/opt/conda/lib/python3.9/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
  warn("The installed version of bitsandbytes was compiled without GPU support. " [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:36:31.932556Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard process was signaled to shutdown with signal 9 [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T09:36:32.029976Z[0m [31mERROR[0m [2mtext_generation_launcher[0m[2m:[0m Shard 0 failed to start
[2m2024-08-11T09:36:32.030017Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Shutting down shards
Error: ShardCannotStart
```

Th·ª≠ chuy·ªÉn sang instance type ko c√≥ GPU: `ml.m5.xlarge` v·∫´n l·ªói:

```s
# Cloudwatch Log:

[2m2024-08-11T10:04:22.959345Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "TheBloke/TinyLlama-1.1B-python-v0.1-AWQ", revision: None, validation_workers: 2, sharded: None, num_shard: Some(1), quantize: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T10:04:22.959526Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T10:04:28.587578Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: model.safetensors
[2m2024-08-11T10:04:35.676244Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Downloaded /tmp/models--TheBloke--TinyLlama-1.1B-python-v0.1-AWQ/snapshots/a41947ea2b361cc12892f005aaeb8a55151e2015/model.safetensors in 0:00:07.
[2m2024-08-11T10:04:35.676297Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download: [1/1] -- ETA: 0
[2m2024-08-11T10:04:37.113071Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights.
[2m2024-08-11T10:04:37.113278Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard 0 [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:04:41.508273Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Attention enabled models: CUDA is not available
[2m2024-08-11T10:04:47.151309Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:04:52.602876Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard complete standard error output:
/opt/conda/lib/python3.9/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
  warn("The installed version of bitsandbytes was compiled without GPU support. " [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:04:52.603008Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard process was signaled to shutdown with signal 9 [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:04:52.699716Z[0m [31mERROR[0m [2mtext_generation_launcher[0m[2m:[0m Shard 0 failed to start
[2m2024-08-11T10:04:52.699760Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Shutting down shards
Error: ShardCannotStart
```

#### L·ªói ValueError: quantization is not available on CPU

Th·ª≠ S·ª≠a code b·ªè 2 d√≤ng n√†y:
```py
'SM_NUM_GPUS': json.dumps(1),
'HF_MODEL_QUANTIZE': "bitsandbytes",
```
V√† s·ª≠a `health_check_timeout = 1200` (20min)

Th√¨ l·∫°i b·ªã l·ªói:
```s
[2m2024-08-11T10:36:18.314852Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "TheBloke/TinyLlama-1.1B-python-v0.1-AWQ", revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize: Some(Bitsandbytes), dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 2048, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: 8192, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 8080, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: Some("/tmp"), weights_cache_override: None, disable_custom_kernels: false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_domain: None, ngrok_username: None, ngrok_password: None, env: false }
[2m2024-08-11T10:36:18.315055Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-08-11T10:37:43.378840Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download file: model.safetensors
[2m2024-08-11T10:37:51.742956Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Downloaded /tmp/models--TheBloke--TinyLlama-1.1B-python-v0.1-AWQ/snapshots/a41947ea2b361cc12892f005aaeb8a55151e2015/model.safetensors in 0:00:08.
[2m2024-08-11T10:37:51.743918Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Download: [1/1] -- ETA: 0
[2m2024-08-11T10:38:11.654111Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights.
[2m2024-08-11T10:38:11.654344Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard 0 [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:38:21.666273Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:38:31.678217Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:38:41.706131Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:38:51.724629Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:39:01.747657Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard 0 to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2024-08-11T10:39:01.796340Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Attention enabled models: CUDA is not available
[2m2024-08-11T10:39:02.458703Z[0m [31mERROR[0m [2mtext_generation_launcher[0m[2m:[0m Error when initializing model
Traceback (most recent call last):
  File "/opt/conda/bin/text-generation-server", line 8, in <module>
    sys.exit(app())
  File "/opt/conda/lib/python3.9/site-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/typer/core.py", line 778, in main
    return _main(
  File "/opt/conda/lib/python3.9/site-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py", line 78, in serve
    server.serve(
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py", line 175, in serve
    asyncio.run(
  File "/opt/conda/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/lib/python3.9/asyncio/base_events.py", line 634, in run_until_complete
    self.run_forever()
  File "/opt/conda/lib/python3.9/asyncio/base_events.py", line 601, in run_forever
    self._run_once()
  File "/opt/conda/lib/python3.9/asyncio/base_events.py", line 1905, in _run_once
    handle._run()
  File "/opt/conda/lib/python3.9/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
> File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py", line 142, in serve_inner
    model = get_model(
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py", line 195, in get_model
    return CausalLM(
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/causal_lm.py", line 465, in __init__
    raise ValueError("quantization is not available on CPU")
ValueError: quantization is not available on CPU
[2m2024-08-11T10:39:03.149677Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard complete standard error output:
/opt/conda/lib/python3.9/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
  warn("The installed version of bitsandbytes was compiled without GPU support. "
Traceback (most recent call last):
  File "/opt/conda/bin/text-generation-server", line 8, in <module>
    sys.exit(app())
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py", line 78, in serve
    server.serve(
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py", line 175, in serve
    asyncio.run(
  File "/opt/conda/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py", line 142, in serve_inner
    model = get_model(
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py", line 195, in get_model
    return CausalLM(
  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/causal_lm.py", line 465, in __init__
    raise ValueError("quantization is not available on CPU")
ValueError: quantization is not available on CPU
 [2m[3mrank[0m[2m=[0m0[0m
Error: ShardCannotStart
[2m2024-08-11T10:39:03.222448Z[0m [31mERROR[0m [2mtext_generation_launcher[0m[2m:[0m Shard 0 failed to start
[2m2024-08-11T10:39:03.222492Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Shutting down shards
```

#### L·ªói Image size 11464101106 is greater than supported size 10737418240

N·∫øu d√πng `get_huggingface_llm_image_uri("huggingface",version="2.2.0")` th√¨ s·∫Ω b·ªã l·ªói:
```s
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint tinyllama1-1b-python-v01-hoangmnsd-endpoint: Failed. Reason: Image size 11464101106 is greater than supported size 10737418240. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html

```

T·ªõi ƒë√¢y Ch·ªãu, Chuy·ªÉn sang l√†m theo tutorial n√†y th√¨ l·∫°i OK:
https://www.philschmid.de/sagemaker-serverless-huggingface-distilbert

v√† l√†m theo y nguy√™n tutorial n√†y c≈©ng OK: https://github.com/huggingface/notebooks/blob/main/sagemaker/19_serverless_inference/sagemaker-notebook.ipynb

Log OK:
```s
$ python3.10 create-sagemaker-serverless-endpoint.py 
sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml
sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml
Couldn't call 'get_role' to get Role ARN from role name sagemaker_deployment to get Role path.
---!
model deployed to Sagemaker
```

![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-endpoint-serverless-1.jpg)
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-endpoint-serverless.jpg)
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-endpoint-serverless-config.jpg)
![](https://d32yh8fbac5ivo.cloudfront.net/static/images/aws-sagemaker-endpoint-serverless-model.jpg)

model tr√™n nh·∫π/ƒë∆°n gi·∫£n qu√°, invoke_endpoint ko c√≥ l·ªói g√¨.

M√¨nh s·∫Ω s·ª≠a model th√†nh c√°i `TheBloke/TinyLlama-1.1B-python-v0.1-AWQ`

#### Deploy OK nh∆∞ng l·ªói khi invoke_endpoint

Th·ª≠ s·ª´a file tr√™n ch·ªó HD_MODEL_ID th√¥i th√†nh `'HF_MODEL_ID':'TheBloke/TinyLlama-1.1B-python-v0.1-AWQ'`

Deploy model ƒëc nh∆∞ng invoke_endpoint l·∫°i b·ªã l·ªói:
```s
$ python3.10 create-sagemaker-serverless-endpoint.py 
sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml
sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml
Couldn't call 'get_role' to get Role ARN from role name sagemaker_deployment to get Role path.
---!
model deployed to Sagemaker
Traceback (most recent call last):
  File "/opt/devops/LLM-lab/sagemaker-lab/create-sagemaker-serverless-endpoint.py", line 67, in <module>
    res = predictor.predict(data=data)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sagemaker/base_predictor.py", line 212, in predict
    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 565, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/botocore/client.py", line 1017, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message "{
  "code": 400,
  "type": "InternalServerException",
  "message": "\u0027llama\u0027"
}
". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/tinyllama1-1b-python-v01-hoangmnsd-endpoint in account 856233045188 for more information.


# Cloudwatch log:
https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/$252Faws$252Fsagemaker$252FEndpoints$252Ftinyllama1-1b-python-v01-hoangmnsd-endpoint/log-events/AllTraffic$252F1c17ee2437692fb56aea43a82351435b-67eb496d28b24fd9a659037bcb81baf5

python: can't open file '/usr/local/bin/deep_learning_container.py': [Errno 13] Permission denied
KeyError: 'llama'
```

Th·ª≠ deploy l·∫°i, s·ª≠a code ƒëo·∫°n n√†y:
```py
# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
  name=model_name,
  env=hub,                        # configuration for loading model from Hub
  role=role,                      # iam role with permissions to create an Endpoint
  transformers_version="4.27",    # transformers version used
  pytorch_version="1.13",         # pytorch version used
  py_version='py39',              # python version used
)
```

Th√¨ l·∫°i v·∫´n deploy OK nh∆∞ng invoke_endpoint c√≥ l·ªói:
```s
KeyError: 'llama'
```

Link n√†y n√≥i l·ªói tr√™n c√≥ th·ªÉ do version c·ªßa transformers_version < 4.28: https://github.com/huggingface/transformers/issues/23129

Th·ª≠ deploy l·∫°i, s·ª≠a code ƒëo·∫°n n√†y:
```py
# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
  name=model_name,
  env=hub,                        # configuration for loading model from Hub
  role=role,                      # iam role with permissions to create an Endpoint
  transformers_version="4.28.1",  # transformers version used
  pytorch_version="2.0.0",        # pytorch version used
  py_version='py310',             # python version used
)
```

Th√¨ l·∫°i b·ªã l·ªói:
```s
    predictor = huggingface_model.deploy(
  File "/home/ubuntu/.local/lib/python3.9/site-packages/sagemaker/huggingface/model.py", line 319, in deploy
    return super(HuggingFaceModel, self).deploy(
  File "/home/ubuntu/.local/lib/python3.9/site-packages/sagemaker/model.py", line 1749, in deploy
    self.sagemaker_session.endpoint_from_production_variants(
  File "/home/ubuntu/.local/lib/python3.9/site-packages/sagemaker/session.py", line 5728, in endpoint_from_production_variants
    return self.create_endpoint(
  File "/home/ubuntu/.local/lib/python3.9/site-packages/sagemaker/session.py", line 4586, in create_endpoint
    self.wait_for_endpoint(endpoint_name, live_logging=live_logging)
  File "/home/ubuntu/.local/lib/python3.9/site-packages/sagemaker/session.py", line 5371, in wait_for_endpoint
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint tinyllama1-1b-python-v01-hoangmnsd-endpoint: Failed. Reason: Image size 13605595695 is greater than supported size 10737418240. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html
```

#### K·∫øt lu·∫≠n t·∫°m th·ªùi

Nh∆∞ v·∫≠y l√† v·∫•n ƒë·ªÅ ·ªü ch·ªó khi deploy OK th√¨ invoke_endpoint b·ªã l·ªói `KeyError: 'llama'`, mu·ªën fix th√¨ ph·∫£i upgrade version c·ªßa `huggingface_model` (c√°c `py_version, pytorch_version, transformers_version`).

Nh∆∞ng upgrade version c·ªßa ch√∫ng th√¨ l√†m Image to ra v√† size image l·ªõn h∆°n m·ª©c support c·ªßa SageMaker serverless endpoint (10GB)

# REFERENCES

Docs v·ªÅ Serverless endpoint: https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html

https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/

https://www.philschmid.de/sagemaker-llama-llm

https://medium.com/@michael.leigh.stewart/self-hosting-large-language-models-using-aws-sagemaker-2568965159f1

https://github.com/aws/deep-learning-containers/tree/master

Create a web UI to interact with LLMs using Amazon SageMaker JumpStart:  
https://aws.amazon.com/blogs/machine-learning/create-a-web-ui-to-interact-with-llms-using-amazon-sagemaker-jumpstart/

C√≥ v·∫ª d·ªÖ hi·ªÉu nh·∫•t, h∆∞·ªõng d·∫´n d√πng JupyterNoteBook instance, deploy model l√™n SageMaker, r·ªìi c√≤n test connect t·ª´ Lambda, API Gateway lu√¥n: 
https://www.youtube.com/watch?v=A9Pu4xg-Nas

Repo c·ªßa Youtuber m√¨nh xem, c√≥ nhi·ªÅu repo r·∫•t hay:  
https://github.com/AIAnytime/Search-Your-PDF-App

https://www.youtube.com/watch?v=U72q95dHpRo

Notebook h∆∞·ªõng d·∫´n deploy Stable Diffusion l√™n SageMaker: 
https://github.com/Stability-AI/aws-jumpstart-examples/blob/main/sdxl-v1-0/sdxl-1-0-jumpstart.ipynb
> This sample notebook shows you how to deploy Stable Diffusion SDXL 1.0 from Stability AI as an endpoint on Amazon SageMaker.

Deploy model tr√™n SageMaker UI: https://www.youtube.com/watch?v=3y_TcDNC0HE

Let's run Llama 3.1 8B Model (Different Ways) using Transformers, Unsloth Chat Studio, and Ollama locally: https://www.youtube.com/watch?v=QpGVF_kElQY

Tutorial ch√≠nh th·ª©c c·ªßa huggingface: https://github.com/huggingface/notebooks/blob/main/sagemaker/19_serverless_inference/sagemaker-notebook.ipynb