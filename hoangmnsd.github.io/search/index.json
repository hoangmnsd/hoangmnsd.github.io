[{"content":" \u0026ldquo;Danger is very real, but fear is a choice\u0026rdquo; ‚Äî After Earth\n ","href":"/","title":"Home"},{"content":"","href":"/posts/","title":"All my posts"},{"content":"Love football, a Man United fan, also Messi fan, people around describe me as a silent who doesn\u0026rsquo;t speak often üëª\n\u0026ldquo;Messi c≈©ng nguy hi·ªÉm nh∆∞ ti·∫øng h√°t c·ªßa nh√¢n ng∆∞ trong truy·ªÅn thuy·∫øt. K·ªÉ c·∫£ khi l√† ƒë·ªëi th·ªß, b·∫°n c≈©ng r·∫•t th√≠ch theo d√µi c·∫≠u ·∫•y ch∆°i b√≥ng v√† th·∫≠t s·ª± th√¨ c·∫≠u ·∫•y ƒë√£ \u0026ldquo;quy·∫øn r≈©\u0026rdquo; b·∫°n.\u0026rdquo; ‚Äî Prandelli\n\u0026ldquo;S·ªõm hay mu·ªôn, ng∆∞·ªùi th·∫Øng l√† ng∆∞·ªùi nghƒ© m√¨nh c√≥ th·ªÉ. Xung ƒë·ªôt c√†ng l·ªõn th√¨ th·∫Øng l·ª£i c√†ng vinh quang.\u0026rdquo;\n","href":"/about/","title":"About"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/tags/azure/","title":"Azure"},{"content":"","href":"/bk/","title":"Bks"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/authors/hoangmnsd/","title":"hoangmnsd"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/categories/tech-tutorials/","title":"Tech-Tutorials"},{"content":"1. Story M·ªõi ƒë√¢y m√¨nh g·∫∑p 1 t√¨nh hu·ªëng l√† t·∫°o 1 VM (Ubuntu18.04-LTS) tr√™n Azure cho Dev s·ª≠ d·ª•ng, c√≥ to√†n quy·ªÅn sudo lu√¥n.\nDev setup nhi·ªÅu ph·∫ßn m·ªÅm, v√† sau 1 th·ªùi gian th√¨ VM b·ªã l·ªói ko th·ªÉ SSH v√†o ƒë∆∞·ª£c n·ªØa.\nssh: port 22: Connection timed out N√≥i chung l√† s·ª≠ d·ª•ng h√†ng Cloud th√¨ x·ª≠ l√Ω nh·ªØng case ki·ªÉu n√†y c√≥ t√†i li·ªáu kh√° ƒë·∫ßy ƒë·ªß, v√¨ Azure l√† 1 ƒë∆°n v·ªã cung c·∫•p Cloud l·ªõn.\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-ssh-connection\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/connect-linux-vm-sshconnection\n2. Connection troubleshooting 2.1. M·ªôt s·ªë b∆∞·ªõc n√™n check tr∆∞·ªõc Check xem CPU v√† Disk ƒëang nh∆∞ n√†o, c√≥ th·ªÉ full disk v√† full CPU d·∫´n ƒë·∫øn SSH ƒë·∫øn VM b·ªã timeout?\nTh·ª≠ ping ƒë·∫øn VM ƒëang l·ªói t·ª´ 1 VM kh√°c c√πng subnet xem?\nCheck xem NSG c√≥ ƒëang open port 22 ko?\nCheck xem VM c√≥ ƒëang ·ªü sau 1 Internal Load Balancer ko? ƒêi·ªÅu n√†y ko ngƒÉn c·∫£n vi·ªác SSH v√†o VM nh∆∞ng c√≥ th·ªÉ n√≥ ƒë√£ ch·∫∑n k·∫øt n·ªëi ra ngo√†i internet c·ªßa VM\nCheck NIC xem c√≥ ƒëang g·∫Øn v√†o NSG ko?\n2.2. C√°c c√°ch ƒë√£ ƒë·ªçc nh∆∞ng ch∆∞a th·ª≠   Delete VM nh∆∞ng gi·ªØ l·∫°i OS disk, deploy l·∫°i VM v·ªõi OS disk c≈© ƒë∆∞·ª£c attach v√†o\n  Run command trong VM b·∫±ng AZ cli:\n  vmname=yourvm;rg=yourrg;timestamp=`date +%d%Y%H%M%S`;az vm extension set ‚Äìresource-group $rg ‚Äìvm-name $vmname ‚Äìname customScript ‚Äìpublisher Microsoft.Azure.Extensions ‚Äìsettings \u0026#34;{\u0026#39;commandToExecute\u0026#39;: \u0026#39;bash -c \\\u0026#39;chmod 755 /var/empty/sshd;chown root:root /var/empty/sshd;systemctl start sshd;ps -eaf | grep sshd\\\u0026#34;,\u0026#39;timestamp\u0026#39;: \u0026#34;$((timestamp))\u0026#34;}\u0026#34; 2.3. C√°c c√°ch ƒë√£ th·ª≠ v√† b·ªã l·ªói  Reset ssh b·∫±ng az cli nh∆∞ n√†y, hi·ªán v·∫´n b·ªã l·ªói:  $ az vm user reset-ssh --resource-group rg-20230222124816 --name vm-123 (VMAgentStatusCommunicationError) VM 'vm-123' has not reported status for VM agent or extensions. Verify that the OS is up and healthy, the VM has a running VM agent, and that it can establish outbound connections to Azure storage. Please refer to https://aka.ms/vmextensionlinuxtroubleshoot for additional VM agent troubleshooting information. Code: VMAgentStatusCommunicationError Message: VM 'vm-123' has not reported status for VM agent or extensions. Verify that the OS is up and healthy, the VM has a running VM agent, and that it can establish outbound connections to Azure storage. Please refer to https://aka.ms/vmextensionlinuxtroubleshoot for additional VM agent troubleshooting information.  D√πng Serial Console tr√™n Azure Portal: (https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/connect-linux-vm-sshconnection#serial-console) C√≥ th·ªÉ s·∫Ω b·ªã l·ªói:  For more information on the Azure Serial Console, see \u0026lt;https://aka.ms/serialconsolelinux\u0026gt;. Preparing console connection to vm-123 ‚ñ† ‚ñ° ‚ñ° The serial console connection to the VM encountered an error: 'DenyAssignmentAuthorizationFailed' (403) - The client '' with object id '' has permission to perform action 'Microsoft.SerialConsole/serialPorts/connect/action' on scope '/subscriptions/subscriptionID/resourcegroups/rg/providers/Microsoft.Compute/virtualMachines/vm-123/providers/Microsoft.SerialConsole/serialPorts/0'; however, the access is denied because of the deny assignment with name 'System deny assignment created by managed application /subscriptions/subscriptionID/resourceGroups/rg/providers/Microsoft.Solutions/applications/AppsName' and Id '' at scope '/subscriptions/subscriptionID/resourceGroups/rg'. -\u0026gt; C√°ch n√†y th√¨ c·∫ßn d√πng account c√≥ quy·ªÅn cao h∆°n l√† OK.\n2.4. SSH ƒë∆∞·ª£c v√†o VM, x√°c ƒë·ªãnh v·∫•n ƒë·ªÅ D√πng Serial Console tr√™n Azure Portal\nSau khi d√πng th√¨ s·∫Ω c√≥ giao di·ªán ƒë·ªÉ SSH v√†o VM ngay tr√™n Browser\nM√¨nh ƒë√£ check connection th√¨ th·∫•y Vm n√†y g·∫∑p 3 l·ªói:\n 1 l√† ko ping ƒë·∫øn c√°c VM private c√πng m·∫°ng ƒë∆∞·ª£c 2 l√† c√°c version python v√† python3 default c·ªßa h·ªá th·ªëng b·ªã sai, d·∫´n ƒë·∫øn c√°c service c·ªßa h·ªá th·ªëng c≈©ng b·ªã die nh∆∞:\ncloud-config, cloud-init, cloud-final, cloud-init-local, networkd-dispatcher, walinuxagent, unattended-upgrades 3 l√† ko ping ra ngo√†i internet ƒë∆∞·ª£c:  # curl google.com curl: (6) Could not resolve host: google.com 2.5. T√¨m c√°ch x·ª≠ l√Ω c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn connection tr∆∞·ªõc T√¨m c√°ch x·ª≠ l√Ω c√°c l·ªói 1 v√† 3 tr∆∞·ªõc\n2.5.1. Fix l·ªói 1: private connection ƒë·∫øn c√°c VM c√πng m·∫°ng Th·ª≠ c√°ch n√†y: Reset NIC b·∫±ng giao di·ªán portal, ƒë·ªïi qua l·∫°i private IP: (ngu·ªìn https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/reset-network-interface-azure-linux-vm)\n-\u0026gt; V·∫´n l·ªói ko ping ƒë·∫øn c√°c VM c√πng m·∫°ng\nCheck nic status b·∫±ng ifconfig th√¨ nh∆∞ n√†y:\n$ ifconfig lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 14646 bytes 1143824 (1.1 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14646 bytes 1143824 (1.1 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 -\u0026gt; Hi·ªán ch·ªâ c√≥ interface lo l√† ƒëang UP.\nTrong check ifconfig -a l·∫°i ra 3 c√°i eth0, enP39453s1, lo, nh∆∞ng ko c√≥ c√°i n√†o UP.\n·ªû 1 VM b√¨nh th∆∞·ªùng s·∫Ω ph·∫£i nh∆∞ n√†y:\n$ ifconfig enP10290s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 ether 00:0d:3a:29:26:cf txqueuelen 1000 (Ethernet) RX packets 1081434 bytes 324837416 (324.8 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1450041 bytes 554743361 (554.7 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.16.10 netmask 255.255.255.0 broadcast 10.10.16.255 inet6 fe80::20d:3aff:fe29:26cf prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 00:0d:3a:29:26:cf txqueuelen 1000 (Ethernet) RX packets 981774 bytes 306181728 (306.1 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1286521 bytes 543930689 (543.9 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 952 bytes 128818 (128.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 952 bytes 128818 (128.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 -\u0026gt; V·∫≠y c·∫ßn enable interface eth0 v√† enP39453s1, D√πng command sau:\nifconfig eth0 up Check l·∫°i th√¨ ƒë√£ th·∫•y c·∫£ 3 interface ƒë√£ UP. Nh∆∞ng UP r·ªìi tuy nhi√™n ko c√≥ private IP trong eth0 nh∆∞ b√¨nh th∆∞·ªùng:\nenP39453s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 1 bytes 86 (86.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11 bytes 866 (866.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::6245:bdff:fe91:b0e2 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 1 bytes 72 (72.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11 bytes 866 (866.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 480 bytes 35664 (35.6 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 480 bytes 35664 (35.6 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 -\u0026gt; C·∫ßn ph·∫£i add Private IP v√†o interface l·∫°i: (ngu·ªìn: https://www.computerhope.com/unix/uifconfi.htm)\nifconfig eth0 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 # N·∫øu mu·ªën x√≥a ƒëi: # ip address del 10.10.10.6 dev eth0 Sau khi add private IP v√†o n√≥ s·∫Ω nh∆∞ n√†y:\n$ ifconfig enP39453s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 2061 bytes 125890 (125.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2027 bytes 123142 (123.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 inet6 fe80::6245:bdff:fe91:b0e2 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 2217 bytes 108256 (108.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 19 bytes 2054 (2.0 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 14646 bytes 1143824 (1.1 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14646 bytes 1143824 (1.1 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Gi·ªù b·∫°n s·∫Ω ping ƒë∆∞·ª£c ƒë·∫øn private ip kh√°c.\n$ ping 10.10.10.5 PING 10.10.10.5 56(84) bytes of data. 64 bytes from 10.10.10.5: icmp_seq=1 ttl=64 time=0.029 ms 64 bytes from 10.10.10.5: icmp_seq=2 ttl=64 time=0.067 ms 64 bytes from 10.10.10.5: icmp_seq=3 ttl=64 time=0.066 ms -\u0026gt; OK! Fix ƒë∆∞·ª£c l·ªói 1 ü§ó\n2.5.2. Fix l·ªói 3: public connection ƒë·∫øn Internet Ti·∫øp t·ª•c fix l·ªói 3. Hi·ªán v·∫´n ko c√≥ internet (t·ª´ VM ko connect ƒë·∫øn google.com ƒë∆∞·ª£c).\n# curl google.com curl: (6) Could not resolve host: google.com Run v√†i c√¢u l·ªánh linh tinh:\nservice hostname restart sudo dhclient enP1219s1 sudo dhclient eth0 sau ƒë√≥ t·ª± nhi√™n 1 l√∫c sau v√†o ƒë∆∞·ª£c internet:\ncheck file: /etc/resolv.conf th·∫•y t·ª± s·ª≠a th√†nh nh∆∞ n√†y:\ndomain xsdfsdgfegbtergfvwefweee.ax.internal.cloudapp.net search xsdfsdgfegbtergfvwefweee.ax.internal.cloudapp.net nameserver 168.63.129.16 v√†o ifconfig th·∫•y nh∆∞ sau, ko c√≥ g√¨ ƒë·∫∑c bi·ªát l·∫Øm, d√π IP ƒë∆∞·ª£c add th√™m v√†o enP62109s1, nh∆∞ng tr∆∞·ªõc ƒë√≥ m√¨nh c≈©ng add r·ªìi, ko c√≥ m·∫°ng:\n$ ifconfig enP62109s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 ether 60:45:bd:87:c4:a6 txqueuelen 1000 (Ethernet) RX packets 1350521 bytes 1871982693 (1.8 GB) RX errors 0 dropped 1010 overruns 0 frame 0 TX packets 66743 bytes 26351775 (26.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 inet6 fe80::6245:bdff:fe87:c4a6 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 60:45:bd:87:c4:a6 txqueuelen 1000 (Ethernet) RX packets 98512 bytes 1851838727 (1.8 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 56806 bytes 25686229 (25.6 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 14992 bytes 1216028 (1.2 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14992 bytes 1216028 (1.2 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Restart l·∫°i VM th√¨ v·∫´n b·ªã l·ªói khi curl connect ra internet, nh∆∞ng sau khi run m·∫•y c√¢u l·ªánh n√†y th√¨ restart l·∫°i ko th·∫•y l·ªói n·ªØa: (ngu·ªìn: http://www.freekb.net/Article?id=1196#:~:text=The%20dhclient%20command%20is%20used,will%20use%20the%20service%20command)\n$ curl google.com curl: (6) Could not resolve host: google.com $ sudo dhclient enP1219s1 sudo: unable to resolve host vm-123 RTNETLINK answers: File exists $ sudo dhclient eth0 sudo: unable to resolve host vm-123: Resource temporarily unavailable RTNETLINK answers: File exists $ service hostname restart Failed to restart hostname.service: Unit hostname.service is masked. $ curl google.com \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026quot;content-type\u0026quot; content=\u0026quot;text/html;charset=utf-8\u0026quot;\u0026gt; \u0026lt;TITLE\u0026gt;301 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;301 Moved\u0026lt;/H1\u0026gt; The document has moved \u0026lt;A HREF=\u0026quot;http://www.google.com/\u0026quot;\u0026gt;here\u0026lt;/A\u0026gt;. \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt; OK! ƒê√£ fix ƒë∆∞·ª£c l·ªói 3. ü•∞\n-\u0026gt; Nh∆∞ v·∫≠y c√≥ v·∫ª nh∆∞ c√°c command n√†y d√π b·ªã l·ªói nh∆∞ng v·∫´n c√≥ t√°c d·ª•ng:\nsudo dhclient eth0 service hostname restart 2.6. X·ª≠ l√Ω c√°c service h·ªá th·ªëng b·ªã l·ªói T√¨m c√°ch fix l·ªói 2.\nSau khi s·ª≠a python version m·∫∑c ƒë·ªãnh, c√°c service sau c·∫ßn check l·∫°i status:\nservice walinuxagent status service unattended-upgrades status service networkd-dispatcher status service cloud-init-local status service cloud-final status service cloud-init status service cloud-config status service sshd status N·∫øu s·ª≠a file .servie th√¨ ph·∫£i reload b·∫±ng command n√†y:\nsystemctl daemon-reload Debug:\ntail -n 200 /var/log/waagent.log N·∫øu service walinuxagent b·ªã l·ªói ko bi·∫øt v√¨ sao nh∆∞ n√†y (check log l·ªói ko c√≥ log):\n# service walinuxagent status ‚óè walinuxagent.service - Azure Linux Agent Loaded: loaded (/lib/systemd/system/walinuxagent.service; enabled; vendor preset: enabled) Drop-In: /lib/systemd/system/walinuxagent.service.d ‚îî‚îÄ10-Slice.conf, 11-CPUAccounting.conf, 12-CPUQuota.conf, 13-MemoryAccounting.conf Active: failed (Result: exit-code) since Tue 2023-03-14 02:29:09 UTC; 805ms ago Process: 9679 ExecStart=/usr/bin/python3 -u /usr/sbin/waagent -daemon (code=exited, status=1/FAILURE) Main PID: 9679 (code=exited, status=1/FAILURE) CPU: 26ms Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Consumed 26ms CPU time Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Service hold-off time Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Scheduled restart job, Mar 14 02:29:09 vm-123 systemd[1]: Stopped Azure Linux Agent. Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Consumed 26ms CPU time Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Start request repeated Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Failed with result 'ex Mar 14 02:29:09 vm-123 systemd[1]: Failed to start Azure Linux Agent. C√≥ v·∫ª service ƒë√£ b·ªã broken, fix b·∫±ng command sau:\nsudo apt-get --fix-broken install walinuxagent --fix-missing N·∫øu c√°c service h·ªá th·ªëng b·ªã l·ªói masked nh∆∞ n√†y:\n$ service unattended-upgrades status ‚óè unattended-upgrades.service Loaded: masked (/dev/null; bad) Active: inactive (dead) $ service unattended-upgrades restart Failed to restart unattended-upgrades.service: Unit unattended-upgrades.service is masked. $ systemctl enable networkd-dispatcher Failed to enable unit: Unit file networkd-dispatcher.service does not exist. $ service networkd-dispatcher start Failed to start networkd-dispatcher.service: Unit networkd-dispatcher.service not found. Fix = command sau:\n$ systemctl unmask unattended-upgrades Removed /etc/systemd/system/unattended-upgrades.service. $ systemctl enable unattended-upgrades unattended-upgrades.service is not a native service, redirecting to systemd-sysv-install. Executing: /lib/systemd/systemd-sysv-install enable unattended-upgrades $ systemctl start unattended-upgrades $ sudo apt-get --fix-broken install networkd-dispatcher --fix-missing V·∫≠y l√† xong, ƒë√£ fix ƒë∆∞·ª£c t·∫•t c·∫£ c√°c l·ªói, restart l·∫°i VM c≈©ng ko c√≥ l·ªói n·ªØa, SSH ƒë∆∞·ª£c b√¨nh th∆∞·ªùng, connect Internet b√¨nh th∆∞·ªùng.\nC·∫ßn ch√∫ √Ω Azure Ubuntu 18.04 v·∫´n d√πng Python3.6 l√† default cho python3. Python2.7 l√† default cho python. ƒê·ª´ng c·ªë g·∫Øng chuy·ªÉn default Python v√¨ c√≥ r·∫•t nhi·ªÅu service h·ªá th·ªëng ph·ª• thu·ªôc v√†o c√°c python default. Vi·ªác thay ƒë·ªïi default s·∫Ω g√¢y l·ªói ko ƒë√°ng c√≥.\nCREDIT https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-ssh-connection\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/connect-linux-vm-sshconnection\n","href":"/bk/encrypt-troubleshoot-azure-linux-vm-internet-ssh-connections/","title":"Troubleshoot Azure Linux VM: Internet \u0026 SSH Connections"},{"content":"1. Story M·ªõi ƒë√¢y m√¨nh g·∫∑p 1 t√¨nh hu·ªëng l√† t·∫°o 1 VM (Ubuntu18.04-LTS) tr√™n Azure cho Dev s·ª≠ d·ª•ng, c√≥ to√†n quy·ªÅn sudo lu√¥n.\nDev setup nhi·ªÅu ph·∫ßn m·ªÅm, v√† sau 1 th·ªùi gian th√¨ VM b·ªã l·ªói ko th·ªÉ SSH v√†o ƒë∆∞·ª£c n·ªØa.\nssh: port 22: Connection timed out N√≥i chung l√† s·ª≠ d·ª•ng h√†ng Cloud th√¨ x·ª≠ l√Ω nh·ªØng case ki·ªÉu n√†y c√≥ t√†i li·ªáu kh√° ƒë·∫ßy ƒë·ªß, v√¨ Azure l√† 1 ƒë∆°n v·ªã cung c·∫•p Cloud l·ªõn.\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-ssh-connection\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/connect-linux-vm-sshconnection\n2. Connection troubleshooting 2.1. M·ªôt s·ªë b∆∞·ªõc n√™n check tr∆∞·ªõc Check xem CPU v√† Disk ƒëang nh∆∞ n√†o, c√≥ th·ªÉ full disk v√† full CPU d·∫´n ƒë·∫øn SSH ƒë·∫øn VM b·ªã timeout?\nTh·ª≠ ping ƒë·∫øn VM ƒëang l·ªói t·ª´ 1 VM kh√°c c√πng subnet xem?\nCheck xem NSG c√≥ ƒëang open port 22 ko?\nCheck xem VM c√≥ ƒëang ·ªü sau 1 Internal Load Balancer ko? ƒêi·ªÅu n√†y ko ngƒÉn c·∫£n vi·ªác SSH v√†o VM nh∆∞ng c√≥ th·ªÉ n√≥ ƒë√£ ch·∫∑n k·∫øt n·ªëi ra ngo√†i internet c·ªßa VM\nCheck NIC xem c√≥ ƒëang g·∫Øn v√†o NSG ko?\n2.2. C√°c c√°ch ƒë√£ ƒë·ªçc nh∆∞ng ch∆∞a th·ª≠   Delete VM nh∆∞ng gi·ªØ l·∫°i OS disk, deploy l·∫°i VM v·ªõi OS disk c≈© ƒë∆∞·ª£c attach v√†o\n  Run command trong VM b·∫±ng AZ cli:\n  vmname=yourvm;rg=yourrg;timestamp=`date +%d%Y%H%M%S`;az vm extension set ‚Äìresource-group $rg ‚Äìvm-name $vmname ‚Äìname customScript ‚Äìpublisher Microsoft.Azure.Extensions ‚Äìsettings \u0026#34;{\u0026#39;commandToExecute\u0026#39;: \u0026#39;bash -c \\\u0026#39;chmod 755 /var/empty/sshd;chown root:root /var/empty/sshd;systemctl start sshd;ps -eaf | grep sshd\\\u0026#34;,\u0026#39;timestamp\u0026#39;: \u0026#34;$((timestamp))\u0026#34;}\u0026#34; 2.3. C√°c c√°ch ƒë√£ th·ª≠ v√† b·ªã l·ªói  Reset ssh b·∫±ng az cli nh∆∞ n√†y, hi·ªán v·∫´n b·ªã l·ªói:  $ az vm user reset-ssh --resource-group rg-20230222124816 --name vm-123 (VMAgentStatusCommunicationError) VM 'vm-123' has not reported status for VM agent or extensions. Verify that the OS is up and healthy, the VM has a running VM agent, and that it can establish outbound connections to Azure storage. Please refer to https://aka.ms/vmextensionlinuxtroubleshoot for additional VM agent troubleshooting information. Code: VMAgentStatusCommunicationError Message: VM 'vm-123' has not reported status for VM agent or extensions. Verify that the OS is up and healthy, the VM has a running VM agent, and that it can establish outbound connections to Azure storage. Please refer to https://aka.ms/vmextensionlinuxtroubleshoot for additional VM agent troubleshooting information.  D√πng Serial Console tr√™n Azure Portal: (https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/connect-linux-vm-sshconnection#serial-console) C√≥ th·ªÉ s·∫Ω b·ªã l·ªói:  For more information on the Azure Serial Console, see \u0026lt;https://aka.ms/serialconsolelinux\u0026gt;. Preparing console connection to vm-123 ‚ñ† ‚ñ° ‚ñ° The serial console connection to the VM encountered an error: 'DenyAssignmentAuthorizationFailed' (403) - The client '' with object id '' has permission to perform action 'Microsoft.SerialConsole/serialPorts/connect/action' on scope '/subscriptions/subscriptionID/resourcegroups/rg/providers/Microsoft.Compute/virtualMachines/vm-123/providers/Microsoft.SerialConsole/serialPorts/0'; however, the access is denied because of the deny assignment with name 'System deny assignment created by managed application /subscriptions/subscriptionID/resourceGroups/rg/providers/Microsoft.Solutions/applications/AppsName' and Id '' at scope '/subscriptions/subscriptionID/resourceGroups/rg'. -\u0026gt; C√°ch n√†y th√¨ c·∫ßn d√πng account c√≥ quy·ªÅn cao h∆°n l√† OK.\n2.4. SSH ƒë∆∞·ª£c v√†o VM, x√°c ƒë·ªãnh v·∫•n ƒë·ªÅ D√πng Serial Console tr√™n Azure Portal\nSau khi d√πng th√¨ s·∫Ω c√≥ giao di·ªán ƒë·ªÉ SSH v√†o VM ngay tr√™n Browser\nM√¨nh ƒë√£ check connection th√¨ th·∫•y Vm n√†y g·∫∑p 3 l·ªói:\n 1 l√† ko ping ƒë·∫øn c√°c VM private c√πng m·∫°ng ƒë∆∞·ª£c 2 l√† c√°c version python v√† python3 default c·ªßa h·ªá th·ªëng b·ªã sai, d·∫´n ƒë·∫øn c√°c service c·ªßa h·ªá th·ªëng c≈©ng b·ªã die nh∆∞:\ncloud-config, cloud-init, cloud-final, cloud-init-local, networkd-dispatcher, walinuxagent, unattended-upgrades 3 l√† ko ping ra ngo√†i internet ƒë∆∞·ª£c:  # curl google.com curl: (6) Could not resolve host: google.com 2.5. T√¨m c√°ch x·ª≠ l√Ω c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn connection tr∆∞·ªõc T√¨m c√°ch x·ª≠ l√Ω c√°c l·ªói 1 v√† 3 tr∆∞·ªõc\n2.5.1. Fix l·ªói 1: private connection ƒë·∫øn c√°c VM c√πng m·∫°ng Th·ª≠ c√°ch n√†y: Reset NIC b·∫±ng giao di·ªán portal, ƒë·ªïi qua l·∫°i private IP: (ngu·ªìn https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/reset-network-interface-azure-linux-vm)\n-\u0026gt; V·∫´n l·ªói ko ping ƒë·∫øn c√°c VM c√πng m·∫°ng\nCheck nic status b·∫±ng ifconfig th√¨ nh∆∞ n√†y:\n$ ifconfig lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 14646 bytes 1143824 (1.1 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14646 bytes 1143824 (1.1 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 -\u0026gt; Hi·ªán ch·ªâ c√≥ interface lo l√† ƒëang UP.\nTrong check ifconfig -a l·∫°i ra 3 c√°i eth0, enP39453s1, lo, nh∆∞ng ko c√≥ c√°i n√†o UP.\n·ªû 1 VM b√¨nh th∆∞·ªùng s·∫Ω ph·∫£i nh∆∞ n√†y:\n$ ifconfig enP10290s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 ether 00:0d:3a:29:26:cf txqueuelen 1000 (Ethernet) RX packets 1081434 bytes 324837416 (324.8 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1450041 bytes 554743361 (554.7 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.16.10 netmask 255.255.255.0 broadcast 10.10.16.255 inet6 fe80::20d:3aff:fe29:26cf prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 00:0d:3a:29:26:cf txqueuelen 1000 (Ethernet) RX packets 981774 bytes 306181728 (306.1 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1286521 bytes 543930689 (543.9 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 952 bytes 128818 (128.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 952 bytes 128818 (128.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 -\u0026gt; V·∫≠y c·∫ßn enable interface eth0 v√† enP39453s1, D√πng command sau:\nifconfig eth0 up Check l·∫°i th√¨ ƒë√£ th·∫•y c·∫£ 3 interface ƒë√£ UP. Nh∆∞ng UP r·ªìi tuy nhi√™n ko c√≥ private IP trong eth0 nh∆∞ b√¨nh th∆∞·ªùng:\nenP39453s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 1 bytes 86 (86.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11 bytes 866 (866.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet6 fe80::6245:bdff:fe91:b0e2 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 1 bytes 72 (72.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11 bytes 866 (866.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 480 bytes 35664 (35.6 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 480 bytes 35664 (35.6 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 -\u0026gt; C·∫ßn ph·∫£i add Private IP v√†o interface l·∫°i: (ngu·ªìn: https://www.computerhope.com/unix/uifconfi.htm)\nifconfig eth0 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 # N·∫øu mu·ªën x√≥a ƒëi: # ip address del 10.10.10.6 dev eth0 Sau khi add private IP v√†o n√≥ s·∫Ω nh∆∞ n√†y:\n$ ifconfig enP39453s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 2061 bytes 125890 (125.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2027 bytes 123142 (123.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 inet6 fe80::6245:bdff:fe91:b0e2 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 60:45:bd:91:b0:e2 txqueuelen 1000 (Ethernet) RX packets 2217 bytes 108256 (108.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 19 bytes 2054 (2.0 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 14646 bytes 1143824 (1.1 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14646 bytes 1143824 (1.1 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Gi·ªù b·∫°n s·∫Ω ping ƒë∆∞·ª£c ƒë·∫øn private ip kh√°c.\n$ ping 10.10.10.5 PING 10.10.10.5 56(84) bytes of data. 64 bytes from 10.10.10.5: icmp_seq=1 ttl=64 time=0.029 ms 64 bytes from 10.10.10.5: icmp_seq=2 ttl=64 time=0.067 ms 64 bytes from 10.10.10.5: icmp_seq=3 ttl=64 time=0.066 ms -\u0026gt; OK! Fix ƒë∆∞·ª£c l·ªói 1 ü§ó\n2.5.2. Fix l·ªói 3: public connection ƒë·∫øn Internet Ti·∫øp t·ª•c fix l·ªói 3. Hi·ªán v·∫´n ko c√≥ internet (t·ª´ VM ko connect ƒë·∫øn google.com ƒë∆∞·ª£c).\n# curl google.com curl: (6) Could not resolve host: google.com Run v√†i c√¢u l·ªánh linh tinh:\nservice hostname restart sudo dhclient enP1219s1 sudo dhclient eth0 sau ƒë√≥ t·ª± nhi√™n 1 l√∫c sau v√†o ƒë∆∞·ª£c internet:\ncheck file: /etc/resolv.conf th·∫•y t·ª± s·ª≠a th√†nh nh∆∞ n√†y:\ndomain xsdfsdgfegbtergfvwefweee.ax.internal.cloudapp.net search xsdfsdgfegbtergfvwefweee.ax.internal.cloudapp.net nameserver 168.63.129.16 v√†o ifconfig th·∫•y nh∆∞ sau, ko c√≥ g√¨ ƒë·∫∑c bi·ªát l·∫Øm, d√π IP ƒë∆∞·ª£c add th√™m v√†o enP62109s1, nh∆∞ng tr∆∞·ªõc ƒë√≥ m√¨nh c≈©ng add r·ªìi, ko c√≥ m·∫°ng:\n$ ifconfig enP62109s1: flags=6211\u0026lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 ether 60:45:bd:87:c4:a6 txqueuelen 1000 (Ethernet) RX packets 1350521 bytes 1871982693 (1.8 GB) RX errors 0 dropped 1010 overruns 0 frame 0 TX packets 66743 bytes 26351775 (26.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.6 netmask 255.255.255.0 broadcast 10.10.48.255 inet6 fe80::6245:bdff:fe87:c4a6 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 60:45:bd:87:c4:a6 txqueuelen 1000 (Ethernet) RX packets 98512 bytes 1851838727 (1.8 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 56806 bytes 25686229 (25.6 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 14992 bytes 1216028 (1.2 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14992 bytes 1216028 (1.2 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Restart l·∫°i VM th√¨ v·∫´n b·ªã l·ªói khi curl connect ra internet, nh∆∞ng sau khi run m·∫•y c√¢u l·ªánh n√†y th√¨ restart l·∫°i ko th·∫•y l·ªói n·ªØa: (ngu·ªìn: http://www.freekb.net/Article?id=1196#:~:text=The%20dhclient%20command%20is%20used,will%20use%20the%20service%20command)\n$ curl google.com curl: (6) Could not resolve host: google.com $ sudo dhclient enP1219s1 sudo: unable to resolve host vm-123 RTNETLINK answers: File exists $ sudo dhclient eth0 sudo: unable to resolve host vm-123: Resource temporarily unavailable RTNETLINK answers: File exists $ service hostname restart Failed to restart hostname.service: Unit hostname.service is masked. $ curl google.com \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026quot;content-type\u0026quot; content=\u0026quot;text/html;charset=utf-8\u0026quot;\u0026gt; \u0026lt;TITLE\u0026gt;301 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;301 Moved\u0026lt;/H1\u0026gt; The document has moved \u0026lt;A HREF=\u0026quot;http://www.google.com/\u0026quot;\u0026gt;here\u0026lt;/A\u0026gt;. \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt; OK! ƒê√£ fix ƒë∆∞·ª£c l·ªói 3. ü•∞\n-\u0026gt; Nh∆∞ v·∫≠y c√≥ v·∫ª nh∆∞ c√°c command n√†y d√π b·ªã l·ªói nh∆∞ng v·∫´n c√≥ t√°c d·ª•ng:\nsudo dhclient eth0 service hostname restart 2.6. X·ª≠ l√Ω c√°c service h·ªá th·ªëng b·ªã l·ªói T√¨m c√°ch fix l·ªói 2.\nSau khi s·ª≠a python version m·∫∑c ƒë·ªãnh, c√°c service sau c·∫ßn check l·∫°i status:\nservice walinuxagent status service unattended-upgrades status service networkd-dispatcher status service cloud-init-local status service cloud-final status service cloud-init status service cloud-config status service sshd status N·∫øu s·ª≠a file .servie th√¨ ph·∫£i reload b·∫±ng command n√†y:\nsystemctl daemon-reload Debug:\ntail -n 200 /var/log/waagent.log N·∫øu service walinuxagent b·ªã l·ªói ko bi·∫øt v√¨ sao nh∆∞ n√†y (check log l·ªói ko c√≥ log):\n# service walinuxagent status ‚óè walinuxagent.service - Azure Linux Agent Loaded: loaded (/lib/systemd/system/walinuxagent.service; enabled; vendor preset: enabled) Drop-In: /lib/systemd/system/walinuxagent.service.d ‚îî‚îÄ10-Slice.conf, 11-CPUAccounting.conf, 12-CPUQuota.conf, 13-MemoryAccounting.conf Active: failed (Result: exit-code) since Tue 2023-03-14 02:29:09 UTC; 805ms ago Process: 9679 ExecStart=/usr/bin/python3 -u /usr/sbin/waagent -daemon (code=exited, status=1/FAILURE) Main PID: 9679 (code=exited, status=1/FAILURE) CPU: 26ms Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Consumed 26ms CPU time Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Service hold-off time Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Scheduled restart job, Mar 14 02:29:09 vm-123 systemd[1]: Stopped Azure Linux Agent. Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Consumed 26ms CPU time Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Start request repeated Mar 14 02:29:09 vm-123 systemd[1]: walinuxagent.service: Failed with result 'ex Mar 14 02:29:09 vm-123 systemd[1]: Failed to start Azure Linux Agent. C√≥ v·∫ª service ƒë√£ b·ªã broken, fix b·∫±ng command sau:\nsudo apt-get --fix-broken install walinuxagent --fix-missing N·∫øu c√°c service h·ªá th·ªëng b·ªã l·ªói masked nh∆∞ n√†y:\n$ service unattended-upgrades status ‚óè unattended-upgrades.service Loaded: masked (/dev/null; bad) Active: inactive (dead) $ service unattended-upgrades restart Failed to restart unattended-upgrades.service: Unit unattended-upgrades.service is masked. $ systemctl enable networkd-dispatcher Failed to enable unit: Unit file networkd-dispatcher.service does not exist. $ service networkd-dispatcher start Failed to start networkd-dispatcher.service: Unit networkd-dispatcher.service not found. Fix = command sau:\n$ systemctl unmask unattended-upgrades Removed /etc/systemd/system/unattended-upgrades.service. $ systemctl enable unattended-upgrades unattended-upgrades.service is not a native service, redirecting to systemd-sysv-install. Executing: /lib/systemd/systemd-sysv-install enable unattended-upgrades $ systemctl start unattended-upgrades $ sudo apt-get --fix-broken install networkd-dispatcher --fix-missing V·∫≠y l√† xong, ƒë√£ fix ƒë∆∞·ª£c t·∫•t c·∫£ c√°c l·ªói, restart l·∫°i VM c≈©ng ko c√≥ l·ªói n·ªØa, SSH ƒë∆∞·ª£c b√¨nh th∆∞·ªùng, connect Internet b√¨nh th∆∞·ªùng.\nC·∫ßn ch√∫ √Ω Azure Ubuntu 18.04 v·∫´n d√πng Python3.6 l√† default cho python3. Python2.7 l√† default cho python. ƒê·ª´ng c·ªë g·∫Øng chuy·ªÉn default Python v√¨ c√≥ r·∫•t nhi·ªÅu service h·ªá th·ªëng ph·ª• thu·ªôc v√†o c√°c python default. Vi·ªác thay ƒë·ªïi default s·∫Ω g√¢y l·ªói ko ƒë√°ng c√≥.\nCREDIT https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-ssh-connection\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/connect-linux-vm-sshconnection\n","href":"/posts/encrypt-troubleshoot-azure-linux-vm-internet-ssh-connections/","title":"Troubleshoot Azure Linux VM: Internet \u0026 SSH Connections"},{"content":"","href":"/tags/python/","title":"Python"},{"content":"1. Create USB Boots Win10 1.1. Download windows 10 t·ª´ https://www.microsoft.com/software-download/windows10.\nB·∫≠t F12 v√¨ MS ch·ªâ cho ph√©p t·∫£i t·ª´ tr√¨nh duy·ªát mobile.\n1.2. Download Rufus t·ª´ https://rufus.ie/ Xong th√¨ c√≥ th·ªÉ eject USB v√† r√∫t ra\n2. Install Win10 from USB Boots 2.1. C·∫Øm USB ƒë√£ c√†i Win10 v√†o 2.2. Start/restart m√°y ·∫•n li√™n t·ª•c F10 ƒë·ªÉ v√†o Boots Menu screen\n2.3. Ch·ªçn c√°i USB b√™n tr√™n V√†o m√†n h√¨nh c√†i Win, ch·ªçn Win 10 Pro - Accept terms.\nC√≥ th·ªÉ ch·ªçn Format Partition, Delete Partition, Extend ƒë·ªÉ g·ªôp c√°c ph√¢n v√πng l·∫°i th√†nh 1 ·ªï.\nCh·ªù c√†i xong\nCh√∫ √Ω: n√™n ch·ªçn layout US Keyboard (ko n√™n ch·ªçn layout UK v√¨ 1 s·ªë ph√≠m s·∫Ω kh√°c).\n2.4. Sau khi v√†o th√¨ update Windows drivers B·∫±ng c√°ch v√†o Settings -\u0026gt; Update \u0026amp; Security -\u0026gt; Windows Update -\u0026gt; Check for Updates -\u0026gt; install\n2.5. Udate Intel drivers M·ªü Edge browser, search intel driver -\u0026gt; Get Start -\u0026gt; Download - Install - Accept, v√†o l·∫°i browser s·∫Ω th·∫•y nhi·ªÅu update availables -\u0026gt; Download all\nCh·ªù t·∫•t c·∫£ c√°c Drivers download h·∫øt r·ªìi th√¨ restart 1 th·ªÉ.\nC√≥ th·ªÉ s·∫Ω ph·∫£i restart v√†i l·∫ßn.\n3. C√†i c√°c ph·∫ßn m·ªÅm c·∫ßn thi·∫øt cho c√¥ng vi·ªác 3.1. Disable Bitlocker Encryption Check trong Disk Management, th·∫•y ·ªï C b·ªã Bitlocker Encrpyted, mu·ªën x√≥a disable Bitlocker ƒëi\nhttps://vn-z.vn/threads/o-cung-bi-khoa-bitlocker-xin-cac-cao-nhan-chi-giup.45182/page-4\nb·∫≠t CMD as Administrator, run command:\nreg add \u0026#34;HKLM\\SYSTEM\\CurrentControlSet\\Control\\BitLocker\u0026#34; /v PreventDeviceEncryption /t REG_DWORD /d 1 /f https://www.manageengine.com/products/os-deployer/help/how-to-disable-bitlocker-encryption.html\nOpen Windows Powershell in Administrator mode:\nDisable-BitLocker -MountPoint \u0026#34;C:\u0026#34; V√†o check l·∫°i trong Disk Management l√† th·∫•y c√°i d√≤ng ch·ªØ \u0026ldquo;Bitlocker Encrypted\u0026rdquo; b√™n c·∫°nh ·ªï C ƒë√£ b·ªã x√≥a\n3.2. Install MS Office T·∫°o account MS Developer E5 link v·ªõi github theo guideline tr√™n m·∫°ng\nLogin v√†o portal.office.com\nDownload and Install Office application\nSau ƒë√≥ v√†o Excel, Signin v√†o Excel b·∫±ng account b√™n tr√™n\n3.3. Copy b·ªô c√†i ƒë√£ download \u0026hellip;t·ª´ trong ·ªï c·ª©ng di d·ªông ra, c·∫ßn ph·∫ßn m·ªÅm n√†o th√¨ c√†i.\nRi√™ng IDM th√¨ c·ª© c√†i latest, xong r·ªìi d√πng script n√†y ƒë·ªÉ activate:\nhttps://github.com/lstprjct/IDM-Activation-Script\nRun on Powershell:\niwr -useb https://raw.githubusercontent.com/lstprjct/IDM-Activation-Script/main/IAS.ps1 | iex Ri√™ng Your Uninstaller Pro 7.5(2014), c√†i xong th√¨ ƒëi·ªÅn key sau:\nName: sharyn kolibob Registration code: 000016-9P0U6X-N5BBFB-EH9ZTE-DEZ8P0-9U4R72-RGZ6PF-EMYUAZ-9J6XQQ-89BV1Z List c√°c ph·∫ßn m·ªÅm ƒë√£ c√†i:\nFirefox Chrome PotPLayer OpenVPN Wireguard Putty Ubuntu1804LTS TeamViewer Notepad++ Discord PowerBIDesktop GitBash TorBrowser Utorrent FileZilla Zoom PasswordSafe Slack Telegram VLC WinSCP DockerDesktop Skype Postman Drawio Dropbox sakura SublimeText ACEStream Anki VSCode DaemonToolLite (C√≥ file) IDM (C√≥ script activate reset trial) YourUninstaller7.5-2014 (tr√™n m·∫°ng c√≥ account key) AdobeIllustrator2020 (c√≥ file iso v√† h∆∞·ªõng d·∫´n) AdobePhotoshop2021 (C√≥ file) BeyonCompare (C√≥ key) FastonCapture (C√≥ account tr√™n m·∫°ng share) KasperskyTotalSecurity (tr√™n vn-z c√≥ share key) Tableau (cho Linh) SQLServer2022 Developer (cho Linh) Azure Data Studio (cho Linh) MS SQL Server Tools 19 (cho Linh) C√†i extension cho Chrome:\nAdblock I still dont care about cookie The Great suspender 3.4. Activate Kaspersky Total Security L√™n topic n√†y xin key: https://vn-z.vn/threads/kaspersky-total-security-500-days.15942/page-380\nC√≥ th·ªÉ s·∫Ω ph·∫£i d√πng OpenVPN sang ƒê·ª©c ho·∫∑c Brazil Italia Egypt ƒë·ªÉ active key, trung binh key kho·∫£ng v√†i th√°ng 1 nƒÉm\nv√≠ d·ª• m√¨nh xin ƒëc key n√†y: A989W-BBYCM-B1NH5-82T2T - ƒê·ª©c (h·∫°n March 5, 2024)\n3.5. Troubeshooting Khi c√†i MS SQL Server:\n test connection t·ª´ MS SQL Server Studio 2019 xem c√≥ t·∫°o ƒë∆∞·ª£c Database ko (CREATE DATABASE test -\u0026gt; Execute). test connection t·ª´ Azure Data Studio ƒë·∫øn SQL Server 2022 xem ƒë∆∞·ª£c ko.  C√≥ th·ªÉ b·ªã l·ªói nh∆∞ n√†y:\nAzure data studio cannot connect to the database due to invalid owneruri (parameter 'owneruri') ho·∫∑c: A connection was successfully established with the server, but then an error occurred during the pre-login handshake. (provider: Shared Memory Provider, error: 0 - No process is on the other end of the pipe.) SQL Server Network Configuration -\u0026gt; Protocol for XXX -\u0026gt; Disable c√°i Shared Memory nh∆∞ n√†y:\nrestart\n4. Backup 4.1. Backup b·∫±ng ch·ª©c nƒÉng c·ªßa Win10 tr√™n ch√≠nh ·ªï SSD ƒë√≥ (ph√¢n v√πng kh√°c) b·ªã failed V√†o Setting -\u0026gt; Update \u0026amp; security -\u0026gt; Backup\nGo to Backup and restore (Windows 7)\nSetup a backup\nCh·ªçn ·ªï s·∫Ω l∆∞u file backup\nQu√° tr√¨nh run backup b·ªã failed, th√¥i b·ªè ko d√πng c√°ch n√†y n·ªØa\n4.2. Ghost Win10 with Acronis True Image Sau khi c√†i t·∫•t c·∫£ c√°c ph·∫ßn m·ªÅm c·∫ßn thi·∫øt, s·ª≠ d·ª•ng OK 1 th·ªùi gian th√¨ n√™n Ghost l·∫°i 1 image ƒë·ªÉ backup.\nSau n√†y n·∫øu d√≠nh virus, m√°y ch·∫≠m qu√°, th√¨ \u0026ldquo;bung\u0026rdquo; ra, ko c·∫ßn ph·∫£i c√†i ph·∫ßn m·ªÅm n·ªç kia g√¨ n·ªØa. C·ª© th·∫ø d√πng lu√¥n.\n4.2.1. T·∫°o usb boot anhdv https://anh-dv.com/usb-boot/tao-usb-boot-uefi-legacy-cuu-ho-may-tinh-chuyen\nch·ªù v√†i ph√∫t\u0026hellip; 4.2.2. T·∫°o file ghost .tib b·∫±ng Acronis True Image c√≥ trong usb tr√™n https://anh-dv.com/windows/tao-ghost-windows-10-uefi-voi-acronis-true-image/comment-page-2#comments\n4.2.3. Restore t·ª´ file ghost .tib b·∫±ng usb tr√™n https://anh-dv.com/ghost/ghost-windows-10-uefi-voi-acronis-true-image/comment-page-1#comments\n5. Install Python3.9 for Ubuntu install python3.9:\n# install python3.9 sudo apt-get update sudo apt-get upgrade -y sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.9 sudo apt install python3-pip But when I check version of pip3:\n# check pip3 version $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) # =\u0026gt; means you are using pip3 from python3.6, i want to install pip3 for python3.9 install pip3 for python3.9:\n# install pip3 for python3.9 # https://stackoverflow.com/questions/65644782/how-to-install-pip-for-python-3-9-on-ubuntu-20-04 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py sudo apt install python3.9-distutils python3.9 get-pip.py # check default python3 version $ python3 --version Python 3.6.9 # change default python3 to python3.9 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 update-alternatives --config python3 # =\u0026gt; select number of python3.9 # check default python3 version $ python3 --version Python 3.9.16 # =\u0026gt; ok, default python3 is now python3.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.9) # =\u0026gt; OK Troubleshooting khi d√πng pip3 install 1 package n√†o ƒë√≥:\n# Fix tr∆∞·ªõc 1 l·ªói https://bobbyhadz.com/blog/python-attributeerror-htmlparser-object-has-no-attribute-unescape pip install --upgrade setuptools pip3 install --upgrade setuptools pip install --upgrade distlib pip3 install --upgrade distlib pip3 install --upgrade pip Gi·ªù b·∫°n ƒë√£ c√≥ pip3 t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng version Python3.6 / Python3.9.\nƒêi·ªÅu quan tr·ªçng m√† b·∫°n c·∫ßn nh·ªõ l√† ƒë√¥i khi c√≥ nh·ªØng package c·∫ßn ph·∫£i install b·∫±ng python3.6 th√¨ m·ªõi work, n√™n ph·∫£i d√πng pip3 t∆∞∆°ng ·ª©ng c·ªßa Python3.6 ƒë·ªÉ install.\nT·∫•t nhi√™n h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p l√† c·ª© d√πng version cao h∆°n th√¨ t·ªët h∆°n.\nCREDIT https://quantrimang.com/cong-nghe/cach-tao-usb-boot-usb-cai-windows-bang-rufus-118460\nhttps://www.youtube.com/watch?v=I4_50zV63pA\u0026amp;ab_channel=TechwithStefan\nhttps://blogchiasekienthuc.com/thu-thuat-may-tinh/sao-luu-va-phuc-hoi-windows-tib.html\nhttps://anh-dv.com/usb-boot/tao-usb-boot-uefi-legacy-cuu-ho-may-tinh-chuyen\nhttps://anh-dv.com/ghost/ghost-windows-10-uefi-voi-acronis-true-image/comment-page-1#comments\n","href":"/bk/encrypt-setting-up-my-environments/","title":"Setting up my environments Windows, Ubuntu"},{"content":"1. Create USB Boots Win10 1.1. Download windows 10 t·ª´ https://www.microsoft.com/software-download/windows10.\nB·∫≠t F12 v√¨ MS ch·ªâ cho ph√©p t·∫£i t·ª´ tr√¨nh duy·ªát mobile.\n1.2. Download Rufus t·ª´ https://rufus.ie/ Xong th√¨ c√≥ th·ªÉ eject USB v√† r√∫t ra\n2. Install Win10 from USB Boots 2.1. C·∫Øm USB ƒë√£ c√†i Win10 v√†o 2.2. Start/restart m√°y ·∫•n li√™n t·ª•c F10 ƒë·ªÉ v√†o Boots Menu screen\n2.3. Ch·ªçn c√°i USB b√™n tr√™n V√†o m√†n h√¨nh c√†i Win, ch·ªçn Win 10 Pro - Accept terms.\nC√≥ th·ªÉ ch·ªçn Format Partition, Delete Partition, Extend ƒë·ªÉ g·ªôp c√°c ph√¢n v√πng l·∫°i th√†nh 1 ·ªï.\nCh·ªù c√†i xong\nCh√∫ √Ω: n√™n ch·ªçn layout US Keyboard (ko n√™n ch·ªçn layout UK v√¨ 1 s·ªë ph√≠m s·∫Ω kh√°c).\n2.4. Sau khi v√†o th√¨ update Windows drivers B·∫±ng c√°ch v√†o Settings -\u0026gt; Update \u0026amp; Security -\u0026gt; Windows Update -\u0026gt; Check for Updates -\u0026gt; install\n2.5. Udate Intel drivers M·ªü Edge browser, search intel driver -\u0026gt; Get Start -\u0026gt; Download - Install - Accept, v√†o l·∫°i browser s·∫Ω th·∫•y nhi·ªÅu update availables -\u0026gt; Download all\nCh·ªù t·∫•t c·∫£ c√°c Drivers download h·∫øt r·ªìi th√¨ restart 1 th·ªÉ.\nC√≥ th·ªÉ s·∫Ω ph·∫£i restart v√†i l·∫ßn.\n3. C√†i c√°c ph·∫ßn m·ªÅm c·∫ßn thi·∫øt cho c√¥ng vi·ªác 3.1. Disable Bitlocker Encryption Check trong Disk Management, th·∫•y ·ªï C b·ªã Bitlocker Encrpyted, mu·ªën x√≥a disable Bitlocker ƒëi\nhttps://vn-z.vn/threads/o-cung-bi-khoa-bitlocker-xin-cac-cao-nhan-chi-giup.45182/page-4\nb·∫≠t CMD as Administrator, run command:\nreg add \u0026#34;HKLM\\SYSTEM\\CurrentControlSet\\Control\\BitLocker\u0026#34; /v PreventDeviceEncryption /t REG_DWORD /d 1 /f https://www.manageengine.com/products/os-deployer/help/how-to-disable-bitlocker-encryption.html\nOpen Windows Powershell in Administrator mode:\nDisable-BitLocker -MountPoint \u0026#34;C:\u0026#34; V√†o check l·∫°i trong Disk Management l√† th·∫•y c√°i d√≤ng ch·ªØ \u0026ldquo;Bitlocker Encrypted\u0026rdquo; b√™n c·∫°nh ·ªï C ƒë√£ b·ªã x√≥a\n3.2. Install MS Office T·∫°o account MS Developer E5 link v·ªõi github theo guideline tr√™n m·∫°ng\nLogin v√†o portal.office.com\nDownload and Install Office application\nSau ƒë√≥ v√†o Excel, Signin v√†o Excel b·∫±ng account b√™n tr√™n\n3.3. Copy b·ªô c√†i ƒë√£ download \u0026hellip;t·ª´ trong ·ªï c·ª©ng di d·ªông ra, c·∫ßn ph·∫ßn m·ªÅm n√†o th√¨ c√†i.\nRi√™ng IDM th√¨ c·ª© c√†i latest, xong r·ªìi d√πng script n√†y ƒë·ªÉ activate:\nhttps://github.com/lstprjct/IDM-Activation-Script\nRun on Powershell:\niwr -useb https://raw.githubusercontent.com/lstprjct/IDM-Activation-Script/main/IAS.ps1 | iex Ri√™ng Your Uninstaller Pro 7.5(2014), c√†i xong th√¨ ƒëi·ªÅn key sau:\nName: sharyn kolibob Registration code: 000016-9P0U6X-N5BBFB-EH9ZTE-DEZ8P0-9U4R72-RGZ6PF-EMYUAZ-9J6XQQ-89BV1Z List c√°c ph·∫ßn m·ªÅm ƒë√£ c√†i:\nFirefox Chrome PotPLayer OpenVPN Wireguard Putty Ubuntu1804LTS TeamViewer Notepad++ Discord PowerBIDesktop GitBash TorBrowser Utorrent FileZilla Zoom PasswordSafe Slack Telegram VLC WinSCP DockerDesktop Skype Postman Drawio Dropbox sakura SublimeText ACEStream Anki VSCode DaemonToolLite (C√≥ file) IDM (C√≥ script activate reset trial) YourUninstaller7.5-2014 (tr√™n m·∫°ng c√≥ account key) AdobeIllustrator2020 (c√≥ file iso v√† h∆∞·ªõng d·∫´n) AdobePhotoshop2021 (C√≥ file) BeyonCompare (C√≥ key) FastonCapture (C√≥ account tr√™n m·∫°ng share) KasperskyTotalSecurity (tr√™n vn-z c√≥ share key) Tableau (cho Linh) SQLServer2022 Developer (cho Linh) Azure Data Studio (cho Linh) MS SQL Server Tools 19 (cho Linh) C√†i extension cho Chrome:\nAdblock I still dont care about cookie The Great suspender 3.4. Activate Kaspersky Total Security L√™n topic n√†y xin key: https://vn-z.vn/threads/kaspersky-total-security-500-days.15942/page-380\nC√≥ th·ªÉ s·∫Ω ph·∫£i d√πng OpenVPN sang ƒê·ª©c ho·∫∑c Brazil Italia Egypt ƒë·ªÉ active key, trung binh key kho·∫£ng v√†i th√°ng 1 nƒÉm\nv√≠ d·ª• m√¨nh xin ƒëc key n√†y: A989W-BBYCM-B1NH5-82T2T - ƒê·ª©c (h·∫°n March 5, 2024)\n3.5. Troubeshooting Khi c√†i MS SQL Server:\n test connection t·ª´ MS SQL Server Studio 2019 xem c√≥ t·∫°o ƒë∆∞·ª£c Database ko (CREATE DATABASE test -\u0026gt; Execute). test connection t·ª´ Azure Data Studio ƒë·∫øn SQL Server 2022 xem ƒë∆∞·ª£c ko.  C√≥ th·ªÉ b·ªã l·ªói nh∆∞ n√†y:\nAzure data studio cannot connect to the database due to invalid owneruri (parameter 'owneruri') ho·∫∑c: A connection was successfully established with the server, but then an error occurred during the pre-login handshake. (provider: Shared Memory Provider, error: 0 - No process is on the other end of the pipe.) SQL Server Network Configuration -\u0026gt; Protocol for XXX -\u0026gt; Disable c√°i Shared Memory nh∆∞ n√†y:\nrestart\n4. Backup 4.1. Backup b·∫±ng ch·ª©c nƒÉng c·ªßa Win10 tr√™n ch√≠nh ·ªï SSD ƒë√≥ (ph√¢n v√πng kh√°c) b·ªã failed V√†o Setting -\u0026gt; Update \u0026amp; security -\u0026gt; Backup\nGo to Backup and restore (Windows 7)\nSetup a backup\nCh·ªçn ·ªï s·∫Ω l∆∞u file backup\nQu√° tr√¨nh run backup b·ªã failed, th√¥i b·ªè ko d√πng c√°ch n√†y n·ªØa\n4.2. Ghost Win10 with Acronis True Image Sau khi c√†i t·∫•t c·∫£ c√°c ph·∫ßn m·ªÅm c·∫ßn thi·∫øt, s·ª≠ d·ª•ng OK 1 th·ªùi gian th√¨ n√™n Ghost l·∫°i 1 image ƒë·ªÉ backup.\nSau n√†y n·∫øu d√≠nh virus, m√°y ch·∫≠m qu√°, th√¨ \u0026ldquo;bung\u0026rdquo; ra, ko c·∫ßn ph·∫£i c√†i ph·∫ßn m·ªÅm n·ªç kia g√¨ n·ªØa. C·ª© th·∫ø d√πng lu√¥n.\n4.2.1. T·∫°o usb boot anhdv https://anh-dv.com/usb-boot/tao-usb-boot-uefi-legacy-cuu-ho-may-tinh-chuyen\nch·ªù v√†i ph√∫t\u0026hellip; 4.2.2. T·∫°o file ghost .tib b·∫±ng Acronis True Image c√≥ trong usb tr√™n https://anh-dv.com/windows/tao-ghost-windows-10-uefi-voi-acronis-true-image/comment-page-2#comments\n4.2.3. Restore t·ª´ file ghost .tib b·∫±ng usb tr√™n https://anh-dv.com/ghost/ghost-windows-10-uefi-voi-acronis-true-image/comment-page-1#comments\n5. Install Python3.9 for Ubuntu install python3.9:\n# install python3.9 sudo apt-get update sudo apt-get upgrade -y sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.9 sudo apt install python3-pip But when I check version of pip3:\n# check pip3 version $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) # =\u0026gt; means you are using pip3 from python3.6, i want to install pip3 for python3.9 install pip3 for python3.9:\n# install pip3 for python3.9 # https://stackoverflow.com/questions/65644782/how-to-install-pip-for-python-3-9-on-ubuntu-20-04 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py sudo apt install python3.9-distutils python3.9 get-pip.py # check default python3 version $ python3 --version Python 3.6.9 # change default python3 to python3.9 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 update-alternatives --config python3 # =\u0026gt; select number of python3.9 # check default python3 version $ python3 --version Python 3.9.16 # =\u0026gt; ok, default python3 is now python3.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.9) # =\u0026gt; OK Troubleshooting khi d√πng pip3 install 1 package n√†o ƒë√≥:\n# Fix tr∆∞·ªõc 1 l·ªói https://bobbyhadz.com/blog/python-attributeerror-htmlparser-object-has-no-attribute-unescape pip install --upgrade setuptools pip3 install --upgrade setuptools pip install --upgrade distlib pip3 install --upgrade distlib pip3 install --upgrade pip Gi·ªù b·∫°n ƒë√£ c√≥ pip3 t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng version Python3.6 / Python3.9.\nƒêi·ªÅu quan tr·ªçng m√† b·∫°n c·∫ßn nh·ªõ l√† ƒë√¥i khi c√≥ nh·ªØng package c·∫ßn ph·∫£i install b·∫±ng python3.6 th√¨ m·ªõi work, n√™n ph·∫£i d√πng pip3 t∆∞∆°ng ·ª©ng c·ªßa Python3.6 ƒë·ªÉ install.\nT·∫•t nhi√™n h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p l√† c·ª© d√πng version cao h∆°n th√¨ t·ªët h∆°n.\nCREDIT https://quantrimang.com/cong-nghe/cach-tao-usb-boot-usb-cai-windows-bang-rufus-118460\nhttps://www.youtube.com/watch?v=I4_50zV63pA\u0026amp;ab_channel=TechwithStefan\nhttps://blogchiasekienthuc.com/thu-thuat-may-tinh/sao-luu-va-phuc-hoi-windows-tib.html\nhttps://anh-dv.com/usb-boot/tao-usb-boot-uefi-legacy-cuu-ho-may-tinh-chuyen\nhttps://anh-dv.com/ghost/ghost-windows-10-uefi-voi-acronis-true-image/comment-page-1#comments\n","href":"/posts/encrypt-setting-up-my-environments/","title":"Setting up my environments Windows, Ubuntu"},{"content":"","href":"/tags/ubuntu/","title":"Ubuntu"},{"content":"","href":"/tags/windows/","title":"Windows"},{"content":"","href":"/tags/chatgpt/","title":"ChatGPT"},{"content":"1. Run tr√™n m√°y local m√¨nh s·∫Ω s·ª≠ d·ª•ng repo n√†y: https://github.com/acheong08/ChatGPT\nM√¨nh s·∫Ω c·∫ßn install python3 tr∆∞·ªõc\npython -m pip install --upgrade pip pip3 install --upgrade revChatGPT Get API Key: https://platform.openai.com/account/api-keys\n(B∆∞·ªõc n√†y v√¨ m√¨nh ƒë√£ mua 1 account ChatGPT 250k c√≥ s·∫µn 18$ credit r·ªìi n√™n l·∫•y API key kh√° d·ªÖ)\nAPI Key c√≥ d·∫°ng: sk-xxx\nfile basic-sample.py:\nfrom revChatGPT.V3 import Chatbot chatbot = Chatbot(api_key=\u0026quot;sk-xxx\u0026quot;) response = chatbot.ask(\u0026quot;Hello\u0026quot;) print(\u0026quot;ChatGPT: \u0026quot; + response) Run: python3 basic-sample.py, ChatGPT s·∫Ω in ra c√¢u tr·∫£ l·ªùi.\n2. Run on AWS Lambda 1 s·ªë th·ª© c·∫ßn chu·∫©n b·ªã m√† m√¨nh s·∫Ω b·ªè qua v√¨ ƒë√£ vi·∫øt nhi·ªÅu r·ªìi:\n Setup AWS API Gateway. Setup ƒë·ªÉ l·∫•y Telegram token c·ªßa bot.  2.1. Package Lambda layer revChatGPT M√¨nh ch·ªçn python3.9 v√¨ tr√™n AWS Lambda h·∫ßu h·∫øt c√°c function c·ªßa m√¨nh ƒëang d√πng python3.9\nD∆∞·ªõi m√°y local ƒëang l√† python3.6 m·∫∑c ƒë·ªãnh:\n# check default python3 version $ python3 --version Python 3.6.9 # check default pip3 version $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) # =\u0026gt; means you are using pip3 from python3.6 M√¨nh mu·ªën chuy·ªÉn sang m·∫∑c ƒë·ªãnh python3.9 ƒë·ªÉ package layer:\n# install python3.9 sudo apt-get update sudo apt-get upgrade -y sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.9 sudo apt install python3-pip # check default python3 version, still python3.6 $ python3 --version Python 3.6.9 # now, check pip3 version, still pip3 from python3.6 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) # =\u0026gt; understandable # now install pip3 for python3.9 # https://stackoverflow.com/questions/65644782/how-to-install-pip-for-python-3-9-on-ubuntu-20-04 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py sudo apt install python3.9-distutils python3.9 get-pip.py # change default python3 to python3.9 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 update-alternatives --config python3 # =\u0026gt; select number of python3.9 # check default python3 version $ python3 --version Python 3.9.16 # =\u0026gt; ok, default python3 is now python3.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.9) # =\u0026gt; OK, default pip3 is from python3.9 C√≥ th·ªÉ s·∫Ω g·∫∑p 1 s·ªë l·ªói khi install package, n√™n run m·∫•y command n√†y tr∆∞·ªõc, c≈©ng ch·∫£ m·∫•t g√¨:\n# Some recommend command to fix errors # https://bobbyhadz.com/blog/python-attributeerror-htmlparser-object-has-no-attribute-unescape pip install --upgrade setuptools pip3 install --upgrade setuptools pip install --upgrade distlib pip3 install --upgrade distlib pip3 install --upgrade pip ƒêinh ninh l√† d√πng python3.9 l√† ki·ªÉu g√¨ c≈©ng ok:\n# Now install chatGPT to folder layer mkdir -p ./package/python # package to zip layer for using on AWS LAMBDA pip3 install --upgrade --target ./package/python revChatGPT cd package/ zip -r9 ${OLDPWD}/layer.zip . Cu·ªëi c√πng upload package kho·∫£ng 15MB l√™n AWS Lambda layer xong run Lambda th√¨ b·ªã l·ªói syntax g√¨ ƒë√≥:\n{ \u0026quot;errorMessage\u0026quot;: \u0026quot;Syntax error in module 'lambda_function': invalid syntax (lambda_function.py, line 14)\u0026quot;, \u0026quot;errorType\u0026quot;: \u0026quot;Runtime.UserCodeSyntaxError\u0026quot;, \u0026quot;stackTrace\u0026quot;: [ \u0026quot; File \\\u0026quot;/var/task/lambda_function.py\\\u0026quot; Line 14\\n def get_inventory(instanceid)\\n\u0026quot; ] } Sau 1 h·ªìi debug th√¨ l·ªói ·ªü c√°i library m√† m√¨nh import t·ª´ layer revChatGPT.\nKh√¥ng hi·ªÉu v√¨ sao Lambda b·ªã l·ªói, sau khi th·ª≠ ƒë·ªß c√°c th·ªÉ lo·∫°i, m·∫•t c√¥ng c√†i python3.9 xong l·∫°i ch·∫£ d√πng ƒë∆∞·ª£c üò´\nƒê·∫øn khi d√πng python3.6 v√† pip3 from python3.6 ƒë·ªÉ package th√¨ Lambda l·∫°i run OK.\n\u0026hellip;Sau v√†i h√¥m th√¨ t√°c gi·∫£ c·ªßa Library update version m·ªõi, m√¨nh c√≥ th·ªÉ d√πng python3.9 ƒë·ªÉ package layer v√† Lambda l·∫°i run OK.\nCh√∫ √Ω l√† sau khi s·ª≠ d·ª•ng layer m·ªõi - ƒê∆∞·ª£c package t·ª´ pip3 (python3.9) kho·∫£ng 15MB, th√¨ Lambda li√™n t·ª•c b·ªã timeout error. Ph·∫£i tƒÉng CPU l√™n 512MB, c√≤n RAM v·∫´n 512MB th√¨ k·∫øt qu·∫£ m·ªõi tr·∫£ v·ªÅ nhanh ƒë∆∞·ª£c.\n2.2. Code Lambda Tr√™n th·ª±c t·∫ø m√¨nh ƒë∆∞·ª£c chatGPT v√†o 1 function Lambda c√≥ s·∫µn ƒëang l√†m nhi·ªám v·ª• kh√°c (get blog pwd),\nV·ªÅ c∆° b·∫£n code s·∫Ω nh∆∞ n√†y, ƒë√¢y l√† b·∫£n m√¨nh ƒë√£ l∆∞·ª£c b·ªè 1 s·ªë function ko li√™n quan, n√™n c≈©ng ch∆∞a test xem ch·∫°y ok ko:\nimport boto3 import requests import logging import datetime from botocore.exceptions import ClientError from datetime import datetime import json import os import sys from os.path import exists from revChatGPT.V3 import Chatbot \u0026#39;\u0026#39;\u0026#39; Please config Environment variable: ALLOWED_CHAT_ID (comma separated), CHATGPT_API_KEY, CONVERSATION_ID, FUNCTION_NAME, TELEGRAM_TOKEN Install library: pip3 install --upgrade revChatGPT (use pip3 of python3.6, if you use pip3 of python3.9 lambda will raise error) Functions: This script also forwards your question to ChatGPT and response its answer To add this bot to a group chat: You should talk to BotFather to disable privacy first, then add it to group chat, and promote it as administrator. It may takes some time to Telegram takes the change (1 hour in my case) \u0026#39;\u0026#39;\u0026#39; here = os.path.dirname(os.path.realpath(__file__)) sys.path.append(os.path.join(here, \u0026#34;./vendored\u0026#34;)) # Logger settings - CloudWatch # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) # Get Lambda environment variables TOKEN = os.environ.get(\u0026#39;TELEGRAM_TOKEN\u0026#39;) ALLOWED_CHAT_ID = os.environ.get(\u0026#39;ALLOWED_CHAT_ID\u0026#39;) CHATGPT_API_KEY = os.environ.get(\u0026#39;CHATGPT_API_KEY\u0026#39;) CONVERSATION_ID = os.environ.get(\u0026#39;CONVERSATION_ID\u0026#39;) FUNCTION_NAME = os.environ.get(\u0026#39;FUNCTION_NAME\u0026#39;) # Get current datetime currentDT = datetime.now() # supported command SUPPORTED_CMD = [\u0026#34;/help\u0026#34;, \u0026#34;/gpt YOUR_QUESTION\u0026#34;, \u0026#34;/new\u0026#34;] # conversation data file, store in temporary of Lambda instance TMP_CONVERSATION_DATA = \u0026#34;/tmp/conversation.json\u0026#34; def send_telegram_msg(msg, chat_id): \u0026#34;\u0026#34;\u0026#34; Tries to Send Telegram message. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: BASE_URL = \u0026#34;https://api.telegram.org/bot{}\u0026#34;.format(TOKEN) url = BASE_URL + \u0026#34;/sendMessage\u0026#34; data = {\u0026#34;text\u0026#34;: msg.encode(\u0026#34;utf8\u0026#34;), \u0026#34;chat_id\u0026#34;: chat_id} requests.post(url, data) except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info( \u0026#34;Send message to Telegram command throttled, automatically retrying...\u0026#34;) send_telegram_msg(msg, chat_id) else: logger.error( \u0026#34;Send message to Telegram command Failed!\\n%s\u0026#34;, str(err)) return False except: raise def chat_gpt(question, conversation_id, chat_id): \u0026#34;\u0026#34;\u0026#34; Tries to ask ChatGPT a question \u0026#34;\u0026#34;\u0026#34; try: chatbot = Chatbot(api_key=CHATGPT_API_KEY) # Check if conversation data file is exist or not file_exists = exists(TMP_CONVERSATION_DATA) logger.info(\u0026#34;Conversation data exist?: %s\u0026#34; % file_exists) logger.info(\u0026#34;Conversation ID: %s\u0026#34; % conversation_id) # if data file exist, load it to conversation if file_exists == True: chatbot.load(TMP_CONVERSATION_DATA) # send question to chatGPT, belong to specific conversation response = chatbot.ask(question, role=\u0026#34;user\u0026#34;, convo_id=conversation_id) # save conversation data to a json file chatbot.save(TMP_CONVERSATION_DATA) return response except Exception as err: if \u0026#39;500 Internal Server Error\u0026#39; in str(err): logger.error(\u0026#34;Error happens!\\n%s\u0026#34;, str(err)) msg = \u0026#34;Please try again. Internal Server Error: %s\u0026#34; + str(err) send_telegram_msg(msg, chat_id) return False except: raise def update_lambda_env_variable(function_name, key, value): \u0026#34;\u0026#34;\u0026#34; Tries to update Lambda Env variable. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: client = boto3.client(\u0026#39;lambda\u0026#39;) client.update_function_configuration( FunctionName=function_name, Environment={ \u0026#39;Variables\u0026#39;: { key: value, \u0026#34;ALLOWED_CHAT_ID\u0026#34;: ALLOWED_CHAT_ID, \u0026#34;CHATGPT_API_KEY\u0026#34;: CHATGPT_API_KEY, \u0026#34;PASSWORD\u0026#34;: ENCRYPT_PASSWORD, \u0026#34;TELEGRAM_TOKEN\u0026#34;: TOKEN, \u0026#34;FUNCTION_NAME\u0026#34;: function_name } } ) logger.info(\u0026#39;============Update Lambda Env Variable Successfully\u0026#39;) return True except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info(\u0026#34;Update Lambda Env Variable throttled, automatically retrying...\u0026#34;) update_lambda_env_variable(function_name, key, value) else: logger.error(\u0026#34;Update Lambda Env Variable Failed!\\n%s\u0026#34;, str(err)) return False except: raise def lambda_handler(event, context): print(\u0026#39;------------------\u0026#39;) logger.debug(event) data = json.loads(event[\u0026#34;body\u0026#34;]) try: # if you not set enough admin right for Telegram bot in a group chat,  # it may get some error since response will not have `message` element # that\u0026#39;s why I put this in try catch message = str(data[\u0026#34;message\u0026#34;][\u0026#34;text\u0026#34;]) # need to convert chat_id to string to find it it list chat_id = str(data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;id\u0026#34;]) logger.info(chat_id) allowed_chat_id_list = list(ALLOWED_CHAT_ID.split(\u0026#34;,\u0026#34;)) if chat_id in allowed_chat_id_list: logger.info(\u0026#34;chat_id is allowed\u0026#34;) encrypt_password = ENCRYPT_PASSWORD # first_name = data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;first_name\u0026#34;] # msg = \u0026#34;Please /help, {}\u0026#34;.format(first_name) # help command if SUPPORTED_CMD[0] in message: joined_msg = \u0026#34;\\n\u0026#34;.join(SUPPORTED_CMD) msg = \u0026#34;There are only a few commands that I understand:\\n%s\u0026#34; % joined_msg # send telegram message logger.info(msg) send_telegram_msg(msg, chat_id) # return answer for question via chatGPT elif message.startswith(\u0026#39;/gpt \u0026#39;): question = message.split(\u0026#34; \u0026#34;, 1)[1] logger.info(question) msg = chat_gpt(question, CONVERSATION_ID, chat_id) # send telegram message # logger.info(msg) send_telegram_msg(msg, chat_id) # increase conversation_id and update the enviroment variable elif message.startswith(\u0026#39;/new\u0026#39;): conversation_id = int(CONVERSATION_ID) + 1 key = \u0026#34;CONVERSATION_ID\u0026#34; value = str(conversation_id) update_lambda_env_variable(FUNCTION_NAME, key, value) msg = \u0026#34;OK, new conversation starts\u0026#34; send_telegram_msg(msg, chat_id) return True else: logger.info(\u0026#34;chat_id is not allowed\u0026#34;) return False except KeyError as err: logger.error(\u0026#34;Invalid response format! \\n%s\u0026#34;, str(err)) return False except: raise V·ªÅ m·∫∑t logic:\n M·ªói khi mu·ªën h·ªèi chatGPT 1 c√¢u th√¨ g√µ nh∆∞ v√≠ d·ª•: \u0026ldquo;/gpt qu·∫£ t√°o l√† g√¨?\u0026rdquo;. Sau m·ªói c√¢u tr·∫£ l·ªùi h√†m chat_gpt s·∫Ω save/load conversation v√†o 1 file /tmp/conversation.json c·ªßa Lambda container. Th·∫ø n√™n n√≥ c√≥ th·ªÉ reply li√™n t·ª•c nh∆∞ 1 conversation ngo√†i ƒë·ªùi th·∫≠t. (Mu·ªën test th√¨ n√≥i \u0026ldquo;continue\u0026rdquo;) ƒë·ªÉ n√≥ ti·∫øp t·ª•c ch·ªß ƒë·ªÅ v·ª´a h·ªèi. Nh∆∞ng v√¨ ƒë·∫∑t trong /tmp/conversation.json c·ªßa Lambda container, n√™n n·∫øu ko chat 1 th·ªùi gian, file n√†y s·∫Ω b·ªã x√≥a v√¨ container ƒë√£ b·ªã AWS delete. N·∫øu ƒëang chat li√™n t·ª•c m√† l·∫°i mu·ªën chatGPT qu√™n h·∫øt nh·ªØng ch·ªß ƒë·ªÅ v·ª´a xong, b·∫Øt ƒë·∫ßu 1 conversation m·ªõi th√¨ g√µ /new.  2.3. Troubleshooting Trong qu√° tr√¨nh l√†m th√¨:\nN·∫øu Telegram bot ko th·ªÉ nh·∫≠n ƒë∆∞·ª£c message t·ª´ 1 group chat. M·∫∑c d√π run OK khi chat ri√™ng v·ªõi n√≥.\nT√¨nh tr·∫°ng y nh∆∞ n√†y: https://stackoverflow.com/questions/64435576/telegram-webhook-doesnt-send-text-from-group-chats\nsau 1 l√∫c th√¨ l·∫°i ko th·∫•y l·ªói n·ªØa, kh√≥ hi·ªÉu. C√≥ v·∫ª nh∆∞ telegram api b·ªã cache ·ªü ƒë√¢u ƒë√≥, n√™n vi·ªác m√¨nh remove/add bot l√†m admin v√† disable privacy m·∫•t 1 l√∫c ƒë·ªÉ n√≥ nh·∫≠n ra.\nN·∫øu mu·ªën test vi·ªác write v√† read file t·ª´ folder /tmp c·ªßa Lambda:\nimport json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) logger.info(\u0026#34;writing...\u0026#34;) body = {\u0026#34;a\u0026#34;: \u0026#34;v\u0026#34;} data = json.dumps(body) with open(\u0026#39;/tmp/conversation.json\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(data) logger.info(\u0026#34;reading...\u0026#34;) with open(/tmp/conversation.json, \u0026#39;r\u0026#39;) as openfile: json_object = json.load(openfile) print(json_object) Nh∆∞·ª£c ƒëi·ªÉm c·ªßa revChatGPT.V1 l√† ch·∫≠m, ph·∫£i authen b·∫±ng access_token (s·ªëng dc 2 tu·∫ßn), ho·∫∑c user/password (d·ªÖ b·ªã Cloudflare h·ªèi thƒÉm), ho·∫∑c session_token (s·ªëng ƒë∆∞·ª£c ng·∫Øn).\nD√πng revChatGPT.V3 th√¨ c√≥ th·ªÉ d√πng API_KEY (s·ªëng l√¢u, nh∆∞ng m·∫•t ti·ªÅn), ph·∫£n h·ªìi kh√° nhanh.\nD√πng revChatGPT.V3 m√† mu·ªën chatGPT nh·ªõ ƒë∆∞·ª£c conversation c≈© th√¨ n√≥ y√™u c·∫ßu save v√† load conversation t·ª´ JSON file nh∆∞ code ph√≠a tr√™n.\nD√πng revChatGPT.V1 m√† mu·ªën chatGPT nh·ªõ ƒë∆∞·ª£c conversation c≈© th√¨ c·∫ßn l√†m ki·ªÉu n√†y:\nfrom revChatGPT.V1 import Chatbot config = { \u0026#34;access_token\u0026#34;: \u0026#34;eyJ...\u0026#34; } chatbot = Chatbot(config) response = \u0026#34;\u0026#34; conversation =\u0026#34;\u0026#34; parent_id =\u0026#34;\u0026#34; chatbot.conversation_id = conversation chatbot.parent_id = parent_id for data in chatbot.ask(\u0026#34;continue\u0026#34;): response = data[\u0026#34;message\u0026#34;] conversation = data[\u0026#39;conversation_id\u0026#39;] parent_id = data[\u0026#39;parent_id\u0026#39;] print(response) print(conversation) print(parent_id) del chatbot chatbot = Chatbot(config) chatbot.conversation_id = conversation chatbot.parent_id = parent_id for data in chatbot.ask(\u0026#34;continue\u0026#34;): response = data[\u0026#34;message\u0026#34;] print(response) Sau ƒë√≥ l√™n khung chat tr√™n Browser s·∫Ω th·∫•y 1 ƒëo·∫°n conversation m·ªõi ƒë∆∞·ª£c t·∫°o ra, ƒëi·ªÉm n√†y revChatGPT.V3 hi·ªán ko l√†m ƒë∆∞·ª£c.\nNh∆∞ng n√≥i chung l√† n√™n d√πng revChatGPT.V3 v√¨ n√≥ l√† official API c·ªßa OpenAI, s·ªõm mu·ªôn c≈©ng s·∫Ω thay th·∫ø V1.\n3. S·ª≠ d·ª•ng framework AWS Chalice https://github.com/aws/chalice\nChalice l√† 1 framework c·ªßa AWS ph√°t tri·ªÉn, d√†nh ri√™ng cho c√°c ·ª©ng d·ª•ng serverless ng√¥n ng·ªØ Python.\nN√≥ gi√∫p b·∫°n deploy 1 ·ª©ng d·ª•ng r·∫•t nhanh l√™n AWS. N√≥ config API Gateway thay b·∫°n, t·∫°o IAM Role thay b·∫°n, t·∫°o Lambda function thay b·∫°n lu√¥n.\nB·∫°n ch·ªâ c·∫ßn ƒë∆∞a Access Key v√† Secret Key ƒë·ªÉ n√≥ l√†m thay b·∫°n th√¥i.\n3.1. Configure aws cli https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update Configure aws credential: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html\nT·∫°o user tr√™n IAM ƒë·ªÉ get Access Key v√† Secret Key. N√™n t·∫°o 1 user v·ªõi quy·ªÅn v·ª´a ƒë·ªß th√¥i.\nT·∫°o inline policy v·ª´a ƒë·ªß quy·ªÅn cho user ƒë√≥: (credit: https://github.com/aws/chalice/issues/59)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565000\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:PassRole\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565001\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:GET\u0026#34;, \u0026#34;apigateway:POST\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565002\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:DELETE\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565003\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:POST\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/deployments\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565004\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:PUT\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/GET\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/GET/*\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/POST\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/POST/*\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/PUT\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/PUT/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565005\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:PATCH\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } (M·∫∑c d√π app chatGPT n√†y ko d√πng API Gateway nh∆∞ng m√¨nh v·∫´n ƒë∆∞a v√†o c√°i json policy tr√™n v√¨ bi·∫øt ƒë√¢u sau n√†y c·∫ßn)\nRun aws configure ƒë·ªÉ nh·∫≠p Access key v√† Secret key b√™n tr√™n\n3.2. Setup m√¥i tr∆∞·ªùng python git clone https://github.com/franalgaba/chatgpt-telegram-bot-serverless cd chatgpt-telegram-bot-serverless s·ª≠a python3 m·∫∑c ƒë·ªãnh sang python3.9, pip3 m·∫∑c ƒë·ªãnh c≈©ng ph·∫£i c·ªßa python3.9:\nupdate-alternatives --config python3 sudo apt-get install python-virtualenv apt install python3.9-dev python3.9-venv python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt Deploy:\n$ chalice deploy Creating deployment package. Reusing existing deployment package. Creating IAM role: chatgpt-telegram-bot-dev-message-handler-lambda Creating lambda function: chatgpt-telegram-bot-dev-message-handler-lambda Resources deployed: - Lambda ARN: arn:aws:lambda:us-east-1:85xxxxx188:function:chatgpt-telegram-bot-dev-message-handler-lambda N·∫øu c√≥ udpate g√¨ trong code th√¨ c·ª© run chalice deploy l·∫°i th√¥i.\nL√™n AWS Lambda s·∫Ω th·∫•y 1 Lambda function m·ªõi ƒë∆∞·ª£c t·∫°o ra: chatgpt-telegram-bot-dev-message-handler-lambda\nL√™n AWS IAM s·∫Ω th·∫•y 1 IAM role m·ªõi ƒë∆∞·ª£c t·∫°o ra: chatgpt-telegram-bot-dev-message-handler-lambda\n3.3. T·∫°o Telegram Bot ƒë·ªÉ test Chat v·ªõi BotFather, t·∫°o 1 con bot Telegram m·ªõi ƒë·ªÉ test, v√≠ d·ª• ChaliceTL_bot, l·∫•y ƒë∆∞·ª£c token: 5xxxxxx40:AAxxxxxxxxxxxxxxxxxx998k\n3.4. Expose AWS Lambda ra internet H·ªìi x∆∞a th√¨ b·∫Øt bu·ªôc ph·∫£i call Lambda qua API Gateway, gi·ªù AWS ƒë√£ cho th√™m c√°i Function URL n√†y ƒë·ªÉ call tr·ª±c ti·∫øp Lambda kh√° ti·ªán.\nGo to the AWS Console -\u0026gt; Lambda -\u0026gt; chatgpt-telegram-bot-dev-message-handler-lambda -\u0026gt; Configuration -\u0026gt; Function URL.\nClick Create Function URL and set Auth type to NONE.\nƒê∆∞·ª£c 1 Function URL: https://sbhtxxxxxxxxxxxxpn.lambda-url.us-east-1.on.aws/\n3.5. Configure webhook cho Telegram bot $ curl --request POST --url https://api.telegram.org/bot\u0026lt;YOUR_TELEGRAM_TOKEN\u0026gt;/setWebhook --header 'content-type: application/json' --data '{\u0026quot;url\u0026quot;: \u0026quot;\u0026lt;YOUR_FUNCTION_URL\u0026quot;}' {\u0026quot;ok\u0026quot;:true,\u0026quot;result\u0026quot;:true,\u0026quot;description\u0026quot;:\u0026quot;Webhook was set\u0026quot;} Done! Test th√¥i.üòÅ\nƒê·ªçc code th√¨ th·∫•y kh√° ƒë∆°n gi·∫£n (ch·ªâ c√≥ 1 file th√¥i)\nV√≠ d·ª• n√†y gi√∫p ta hi·ªÉu ƒë∆∞·ª£c c√°ch s·ª≠ d·ª•ng AWS Chalice ƒë·ªÉ deploy ·ª©ng d·ª•ng r·∫•t nhanh l√™n AWS (Developer s·∫Ω ko c·∫ßn ph·∫£i l√™n AWS Console upload Layer nh∆∞ m√¨nh l√†m h·ªìi tr∆∞·ªõc n·ªØa)\n3.6. Clean Run chalice delete ƒë·ªÉ x√≥a to√†n b·ªô nh·ªØng g√¨ chalice ƒë√£ t·∫°o ra (c≈©ng kh√° ti·ªán, ko s·ª£ b·ªã x√≥a nh·∫ßm, x√≥a x√≥t resource)\nƒêi·ªÅu m√¨nh concern l√† l√†m th·∫ø n√†o ƒë·ªÉ b·∫£o v·ªá c√°i access_key v√† secret_key th√¥i\nCREDIT So s√°nh AWS Chalice v·ªõi c√°c serverless framework: https://medium.com/@ndh175/should-i-use-aws-chalice-for-my-next-project-e445f262a49b\nhttps://stackoverflow.com/questions/64435576/telegram-webhook-doesnt-send-text-from-group-chats https://github.com/acheong08/ChatGPT\nhttps://github.com/acheong08/ChatGPT/discussions/521#discussioncomment-4878935\nhttps://github.com/acheong08/ChatGPT/issues/7#issuecomment-1338930770\nhttps://github.com/acheong08/ChatGPT/wiki/V3\nhttps://github.com/acheong08/ChatGPT/issues/574 https://github.com/acheong08/ChatGPT/discussions/1055\nhttps://github.com/franalgaba/chatgpt-telegram-bot-serverless\nhttps://github.com/aws/chalice\n","href":"/bk/encrypt-integrate-telegram-bot-with-chatgpt/","title":"Integrate Telegram Bot With ChatGPT"},{"content":"1. Run tr√™n m√°y local m√¨nh s·∫Ω s·ª≠ d·ª•ng repo n√†y: https://github.com/acheong08/ChatGPT\nM√¨nh s·∫Ω c·∫ßn install python3 tr∆∞·ªõc\npython -m pip install --upgrade pip pip3 install --upgrade revChatGPT Get API Key: https://platform.openai.com/account/api-keys\n(B∆∞·ªõc n√†y v√¨ m√¨nh ƒë√£ mua 1 account ChatGPT 250k c√≥ s·∫µn 18$ credit r·ªìi n√™n l·∫•y API key kh√° d·ªÖ)\nAPI Key c√≥ d·∫°ng: sk-xxx\nfile basic-sample.py:\nfrom revChatGPT.V3 import Chatbot chatbot = Chatbot(api_key=\u0026quot;sk-xxx\u0026quot;) response = chatbot.ask(\u0026quot;Hello\u0026quot;) print(\u0026quot;ChatGPT: \u0026quot; + response) Run: python3 basic-sample.py, ChatGPT s·∫Ω in ra c√¢u tr·∫£ l·ªùi.\n2. Run on AWS Lambda 1 s·ªë th·ª© c·∫ßn chu·∫©n b·ªã m√† m√¨nh s·∫Ω b·ªè qua v√¨ ƒë√£ vi·∫øt nhi·ªÅu r·ªìi:\n Setup AWS API Gateway. Setup ƒë·ªÉ l·∫•y Telegram token c·ªßa bot.  2.1. Package Lambda layer revChatGPT M√¨nh ch·ªçn python3.9 v√¨ tr√™n AWS Lambda h·∫ßu h·∫øt c√°c function c·ªßa m√¨nh ƒëang d√πng python3.9\nD∆∞·ªõi m√°y local ƒëang l√† python3.6 m·∫∑c ƒë·ªãnh:\n# check default python3 version $ python3 --version Python 3.6.9 # check default pip3 version $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) # =\u0026gt; means you are using pip3 from python3.6 M√¨nh mu·ªën chuy·ªÉn sang m·∫∑c ƒë·ªãnh python3.9 ƒë·ªÉ package layer:\n# install python3.9 sudo apt-get update sudo apt-get upgrade -y sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.9 sudo apt install python3-pip # check default python3 version, still python3.6 $ python3 --version Python 3.6.9 # now, check pip3 version, still pip3 from python3.6 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) # =\u0026gt; understandable # now install pip3 for python3.9 # https://stackoverflow.com/questions/65644782/how-to-install-pip-for-python-3-9-on-ubuntu-20-04 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py sudo apt install python3.9-distutils python3.9 get-pip.py # change default python3 to python3.9 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 update-alternatives --config python3 # =\u0026gt; select number of python3.9 # check default python3 version $ python3 --version Python 3.9.16 # =\u0026gt; ok, default python3 is now python3.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.9) # =\u0026gt; OK, default pip3 is from python3.9 C√≥ th·ªÉ s·∫Ω g·∫∑p 1 s·ªë l·ªói khi install package, n√™n run m·∫•y command n√†y tr∆∞·ªõc, c≈©ng ch·∫£ m·∫•t g√¨:\n# Some recommend command to fix errors # https://bobbyhadz.com/blog/python-attributeerror-htmlparser-object-has-no-attribute-unescape pip install --upgrade setuptools pip3 install --upgrade setuptools pip install --upgrade distlib pip3 install --upgrade distlib pip3 install --upgrade pip ƒêinh ninh l√† d√πng python3.9 l√† ki·ªÉu g√¨ c≈©ng ok:\n# Now install chatGPT to folder layer mkdir -p ./package/python # package to zip layer for using on AWS LAMBDA pip3 install --upgrade --target ./package/python revChatGPT cd package/ zip -r9 ${OLDPWD}/layer.zip . Cu·ªëi c√πng upload package kho·∫£ng 15MB l√™n AWS Lambda layer xong run Lambda th√¨ b·ªã l·ªói syntax g√¨ ƒë√≥:\n{ \u0026quot;errorMessage\u0026quot;: \u0026quot;Syntax error in module 'lambda_function': invalid syntax (lambda_function.py, line 14)\u0026quot;, \u0026quot;errorType\u0026quot;: \u0026quot;Runtime.UserCodeSyntaxError\u0026quot;, \u0026quot;stackTrace\u0026quot;: [ \u0026quot; File \\\u0026quot;/var/task/lambda_function.py\\\u0026quot; Line 14\\n def get_inventory(instanceid)\\n\u0026quot; ] } Sau 1 h·ªìi debug th√¨ l·ªói ·ªü c√°i library m√† m√¨nh import t·ª´ layer revChatGPT.\nKh√¥ng hi·ªÉu v√¨ sao Lambda b·ªã l·ªói, sau khi th·ª≠ ƒë·ªß c√°c th·ªÉ lo·∫°i, m·∫•t c√¥ng c√†i python3.9 xong l·∫°i ch·∫£ d√πng ƒë∆∞·ª£c üò´\nƒê·∫øn khi d√πng python3.6 v√† pip3 from python3.6 ƒë·ªÉ package th√¨ Lambda l·∫°i run OK.\n\u0026hellip;Sau v√†i h√¥m th√¨ t√°c gi·∫£ c·ªßa Library update version m·ªõi, m√¨nh c√≥ th·ªÉ d√πng python3.9 ƒë·ªÉ package layer v√† Lambda l·∫°i run OK.\nCh√∫ √Ω l√† sau khi s·ª≠ d·ª•ng layer m·ªõi - ƒê∆∞·ª£c package t·ª´ pip3 (python3.9) kho·∫£ng 15MB, th√¨ Lambda li√™n t·ª•c b·ªã timeout error. Ph·∫£i tƒÉng CPU l√™n 512MB, c√≤n RAM v·∫´n 512MB th√¨ k·∫øt qu·∫£ m·ªõi tr·∫£ v·ªÅ nhanh ƒë∆∞·ª£c.\n2.2. Code Lambda Tr√™n th·ª±c t·∫ø m√¨nh ƒë∆∞·ª£c chatGPT v√†o 1 function Lambda c√≥ s·∫µn ƒëang l√†m nhi·ªám v·ª• kh√°c (get blog pwd),\nV·ªÅ c∆° b·∫£n code s·∫Ω nh∆∞ n√†y, ƒë√¢y l√† b·∫£n m√¨nh ƒë√£ l∆∞·ª£c b·ªè 1 s·ªë function ko li√™n quan, n√™n c≈©ng ch∆∞a test xem ch·∫°y ok ko:\nimport boto3 import requests import logging import datetime from botocore.exceptions import ClientError from datetime import datetime import json import os import sys from os.path import exists from revChatGPT.V3 import Chatbot \u0026#39;\u0026#39;\u0026#39; Please config Environment variable: ALLOWED_CHAT_ID (comma separated), CHATGPT_API_KEY, CONVERSATION_ID, FUNCTION_NAME, TELEGRAM_TOKEN Install library: pip3 install --upgrade revChatGPT (use pip3 of python3.6, if you use pip3 of python3.9 lambda will raise error) Functions: This script also forwards your question to ChatGPT and response its answer To add this bot to a group chat: You should talk to BotFather to disable privacy first, then add it to group chat, and promote it as administrator. It may takes some time to Telegram takes the change (1 hour in my case) \u0026#39;\u0026#39;\u0026#39; here = os.path.dirname(os.path.realpath(__file__)) sys.path.append(os.path.join(here, \u0026#34;./vendored\u0026#34;)) # Logger settings - CloudWatch # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) # Get Lambda environment variables TOKEN = os.environ.get(\u0026#39;TELEGRAM_TOKEN\u0026#39;) ALLOWED_CHAT_ID = os.environ.get(\u0026#39;ALLOWED_CHAT_ID\u0026#39;) CHATGPT_API_KEY = os.environ.get(\u0026#39;CHATGPT_API_KEY\u0026#39;) CONVERSATION_ID = os.environ.get(\u0026#39;CONVERSATION_ID\u0026#39;) FUNCTION_NAME = os.environ.get(\u0026#39;FUNCTION_NAME\u0026#39;) # Get current datetime currentDT = datetime.now() # supported command SUPPORTED_CMD = [\u0026#34;/help\u0026#34;, \u0026#34;/gpt YOUR_QUESTION\u0026#34;, \u0026#34;/new\u0026#34;] # conversation data file, store in temporary of Lambda instance TMP_CONVERSATION_DATA = \u0026#34;/tmp/conversation.json\u0026#34; def send_telegram_msg(msg, chat_id): \u0026#34;\u0026#34;\u0026#34; Tries to Send Telegram message. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: BASE_URL = \u0026#34;https://api.telegram.org/bot{}\u0026#34;.format(TOKEN) url = BASE_URL + \u0026#34;/sendMessage\u0026#34; data = {\u0026#34;text\u0026#34;: msg.encode(\u0026#34;utf8\u0026#34;), \u0026#34;chat_id\u0026#34;: chat_id} requests.post(url, data) except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info( \u0026#34;Send message to Telegram command throttled, automatically retrying...\u0026#34;) send_telegram_msg(msg, chat_id) else: logger.error( \u0026#34;Send message to Telegram command Failed!\\n%s\u0026#34;, str(err)) return False except: raise def chat_gpt(question, conversation_id, chat_id): \u0026#34;\u0026#34;\u0026#34; Tries to ask ChatGPT a question \u0026#34;\u0026#34;\u0026#34; try: chatbot = Chatbot(api_key=CHATGPT_API_KEY) # Check if conversation data file is exist or not file_exists = exists(TMP_CONVERSATION_DATA) logger.info(\u0026#34;Conversation data exist?: %s\u0026#34; % file_exists) logger.info(\u0026#34;Conversation ID: %s\u0026#34; % conversation_id) # if data file exist, load it to conversation if file_exists == True: chatbot.load(TMP_CONVERSATION_DATA) # send question to chatGPT, belong to specific conversation response = chatbot.ask(question, role=\u0026#34;user\u0026#34;, convo_id=conversation_id) # save conversation data to a json file chatbot.save(TMP_CONVERSATION_DATA) return response except Exception as err: if \u0026#39;500 Internal Server Error\u0026#39; in str(err): logger.error(\u0026#34;Error happens!\\n%s\u0026#34;, str(err)) msg = \u0026#34;Please try again. Internal Server Error: %s\u0026#34; + str(err) send_telegram_msg(msg, chat_id) return False except: raise def update_lambda_env_variable(function_name, key, value): \u0026#34;\u0026#34;\u0026#34; Tries to update Lambda Env variable. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: client = boto3.client(\u0026#39;lambda\u0026#39;) client.update_function_configuration( FunctionName=function_name, Environment={ \u0026#39;Variables\u0026#39;: { key: value, \u0026#34;ALLOWED_CHAT_ID\u0026#34;: ALLOWED_CHAT_ID, \u0026#34;CHATGPT_API_KEY\u0026#34;: CHATGPT_API_KEY, \u0026#34;PASSWORD\u0026#34;: ENCRYPT_PASSWORD, \u0026#34;TELEGRAM_TOKEN\u0026#34;: TOKEN, \u0026#34;FUNCTION_NAME\u0026#34;: function_name } } ) logger.info(\u0026#39;============Update Lambda Env Variable Successfully\u0026#39;) return True except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info(\u0026#34;Update Lambda Env Variable throttled, automatically retrying...\u0026#34;) update_lambda_env_variable(function_name, key, value) else: logger.error(\u0026#34;Update Lambda Env Variable Failed!\\n%s\u0026#34;, str(err)) return False except: raise def lambda_handler(event, context): print(\u0026#39;------------------\u0026#39;) logger.debug(event) data = json.loads(event[\u0026#34;body\u0026#34;]) try: # if you not set enough admin right for Telegram bot in a group chat,  # it may get some error since response will not have `message` element # that\u0026#39;s why I put this in try catch message = str(data[\u0026#34;message\u0026#34;][\u0026#34;text\u0026#34;]) # need to convert chat_id to string to find it it list chat_id = str(data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;id\u0026#34;]) logger.info(chat_id) allowed_chat_id_list = list(ALLOWED_CHAT_ID.split(\u0026#34;,\u0026#34;)) if chat_id in allowed_chat_id_list: logger.info(\u0026#34;chat_id is allowed\u0026#34;) encrypt_password = ENCRYPT_PASSWORD # first_name = data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;first_name\u0026#34;] # msg = \u0026#34;Please /help, {}\u0026#34;.format(first_name) # help command if SUPPORTED_CMD[0] in message: joined_msg = \u0026#34;\\n\u0026#34;.join(SUPPORTED_CMD) msg = \u0026#34;There are only a few commands that I understand:\\n%s\u0026#34; % joined_msg # send telegram message logger.info(msg) send_telegram_msg(msg, chat_id) # return answer for question via chatGPT elif message.startswith(\u0026#39;/gpt \u0026#39;): question = message.split(\u0026#34; \u0026#34;, 1)[1] logger.info(question) msg = chat_gpt(question, CONVERSATION_ID, chat_id) # send telegram message # logger.info(msg) send_telegram_msg(msg, chat_id) # increase conversation_id and update the enviroment variable elif message.startswith(\u0026#39;/new\u0026#39;): conversation_id = int(CONVERSATION_ID) + 1 key = \u0026#34;CONVERSATION_ID\u0026#34; value = str(conversation_id) update_lambda_env_variable(FUNCTION_NAME, key, value) msg = \u0026#34;OK, new conversation starts\u0026#34; send_telegram_msg(msg, chat_id) return True else: logger.info(\u0026#34;chat_id is not allowed\u0026#34;) return False except KeyError as err: logger.error(\u0026#34;Invalid response format! \\n%s\u0026#34;, str(err)) return False except: raise V·ªÅ m·∫∑t logic:\n M·ªói khi mu·ªën h·ªèi chatGPT 1 c√¢u th√¨ g√µ nh∆∞ v√≠ d·ª•: \u0026ldquo;/gpt qu·∫£ t√°o l√† g√¨?\u0026rdquo;. Sau m·ªói c√¢u tr·∫£ l·ªùi h√†m chat_gpt s·∫Ω save/load conversation v√†o 1 file /tmp/conversation.json c·ªßa Lambda container. Th·∫ø n√™n n√≥ c√≥ th·ªÉ reply li√™n t·ª•c nh∆∞ 1 conversation ngo√†i ƒë·ªùi th·∫≠t. (Mu·ªën test th√¨ n√≥i \u0026ldquo;continue\u0026rdquo;) ƒë·ªÉ n√≥ ti·∫øp t·ª•c ch·ªß ƒë·ªÅ v·ª´a h·ªèi. Nh∆∞ng v√¨ ƒë·∫∑t trong /tmp/conversation.json c·ªßa Lambda container, n√™n n·∫øu ko chat 1 th·ªùi gian, file n√†y s·∫Ω b·ªã x√≥a v√¨ container ƒë√£ b·ªã AWS delete. N·∫øu ƒëang chat li√™n t·ª•c m√† l·∫°i mu·ªën chatGPT qu√™n h·∫øt nh·ªØng ch·ªß ƒë·ªÅ v·ª´a xong, b·∫Øt ƒë·∫ßu 1 conversation m·ªõi th√¨ g√µ /new.  2.3. Troubleshooting Trong qu√° tr√¨nh l√†m th√¨:\nN·∫øu Telegram bot ko th·ªÉ nh·∫≠n ƒë∆∞·ª£c message t·ª´ 1 group chat. M·∫∑c d√π run OK khi chat ri√™ng v·ªõi n√≥.\nT√¨nh tr·∫°ng y nh∆∞ n√†y: https://stackoverflow.com/questions/64435576/telegram-webhook-doesnt-send-text-from-group-chats\nsau 1 l√∫c th√¨ l·∫°i ko th·∫•y l·ªói n·ªØa, kh√≥ hi·ªÉu. C√≥ v·∫ª nh∆∞ telegram api b·ªã cache ·ªü ƒë√¢u ƒë√≥, n√™n vi·ªác m√¨nh remove/add bot l√†m admin v√† disable privacy m·∫•t 1 l√∫c ƒë·ªÉ n√≥ nh·∫≠n ra.\nN·∫øu mu·ªën test vi·ªác write v√† read file t·ª´ folder /tmp c·ªßa Lambda:\nimport json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) logger.info(\u0026#34;writing...\u0026#34;) body = {\u0026#34;a\u0026#34;: \u0026#34;v\u0026#34;} data = json.dumps(body) with open(\u0026#39;/tmp/conversation.json\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(data) logger.info(\u0026#34;reading...\u0026#34;) with open(/tmp/conversation.json, \u0026#39;r\u0026#39;) as openfile: json_object = json.load(openfile) print(json_object) Nh∆∞·ª£c ƒëi·ªÉm c·ªßa revChatGPT.V1 l√† ch·∫≠m, ph·∫£i authen b·∫±ng access_token (s·ªëng dc 2 tu·∫ßn), ho·∫∑c user/password (d·ªÖ b·ªã Cloudflare h·ªèi thƒÉm), ho·∫∑c session_token (s·ªëng ƒë∆∞·ª£c ng·∫Øn).\nD√πng revChatGPT.V3 th√¨ c√≥ th·ªÉ d√πng API_KEY (s·ªëng l√¢u, nh∆∞ng m·∫•t ti·ªÅn), ph·∫£n h·ªìi kh√° nhanh.\nD√πng revChatGPT.V3 m√† mu·ªën chatGPT nh·ªõ ƒë∆∞·ª£c conversation c≈© th√¨ n√≥ y√™u c·∫ßu save v√† load conversation t·ª´ JSON file nh∆∞ code ph√≠a tr√™n.\nD√πng revChatGPT.V1 m√† mu·ªën chatGPT nh·ªõ ƒë∆∞·ª£c conversation c≈© th√¨ c·∫ßn l√†m ki·ªÉu n√†y:\nfrom revChatGPT.V1 import Chatbot config = { \u0026#34;access_token\u0026#34;: \u0026#34;eyJ...\u0026#34; } chatbot = Chatbot(config) response = \u0026#34;\u0026#34; conversation =\u0026#34;\u0026#34; parent_id =\u0026#34;\u0026#34; chatbot.conversation_id = conversation chatbot.parent_id = parent_id for data in chatbot.ask(\u0026#34;continue\u0026#34;): response = data[\u0026#34;message\u0026#34;] conversation = data[\u0026#39;conversation_id\u0026#39;] parent_id = data[\u0026#39;parent_id\u0026#39;] print(response) print(conversation) print(parent_id) del chatbot chatbot = Chatbot(config) chatbot.conversation_id = conversation chatbot.parent_id = parent_id for data in chatbot.ask(\u0026#34;continue\u0026#34;): response = data[\u0026#34;message\u0026#34;] print(response) Sau ƒë√≥ l√™n khung chat tr√™n Browser s·∫Ω th·∫•y 1 ƒëo·∫°n conversation m·ªõi ƒë∆∞·ª£c t·∫°o ra, ƒëi·ªÉm n√†y revChatGPT.V3 hi·ªán ko l√†m ƒë∆∞·ª£c.\nNh∆∞ng n√≥i chung l√† n√™n d√πng revChatGPT.V3 v√¨ n√≥ l√† official API c·ªßa OpenAI, s·ªõm mu·ªôn c≈©ng s·∫Ω thay th·∫ø V1.\n3. S·ª≠ d·ª•ng framework AWS Chalice https://github.com/aws/chalice\nChalice l√† 1 framework c·ªßa AWS ph√°t tri·ªÉn, d√†nh ri√™ng cho c√°c ·ª©ng d·ª•ng serverless ng√¥n ng·ªØ Python.\nN√≥ gi√∫p b·∫°n deploy 1 ·ª©ng d·ª•ng r·∫•t nhanh l√™n AWS. N√≥ config API Gateway thay b·∫°n, t·∫°o IAM Role thay b·∫°n, t·∫°o Lambda function thay b·∫°n lu√¥n.\nB·∫°n ch·ªâ c·∫ßn ƒë∆∞a Access Key v√† Secret Key ƒë·ªÉ n√≥ l√†m thay b·∫°n th√¥i.\n3.1. Configure aws cli https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update Configure aws credential: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html\nT·∫°o user tr√™n IAM ƒë·ªÉ get Access Key v√† Secret Key. N√™n t·∫°o 1 user v·ªõi quy·ªÅn v·ª´a ƒë·ªß th√¥i.\nT·∫°o inline policy v·ª´a ƒë·ªß quy·ªÅn cho user ƒë√≥: (credit: https://github.com/aws/chalice/issues/59)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565000\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:PassRole\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565001\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:GET\u0026#34;, \u0026#34;apigateway:POST\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565002\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:DELETE\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565003\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:POST\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/deployments\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/resources/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565004\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:PUT\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/GET\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/GET/*\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/POST\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/POST/*\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/PUT\u0026#34;, \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*/methods/PUT/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1471020565005\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:PATCH\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:us-east-1::/restapis/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } (M·∫∑c d√π app chatGPT n√†y ko d√πng API Gateway nh∆∞ng m√¨nh v·∫´n ƒë∆∞a v√†o c√°i json policy tr√™n v√¨ bi·∫øt ƒë√¢u sau n√†y c·∫ßn)\nRun aws configure ƒë·ªÉ nh·∫≠p Access key v√† Secret key b√™n tr√™n\n3.2. Setup m√¥i tr∆∞·ªùng python git clone https://github.com/franalgaba/chatgpt-telegram-bot-serverless cd chatgpt-telegram-bot-serverless s·ª≠a python3 m·∫∑c ƒë·ªãnh sang python3.9, pip3 m·∫∑c ƒë·ªãnh c≈©ng ph·∫£i c·ªßa python3.9:\nupdate-alternatives --config python3 sudo apt-get install python-virtualenv apt install python3.9-dev python3.9-venv python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt Deploy:\n$ chalice deploy Creating deployment package. Reusing existing deployment package. Creating IAM role: chatgpt-telegram-bot-dev-message-handler-lambda Creating lambda function: chatgpt-telegram-bot-dev-message-handler-lambda Resources deployed: - Lambda ARN: arn:aws:lambda:us-east-1:85xxxxx188:function:chatgpt-telegram-bot-dev-message-handler-lambda N·∫øu c√≥ udpate g√¨ trong code th√¨ c·ª© run chalice deploy l·∫°i th√¥i.\nL√™n AWS Lambda s·∫Ω th·∫•y 1 Lambda function m·ªõi ƒë∆∞·ª£c t·∫°o ra: chatgpt-telegram-bot-dev-message-handler-lambda\nL√™n AWS IAM s·∫Ω th·∫•y 1 IAM role m·ªõi ƒë∆∞·ª£c t·∫°o ra: chatgpt-telegram-bot-dev-message-handler-lambda\n3.3. T·∫°o Telegram Bot ƒë·ªÉ test Chat v·ªõi BotFather, t·∫°o 1 con bot Telegram m·ªõi ƒë·ªÉ test, v√≠ d·ª• ChaliceTL_bot, l·∫•y ƒë∆∞·ª£c token: 5xxxxxx40:AAxxxxxxxxxxxxxxxxxx998k\n3.4. Expose AWS Lambda ra internet H·ªìi x∆∞a th√¨ b·∫Øt bu·ªôc ph·∫£i call Lambda qua API Gateway, gi·ªù AWS ƒë√£ cho th√™m c√°i Function URL n√†y ƒë·ªÉ call tr·ª±c ti·∫øp Lambda kh√° ti·ªán.\nGo to the AWS Console -\u0026gt; Lambda -\u0026gt; chatgpt-telegram-bot-dev-message-handler-lambda -\u0026gt; Configuration -\u0026gt; Function URL.\nClick Create Function URL and set Auth type to NONE.\nƒê∆∞·ª£c 1 Function URL: https://sbhtxxxxxxxxxxxxpn.lambda-url.us-east-1.on.aws/\n3.5. Configure webhook cho Telegram bot $ curl --request POST --url https://api.telegram.org/bot\u0026lt;YOUR_TELEGRAM_TOKEN\u0026gt;/setWebhook --header 'content-type: application/json' --data '{\u0026quot;url\u0026quot;: \u0026quot;\u0026lt;YOUR_FUNCTION_URL\u0026quot;}' {\u0026quot;ok\u0026quot;:true,\u0026quot;result\u0026quot;:true,\u0026quot;description\u0026quot;:\u0026quot;Webhook was set\u0026quot;} Done! Test th√¥i.üòÅ\nƒê·ªçc code th√¨ th·∫•y kh√° ƒë∆°n gi·∫£n (ch·ªâ c√≥ 1 file th√¥i)\nV√≠ d·ª• n√†y gi√∫p ta hi·ªÉu ƒë∆∞·ª£c c√°ch s·ª≠ d·ª•ng AWS Chalice ƒë·ªÉ deploy ·ª©ng d·ª•ng r·∫•t nhanh l√™n AWS (Developer s·∫Ω ko c·∫ßn ph·∫£i l√™n AWS Console upload Layer nh∆∞ m√¨nh l√†m h·ªìi tr∆∞·ªõc n·ªØa)\n3.6. Clean Run chalice delete ƒë·ªÉ x√≥a to√†n b·ªô nh·ªØng g√¨ chalice ƒë√£ t·∫°o ra (c≈©ng kh√° ti·ªán, ko s·ª£ b·ªã x√≥a nh·∫ßm, x√≥a x√≥t resource)\nƒêi·ªÅu m√¨nh concern l√† l√†m th·∫ø n√†o ƒë·ªÉ b·∫£o v·ªá c√°i access_key v√† secret_key th√¥i\nCREDIT So s√°nh AWS Chalice v·ªõi c√°c serverless framework: https://medium.com/@ndh175/should-i-use-aws-chalice-for-my-next-project-e445f262a49b\nhttps://stackoverflow.com/questions/64435576/telegram-webhook-doesnt-send-text-from-group-chats https://github.com/acheong08/ChatGPT\nhttps://github.com/acheong08/ChatGPT/discussions/521#discussioncomment-4878935\nhttps://github.com/acheong08/ChatGPT/issues/7#issuecomment-1338930770\nhttps://github.com/acheong08/ChatGPT/wiki/V3\nhttps://github.com/acheong08/ChatGPT/issues/574 https://github.com/acheong08/ChatGPT/discussions/1055\nhttps://github.com/franalgaba/chatgpt-telegram-bot-serverless\nhttps://github.com/aws/chalice\n","href":"/posts/encrypt-integrate-telegram-bot-with-chatgpt/","title":"Integrate Telegram Bot With ChatGPT"},{"content":"","href":"/tags/telegram/","title":"Telegram"},{"content":"","href":"/tags/notes/","title":"Notes"},{"content":"L·ªói n√†y x·∫£y ra v·ªõi h√†m schedule-check-blog-comment-v3 c·ªßa m√¨nh,\nC√°i nguy hi·ªÉm c·ªßa n√≥ l√† thi tho·∫£ng m√≥i x·∫£y ra. L√∫c test th√¨ ngon l√†nh, n·∫øu ko monitor th∆∞·ªùng xuy√™n th√¨ s·∫Ω ko bi·∫øt c√≥ l·ªói\nG·∫ßn ƒë√¢y check c√°i log monitoring c·ªßa Lambda m·ªõi th·∫•y t·ªâ l·ªá l·ªói kh√° nhi·ªÅu: (·∫£nh) trong 14 l·∫ßn ch·∫°y g·∫ßn nh·∫•t th√¨ l·ªói ƒë·∫øn 6 l·∫ßn\nSau khi t√¨m xem log th√¨ th·∫•y l·ªói AttributeError: 'ForkAwareLocal' object has no attribute 'connection' n√†y:\n2023-03-07T15:00:10.186+07:00 [INFO] 2023-03-07T08:00:10.186Z Current datetime: 2023-03-07 08:00:10.186406 2023-03-07T15:00:10.209+07:00 START RequestId: 3bd7d8xxxxxd9xxx6b8d1 Version: $LATEST 2023-03-07T15:00:21.933+07:00 Process Process-68: 2023-03-07T15:00:22.214+07:00 Traceback (most recent call last): 2023-03-07T15:00:22.214+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/managers.py\u0026quot;, line 802, in _callmethod 2023-03-07T15:00:22.214+07:00 conn = self._tls.connection 2023-03-07T15:00:22.215+07:00 AttributeError: 'ForkAwareLocal' object has no attribute 'connection' 2023-03-07T15:00:22.215+07:00 During handling of the above exception, another exception occurred: 2023-03-07T15:00:22.215+07:00 Traceback (most recent call last): 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/process.py\u0026quot;, line 315, in _bootstrap 2023-03-07T15:00:22.215+07:00 self.run() 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/process.py\u0026quot;, line 108, in run 2023-03-07T15:00:22.215+07:00 self._target(*self._args, **self._kwargs) 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/task/lambda_function.py\u0026quot;, line 104, in check_comment 2023-03-07T15:00:22.215+07:00 cmt_total.append(\u0026quot;yes\u0026quot;) 2023-03-07T15:00:22.215+07:00 File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 2, in append 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/managers.py\u0026quot;, line 806, in _callmethod 2023-03-07T15:00:22.215+07:00 self._connect() 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/managers.py\u0026quot;, line 793, in _connect 2023-03-07T15:00:22.215+07:00 conn = self._Client(self._token.address, authkey=self._authkey) 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/connection.py\u0026quot;, line 507, in Client 2023-03-07T15:00:22.215+07:00 c = SocketClient(address) 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/connection.py\u0026quot;, line 635, in SocketClient 2023-03-07T15:00:22.215+07:00 s.connect(address) 2023-03-07T15:00:22.215+07:00 ConnectionRefusedError: [Errno 111] Connection refused C√≥ v·∫ª link n√†y h·ªØu d·ª•ng: https://stackoverflow.com/questions/60049527/shutting-down-manager-error-attributeerror-forkawarelocal-object-has-no-attr\nƒê√∫ng l√† hi·ªán t·∫°i code x·ª≠ l√Ω ki·ªÉu d√πng chung nhi·ªÅu multiprocessing.Manager object nh∆∞ n√†y:\n# Prepair variables that use to share in multi-processing manager = multiprocessing.Manager() cmt_total = manager.list() new_cmt = manager.list() post_have_new_cmt = manager.list() # r·ªìi b√™n d∆∞·ªõi ch·∫°y logic ki·ªÉu: for div in comment_meta: new_cmt.append(\u0026quot;xxx\u0026quot;) cmt_total.append(\u0026quot;xxx\u0026quot;) post_have_new_cmt.append(\u0026quot;xxx\u0026quot;) C·∫ßn ph·∫£i s·ª≠a l·∫°i nh∆∞ n√†y:\n# Prepair variables that use to share in multi-processing cmt_total_manager = multiprocessing.Manager() cmt_total = cmt_total_manager.list() new_cmt_manager = multiprocessing.Manager() new_cmt = new_cmt_manager.list() post_have_new_cmt_manager = multiprocessing.Manager() post_have_new_cmt = post_have_new_cmt_manager.list() # r·ªìi b√™n d∆∞·ªõi ch·∫°y logic ki·ªÉu: for div in comment_meta: new_cmt.append(\u0026quot;xxx\u0026quot;) cmt_total.append(\u0026quot;xxx\u0026quot;) post_have_new_cmt.append(\u0026quot;xxx\u0026quot;) Th√¨ hi·ªán t·∫°i sau kho·∫£ng 2 tu·∫ßn th√¨ ko c√≥ t√Ω l·ªói AttributeError: 'ForkAwareLocal' object has no attribute 'connection' n√†o xu·∫•t hi·ªán n·ªØa. üòò\nCREDIT https://stackoverflow.com/questions/60049527/shutting-down-manager-error-attributeerror-forkawarelocal-object-has-no-attr\n","href":"/bk/encrypt-python-error-forkawarelocal-multiprocessing/","title":"Python Error: ForkAwareLocal Multiprocessing"},{"content":"L·ªói n√†y x·∫£y ra v·ªõi h√†m schedule-check-blog-comment-v3 c·ªßa m√¨nh,\nC√°i nguy hi·ªÉm c·ªßa n√≥ l√† thi tho·∫£ng m√≥i x·∫£y ra. L√∫c test th√¨ ngon l√†nh, n·∫øu ko monitor th∆∞·ªùng xuy√™n th√¨ s·∫Ω ko bi·∫øt c√≥ l·ªói\nG·∫ßn ƒë√¢y check c√°i log monitoring c·ªßa Lambda m·ªõi th·∫•y t·ªâ l·ªá l·ªói kh√° nhi·ªÅu: (·∫£nh) trong 14 l·∫ßn ch·∫°y g·∫ßn nh·∫•t th√¨ l·ªói ƒë·∫øn 6 l·∫ßn\nSau khi t√¨m xem log th√¨ th·∫•y l·ªói AttributeError: 'ForkAwareLocal' object has no attribute 'connection' n√†y:\n2023-03-07T15:00:10.186+07:00 [INFO] 2023-03-07T08:00:10.186Z Current datetime: 2023-03-07 08:00:10.186406 2023-03-07T15:00:10.209+07:00 START RequestId: 3bd7d8xxxxxd9xxx6b8d1 Version: $LATEST 2023-03-07T15:00:21.933+07:00 Process Process-68: 2023-03-07T15:00:22.214+07:00 Traceback (most recent call last): 2023-03-07T15:00:22.214+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/managers.py\u0026quot;, line 802, in _callmethod 2023-03-07T15:00:22.214+07:00 conn = self._tls.connection 2023-03-07T15:00:22.215+07:00 AttributeError: 'ForkAwareLocal' object has no attribute 'connection' 2023-03-07T15:00:22.215+07:00 During handling of the above exception, another exception occurred: 2023-03-07T15:00:22.215+07:00 Traceback (most recent call last): 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/process.py\u0026quot;, line 315, in _bootstrap 2023-03-07T15:00:22.215+07:00 self.run() 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/process.py\u0026quot;, line 108, in run 2023-03-07T15:00:22.215+07:00 self._target(*self._args, **self._kwargs) 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/task/lambda_function.py\u0026quot;, line 104, in check_comment 2023-03-07T15:00:22.215+07:00 cmt_total.append(\u0026quot;yes\u0026quot;) 2023-03-07T15:00:22.215+07:00 File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 2, in append 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/managers.py\u0026quot;, line 806, in _callmethod 2023-03-07T15:00:22.215+07:00 self._connect() 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/managers.py\u0026quot;, line 793, in _connect 2023-03-07T15:00:22.215+07:00 conn = self._Client(self._token.address, authkey=self._authkey) 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/connection.py\u0026quot;, line 507, in Client 2023-03-07T15:00:22.215+07:00 c = SocketClient(address) 2023-03-07T15:00:22.215+07:00 File \u0026quot;/var/lang/lib/python3.9/multiprocessing/connection.py\u0026quot;, line 635, in SocketClient 2023-03-07T15:00:22.215+07:00 s.connect(address) 2023-03-07T15:00:22.215+07:00 ConnectionRefusedError: [Errno 111] Connection refused C√≥ v·∫ª link n√†y h·ªØu d·ª•ng: https://stackoverflow.com/questions/60049527/shutting-down-manager-error-attributeerror-forkawarelocal-object-has-no-attr\nƒê√∫ng l√† hi·ªán t·∫°i code x·ª≠ l√Ω ki·ªÉu d√πng chung nhi·ªÅu multiprocessing.Manager object nh∆∞ n√†y:\n# Prepair variables that use to share in multi-processing manager = multiprocessing.Manager() cmt_total = manager.list() new_cmt = manager.list() post_have_new_cmt = manager.list() # r·ªìi b√™n d∆∞·ªõi ch·∫°y logic ki·ªÉu: for div in comment_meta: new_cmt.append(\u0026quot;xxx\u0026quot;) cmt_total.append(\u0026quot;xxx\u0026quot;) post_have_new_cmt.append(\u0026quot;xxx\u0026quot;) C·∫ßn ph·∫£i s·ª≠a l·∫°i nh∆∞ n√†y:\n# Prepair variables that use to share in multi-processing cmt_total_manager = multiprocessing.Manager() cmt_total = cmt_total_manager.list() new_cmt_manager = multiprocessing.Manager() new_cmt = new_cmt_manager.list() post_have_new_cmt_manager = multiprocessing.Manager() post_have_new_cmt = post_have_new_cmt_manager.list() # r·ªìi b√™n d∆∞·ªõi ch·∫°y logic ki·ªÉu: for div in comment_meta: new_cmt.append(\u0026quot;xxx\u0026quot;) cmt_total.append(\u0026quot;xxx\u0026quot;) post_have_new_cmt.append(\u0026quot;xxx\u0026quot;) Th√¨ hi·ªán t·∫°i sau kho·∫£ng 2 tu·∫ßn th√¨ ko c√≥ t√Ω l·ªói AttributeError: 'ForkAwareLocal' object has no attribute 'connection' n√†o xu·∫•t hi·ªán n·ªØa. üòò\nCREDIT https://stackoverflow.com/questions/60049527/shutting-down-manager-error-attributeerror-forkawarelocal-object-has-no-attr\n","href":"/posts/encrypt-python-error-forkawarelocal-multiprocessing/","title":"Python Error: ForkAwareLocal Multiprocessing"},{"content":"","href":"/categories/tech-notes/","title":"Tech-Notes"},{"content":"N·∫øu b·∫°n ƒëang qu·∫£n l√Ω nhi·ªÅu application nh∆∞ng m·ªói app l·∫°i c√≥ 1 database ri√™ng ƒë·ªÉ l∆∞u tr·ªØ user/password,\nb·∫°n s·∫Ω c·∫ßn ph·∫£i nh·ªõ r·∫•t nhi·ªÅu user/password cho m·ªói app. Sau khi ƒë·ªïi password th√¨ l·∫°i c√†ng lo·∫°n h∆°n.\nƒê·ªÉ gi·∫£i quy·∫øt, b·∫°n s·∫Ω mu·ªën ch·ªâ d√πng 1 database cho user/password, t·∫•t c·∫£ application s·∫Ω ch·ªçc v√†o 1 database ƒë√≥ ƒë·ªÉ x√°c th·ª±c user.\nC√≥ nhi·ªÅu ph·∫ßn m·ªÅm ƒë·ªÉ th·ª±c hi·ªán vi·ªác ƒë√≥, trong ƒë√≥ th√¨ OpenLDAP l√† 1 open-source r·∫•t hay, ph√π h·ª£p cho c√°c application n·ªôi b·ªô, s·ªë l∆∞·ª£ng user nh·ªè.\nB√™n c·∫°nh OpenLDAP th√¨ c√≤n c√≥ Azure Active Directory, AWS Cognito, Auth0, Keycloak,\u0026hellip;etc\nB√†i n√†y m√¨nh s·∫Ω ch·ªâ n√≥i v·ªÅ OpenLDAP th√¥i v√¨ n√≥ free. Tuy nhi√™n t∆∞∆°ng lai m√¨nh c≈©ng mu·ªën t√¨m hi·ªÉu v·ªÅ Auth0 n·ªØa v√¨ th·∫•y nhi·ªÅu ng∆∞·ªùi recommend. (C√≤n AWS Cognito th√¨ m√¨nh ƒë√£ c√≥ 1,2 b√†i n√≥i v·ªÅ n√≥ r·ªìi - apply v√†o ReactJS Gatsby framework)\n1. Install C√†i ƒë·∫∑t OpenLDAP trong b√†i n√†y m√¨nh s·∫Ω d√πng Docker: https://github.com/osixia/docker-openldap\nV√† guidline m√¨nh follow ƒë·ªÉ config OpenLDAP th√¨ ·ªü ƒë√¢y: https://doc.digdash.com/xwiki/wiki/dd2022r2/view/Digdash/deployment/installation/install_guide_ubuntu/...\nV√¨ m√¨nh l·∫•y credit t·ª´ document c·ªßa Digdash n√™n N√™n digdash s·∫Ω ƒë∆∞·ª£c d√πng xuy√™n su·ªët trong b√†i, b·∫°n c√≥ th·ªÉ s·ª≠a th√†nh digdash th√†nh b·∫•t c·ª© th·ª© g√¨ b·∫°n mu·ªën\nChu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ s·ª≠ d·ª•ng trong b√†i n√†y:\nexport LDAP_ADMIN_PASSWORD=\u0026#34;Abcd1234\u0026#34; Chu·∫©n b·ªã th∆∞ m·ª•c, c√°c file ldif ch∆∞a c√≥ c·ª© b·ªè qua:\n. |__docker-compose.yml |__docker-resources |__openldap |__database |__config |__ldif-custom |__ppolicy-module.ldif |__ppolicy-conf.ldif |__ppolicy-defaut.ldif |__neworganisation.ldif |__create_user_admin.ldif |__add_right_admin.ldif |__extend_limit_search.ldif |__increase_mem.ldif File docker-compose.yml:\nversion: \u0026#39;3\u0026#39; services: openldap-server: container_name: openldap-server image: osixia/openldap:1.5.0 restart: unless-stopped stdin_open: true tty: true ports: - \u0026#34;11389:389\u0026#34; environment: - LDAP_ORGANISATION=abc - LDAP_DOMAIN=digdash.com - LDAP_ADMIN_PASSWORD=$LDAP_ADMIN_PASSWORD - LDAP_BASE_DN=dc=digdash,dc=com volumes: - ./docker-resources/openldap/database:/var/lib/ldap  # N·∫øu mu·ªën persist data th√¨ chu·∫©n b·ªã c√°c folder n√†y - ./docker-resources/openldap/config:/etc/ldap/slapd.d  # N·∫øu mu·ªën persist data th√¨ chu·∫©n b·ªã c√°c folder n√†y - ./docker-resources/openldap/ldif-custom:/etc/ldap/ldif-custom # N·∫øu mu·ªën execute c√°c file ldif th√¨ ƒë·∫∑t ·ªü ƒë√¢y Ch√∫ √Ω volume /etc/ldap/ldif-custom s·∫Ω ch·ª©a c√°c file ldif ƒë·ªÉ sau n√†y m√¨nh run config l·∫°i v·ªõi openLDAP\nM·∫∑c d√π ·ªü ƒë√¢y c√≥ h∆∞·ªõng d·∫´n ƒë·ªÉ seed c√°c file ldif v√†o lu√¥n, nh∆∞ng m√¨nh th·∫•y n√≥ ko th·ªÉ s·∫Øp x·∫øp th·ª© t·ª± execute c·ªßa c√°c file ldif ƒë√≥ n√™n m√¨nh ko d√πng.\nThay v√†o ƒë√≥ sau khi docker-compose up -d xong, m√¨nh s·∫Ω run command ƒë·ªÉ ch·∫°y 1 shell script, execute c√°c ldif file.\nRun:\ndocker-compose up -d # check xem c√≥ log l·ªói ko docker logs openldap-server 2. Run some commands with ldap daemon Gi·ªù ssh v√†o trong container ƒë·ªÉ execute c√°c file ldif file:\ndocker exec -it openldap-server /bin/bash cd /etc/ldap/ldif-custom Add ppolicy module Check xem ppolicy module ƒë√£ ƒë∆∞·ª£c add v√†o ch∆∞a, nh∆∞ n√†y l√† ok:\nroot@4366cd65ca46:/# ldapsearch -Y EXTERNAL -s one -H ldapi:/// -b cn=schema,cn=config cn -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: cn={0}core,cn=schema,cn=config cn: {0}core dn: cn={1}cosine,cn=schema,cn=config cn: {1}cosine dn: cn={2}nis,cn=schema,cn=config cn: {2}nis dn: cn={3}inetorgperson,cn=schema,cn=config cn: {3}inetorgperson dn: cn={4}ppolicy,cn=schema,cn=config cn: {4}ppolicy dn: cn={5}kopano,cn=schema,cn=config cn: {5}kopano dn: cn={6}openssh-lpk,cn=schema,cn=config cn: {6}openssh-lpk dn: cn={7}postfix-book,cn=schema,cn=config cn: {7}postfix-book dn: cn={8}samba,cn=schema,cn=config cn: {8}samba N·∫øu ch∆∞a th·∫•y ppolicy ƒë∆∞·ª£c add th√¨ d√πng command n√†y ƒë·ªÉ add:\nroot@4366cd65ca46:/# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/ppolicy.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;cn=ppolicy,cn=schema,cn=config\u0026#34; Activate module ppolicy then verify File ppolicy-module.ldif:\ndn: cn=module{0},cn=config changeType: modify add: olcModuleLoad olcModuleLoad: ppolicy Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -Y EXTERNAL -H ldapi:/// -f ppolicy-module.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;cn=module{0},cn=config\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026#34;(objectClass=olcModuleList)\u0026#34; olcModuleLoad -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: cn=module{0},cn=config olcModuleLoad: {0}back_mdb olcModuleLoad: {1}memberof olcModuleLoad: {2}refint olcModuleLoad: {3}ppolicy Config hash password c·ªßa c√°c user File ppolicy-conf.ldif:\ndn: olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config objectClass: olcPpolicyConfig olcOverlay: ppolicy olcPPolicyDefault: cn=ppolicy,dc=digdash,dc=com olcPPolicyUseLockout: TRUE olcPPolicyHashCleartext: TRUE Run command:\n# Before  root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026#34;(objectClass=olcPpolicyConfig)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 # Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f ppolicy-conf.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026#34;(objectClass=olcPpolicyConfig)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: olcOverlay={2}ppolicy,olcDatabase={1}mdb,cn=config objectClass: olcPPolicyConfig olcOverlay: {2}ppolicy olcPPolicyDefault: cn=ppolicy,dc=digdash,dc=com olcPPolicyHashCleartext: TRUE olcPPolicyUseLockout: TRUE Define password policy File ppolicy-defaut.ldif s·∫Ω define user password c·∫ßn tu√¢n th·ªß nh·ªØng policy g√¨:\ndn: cn=ppolicy,dc=digdash,dc=com objectClass: device objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: ppolicy pwdAllowUserChange: TRUE pwdAttribute: userPassword pwdCheckQuality: 1 pwdExpireWarning: 600 pwdFailureCountInterval: 30 pwdGraceAuthNLimit: 5 pwdInHistory: 5 pwdLockout: TRUE pwdLockoutDuration: 900 pwdMaxAge: 0 pwdMaxFailure: 5 pwdMinAge: 0 pwdMinLength: 12 pwdMustChange: FALSE pwdSafeModify: FALSE Run command:\n# Before  root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b dc=digdash,dc=com \u0026#34;(objectClass=pwdPolicy)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 # Run command root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -x -a -H ldap://localhost -D cn=admin,dc=digdash,dc=com -f ppolicy-defaut.ldif -w Abcd1234 adding new entry \u0026#34;cn=ppolicy,dc=digdash,dc=com\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b dc=digdash,dc=com \u0026#34;(objectClass=pwdPolicy)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: cn=ppolicy,dc=digdash,dc=com objectClass: device objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: ppolicy pwdAllowUserChange: TRUE pwdAttribute: userPassword pwdCheckQuality: 1 pwdExpireWarning: 600 pwdFailureCountInterval: 30 pwdGraceAuthNLimit: 5 pwdInHistory: 5 pwdLockout: TRUE pwdLockoutDuration: 900 pwdMaxAge: 0 pwdMaxFailure: 5 pwdMinAge: 0 pwdMinLength: 12 pwdMustChange: FALSE pwdSafeModify: FALSE Create a new organization File neworganisation.ldif d√πng ƒë·ªÉ t·∫°o m·ªõi organization t√™n l√† ou=default,dc=digdash,dc=com:\ndn: ou=default,dc=digdash,dc=com objectClass: organizationalUnit ou: default Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapadd -H ldap://localhost -D cn=admin,dc=digdash,dc=com -x -f neworganisation.ldif -w Abcd1234 adding new entry \u0026#34;ou=default,dc=digdash,dc=com\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -x -H ldap://localhost -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 # extended LDIF # # LDAPv3 # base \u0026lt;dc=digdash,dc=com\u0026gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # digdash.com dn: dc=digdash,dc=com objectClass: top objectClass: dcObject objectClass: organization o: loamics dc: digdash # ppolicy, digdash.com dn: cn=ppolicy,dc=digdash,dc=com objectClass: device objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: ppolicy pwdAllowUserChange: TRUE pwdAttribute: userPassword pwdCheckQuality: 1 pwdExpireWarning: 600 pwdFailureCountInterval: 30 pwdGraceAuthNLimit: 5 pwdInHistory: 5 pwdLockout: TRUE pwdLockoutDuration: 900 pwdMaxAge: 0 pwdMaxFailure: 5 pwdMinAge: 0 pwdMinLength: 12 pwdMustChange: FALSE pwdSafeModify: FALSE # default, digdash.com dn: ou=default,dc=digdash,dc=com objectClass: organizationalUnit ou: default # search result search: 2 result: 0 Success # numResponses: 4 # numEntries: 3 Create an admin user with the right File create_user_admin.ldif ƒë·ªÉ t·∫°o m·ªõi 1 user v·ªõi uid admin_default:\ndn: uid=admin,ou=default,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Default sn: Default uid: admin_default Run command:\nroot@4366cd65ca46:/etc/ldap/ldif-custom# ldapadd -H ldap://localhost -D cn=admin,dc=digdash,dc=com -x -f create_user_admin.ldif -w Abcd1234 adding new entry \u0026#34;uid=admin,ou=default,dc=digdash,dc=com\u0026#34; Add admin right File add_right_admin.ldif ƒë·ªÉ grant quy·ªÅn cho user:\ndn: olcDatabase={1}mdb,cn=config changetype: modify add: olcAccess olcaccess: {0}to dn.subtree=ou=default,dc=digdash,dc=com attrs=userpassword,shadowlastchange by dn.exact=uid=admin,ou=default,dc=digdash,dc=com write by self write by anonymous auth by * read olcaccess: {1}to dn.subtree=ou=default,dc=digdash,dc=com by dn.exact=uid=admin,ou=default,dc=digdash,dc=com write by * read Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f add_right_admin.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;olcDatabase={1}mdb,cn=config\u0026#34; Set new password for uid=admin,ou=default,dc=digdash,dc=com root@4366cd65ca46:/etc/ldap/ldif-custom# ldappasswd -H ldap://localhost -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -S \u0026#34;uid=admin,ou=default,dc=digdash,dc=com\u0026#34; -w Abcd1234 New password: Re-enter new password: Extend the 500 limit for LDAP searches File extend_limit_search.ldif ƒë·ªÉ extend gi·ªõi h·∫°n search c·ªßa ldap:\ndn: olcDatabase={-1}frontend,cn=config changetype: modify replace: olcSizeLimit olcSizeLimit: 3000 Run command:\nroot@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f extend_limit_search.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;olcDatabase={-1}frontend,cn=config\u0026#34; Increase MDB database memory to 10GB File increase_mem.ldif ƒë·ªÉ tƒÉng gi·ªõi h·∫°n c·ªßa DB MDB:\ndn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcDbMaxSize olcDbMaxSize: 10000000000 Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -H ldapi:/// -Y EXTERNAL -f increase_mem.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;olcDatabase={1}mdb,cn=config\u0026#34; List all group and users trong openldap ldapsearch -x -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 List a specific user identified by uid=hoangdocker ldapsearch -x -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; \u0026#39;(uid=hoangdocker)\u0026#39; -w Abcd1234 Change password user Tr∆∞·ªõc ti√™n h√£y list user ƒë√≥ ra b·∫±ng command b√™n tr√™n ƒë·ªÉ bi·∫øt c√°c th√¥ng s·ªë kh√°c c·ªßa user ƒë√≥, nh∆∞ ou,dc l√† g√¨, sau ƒë√≥ th√¨ change password nh∆∞ sau:\nldappasswd -H ldapi:/// -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 -S uid=hoangdocker,ou=users,dc=digdash,dc=com New password: Re-enter new password: Check c√°c user v√† password ƒë√£ encrypted c·ªßa ch√∫ng trong cn=config ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config Find the RootDN account and the current RootDN password hash: (theo https://www.digitalocean.com/community/tutorials/how-to-change-account-passwords-on-an-openldap-server)\nroot@ded2e404d270:/# ldapsearch -H ldapi:// -LLL -Q -Y EXTERNAL -b \u0026#34;cn=config\u0026#34; \u0026#34;(olcRootDN=*)\u0026#34; dn olcRootDN olcRootPW dn: olcDatabase={0}config,cn=config olcRootDN: cn=admin,cn=config olcRootPW: {SSHA}kDomIgiYEPYYe2tJRrLkFO4jxrayzt0n dn: olcDatabase={1}mdb,cn=config olcRootDN: cn=admin,dc=digdash,dc=com olcRootPW: {SSHA}bq06veDempYLWI3VgWGO10mFaaPhxiwk Create a password hashed https://tech.feedyourhead.at/content/openldap-set-config-admin-password\nslappasswd -h {SSHA} Issue c·ªßa Digdash v√† OpenLDAP Khi m√¨nh mu·ªën Thay ƒë·ªïi password c·ªßa user admin. M√¨nh ti·∫øn h√†nh list c√°c user c√≥ uid=admin ra:\nldapsearch -x -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; \u0026#39;(uid=admin)\u0026#39; -w Abcd1234 # admin, default, digdash.com dn: uid=admin,ou=default,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Default sn: Default uid: admin_default uid: admin userPassword:: e1NTSEF9MVJhckpidkZMaFNudHJsZzQya0JFMHpCWlg3a3pEMlA= # admin, users, digdash.com dn: uid=admin,ou=users,dc=digdash,dc=com uid: admin description:: ... objectClass: top objectClass: person objectClass: organizationalPerson objectClass: inetOrgPerson sn: admin cn: admin userPassword:: e1NTSEF9cnJiTUJmOFRiL2d5TVRPc25tbE1WYVBBZjlITFVWQXQ= =\u0026gt; l√† uid=admin,ou=default,dc=digdash,dc=com v√† uid=admin,ou=users,dc=digdash,dc=com\nN·∫øu ch·ªâ ƒë·ªïi password c·ªßa uid=admin,ou=users,dc=digdash,dc=com th√¨ s·∫Ω c√≥ l·ªói l√† login v√†o ƒë∆∞·ª£c b·∫±ng c·∫£ admin/admin v√† admin/New_Password\n-\u0026gt; N√™n m√¨nh th·ª≠ ƒë·ªïi pass c·ªßa c·∫£ 2 admin user ƒë√≥:\nldappasswd -H ldapi:/// -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 -S uid=admin,ou=users,dc=digdash,dc=com -\u0026gt; th√†nh Abcd1234u ldappasswd -H ldapi:/// -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 -S uid=admin,ou=default,dc=digdash,dc=com -\u0026gt; th√†nh Abcd1234d -\u0026gt; ko ƒÉn thua, ko login v·ªõi password n√†y ƒë∆∞·ª£c -\u0026gt; v·∫´n login v√†o b·∫±ng c·∫£ admin/admin v√† admin/Abcd1234u\n-\u0026gt; ph·∫£i l√™n http://DNS:8080/ddenterpriseapi/serversettings\nth√¨ m·ªõi s·ª≠a ƒë∆∞·ª£c c√°i password admin/admin, nh∆∞ng v·∫´n login ƒë∆∞·ª£c b·∫±ng admin/Abcd1234u\n-\u0026gt; ƒê√¢y l√† known issue c·ªßa OpenLDAP: https://github.com/osixia/docker-openldap/issues/161\nC√°ch gi·∫£i quy·∫øt\nDelete admin user (uid=admin,ou=users,dc=digdash,dc=com) in LDAP server:\nldapdelete -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 uid=admin,ou=users,dc=digdash,dc=com Delete admin user (uid=admin,ou=default,dc=digdash,dc=com) in LDAP server:\nldapdelete -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 uid=admin,ou=default,dc=digdash,dc=com Nh∆∞ v·∫≠y LDAP s·∫Ω ko c√≥ user admin, s·∫Ω ch·ªâ c√≥ password ƒë∆∞·ª£c encrpyted trong file: \u0026lsquo;/digdash/appdata/default/Enterprise Server/ddenterpriseapi/config/serversettings.xml\u0026rsquo;\nTuy nhi√™n n·∫øu x√≥a c·∫£ 2 admin user trong LDAP ƒëi th√¨ khi tr√™n UI, list all user ra s·∫Ω ko list dc user admin -\u0026gt; n√™n ko g√°n license cho admin ƒë∆∞·ª£c -\u0026gt; 1 s·ªë trang nh∆∞ Dashboard editor.. th√¨ admin s·∫Ω ko login ƒë∆∞·ª£c v√¨ ko c√≥ license\n=\u0026gt; nh∆∞ v·∫≠y v·∫´n c·∫ßn t·∫°o user admin trong LDAP server, nh∆∞ng ko t·∫°o password cho n√≥. ƒê·ªÉ password s·∫Ω l·∫•y t·ª´ file serversettings.xml\nFile t·∫°o user admin c·∫ßn s·ª≠a l·∫°i nh∆∞ sau create_user_admin.ldif:\ndn: uid=admin,ou=users,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Users sn: admin uid: admin dn: uid=admin,ou=default,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Default sn: Default uid: admin_default Ch√∫ √Ω uid=admin,ou=users,dc=digdash,dc=com th√¨ m·ªõi hi·ªÉn th·ªã tr√™n list user,\nch·ª© uid=admin,ou=default,dc=digdash,dc=com s·∫Ω ko hi·ªÉn th·ªã tr√™n list user, m√† c≈©ng ko login ƒë∆∞·ª£c.\nM√¨nh t·∫°o ra n√≥ ch·ªâ ƒë·ªÉ ƒë√∫ng v·ªõi document m√† DigDash cung c·∫•p th√¥i.\nN·∫øu b·ªã l·ªói khi t·∫°o uid=admin,ou=users,dc=digdash,dc=com th√¨ c√≥ kh·∫£ nƒÉng b·∫°n ƒëang deploy openLDAP tr∆∞·ªõc (ch∆∞a deploy DigDash) n√™n c·∫ßn t·∫°o organization users ƒë√£.\nV√¨ th·∫ø n√™n file neworganisation.ldif m√¨nh ƒë√£ add th√™m:\ndn: ou=default,dc=digdash,dc=com objectClass: organizationalUnit ou: default dn: ou=users,dc=digdash,dc=com objectClass: organizationalUnit ou: users CREDIT https://doc.digdash.com/xwiki/wiki/dd2022r2/view/Digdash/deployment/installation/install_guide_ubuntu/\nhttps://github.com/osixia/docker-openldap#seed-ldap-database-with-ldif\n","href":"/bk/encrypt-notes-about-docker-openldap/","title":"Notes About Docker OpenLDAP"},{"content":"N·∫øu b·∫°n ƒëang qu·∫£n l√Ω nhi·ªÅu application nh∆∞ng m·ªói app l·∫°i c√≥ 1 database ri√™ng ƒë·ªÉ l∆∞u tr·ªØ user/password,\nb·∫°n s·∫Ω c·∫ßn ph·∫£i nh·ªõ r·∫•t nhi·ªÅu user/password cho m·ªói app. Sau khi ƒë·ªïi password th√¨ l·∫°i c√†ng lo·∫°n h∆°n.\nƒê·ªÉ gi·∫£i quy·∫øt, b·∫°n s·∫Ω mu·ªën ch·ªâ d√πng 1 database cho user/password, t·∫•t c·∫£ application s·∫Ω ch·ªçc v√†o 1 database ƒë√≥ ƒë·ªÉ x√°c th·ª±c user.\nC√≥ nhi·ªÅu ph·∫ßn m·ªÅm ƒë·ªÉ th·ª±c hi·ªán vi·ªác ƒë√≥, trong ƒë√≥ th√¨ OpenLDAP l√† 1 open-source r·∫•t hay, ph√π h·ª£p cho c√°c application n·ªôi b·ªô, s·ªë l∆∞·ª£ng user nh·ªè.\nB√™n c·∫°nh OpenLDAP th√¨ c√≤n c√≥ Azure Active Directory, AWS Cognito, Auth0, Keycloak,\u0026hellip;etc\nB√†i n√†y m√¨nh s·∫Ω ch·ªâ n√≥i v·ªÅ OpenLDAP th√¥i v√¨ n√≥ free. Tuy nhi√™n t∆∞∆°ng lai m√¨nh c≈©ng mu·ªën t√¨m hi·ªÉu v·ªÅ Auth0 n·ªØa v√¨ th·∫•y nhi·ªÅu ng∆∞·ªùi recommend. (C√≤n AWS Cognito th√¨ m√¨nh ƒë√£ c√≥ 1,2 b√†i n√≥i v·ªÅ n√≥ r·ªìi - apply v√†o ReactJS Gatsby framework)\n1. Install C√†i ƒë·∫∑t OpenLDAP trong b√†i n√†y m√¨nh s·∫Ω d√πng Docker: https://github.com/osixia/docker-openldap\nV√† guidline m√¨nh follow ƒë·ªÉ config OpenLDAP th√¨ ·ªü ƒë√¢y: https://doc.digdash.com/xwiki/wiki/dd2022r2/view/Digdash/deployment/installation/install_guide_ubuntu/...\nV√¨ m√¨nh l·∫•y credit t·ª´ document c·ªßa Digdash n√™n N√™n digdash s·∫Ω ƒë∆∞·ª£c d√πng xuy√™n su·ªët trong b√†i, b·∫°n c√≥ th·ªÉ s·ª≠a th√†nh digdash th√†nh b·∫•t c·ª© th·ª© g√¨ b·∫°n mu·ªën\nChu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ s·ª≠ d·ª•ng trong b√†i n√†y:\nexport LDAP_ADMIN_PASSWORD=\u0026#34;Abcd1234\u0026#34; Chu·∫©n b·ªã th∆∞ m·ª•c, c√°c file ldif ch∆∞a c√≥ c·ª© b·ªè qua:\n. |__docker-compose.yml |__docker-resources |__openldap |__database |__config |__ldif-custom |__ppolicy-module.ldif |__ppolicy-conf.ldif |__ppolicy-defaut.ldif |__neworganisation.ldif |__create_user_admin.ldif |__add_right_admin.ldif |__extend_limit_search.ldif |__increase_mem.ldif File docker-compose.yml:\nversion: \u0026#39;3\u0026#39; services: openldap-server: container_name: openldap-server image: osixia/openldap:1.5.0 restart: unless-stopped stdin_open: true tty: true ports: - \u0026#34;11389:389\u0026#34; environment: - LDAP_ORGANISATION=abc - LDAP_DOMAIN=digdash.com - LDAP_ADMIN_PASSWORD=$LDAP_ADMIN_PASSWORD - LDAP_BASE_DN=dc=digdash,dc=com volumes: - ./docker-resources/openldap/database:/var/lib/ldap  # N·∫øu mu·ªën persist data th√¨ chu·∫©n b·ªã c√°c folder n√†y - ./docker-resources/openldap/config:/etc/ldap/slapd.d  # N·∫øu mu·ªën persist data th√¨ chu·∫©n b·ªã c√°c folder n√†y - ./docker-resources/openldap/ldif-custom:/etc/ldap/ldif-custom # N·∫øu mu·ªën execute c√°c file ldif th√¨ ƒë·∫∑t ·ªü ƒë√¢y Ch√∫ √Ω volume /etc/ldap/ldif-custom s·∫Ω ch·ª©a c√°c file ldif ƒë·ªÉ sau n√†y m√¨nh run config l·∫°i v·ªõi openLDAP\nM·∫∑c d√π ·ªü ƒë√¢y c√≥ h∆∞·ªõng d·∫´n ƒë·ªÉ seed c√°c file ldif v√†o lu√¥n, nh∆∞ng m√¨nh th·∫•y n√≥ ko th·ªÉ s·∫Øp x·∫øp th·ª© t·ª± execute c·ªßa c√°c file ldif ƒë√≥ n√™n m√¨nh ko d√πng.\nThay v√†o ƒë√≥ sau khi docker-compose up -d xong, m√¨nh s·∫Ω run command ƒë·ªÉ ch·∫°y 1 shell script, execute c√°c ldif file.\nRun:\ndocker-compose up -d # check xem c√≥ log l·ªói ko docker logs openldap-server 2. Run some commands with ldap daemon Gi·ªù ssh v√†o trong container ƒë·ªÉ execute c√°c file ldif file:\ndocker exec -it openldap-server /bin/bash cd /etc/ldap/ldif-custom Add ppolicy module Check xem ppolicy module ƒë√£ ƒë∆∞·ª£c add v√†o ch∆∞a, nh∆∞ n√†y l√† ok:\nroot@4366cd65ca46:/# ldapsearch -Y EXTERNAL -s one -H ldapi:/// -b cn=schema,cn=config cn -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: cn={0}core,cn=schema,cn=config cn: {0}core dn: cn={1}cosine,cn=schema,cn=config cn: {1}cosine dn: cn={2}nis,cn=schema,cn=config cn: {2}nis dn: cn={3}inetorgperson,cn=schema,cn=config cn: {3}inetorgperson dn: cn={4}ppolicy,cn=schema,cn=config cn: {4}ppolicy dn: cn={5}kopano,cn=schema,cn=config cn: {5}kopano dn: cn={6}openssh-lpk,cn=schema,cn=config cn: {6}openssh-lpk dn: cn={7}postfix-book,cn=schema,cn=config cn: {7}postfix-book dn: cn={8}samba,cn=schema,cn=config cn: {8}samba N·∫øu ch∆∞a th·∫•y ppolicy ƒë∆∞·ª£c add th√¨ d√πng command n√†y ƒë·ªÉ add:\nroot@4366cd65ca46:/# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/ppolicy.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;cn=ppolicy,cn=schema,cn=config\u0026#34; Activate module ppolicy then verify File ppolicy-module.ldif:\ndn: cn=module{0},cn=config changeType: modify add: olcModuleLoad olcModuleLoad: ppolicy Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -Y EXTERNAL -H ldapi:/// -f ppolicy-module.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;cn=module{0},cn=config\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026#34;(objectClass=olcModuleList)\u0026#34; olcModuleLoad -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: cn=module{0},cn=config olcModuleLoad: {0}back_mdb olcModuleLoad: {1}memberof olcModuleLoad: {2}refint olcModuleLoad: {3}ppolicy Config hash password c·ªßa c√°c user File ppolicy-conf.ldif:\ndn: olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config objectClass: olcPpolicyConfig olcOverlay: ppolicy olcPPolicyDefault: cn=ppolicy,dc=digdash,dc=com olcPPolicyUseLockout: TRUE olcPPolicyHashCleartext: TRUE Run command:\n# Before  root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026#34;(objectClass=olcPpolicyConfig)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 # Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f ppolicy-conf.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026#34;(objectClass=olcPpolicyConfig)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: olcOverlay={2}ppolicy,olcDatabase={1}mdb,cn=config objectClass: olcPPolicyConfig olcOverlay: {2}ppolicy olcPPolicyDefault: cn=ppolicy,dc=digdash,dc=com olcPPolicyHashCleartext: TRUE olcPPolicyUseLockout: TRUE Define password policy File ppolicy-defaut.ldif s·∫Ω define user password c·∫ßn tu√¢n th·ªß nh·ªØng policy g√¨:\ndn: cn=ppolicy,dc=digdash,dc=com objectClass: device objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: ppolicy pwdAllowUserChange: TRUE pwdAttribute: userPassword pwdCheckQuality: 1 pwdExpireWarning: 600 pwdFailureCountInterval: 30 pwdGraceAuthNLimit: 5 pwdInHistory: 5 pwdLockout: TRUE pwdLockoutDuration: 900 pwdMaxAge: 0 pwdMaxFailure: 5 pwdMinAge: 0 pwdMinLength: 12 pwdMustChange: FALSE pwdSafeModify: FALSE Run command:\n# Before  root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b dc=digdash,dc=com \u0026#34;(objectClass=pwdPolicy)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 # Run command root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -x -a -H ldap://localhost -D cn=admin,dc=digdash,dc=com -f ppolicy-defaut.ldif -w Abcd1234 adding new entry \u0026#34;cn=ppolicy,dc=digdash,dc=com\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -Y EXTERNAL -H ldapi:/// -b dc=digdash,dc=com \u0026#34;(objectClass=pwdPolicy)\u0026#34; -LLL SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: cn=ppolicy,dc=digdash,dc=com objectClass: device objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: ppolicy pwdAllowUserChange: TRUE pwdAttribute: userPassword pwdCheckQuality: 1 pwdExpireWarning: 600 pwdFailureCountInterval: 30 pwdGraceAuthNLimit: 5 pwdInHistory: 5 pwdLockout: TRUE pwdLockoutDuration: 900 pwdMaxAge: 0 pwdMaxFailure: 5 pwdMinAge: 0 pwdMinLength: 12 pwdMustChange: FALSE pwdSafeModify: FALSE Create a new organization File neworganisation.ldif d√πng ƒë·ªÉ t·∫°o m·ªõi organization t√™n l√† ou=default,dc=digdash,dc=com:\ndn: ou=default,dc=digdash,dc=com objectClass: organizationalUnit ou: default Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapadd -H ldap://localhost -D cn=admin,dc=digdash,dc=com -x -f neworganisation.ldif -w Abcd1234 adding new entry \u0026#34;ou=default,dc=digdash,dc=com\u0026#34; # Verify root@4366cd65ca46:/etc/ldap/ldif-custom# ldapsearch -x -H ldap://localhost -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 # extended LDIF # # LDAPv3 # base \u0026lt;dc=digdash,dc=com\u0026gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # digdash.com dn: dc=digdash,dc=com objectClass: top objectClass: dcObject objectClass: organization o: loamics dc: digdash # ppolicy, digdash.com dn: cn=ppolicy,dc=digdash,dc=com objectClass: device objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: ppolicy pwdAllowUserChange: TRUE pwdAttribute: userPassword pwdCheckQuality: 1 pwdExpireWarning: 600 pwdFailureCountInterval: 30 pwdGraceAuthNLimit: 5 pwdInHistory: 5 pwdLockout: TRUE pwdLockoutDuration: 900 pwdMaxAge: 0 pwdMaxFailure: 5 pwdMinAge: 0 pwdMinLength: 12 pwdMustChange: FALSE pwdSafeModify: FALSE # default, digdash.com dn: ou=default,dc=digdash,dc=com objectClass: organizationalUnit ou: default # search result search: 2 result: 0 Success # numResponses: 4 # numEntries: 3 Create an admin user with the right File create_user_admin.ldif ƒë·ªÉ t·∫°o m·ªõi 1 user v·ªõi uid admin_default:\ndn: uid=admin,ou=default,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Default sn: Default uid: admin_default Run command:\nroot@4366cd65ca46:/etc/ldap/ldif-custom# ldapadd -H ldap://localhost -D cn=admin,dc=digdash,dc=com -x -f create_user_admin.ldif -w Abcd1234 adding new entry \u0026#34;uid=admin,ou=default,dc=digdash,dc=com\u0026#34; Add admin right File add_right_admin.ldif ƒë·ªÉ grant quy·ªÅn cho user:\ndn: olcDatabase={1}mdb,cn=config changetype: modify add: olcAccess olcaccess: {0}to dn.subtree=ou=default,dc=digdash,dc=com attrs=userpassword,shadowlastchange by dn.exact=uid=admin,ou=default,dc=digdash,dc=com write by self write by anonymous auth by * read olcaccess: {1}to dn.subtree=ou=default,dc=digdash,dc=com by dn.exact=uid=admin,ou=default,dc=digdash,dc=com write by * read Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f add_right_admin.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;olcDatabase={1}mdb,cn=config\u0026#34; Set new password for uid=admin,ou=default,dc=digdash,dc=com root@4366cd65ca46:/etc/ldap/ldif-custom# ldappasswd -H ldap://localhost -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -S \u0026#34;uid=admin,ou=default,dc=digdash,dc=com\u0026#34; -w Abcd1234 New password: Re-enter new password: Extend the 500 limit for LDAP searches File extend_limit_search.ldif ƒë·ªÉ extend gi·ªõi h·∫°n search c·ªßa ldap:\ndn: olcDatabase={-1}frontend,cn=config changetype: modify replace: olcSizeLimit olcSizeLimit: 3000 Run command:\nroot@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -a -Y EXTERNAL -H ldapi:/// -f extend_limit_search.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;olcDatabase={-1}frontend,cn=config\u0026#34; Increase MDB database memory to 10GB File increase_mem.ldif ƒë·ªÉ tƒÉng gi·ªõi h·∫°n c·ªßa DB MDB:\ndn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcDbMaxSize olcDbMaxSize: 10000000000 Run command:\n# Run root@4366cd65ca46:/etc/ldap/ldif-custom# ldapmodify -H ldapi:/// -Y EXTERNAL -f increase_mem.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 modifying entry \u0026#34;olcDatabase={1}mdb,cn=config\u0026#34; List all group and users trong openldap ldapsearch -x -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 List a specific user identified by uid=hoangdocker ldapsearch -x -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; \u0026#39;(uid=hoangdocker)\u0026#39; -w Abcd1234 Change password user Tr∆∞·ªõc ti√™n h√£y list user ƒë√≥ ra b·∫±ng command b√™n tr√™n ƒë·ªÉ bi·∫øt c√°c th√¥ng s·ªë kh√°c c·ªßa user ƒë√≥, nh∆∞ ou,dc l√† g√¨, sau ƒë√≥ th√¨ change password nh∆∞ sau:\nldappasswd -H ldapi:/// -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 -S uid=hoangdocker,ou=users,dc=digdash,dc=com New password: Re-enter new password: Check c√°c user v√† password ƒë√£ encrypted c·ªßa ch√∫ng trong cn=config ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config Find the RootDN account and the current RootDN password hash: (theo https://www.digitalocean.com/community/tutorials/how-to-change-account-passwords-on-an-openldap-server)\nroot@ded2e404d270:/# ldapsearch -H ldapi:// -LLL -Q -Y EXTERNAL -b \u0026#34;cn=config\u0026#34; \u0026#34;(olcRootDN=*)\u0026#34; dn olcRootDN olcRootPW dn: olcDatabase={0}config,cn=config olcRootDN: cn=admin,cn=config olcRootPW: {SSHA}kDomIgiYEPYYe2tJRrLkFO4jxrayzt0n dn: olcDatabase={1}mdb,cn=config olcRootDN: cn=admin,dc=digdash,dc=com olcRootPW: {SSHA}bq06veDempYLWI3VgWGO10mFaaPhxiwk Create a password hashed https://tech.feedyourhead.at/content/openldap-set-config-admin-password\nslappasswd -h {SSHA} Issue c·ªßa Digdash v√† OpenLDAP Khi m√¨nh mu·ªën Thay ƒë·ªïi password c·ªßa user admin. M√¨nh ti·∫øn h√†nh list c√°c user c√≥ uid=admin ra:\nldapsearch -x -b dc=digdash,dc=com -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; \u0026#39;(uid=admin)\u0026#39; -w Abcd1234 # admin, default, digdash.com dn: uid=admin,ou=default,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Default sn: Default uid: admin_default uid: admin userPassword:: e1NTSEF9MVJhckpidkZMaFNudHJsZzQya0JFMHpCWlg3a3pEMlA= # admin, users, digdash.com dn: uid=admin,ou=users,dc=digdash,dc=com uid: admin description:: ... objectClass: top objectClass: person objectClass: organizationalPerson objectClass: inetOrgPerson sn: admin cn: admin userPassword:: e1NTSEF9cnJiTUJmOFRiL2d5TVRPc25tbE1WYVBBZjlITFVWQXQ= =\u0026gt; l√† uid=admin,ou=default,dc=digdash,dc=com v√† uid=admin,ou=users,dc=digdash,dc=com\nN·∫øu ch·ªâ ƒë·ªïi password c·ªßa uid=admin,ou=users,dc=digdash,dc=com th√¨ s·∫Ω c√≥ l·ªói l√† login v√†o ƒë∆∞·ª£c b·∫±ng c·∫£ admin/admin v√† admin/New_Password\n-\u0026gt; N√™n m√¨nh th·ª≠ ƒë·ªïi pass c·ªßa c·∫£ 2 admin user ƒë√≥:\nldappasswd -H ldapi:/// -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 -S uid=admin,ou=users,dc=digdash,dc=com -\u0026gt; th√†nh Abcd1234u ldappasswd -H ldapi:/// -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 -S uid=admin,ou=default,dc=digdash,dc=com -\u0026gt; th√†nh Abcd1234d -\u0026gt; ko ƒÉn thua, ko login v·ªõi password n√†y ƒë∆∞·ª£c -\u0026gt; v·∫´n login v√†o b·∫±ng c·∫£ admin/admin v√† admin/Abcd1234u\n-\u0026gt; ph·∫£i l√™n http://DNS:8080/ddenterpriseapi/serversettings\nth√¨ m·ªõi s·ª≠a ƒë∆∞·ª£c c√°i password admin/admin, nh∆∞ng v·∫´n login ƒë∆∞·ª£c b·∫±ng admin/Abcd1234u\n-\u0026gt; ƒê√¢y l√† known issue c·ªßa OpenLDAP: https://github.com/osixia/docker-openldap/issues/161\nC√°ch gi·∫£i quy·∫øt\nDelete admin user (uid=admin,ou=users,dc=digdash,dc=com) in LDAP server:\nldapdelete -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 uid=admin,ou=users,dc=digdash,dc=com Delete admin user (uid=admin,ou=default,dc=digdash,dc=com) in LDAP server:\nldapdelete -x -D \u0026#34;cn=admin,dc=digdash,dc=com\u0026#34; -w Abcd1234 uid=admin,ou=default,dc=digdash,dc=com Nh∆∞ v·∫≠y LDAP s·∫Ω ko c√≥ user admin, s·∫Ω ch·ªâ c√≥ password ƒë∆∞·ª£c encrpyted trong file: \u0026lsquo;/digdash/appdata/default/Enterprise Server/ddenterpriseapi/config/serversettings.xml\u0026rsquo;\nTuy nhi√™n n·∫øu x√≥a c·∫£ 2 admin user trong LDAP ƒëi th√¨ khi tr√™n UI, list all user ra s·∫Ω ko list dc user admin -\u0026gt; n√™n ko g√°n license cho admin ƒë∆∞·ª£c -\u0026gt; 1 s·ªë trang nh∆∞ Dashboard editor.. th√¨ admin s·∫Ω ko login ƒë∆∞·ª£c v√¨ ko c√≥ license\n=\u0026gt; nh∆∞ v·∫≠y v·∫´n c·∫ßn t·∫°o user admin trong LDAP server, nh∆∞ng ko t·∫°o password cho n√≥. ƒê·ªÉ password s·∫Ω l·∫•y t·ª´ file serversettings.xml\nFile t·∫°o user admin c·∫ßn s·ª≠a l·∫°i nh∆∞ sau create_user_admin.ldif:\ndn: uid=admin,ou=users,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Users sn: admin uid: admin dn: uid=admin,ou=default,dc=digdash,dc=com objectClass: shadowAccount objectClass: inetOrgPerson cn: Admin Domain Default sn: Default uid: admin_default Ch√∫ √Ω uid=admin,ou=users,dc=digdash,dc=com th√¨ m·ªõi hi·ªÉn th·ªã tr√™n list user,\nch·ª© uid=admin,ou=default,dc=digdash,dc=com s·∫Ω ko hi·ªÉn th·ªã tr√™n list user, m√† c≈©ng ko login ƒë∆∞·ª£c.\nM√¨nh t·∫°o ra n√≥ ch·ªâ ƒë·ªÉ ƒë√∫ng v·ªõi document m√† DigDash cung c·∫•p th√¥i.\nN·∫øu b·ªã l·ªói khi t·∫°o uid=admin,ou=users,dc=digdash,dc=com th√¨ c√≥ kh·∫£ nƒÉng b·∫°n ƒëang deploy openLDAP tr∆∞·ªõc (ch∆∞a deploy DigDash) n√™n c·∫ßn t·∫°o organization users ƒë√£.\nV√¨ th·∫ø n√™n file neworganisation.ldif m√¨nh ƒë√£ add th√™m:\ndn: ou=default,dc=digdash,dc=com objectClass: organizationalUnit ou: default dn: ou=users,dc=digdash,dc=com objectClass: organizationalUnit ou: users CREDIT https://doc.digdash.com/xwiki/wiki/dd2022r2/view/Digdash/deployment/installation/install_guide_ubuntu/\nhttps://github.com/osixia/docker-openldap#seed-ldap-database-with-ldif\n","href":"/posts/encrypt-notes-about-docker-openldap/","title":"Notes About Docker OpenLDAP"},{"content":"","href":"/tags/openldap/","title":"OpenLDAP"},{"content":"Story G·∫ßn ƒë√¢y sau khi t·∫°o account Oracle m·ªõi (region Singapore), m√¨nh g·∫∑p kh√≥ khƒÉn khi ko th·ªÉ t·∫°o VM ƒë∆∞·ª£c v√¨ l·ªói: Error: 500-InternalError, Out of host capacity.\nM·ªói region ch·ªâ c√≥ 1 s·ªë l∆∞·ª£ng c√≥ h·∫°n c√°c VM CPU v√† RAM th√¥i, n√™n n·∫øu ai h√™n th√¨ m·ªõi t·∫°o ƒë∆∞·ª£c VM. N·∫øu c·ª© l√∫c n√†o nghƒ© ƒë·∫øn m·ªõi v√†o t·∫°o th√¨ s·∫Ω ch·∫≥ng bao gi·ªù c·∫°nh tranh ƒë∆∞·ª£c.\nC·∫ßn ph·∫£i scripting v√† auto create VM th√¥i. 1 Bash shell script s·ª≠ d·ª•ng oci-cli l√† ƒë·ªß.\n1. oci-cli Install oci-cli theo guideline\nsudo apt update sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev sudo apt update \u0026amp;\u0026amp; sudo apt install python3 python3-pip python3-venv python3 -m pip install oci-cli $ oci --version 3.24.0 2. Config credential Tham kh·∫£o b√†i n√†y: https://archive.ph/OjSqw#selection-755.0-755.27\nT·∫°o API Key: Download Private key, public key (optional), ·∫•n Add:\nHi·ªán ra 1 b·∫£ng h∆∞·ªõng d·∫´n setup config file:\nSau ƒë√≥, tr√™n local c·ªßa m√¨nh (Rasbperry Pi), t·∫°o file ~/.oci/config, paste n·ªôi dung b√™n tr√™n v√†o:\nCh√∫ √Ω ch·ªó b√¥i v√†ng tr·ªè t·ªõi ƒë∆∞·ªùng d·∫´n ch·ª©a Private Key v·ª´a download v·ªÅ nh√©\nmkdir ~/.oci nano ~/.oci/config Set permission cho credential:\noci setup repair-file-permissions --file ~/.oci/config oci setup repair-file-permissions --file \u0026lt;PRIVATE_KEY\u0026gt;.pem Test API Key, nh·ªõ t√¨m user_id c·ªßa b·∫°n ƒë·ªÉ thay v√†o nh√©:\nexport user_id=\u0026#34;ocid1.user.oc1..xyz\u0026#34; oci --config-file ~/.oci/config iam user get --user-id $user_id Tr·∫£ v·ªÅ 1 chu·ªói JSON ko c√≥ l·ªói l√† OK\nVi·ªác t·∫°o API Key tr√™n Oracle nh∆∞ n√†y m√¨nh th·∫•y h∆°i k√©m:\n Key ko c√≥ set th·ªùi gian expire. Key ko ƒë∆∞·ª£c set quy·ªÅn ƒë·ªÉ h·∫°n ch·∫ø vi·ªác s·ª≠ d·ª•ng c√°c resource kh√°c. S·ª≠ d·ª•ng Key kh√° c·ªìng k·ªÅnh, ph·∫£i t·∫°o c·∫£ ~/.oci/config file, trong ƒë√≥ tr·ªè ƒë·∫øn 1 file private key PEM üò´.  3. L√™n Oracle Console t·∫°o Stack T·∫°o VM nh∆∞ b√¨nh th∆∞·ªùng, sau ƒë√≥ ch·ªçn \u0026ldquo;Save as stack\u0026rdquo;:\nStack n√†y ƒë∆∞·ª£c back b·ªüi Terraform:\nNh∆∞ v·∫≠y sau n√†y m·ªói l·∫ßn v√†o Console ch·ªâ c·∫ßn v√†o Stack (resource manager) r·ªìi ·∫•n Apply l√† s·∫Ω t·ª± run code ƒë·ªÉ t·∫°o VM th√¥i:\nGi·ªù t√¨m c√°ch ƒë·ªÉ Run Apply Stack b·∫±ng oci-cli l√† ok\n4. Vi·∫øt script ƒë·ªÉ apply Stack b·∫±ng oci-cli Format command l·∫•y t·ª´ ngu·ªìn official n√†y\noci resource-manager job create-apply-job --execution-plan-strategy AUTO_APPROVED --stack-id \u0026lt;stack_ocid\u0026gt; script create-vm-oracle-sg.sh:\n#!/bin/bash  # Crontab every 2 minutes usage, run crontab -e: # */2 * * * * bash PATH_TO_THIS_SCRIPT.sh \u0026gt;\u0026gt; PATH_TO_LOG.log # Prepare YOUR OWN variable stack_ocid_1cpu=\u0026#34;ocid1.ormstack.oc1.ap-singapore-1.XXX\u0026#34; compartment_id=\u0026#34;ocid1.tenancy.oc1..YYY\u0026#34; TELEGRAM_API_TOKEN=\u0026#34;123456789:ZZZ\u0026#34; TELEGRAM_CHAT_ID=\u0026#34;123123123\u0026#34; MSG_SENT_PATH=\u0026#34;/opt/devops/oracle-rm-lab/MSG_SENT\u0026#34; MSG_SENT=$(cat $MSG_SENT_PATH) oci=\u0026#34;/home/pi/.local/bin/oci --config-file /home/pi/.oci/config-sg\u0026#34; # Print Date datetime=$(date \u0026#39;+%Y%m%d-%H%M%S\u0026#39;); echo \u0026#34;===============\u0026#34; echo \u0026#34;===============\u0026#34; echo \u0026#34;DATE: $datetime\u0026#34; echo \u0026#34;MSG_SENT: $MSG_SENT\u0026#34; echo \u0026#34;oci version: \u0026#34; $oci --version # Get Instance List INSTANCE_LIST=$($oci compute instance list --compartment-id $compartment_id) # if instace list has value, it mean the vm has been created, notify via telegram if [ ! -z ${INSTANCE_LIST} ]; then echo \u0026#34;Instance exist\u0026#34; # if Telegram has not been sent, sent it if [ $MSG_SENT == \u0026#39;FALSE\u0026#39; ]; then MESSAGE=\u0026#34;Your OC VM has been created, check it out!\u0026#34; curl -s -X POST https://api.telegram.org/bot$TELEGRAM_API_TOKEN/sendMessage -d chat_id=$TELEGRAM_CHAT_ID -d text=\u0026#34;$MESSAGE\u0026#34; echo \u0026#34;Message just sent\u0026#34; echo \u0026#34;TRUE\u0026#34; \u0026gt; $MSG_SENT_PATH fi fi # if instance list empty if [ -z ${INSTANCE_LIST} ]; then echo \u0026#34;Instance list empty, applying Job 1cpu...\u0026#34; APPLY_JOB=$($oci resource-manager job create-apply-job --execution-plan-strategy AUTO_APPROVED --stack-id $stack_ocid_1cpu) JOB_ID=$(echo \u0026#34;$APPLY_JOB\u0026#34; | jq -r \u0026#39;.data.id\u0026#39;) echo $JOB_ID fi echo \u0026#34;Done\u0026#34; Gi·∫£i th√≠ch script tr√™n:\n C√°c b·∫°n c·∫ßn t√¨m tr√™n Console c√°c OCID ƒë·ªÉ ƒëi·ªÅn v√†o ph·∫ßn # Prepare YOUR OWN variable Script n√†y check n·∫øu ƒë√£ c√≥ VM r·ªìi th√¨ s·∫Ω g·ª≠i message l√™n Telegram, n·∫øu b·∫°n ko mu·ªën d√πng Telegram th√¨ x√≥a ƒëo·∫°n code ƒë√≥ ƒëi.  5. Edit crontab v√† test crontab -e Run every 2 minutes and log to file:\n# */2 * * * * bash PATH_TO_THIS_SCRIPT.sh \u0026gt;\u0026gt; PATH_TO_LOG.log Ch·ªù 2 ph√∫t v√† l√™n giao di·ªán Console check c√≥ job ƒë∆∞·ª£c t·∫°o ra l√† OK nh√©, h·∫ßu h·∫øt l√† FAILED nh∆∞ n√†y :)) Kh√¥ng bi·∫øt c√°ch n√†y c√≥ work ko, v√¨ ƒë·∫øn hi·ªán t·∫°i sau 1 ng√†y run Script li√™n t·ª•c m·ªói 2 ph√∫t, m√¨nh v·∫´n ch∆∞a t·∫°o ƒë∆∞·ª£c VM\nC√≥ ng∆∞·ªùi n√≥i h·ªç ch·∫°y 2 th√°ng r·ªìi v·∫´n ch∆∞a t·∫°o ƒë∆∞·ª£c VM üòÇ\nC√≥ ng∆∞·ªùi n√≥i N·∫øu b·∫°n upgrade t·ª´ Alway Free Tier l√™n Pay-As-You-Go th√¨ b·∫°n s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n n·∫øu t·∫°o VM (C√°i n√†y m√¨nh ko d√°m l√†m)\nNh∆∞ng √≠t nh·∫•t th√¨ c∆° h·ªôi s·∫Ω nhi·ªÅu h∆°n l√† \u0026ldquo;thi tho·∫£ng nh·ªõ ra th√¨ v√†o Browser t·∫°o\u0026rdquo; ƒë√∫ng ko?\nCh√∫c c√°c b·∫°n th√†nh c√¥ng üòÅ\n6. Script auto-start VM if Oracle stop it G·∫ßn ƒë√¢y m√¨nh nh·∫≠n ƒë∆∞·ª£c 1 email t·ª´ Oracle nh∆∞ n√†y: ƒê·∫°i kh√°i h·ªç s·∫Ω \u0026ldquo;reclaim\u0026rdquo; idle resource b·∫±ng c√°ch stop VM c·ªßa m√¨nh.\nN·∫øu m√¨nh upgrade account Alway Free Tier l√™n Pay-As-You-Go th√¨ s·∫Ω ko sao.\nC√≥ th·ªÉ ƒë√¢y l√† chi·∫øn d·ªãch ƒë·ªÉ d·ªçn ng∆∞·ªùi c≈© cho ng m·ªõi c√≥ c∆° h·ªôi t·∫°o VM ƒë√¢y :))\nM√¨nh nghƒ© ngay ƒë·∫øn vi·ªác s·∫Ω vi·∫øt 1 script ƒë·ªÉ check 1 ph√∫t 1 l·∫ßn xem VM ƒë√£ b·ªã stop ch∆∞a, n·∫øu stop th√¨ restart n√≥ l·∫°i ngay l·∫≠p t·ª©c\nScript start-vm-oracle-london.sh:\n#!/bin/bash  # Crontab every 2 minutes usage, run crontab -e: # */2 * * * * bash PATH_TO_THIS_SCRIPT.sh \u0026gt;\u0026gt; PATH_TO_LOG.log # Prepare YOUR OWN variable compartment_id=\u0026#34;ocid1.tenancy.oc1..xxx\u0026#34; instance_id=\u0026#34;ocid1.instance.oc1.uk-london-1.yyy\u0026#34; TELEGRAM_API_TOKEN=\u0026#34;1234567890:zzz\u0026#34; TELEGRAM_CHAT_ID=\u0026#34;123123123\u0026#34; oci=\u0026#34;/home/pi/.local/bin/oci --config-file /home/pi/.oci/config-london\u0026#34; # Print Date datetime=$(date \u0026#39;+%Y%m%d-%H%M%S\u0026#39;); echo \u0026#34;===============\u0026#34; echo \u0026#34;===============\u0026#34; echo \u0026#34;DATE: $datetime\u0026#34; echo \u0026#34;oci version: \u0026#34; $oci --version # Get instance state INSTANCE_STATE_DETAIL=$($oci compute instance get --instance-id $instance_id) INSTANCE_STATE=$(echo \u0026#34;$INSTANCE_STATE_DETAIL\u0026#34; | jq -r \u0026#39;.data.\u0026#34;lifecycle-state\u0026#34;\u0026#39;) echo $INSTANCE_STATE # if instance is not running, notify via telegram if [ $INSTANCE_STATE != \u0026#39;RUNNING\u0026#39; ]; then echo \u0026#34;Instance is not running\u0026#34; # send Telegram message MESSAGE=\u0026#34;Your OC VM is not running, take a look!\u0026#34; curl -s -X POST https://api.telegram.org/bot$TELEGRAM_API_TOKEN/sendMessage -d chat_id=$TELEGRAM_CHAT_ID -d text=\u0026#34;$MESSAGE\u0026#34; echo \u0026#34;Message just sent\u0026#34; fi # if instance is stopped, notify via telegram, then start it if [ $INSTANCE_STATE == \u0026#39;STOPPED\u0026#39; ]; then echo \u0026#34;Instance stopped\u0026#34; START_VM=$($oci compute instance action --action START --instance-id $instance_id) # send Telegram message MESSAGE=\u0026#34;Your OC VM has been stopped, I just restarted it, take a look!\u0026#34; curl -s -X POST https://api.telegram.org/bot$TELEGRAM_API_TOKEN/sendMessage -d chat_id=$TELEGRAM_CHAT_ID -d text=\u0026#34;$MESSAGE\u0026#34; echo \u0026#34;Message just sent\u0026#34; fi echo \u0026#34;Done\u0026#34; Gi·∫£i th√≠ch script tr√™n:\n N·∫øu th·∫•y VM ƒëang runnning, do nothing. N·∫øu th·∫•y VM ƒëang ko ph·∫£i runnning, g·ª≠i message v√†o Telegram. N·∫øu VM ƒëang stopped, start l√™n, g·ª≠i message v√†o Telegram.  CREDIT https://www.reddit.com/r/oraclecloud/ https://www.reddit.com/r/oraclecloud/comments/11u6t9q/oracle_cloud_out_of_host_capacity_resolving/ https://archive.ph/OjSqw#selection-755.0-755.27 https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/climanualinst.htm https://docs.oracle.com/en-us/iaas/Content/ResourceManager/Tasks/create-job.htm https://docs.oracle.com/en-us/iaas/tools/oci-cli/3.24.0/oci_cli_docs/cmdref/compute/instance.html#available-commands\n","href":"/bk/encrypt-oracle-oci-cli-script/","title":"Oracle Cloud Infrastructure CLI scripts"},{"content":"Story G·∫ßn ƒë√¢y sau khi t·∫°o account Oracle m·ªõi (region Singapore), m√¨nh g·∫∑p kh√≥ khƒÉn khi ko th·ªÉ t·∫°o VM ƒë∆∞·ª£c v√¨ l·ªói: Error: 500-InternalError, Out of host capacity.\nM·ªói region ch·ªâ c√≥ 1 s·ªë l∆∞·ª£ng c√≥ h·∫°n c√°c VM CPU v√† RAM th√¥i, n√™n n·∫øu ai h√™n th√¨ m·ªõi t·∫°o ƒë∆∞·ª£c VM. N·∫øu c·ª© l√∫c n√†o nghƒ© ƒë·∫øn m·ªõi v√†o t·∫°o th√¨ s·∫Ω ch·∫≥ng bao gi·ªù c·∫°nh tranh ƒë∆∞·ª£c.\nC·∫ßn ph·∫£i scripting v√† auto create VM th√¥i. 1 Bash shell script s·ª≠ d·ª•ng oci-cli l√† ƒë·ªß.\n1. oci-cli Install oci-cli theo guideline\nsudo apt update sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev sudo apt update \u0026amp;\u0026amp; sudo apt install python3 python3-pip python3-venv python3 -m pip install oci-cli $ oci --version 3.24.0 2. Config credential Tham kh·∫£o b√†i n√†y: https://archive.ph/OjSqw#selection-755.0-755.27\nT·∫°o API Key: Download Private key, public key (optional), ·∫•n Add:\nHi·ªán ra 1 b·∫£ng h∆∞·ªõng d·∫´n setup config file:\nSau ƒë√≥, tr√™n local c·ªßa m√¨nh (Rasbperry Pi), t·∫°o file ~/.oci/config, paste n·ªôi dung b√™n tr√™n v√†o:\nCh√∫ √Ω ch·ªó b√¥i v√†ng tr·ªè t·ªõi ƒë∆∞·ªùng d·∫´n ch·ª©a Private Key v·ª´a download v·ªÅ nh√©\nmkdir ~/.oci nano ~/.oci/config Set permission cho credential:\noci setup repair-file-permissions --file ~/.oci/config oci setup repair-file-permissions --file \u0026lt;PRIVATE_KEY\u0026gt;.pem Test API Key, nh·ªõ t√¨m user_id c·ªßa b·∫°n ƒë·ªÉ thay v√†o nh√©:\nexport user_id=\u0026#34;ocid1.user.oc1..xyz\u0026#34; oci --config-file ~/.oci/config iam user get --user-id $user_id Tr·∫£ v·ªÅ 1 chu·ªói JSON ko c√≥ l·ªói l√† OK\nVi·ªác t·∫°o API Key tr√™n Oracle nh∆∞ n√†y m√¨nh th·∫•y h∆°i k√©m:\n Key ko c√≥ set th·ªùi gian expire. Key ko ƒë∆∞·ª£c set quy·ªÅn ƒë·ªÉ h·∫°n ch·∫ø vi·ªác s·ª≠ d·ª•ng c√°c resource kh√°c. S·ª≠ d·ª•ng Key kh√° c·ªìng k·ªÅnh, ph·∫£i t·∫°o c·∫£ ~/.oci/config file, trong ƒë√≥ tr·ªè ƒë·∫øn 1 file private key PEM üò´.  3. L√™n Oracle Console t·∫°o Stack T·∫°o VM nh∆∞ b√¨nh th∆∞·ªùng, sau ƒë√≥ ch·ªçn \u0026ldquo;Save as stack\u0026rdquo;:\nStack n√†y ƒë∆∞·ª£c back b·ªüi Terraform:\nNh∆∞ v·∫≠y sau n√†y m·ªói l·∫ßn v√†o Console ch·ªâ c·∫ßn v√†o Stack (resource manager) r·ªìi ·∫•n Apply l√† s·∫Ω t·ª± run code ƒë·ªÉ t·∫°o VM th√¥i:\nGi·ªù t√¨m c√°ch ƒë·ªÉ Run Apply Stack b·∫±ng oci-cli l√† ok\n4. Vi·∫øt script ƒë·ªÉ apply Stack b·∫±ng oci-cli Format command l·∫•y t·ª´ ngu·ªìn official n√†y\noci resource-manager job create-apply-job --execution-plan-strategy AUTO_APPROVED --stack-id \u0026lt;stack_ocid\u0026gt; script create-vm-oracle-sg.sh:\n#!/bin/bash  # Crontab every 2 minutes usage, run crontab -e: # */2 * * * * bash PATH_TO_THIS_SCRIPT.sh \u0026gt;\u0026gt; PATH_TO_LOG.log # Prepare YOUR OWN variable stack_ocid_1cpu=\u0026#34;ocid1.ormstack.oc1.ap-singapore-1.XXX\u0026#34; compartment_id=\u0026#34;ocid1.tenancy.oc1..YYY\u0026#34; TELEGRAM_API_TOKEN=\u0026#34;123456789:ZZZ\u0026#34; TELEGRAM_CHAT_ID=\u0026#34;123123123\u0026#34; MSG_SENT_PATH=\u0026#34;/opt/devops/oracle-rm-lab/MSG_SENT\u0026#34; MSG_SENT=$(cat $MSG_SENT_PATH) oci=\u0026#34;/home/pi/.local/bin/oci --config-file /home/pi/.oci/config-sg\u0026#34; # Print Date datetime=$(date \u0026#39;+%Y%m%d-%H%M%S\u0026#39;); echo \u0026#34;===============\u0026#34; echo \u0026#34;===============\u0026#34; echo \u0026#34;DATE: $datetime\u0026#34; echo \u0026#34;MSG_SENT: $MSG_SENT\u0026#34; echo \u0026#34;oci version: \u0026#34; $oci --version # Get Instance List INSTANCE_LIST=$($oci compute instance list --compartment-id $compartment_id) # if instace list has value, it mean the vm has been created, notify via telegram if [ ! -z ${INSTANCE_LIST} ]; then echo \u0026#34;Instance exist\u0026#34; # if Telegram has not been sent, sent it if [ $MSG_SENT == \u0026#39;FALSE\u0026#39; ]; then MESSAGE=\u0026#34;Your OC VM has been created, check it out!\u0026#34; curl -s -X POST https://api.telegram.org/bot$TELEGRAM_API_TOKEN/sendMessage -d chat_id=$TELEGRAM_CHAT_ID -d text=\u0026#34;$MESSAGE\u0026#34; echo \u0026#34;Message just sent\u0026#34; echo \u0026#34;TRUE\u0026#34; \u0026gt; $MSG_SENT_PATH fi fi # if instance list empty if [ -z ${INSTANCE_LIST} ]; then echo \u0026#34;Instance list empty, applying Job 1cpu...\u0026#34; APPLY_JOB=$($oci resource-manager job create-apply-job --execution-plan-strategy AUTO_APPROVED --stack-id $stack_ocid_1cpu) JOB_ID=$(echo \u0026#34;$APPLY_JOB\u0026#34; | jq -r \u0026#39;.data.id\u0026#39;) echo $JOB_ID fi echo \u0026#34;Done\u0026#34; Gi·∫£i th√≠ch script tr√™n:\n C√°c b·∫°n c·∫ßn t√¨m tr√™n Console c√°c OCID ƒë·ªÉ ƒëi·ªÅn v√†o ph·∫ßn # Prepare YOUR OWN variable Script n√†y check n·∫øu ƒë√£ c√≥ VM r·ªìi th√¨ s·∫Ω g·ª≠i message l√™n Telegram, n·∫øu b·∫°n ko mu·ªën d√πng Telegram th√¨ x√≥a ƒëo·∫°n code ƒë√≥ ƒëi.  5. Edit crontab v√† test crontab -e Run every 2 minutes and log to file:\n# */2 * * * * bash PATH_TO_THIS_SCRIPT.sh \u0026gt;\u0026gt; PATH_TO_LOG.log Ch·ªù 2 ph√∫t v√† l√™n giao di·ªán Console check c√≥ job ƒë∆∞·ª£c t·∫°o ra l√† OK nh√©, h·∫ßu h·∫øt l√† FAILED nh∆∞ n√†y :)) Kh√¥ng bi·∫øt c√°ch n√†y c√≥ work ko, v√¨ ƒë·∫øn hi·ªán t·∫°i sau 1 ng√†y run Script li√™n t·ª•c m·ªói 2 ph√∫t, m√¨nh v·∫´n ch∆∞a t·∫°o ƒë∆∞·ª£c VM\nC√≥ ng∆∞·ªùi n√≥i h·ªç ch·∫°y 2 th√°ng r·ªìi v·∫´n ch∆∞a t·∫°o ƒë∆∞·ª£c VM üòÇ\nC√≥ ng∆∞·ªùi n√≥i N·∫øu b·∫°n upgrade t·ª´ Alway Free Tier l√™n Pay-As-You-Go th√¨ b·∫°n s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n n·∫øu t·∫°o VM (C√°i n√†y m√¨nh ko d√°m l√†m)\nNh∆∞ng √≠t nh·∫•t th√¨ c∆° h·ªôi s·∫Ω nhi·ªÅu h∆°n l√† \u0026ldquo;thi tho·∫£ng nh·ªõ ra th√¨ v√†o Browser t·∫°o\u0026rdquo; ƒë√∫ng ko?\nCh√∫c c√°c b·∫°n th√†nh c√¥ng üòÅ\n6. Script auto-start VM if Oracle stop it G·∫ßn ƒë√¢y m√¨nh nh·∫≠n ƒë∆∞·ª£c 1 email t·ª´ Oracle nh∆∞ n√†y: ƒê·∫°i kh√°i h·ªç s·∫Ω \u0026ldquo;reclaim\u0026rdquo; idle resource b·∫±ng c√°ch stop VM c·ªßa m√¨nh.\nN·∫øu m√¨nh upgrade account Alway Free Tier l√™n Pay-As-You-Go th√¨ s·∫Ω ko sao.\nC√≥ th·ªÉ ƒë√¢y l√† chi·∫øn d·ªãch ƒë·ªÉ d·ªçn ng∆∞·ªùi c≈© cho ng m·ªõi c√≥ c∆° h·ªôi t·∫°o VM ƒë√¢y :))\nM√¨nh nghƒ© ngay ƒë·∫øn vi·ªác s·∫Ω vi·∫øt 1 script ƒë·ªÉ check 1 ph√∫t 1 l·∫ßn xem VM ƒë√£ b·ªã stop ch∆∞a, n·∫øu stop th√¨ restart n√≥ l·∫°i ngay l·∫≠p t·ª©c\nScript start-vm-oracle-london.sh:\n#!/bin/bash  # Crontab every 2 minutes usage, run crontab -e: # */2 * * * * bash PATH_TO_THIS_SCRIPT.sh \u0026gt;\u0026gt; PATH_TO_LOG.log # Prepare YOUR OWN variable compartment_id=\u0026#34;ocid1.tenancy.oc1..xxx\u0026#34; instance_id=\u0026#34;ocid1.instance.oc1.uk-london-1.yyy\u0026#34; TELEGRAM_API_TOKEN=\u0026#34;1234567890:zzz\u0026#34; TELEGRAM_CHAT_ID=\u0026#34;123123123\u0026#34; oci=\u0026#34;/home/pi/.local/bin/oci --config-file /home/pi/.oci/config-london\u0026#34; # Print Date datetime=$(date \u0026#39;+%Y%m%d-%H%M%S\u0026#39;); echo \u0026#34;===============\u0026#34; echo \u0026#34;===============\u0026#34; echo \u0026#34;DATE: $datetime\u0026#34; echo \u0026#34;oci version: \u0026#34; $oci --version # Get instance state INSTANCE_STATE_DETAIL=$($oci compute instance get --instance-id $instance_id) INSTANCE_STATE=$(echo \u0026#34;$INSTANCE_STATE_DETAIL\u0026#34; | jq -r \u0026#39;.data.\u0026#34;lifecycle-state\u0026#34;\u0026#39;) echo $INSTANCE_STATE # if instance is not running, notify via telegram if [ $INSTANCE_STATE != \u0026#39;RUNNING\u0026#39; ]; then echo \u0026#34;Instance is not running\u0026#34; # send Telegram message MESSAGE=\u0026#34;Your OC VM is not running, take a look!\u0026#34; curl -s -X POST https://api.telegram.org/bot$TELEGRAM_API_TOKEN/sendMessage -d chat_id=$TELEGRAM_CHAT_ID -d text=\u0026#34;$MESSAGE\u0026#34; echo \u0026#34;Message just sent\u0026#34; fi # if instance is stopped, notify via telegram, then start it if [ $INSTANCE_STATE == \u0026#39;STOPPED\u0026#39; ]; then echo \u0026#34;Instance stopped\u0026#34; START_VM=$($oci compute instance action --action START --instance-id $instance_id) # send Telegram message MESSAGE=\u0026#34;Your OC VM has been stopped, I just restarted it, take a look!\u0026#34; curl -s -X POST https://api.telegram.org/bot$TELEGRAM_API_TOKEN/sendMessage -d chat_id=$TELEGRAM_CHAT_ID -d text=\u0026#34;$MESSAGE\u0026#34; echo \u0026#34;Message just sent\u0026#34; fi echo \u0026#34;Done\u0026#34; Gi·∫£i th√≠ch script tr√™n:\n N·∫øu th·∫•y VM ƒëang runnning, do nothing. N·∫øu th·∫•y VM ƒëang ko ph·∫£i runnning, g·ª≠i message v√†o Telegram. N·∫øu VM ƒëang stopped, start l√™n, g·ª≠i message v√†o Telegram.  CREDIT https://www.reddit.com/r/oraclecloud/ https://www.reddit.com/r/oraclecloud/comments/11u6t9q/oracle_cloud_out_of_host_capacity_resolving/ https://archive.ph/OjSqw#selection-755.0-755.27 https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/climanualinst.htm https://docs.oracle.com/en-us/iaas/Content/ResourceManager/Tasks/create-job.htm https://docs.oracle.com/en-us/iaas/tools/oci-cli/3.24.0/oci_cli_docs/cmdref/compute/instance.html#available-commands\n","href":"/posts/encrypt-oracle-oci-cli-script/","title":"Oracle Cloud Infrastructure CLI scripts"},{"content":"","href":"/tags/oraclecloud/","title":"OracleCloud"},{"content":"B√†i n√†y note l·∫°i nh·ªØng th·ª© kh√° ph·ªï bi·∫øn trong Python nh∆∞ng m√¨nh √≠t d√πng/ƒë·ªÉ √Ω ƒë·∫øn n√≥.\nV·ªõi m√¨nh th√¨ c√¥ng vi·ªác DevOps c≈©ng c√≥ l√†m vi·ªác v·ªõi Python scripting nh∆∞ng ·ªü level r·∫•t basic th√¥i. Nh·ªØng th·ª© n√†y ƒë√∫ng l√† c√≥ 1 ch√∫t l·∫° l·∫´m.\n1. Walrus Operator To√°n t·ª≠ := trong python ƒë∆∞·ª£c g·ªçi l√† \u0026ldquo;Walrus Operator\u0026rdquo;, ƒë∆∞·ª£c gi·ªõi thi·ªáu t·ª´ Python3.8. Gi√∫p l√†m code ng·∫Øn g·ªçn h∆°n.\nV√≠ d·ª• 1: N·∫øu 1 function t√™n l√† func() ch·∫°y r·∫•t l√¢u, t·ªën nhi·ªÅu th·ªùi gian,\nb·∫°n ko n√™n ch·∫°y l·∫°i function ƒë√≥ nhi·ªÅu l·∫ßn 1 c√°ch ko c·∫ßn thi·∫øt,\nm√† n√™n g√°n k·∫øt qu·∫£ c·ªßa n√≥ v√†o 1 bi·∫øn, r·ªìi s·ª≠ d·ª•ng l·∫°i khi c·∫ßn thi·∫øt.\n# --------------------- # SLOWER # \u0026#34;func\u0026#34; called 3 times result = [func(x), func(x)**2, func(x)**3] # -----\u0026gt; B·∫°n ƒëang ch·∫°y l·∫°i func(x) 3 l·∫ßn trong 1 d√≤ng code # --------------------- # FASTER # Reuse result of \u0026#34;func\u0026#34; without splitting the code into multiple lines result = [y := func(x), y**2, y**3] # -----\u0026gt; B·∫°n g√°n k·∫øt qu·∫£ c·ªßa func(x) cho y, r·ªìi l·∫•y k·∫øt qu·∫£ ƒë√≥ s·ª≠ d·ª•ng l·∫°i trong y**2, y**3 V√≠ d·ª• 2: B·∫°n c·∫ßn d√πng regex ƒë·ªÉ t√¨m ki·∫øm 2 partern trong 1 list. Thay v√¨ if-else l·ªìng nhau h·∫øt partern n√†y ƒë·∫øn partern kh√°c.\nB·∫°n n√™n g√°n gi√° tr·ªã regex v√†o bi·∫øn m k·∫øt h·ª£p v·ªõi or. Tr√°nh vi·ªác t·∫°o c√°c v√≤ng loop if-else l·ªìng nhau s·∫Ω kh√≥ ƒë·ªçc.\nimport re tests = [\u0026#34;Something to match\u0026#34;, \u0026#34;Second one is present\u0026#34;] pattern1 = r\u0026#34;^.*(thing).*\u0026#34; pattern2 = r\u0026#34;^.*(present).*\u0026#34; # --------------------- # LONGER for test in tests: m = re.match(pattern1, test) if m: print(f\u0026#34;Matched the 1st pattern: {m.group(1)}\u0026#34;) else: m = re.match(pattern2, test) if m: print(f\u0026#34;Matched the 2nd pattern: {m.group(1)}\u0026#34;) # Matched the 1st pattern: thing # Matched the 2nd pattern: present # --------------------- # CLEANER for test in tests: if m := (re.match(pattern1, test) or re.match(pattern2, test)): print(f\u0026#34;Matched: \u0026#39;{m.group(1)}\u0026#39;\u0026#34;) # Matched: \u0026#39;thing\u0026#39; # Matched: \u0026#39;present\u0026#39; V√≠ d·ª• 3: B·∫°n s·ª≠ d·ª•ng bi·∫øn today ƒë·ªÉ nh·∫≠n gi√° tr·ªã c·ªßa h√†m datetime.today(), sau ƒë√≥ s·ª≠ d·ª•ng l·∫°i trong c√πng line ƒë√≥.\nM√† ko c·∫ßn ph·∫£i run h√†m datetime.today() 2 l·∫ßn.\nfrom datetime import datetime print(f\u0026#34;Today is: {(today:=datetime.today()):%Y-%m-%d}, which is {today:%A}\u0026#34;) # Today is: 2022-07-01, which is Friday 2. Lambda function ƒê√¢y l√† 1 function kh√° th√¥ng d·ª•ng khi ƒë·ªçc code Python ·ªü tr√™n m·∫°ng.\nM·ª•c ƒë√≠ch c·ªßa n√≥ l√† ƒë·ªÉ khai b√°o h√†m m√† ko c·∫ßn define b·∫±ng def. Ng∆∞·ªùi ta g·ªçi lambda function l√† h√†m ·∫©n danh (anonymous function)\nTh∆∞·ªùng ch√∫ng ta s·ª≠ d·ª•ng lambda function v·ªõi c√°c h√†m ch·ªâ c·∫ßn m·ªôt d√≤ng l·ªánh.\n# --------------------- # LONG def doubler(x): return x*2 print(doubler(4)) # Prints 8 # --------------------- # SHORT doubler = lambda x: x*2 print(doubler(5)) # Prints 10 Nhi·ªÅu tham s·ªë:\nmul = lambda x, y: x*y print(mul(5, 10)) # Prints 50 M·∫∑c d√π v·∫≠y Lambda b·ªã ƒë√°nh gi√° l√† khi·∫øn vi·ªác debug/maintenance tr·ªü n√™n kh√≥ khƒÉn h∆°n\n3. Arrow -\u0026gt; ƒê√¥i khi ƒë·ªçc code Python3 b·∫°n c√≤n th·∫•y k√Ω t·ª± m≈©i t√™n -\u0026gt; khi define function\nK√Ω hi·ªáu n√†y ch·ªâ ƒë∆°n gi·∫£n l√† gi√∫p code c·ªßa b·∫°n d·ªÖ ƒë·ªçc h∆°n m√† th√¥i, n√≥ ko ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn logic c·ªßa code c·∫£.\nKi·ªÉu nh∆∞ thay v√¨ vi·∫øt comment # h√†m n√†y t√≠nh v·∫≠n t·ªëc th√¨ b·∫°n vi·∫øt:\ndef velocity(s, t) -\u0026gt; \u0026#39;h√†m n√†y t√≠nh v·∫≠n t·ªëc\u0026#39;: return s / t ƒê∆°n gi·∫£n th·∫ø th√¥i. Khi print velocity.__annotations__ b·∫°n s·∫Ω th·∫•y:\n{'return': 'h√†m n√†y t√≠nh v·∫≠n t·ªëc'} Ng∆∞·ªùi ta c≈©ng hay vi·∫øt ki·ªÉu th·∫ø n√†y:\ndef velocity(s: \u0026#39;d√πng ƒë∆°n v·ªã kilomet\u0026#39;, t: \u0026#39;d√πng ƒë∆°n v·ªã gi·ªù\u0026#39;) -\u0026gt; \u0026#39;h√†m n√†y t√≠nh v·∫≠n t·ªëc\u0026#39;: return s / t Vi·∫øt nh∆∞ v·∫≠y l√† ƒë·ªÉ ng∆∞·ªùi ƒë·ªçc hi·ªÉu r·∫±ng h√£y input gi√° tr·ªã kilomet cho s, gi√° tr·ªã gi·ªù cho t\nKhi print velocity.__annotations__ b·∫°n s·∫Ω th·∫•y:\n{'return': 'h√†m n√†y t√≠nh v·∫≠n t·ªëc', 's': 'd√πng ƒë∆°n v·ªã kilomet', 't': 'd√πng ƒë∆°n v·ªã gi·ªù'} 4. Class in Python C√≥ th·ªÉ coi class trong Python nh∆∞ 1 blueprint c·ªßa objects. Nh∆∞ l√† prototype c·ªßa 1 chi·∫øc xe m√°y.\nTa c√≥ th·ªÉ t·∫°o ra nhi·ªÅu chi·∫øc xe kh√°c nhau t·ª´ 1 prototype, m·ªói chi·∫øc xe l√† 1 object.\nTrong 1 class th√¨ c√≥ th·ªÉ c√≥ nhi·ªÅu function, c√°c function ƒë√≥ g·ªçi l√† method c·ªßa class.\n# create a class class Room: length = 0.0 # \u0026lt;-------------------- ƒê√¢y l√† c√°c attribute c·ªßa Class breadth = 0.0 # method to calculate area def calculate_area(self): # \u0026lt;---------------- ƒê√¢y g·ªçi l√† Method print(\u0026#34;Area of Room =\u0026#34;, self.length * self.breadth) # create object of Room class study_room = Room() # assign values to all the attributes  study_room.length = 42.5 study_room.breadth = 30.8 # access method inside class study_room.calculate_area() Output:\nArea of Room = 1309.0 4.1. Constructor init __init__ method l√† 1 h√†m ƒë·∫∑c bi·ªát trong Class c·ªßa Python, n√≥ gi·ªëng v·ªõi kh√°i ni·ªám constructor trong Java hay C++.\nConstructors ƒë∆∞·ª£c d√πng ƒë·ªÉ kh·ªüi t·∫°o gi√° tr·ªã cho object.\n__init__ s·∫Ω ƒë∆∞·ª£c g·ªçi b·∫•t c·ª© khi n√†o 1 object m·ªõi ƒë∆∞·ª£c kh·ªüi t·∫°o.\nN·∫øu ko d√πng constructor b·∫°n s·∫Ω vi·∫øt code th·∫ø n√†y ƒë·ªÉ t·∫°o gi√° tr·ªã cho attribute name trong class Bike:\nclass Bike: name = \u0026#34;\u0026#34; ... # create object bike1 = Bike() N·∫øu d√πng constructor:\nclass Bike: # constructor function  def __init__(self, name = \u0026#34;\u0026#34;): self.name = name bike1 = Bike() 4.2. K·∫ø th·ª´a, ƒëa k·∫ø th·ª´a V√¨ Python c≈©ng l√† 1 ng√¥n ng·ªØ OOP n√™n n√≥ c√≥ ƒë·∫ßy ƒë·ªß c√°c thu·ªôc t√≠nh c·ªßa OOP language. V√≠ d·ª• t√≠nh k·∫ø th·ª´a (Inheritance)\nK·∫ø th·ª´a: Trong v√≠ d·ª• sau, class Dog k·∫ø th·ª´a class Animal, thay ƒë·ªïi gi√° tr·ªã c·ªßa thu·ªôc t√≠nh leg, vi·∫øt l·∫°i method eat() v√† th√™m method m·ªõi l√† run()\nclass Animal: legs = \u0026#39;0\u0026#39; def __init__(self): pass def whoAmI(self): print (\u0026#34;Animal\u0026#34;) def eat(self): print (\u0026#34;Eating\u0026#34;) class Dog(Animal): def __init__(self): Animal.__init__(self) print (\u0026#34;Dog created\u0026#34;) self.legs = \u0026#39;4\u0026#39;; def whoAmI(self): print (\u0026#34;Dog go go\u0026#34;) def eat(self): print (\u0026#34;eat eat eat .....\u0026#34;) def run(self): print (\u0026#34;legs: \u0026#34; + self.legs + \u0026#34; run run run .....\u0026#34;) d = Dog() d.whoAmI() d.eat() d.run() ƒêa k·∫ø th·ª´a: trong v√≠ d·ª• sau, class Dog k·∫ø th·ª´a t·ª´ 2 class cha Animal, Entity\nclass Animal: legs = \u0026#39;0\u0026#39; def __init__(self): pass def whoAmI(self): print (\u0026#34;Animal\u0026#34;) def eat(self): print (\u0026#34;Eating\u0026#34;) class Entity: def __init__(self): pass def weight(self): print \u0026#39;weight 88\u0026#39;; class Dog(Entity, Animal): def __init__(self): Animal.__init__(self) Entity.__init__(self) print (\u0026#34;Dog created\u0026#34;) self.legs = \u0026#39;4\u0026#39;; def whoAmI(self): print (\u0026#34;Dog go go\u0026#34;) def eat(self): print (\u0026#34;eat eat eat .....\u0026#34;) def run(self): print (\u0026#34;legs: \u0026#34; + self.legs + \u0026#34; run run run .....\u0026#34;) d = Dog() d.whoAmI() d.eat() d.run() d.weight() 5. *args and **kwargs  V√≠ d·ª• v·ªõi *args (Non Keyword Arguments):  def myCountries(*args): for item in args: print(item) myCountries(\u0026#39;US\u0026#39;, \u0026#39;Cuba\u0026#39;) # \u0026lt;------------------ b·∫°n truy·ªÅn v√†o b·∫•t c·ª© th·ª© g√¨, *args s·∫Ω convert ch√∫ng th√†nh 1 tuple, r·ªìi b·∫°n in ra tuple ƒë√≥ # Output:  # US # Cuba *args lu√¥n convert gi√° tr·ªã sang ki·ªÉu tuple ('US', 'Cuba')\nB·∫°n c≈©ng c√≥ th·ªÉ vi·∫øt th·∫ø n√†y *data. Mi·ªÖn l√† 1 d·∫•u * ·ªü tr∆∞·ªõc, m·ªçi ng∆∞·ªùi s·∫Ω hi·ªÉu ƒë√≥ l√† non-keywork argument, ki·ªÉu tuple:\ndef myCountries(*data): for item in data: print(item)  V√≠ d·ª• v·ªõi **kwargs (Keyword Arguments):  def intro(**kwargs): for key,value in kwargs.items(): print(key, \u0026#34;: \u0026#34;, value) intro(Firstname=\u0026#34;Sita\u0026#34;, Lastname=\u0026#34;Sharma\u0026#34;, Age=22) # Output:  # Firstname: Sita # Lastname: Sharma # Age: 22 **kwargs d√πng khi b·∫°n mu·ªën pass argument ki·ªÉu dictionary g·ªìm key,value v√†o function. V√¨ l√Ω do ƒë√≥ m√† h·ªç g·ªçi n√≥ l√† keywork argument\nB·∫°n c≈©ng c√≥ th·ªÉ vi·∫øt th·∫ø n√†y **data. Mi·ªÖn l√† 2 d·∫•u ** ·ªü tr∆∞·ªõc, m·ªçi ng∆∞·ªùi s·∫Ω hi·ªÉu ƒë√≥ l√† keywork argument, ki·ªÉu dictionary:\ndef intro(**data): for key,value in data.items(): print(key, \u0026#34;: \u0026#34;, value) intro(Firstname=\u0026#34;Sita\u0026#34;, Lastname=\u0026#34;Sharma\u0026#34;, Age=22) # Output:  # Firstname: Sita # Lastname: Sharma # Age: 22 6. Function name with underscore khi ƒë·ªçc code Python b·∫°n c≈©ng s·∫Ω th·∫•y ƒë∆∞·ª£c nhi·ªÅu function/variable c√≥ d·∫•u g·∫°ch d∆∞·ªõi nh∆∞ n√†y __\nclass Foo: __name = \u0026quot;Foo\u0026quot; def __getName(self): print(self.__name) def get(self): self.__getName() v·ªõi 1 d·∫•u g·∫°ch d∆∞·ªõi _, ch√∫ng ta ng·∫ßm hi·ªÉu r·∫±ng bi·∫øn/h√†m ƒë√≥ l√† protected - nghƒ©a l√† ch·ªâ class ƒë√≥ v√† c√°c class con n√™n c√≥ quy·ªÅn truy c·∫≠p\nv·ªõi 2 d·∫•u g·∫°ch d∆∞·ªõi __, ch√∫ng ta ng·∫ßm hi·ªÉu r·∫±ng bi·∫øn/h√†m ƒë√≥ l√† private - nghƒ©a l√† ch·ªâ class ƒë√≥ n√™n c√≥ quy·ªÅn truy c·∫≠p\nT·∫°i sao l·∫°i b√¥i ƒë·∫≠m ch·ªØ n√™n, ƒë√≥ l√† v√¨: ƒê√¢y l√† c√°c quy t·∫Øc ng·∫ßm. N·∫øu b·∫°n v·∫´n c·ªë s·ª≠ d·ª•ng th√¨ c≈©ng s·∫Ω ko c√≥ l·ªói n√†o t·ª´ tr√¨nh bi√™n d·ªãch Python quƒÉng ra c·∫£.\nQuy t·∫Øc n√†y gi√∫p code c·ªßa b·∫°n t∆∞·ªùng minh h∆°n cho ng∆∞·ªùi ƒë·ªçc. Nh∆∞ng ng∆∞·ªùi ko hi·ªÉu s·∫Ω ƒë·∫∑t c√¢u h·ªèi v√¨ sao ph·∫£i ƒë·∫∑t t√™n ki·ªÉu nh∆∞ v·∫≠y? (khi·∫øn h·ªç b·ªëi r·ªëi h∆°n)\nCREDIT https://martinheinz.dev/blog/79\nhttps://www.programiz.com/python-programming/class\nhttps://www.youtube.com/watch?v=6eyOQyG6FpE\u0026amp;ab_channel=PythonforEconometrics\nhttps://www.youtube.com/watch?v=328QCyHrXYA\u0026amp;ab_channel=PythonforEconometrics\nhttps://www.programiz.com/python-programming/args-and-kwargs\nhttps://viblo.asia/p/oop-voi-python-E375zQGblGW\n","href":"/bk/encrypt-python-walrus-lambda-arrow-class-args/","title":"Python: Walrus operator, lambda, arrow, class, (non)keyword arguments"},{"content":"B√†i n√†y note l·∫°i nh·ªØng th·ª© kh√° ph·ªï bi·∫øn trong Python nh∆∞ng m√¨nh √≠t d√πng/ƒë·ªÉ √Ω ƒë·∫øn n√≥.\nV·ªõi m√¨nh th√¨ c√¥ng vi·ªác DevOps c≈©ng c√≥ l√†m vi·ªác v·ªõi Python scripting nh∆∞ng ·ªü level r·∫•t basic th√¥i. Nh·ªØng th·ª© n√†y ƒë√∫ng l√† c√≥ 1 ch√∫t l·∫° l·∫´m.\n1. Walrus Operator To√°n t·ª≠ := trong python ƒë∆∞·ª£c g·ªçi l√† \u0026ldquo;Walrus Operator\u0026rdquo;, ƒë∆∞·ª£c gi·ªõi thi·ªáu t·ª´ Python3.8. Gi√∫p l√†m code ng·∫Øn g·ªçn h∆°n.\nV√≠ d·ª• 1: N·∫øu 1 function t√™n l√† func() ch·∫°y r·∫•t l√¢u, t·ªën nhi·ªÅu th·ªùi gian,\nb·∫°n ko n√™n ch·∫°y l·∫°i function ƒë√≥ nhi·ªÅu l·∫ßn 1 c√°ch ko c·∫ßn thi·∫øt,\nm√† n√™n g√°n k·∫øt qu·∫£ c·ªßa n√≥ v√†o 1 bi·∫øn, r·ªìi s·ª≠ d·ª•ng l·∫°i khi c·∫ßn thi·∫øt.\n# --------------------- # SLOWER # \u0026#34;func\u0026#34; called 3 times result = [func(x), func(x)**2, func(x)**3] # -----\u0026gt; B·∫°n ƒëang ch·∫°y l·∫°i func(x) 3 l·∫ßn trong 1 d√≤ng code # --------------------- # FASTER # Reuse result of \u0026#34;func\u0026#34; without splitting the code into multiple lines result = [y := func(x), y**2, y**3] # -----\u0026gt; B·∫°n g√°n k·∫øt qu·∫£ c·ªßa func(x) cho y, r·ªìi l·∫•y k·∫øt qu·∫£ ƒë√≥ s·ª≠ d·ª•ng l·∫°i trong y**2, y**3 V√≠ d·ª• 2: B·∫°n c·∫ßn d√πng regex ƒë·ªÉ t√¨m ki·∫øm 2 partern trong 1 list. Thay v√¨ if-else l·ªìng nhau h·∫øt partern n√†y ƒë·∫øn partern kh√°c.\nB·∫°n n√™n g√°n gi√° tr·ªã regex v√†o bi·∫øn m k·∫øt h·ª£p v·ªõi or. Tr√°nh vi·ªác t·∫°o c√°c v√≤ng loop if-else l·ªìng nhau s·∫Ω kh√≥ ƒë·ªçc.\nimport re tests = [\u0026#34;Something to match\u0026#34;, \u0026#34;Second one is present\u0026#34;] pattern1 = r\u0026#34;^.*(thing).*\u0026#34; pattern2 = r\u0026#34;^.*(present).*\u0026#34; # --------------------- # LONGER for test in tests: m = re.match(pattern1, test) if m: print(f\u0026#34;Matched the 1st pattern: {m.group(1)}\u0026#34;) else: m = re.match(pattern2, test) if m: print(f\u0026#34;Matched the 2nd pattern: {m.group(1)}\u0026#34;) # Matched the 1st pattern: thing # Matched the 2nd pattern: present # --------------------- # CLEANER for test in tests: if m := (re.match(pattern1, test) or re.match(pattern2, test)): print(f\u0026#34;Matched: \u0026#39;{m.group(1)}\u0026#39;\u0026#34;) # Matched: \u0026#39;thing\u0026#39; # Matched: \u0026#39;present\u0026#39; V√≠ d·ª• 3: B·∫°n s·ª≠ d·ª•ng bi·∫øn today ƒë·ªÉ nh·∫≠n gi√° tr·ªã c·ªßa h√†m datetime.today(), sau ƒë√≥ s·ª≠ d·ª•ng l·∫°i trong c√πng line ƒë√≥.\nM√† ko c·∫ßn ph·∫£i run h√†m datetime.today() 2 l·∫ßn.\nfrom datetime import datetime print(f\u0026#34;Today is: {(today:=datetime.today()):%Y-%m-%d}, which is {today:%A}\u0026#34;) # Today is: 2022-07-01, which is Friday 2. Lambda function ƒê√¢y l√† 1 function kh√° th√¥ng d·ª•ng khi ƒë·ªçc code Python ·ªü tr√™n m·∫°ng.\nM·ª•c ƒë√≠ch c·ªßa n√≥ l√† ƒë·ªÉ khai b√°o h√†m m√† ko c·∫ßn define b·∫±ng def. Ng∆∞·ªùi ta g·ªçi lambda function l√† h√†m ·∫©n danh (anonymous function)\nTh∆∞·ªùng ch√∫ng ta s·ª≠ d·ª•ng lambda function v·ªõi c√°c h√†m ch·ªâ c·∫ßn m·ªôt d√≤ng l·ªánh.\n# --------------------- # LONG def doubler(x): return x*2 print(doubler(4)) # Prints 8 # --------------------- # SHORT doubler = lambda x: x*2 print(doubler(5)) # Prints 10 Nhi·ªÅu tham s·ªë:\nmul = lambda x, y: x*y print(mul(5, 10)) # Prints 50 M·∫∑c d√π v·∫≠y Lambda b·ªã ƒë√°nh gi√° l√† khi·∫øn vi·ªác debug/maintenance tr·ªü n√™n kh√≥ khƒÉn h∆°n\n3. Arrow -\u0026gt; ƒê√¥i khi ƒë·ªçc code Python3 b·∫°n c√≤n th·∫•y k√Ω t·ª± m≈©i t√™n -\u0026gt; khi define function\nK√Ω hi·ªáu n√†y ch·ªâ ƒë∆°n gi·∫£n l√† gi√∫p code c·ªßa b·∫°n d·ªÖ ƒë·ªçc h∆°n m√† th√¥i, n√≥ ko ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn logic c·ªßa code c·∫£.\nKi·ªÉu nh∆∞ thay v√¨ vi·∫øt comment # h√†m n√†y t√≠nh v·∫≠n t·ªëc th√¨ b·∫°n vi·∫øt:\ndef velocity(s, t) -\u0026gt; \u0026#39;h√†m n√†y t√≠nh v·∫≠n t·ªëc\u0026#39;: return s / t ƒê∆°n gi·∫£n th·∫ø th√¥i. Khi print velocity.__annotations__ b·∫°n s·∫Ω th·∫•y:\n{'return': 'h√†m n√†y t√≠nh v·∫≠n t·ªëc'} Ng∆∞·ªùi ta c≈©ng hay vi·∫øt ki·ªÉu th·∫ø n√†y:\ndef velocity(s: \u0026#39;d√πng ƒë∆°n v·ªã kilomet\u0026#39;, t: \u0026#39;d√πng ƒë∆°n v·ªã gi·ªù\u0026#39;) -\u0026gt; \u0026#39;h√†m n√†y t√≠nh v·∫≠n t·ªëc\u0026#39;: return s / t Vi·∫øt nh∆∞ v·∫≠y l√† ƒë·ªÉ ng∆∞·ªùi ƒë·ªçc hi·ªÉu r·∫±ng h√£y input gi√° tr·ªã kilomet cho s, gi√° tr·ªã gi·ªù cho t\nKhi print velocity.__annotations__ b·∫°n s·∫Ω th·∫•y:\n{'return': 'h√†m n√†y t√≠nh v·∫≠n t·ªëc', 's': 'd√πng ƒë∆°n v·ªã kilomet', 't': 'd√πng ƒë∆°n v·ªã gi·ªù'} 4. Class in Python C√≥ th·ªÉ coi class trong Python nh∆∞ 1 blueprint c·ªßa objects. Nh∆∞ l√† prototype c·ªßa 1 chi·∫øc xe m√°y.\nTa c√≥ th·ªÉ t·∫°o ra nhi·ªÅu chi·∫øc xe kh√°c nhau t·ª´ 1 prototype, m·ªói chi·∫øc xe l√† 1 object.\nTrong 1 class th√¨ c√≥ th·ªÉ c√≥ nhi·ªÅu function, c√°c function ƒë√≥ g·ªçi l√† method c·ªßa class.\n# create a class class Room: length = 0.0 # \u0026lt;-------------------- ƒê√¢y l√† c√°c attribute c·ªßa Class breadth = 0.0 # method to calculate area def calculate_area(self): # \u0026lt;---------------- ƒê√¢y g·ªçi l√† Method print(\u0026#34;Area of Room =\u0026#34;, self.length * self.breadth) # create object of Room class study_room = Room() # assign values to all the attributes  study_room.length = 42.5 study_room.breadth = 30.8 # access method inside class study_room.calculate_area() Output:\nArea of Room = 1309.0 4.1. Constructor init __init__ method l√† 1 h√†m ƒë·∫∑c bi·ªát trong Class c·ªßa Python, n√≥ gi·ªëng v·ªõi kh√°i ni·ªám constructor trong Java hay C++.\nConstructors ƒë∆∞·ª£c d√πng ƒë·ªÉ kh·ªüi t·∫°o gi√° tr·ªã cho object.\n__init__ s·∫Ω ƒë∆∞·ª£c g·ªçi b·∫•t c·ª© khi n√†o 1 object m·ªõi ƒë∆∞·ª£c kh·ªüi t·∫°o.\nN·∫øu ko d√πng constructor b·∫°n s·∫Ω vi·∫øt code th·∫ø n√†y ƒë·ªÉ t·∫°o gi√° tr·ªã cho attribute name trong class Bike:\nclass Bike: name = \u0026#34;\u0026#34; ... # create object bike1 = Bike() N·∫øu d√πng constructor:\nclass Bike: # constructor function  def __init__(self, name = \u0026#34;\u0026#34;): self.name = name bike1 = Bike() 4.2. K·∫ø th·ª´a, ƒëa k·∫ø th·ª´a V√¨ Python c≈©ng l√† 1 ng√¥n ng·ªØ OOP n√™n n√≥ c√≥ ƒë·∫ßy ƒë·ªß c√°c thu·ªôc t√≠nh c·ªßa OOP language. V√≠ d·ª• t√≠nh k·∫ø th·ª´a (Inheritance)\nK·∫ø th·ª´a: Trong v√≠ d·ª• sau, class Dog k·∫ø th·ª´a class Animal, thay ƒë·ªïi gi√° tr·ªã c·ªßa thu·ªôc t√≠nh leg, vi·∫øt l·∫°i method eat() v√† th√™m method m·ªõi l√† run()\nclass Animal: legs = \u0026#39;0\u0026#39; def __init__(self): pass def whoAmI(self): print (\u0026#34;Animal\u0026#34;) def eat(self): print (\u0026#34;Eating\u0026#34;) class Dog(Animal): def __init__(self): Animal.__init__(self) print (\u0026#34;Dog created\u0026#34;) self.legs = \u0026#39;4\u0026#39;; def whoAmI(self): print (\u0026#34;Dog go go\u0026#34;) def eat(self): print (\u0026#34;eat eat eat .....\u0026#34;) def run(self): print (\u0026#34;legs: \u0026#34; + self.legs + \u0026#34; run run run .....\u0026#34;) d = Dog() d.whoAmI() d.eat() d.run() ƒêa k·∫ø th·ª´a: trong v√≠ d·ª• sau, class Dog k·∫ø th·ª´a t·ª´ 2 class cha Animal, Entity\nclass Animal: legs = \u0026#39;0\u0026#39; def __init__(self): pass def whoAmI(self): print (\u0026#34;Animal\u0026#34;) def eat(self): print (\u0026#34;Eating\u0026#34;) class Entity: def __init__(self): pass def weight(self): print \u0026#39;weight 88\u0026#39;; class Dog(Entity, Animal): def __init__(self): Animal.__init__(self) Entity.__init__(self) print (\u0026#34;Dog created\u0026#34;) self.legs = \u0026#39;4\u0026#39;; def whoAmI(self): print (\u0026#34;Dog go go\u0026#34;) def eat(self): print (\u0026#34;eat eat eat .....\u0026#34;) def run(self): print (\u0026#34;legs: \u0026#34; + self.legs + \u0026#34; run run run .....\u0026#34;) d = Dog() d.whoAmI() d.eat() d.run() d.weight() 5. *args and **kwargs  V√≠ d·ª• v·ªõi *args (Non Keyword Arguments):  def myCountries(*args): for item in args: print(item) myCountries(\u0026#39;US\u0026#39;, \u0026#39;Cuba\u0026#39;) # \u0026lt;------------------ b·∫°n truy·ªÅn v√†o b·∫•t c·ª© th·ª© g√¨, *args s·∫Ω convert ch√∫ng th√†nh 1 tuple, r·ªìi b·∫°n in ra tuple ƒë√≥ # Output:  # US # Cuba *args lu√¥n convert gi√° tr·ªã sang ki·ªÉu tuple ('US', 'Cuba')\nB·∫°n c≈©ng c√≥ th·ªÉ vi·∫øt th·∫ø n√†y *data. Mi·ªÖn l√† 1 d·∫•u * ·ªü tr∆∞·ªõc, m·ªçi ng∆∞·ªùi s·∫Ω hi·ªÉu ƒë√≥ l√† non-keywork argument, ki·ªÉu tuple:\ndef myCountries(*data): for item in data: print(item)  V√≠ d·ª• v·ªõi **kwargs (Keyword Arguments):  def intro(**kwargs): for key,value in kwargs.items(): print(key, \u0026#34;: \u0026#34;, value) intro(Firstname=\u0026#34;Sita\u0026#34;, Lastname=\u0026#34;Sharma\u0026#34;, Age=22) # Output:  # Firstname: Sita # Lastname: Sharma # Age: 22 **kwargs d√πng khi b·∫°n mu·ªën pass argument ki·ªÉu dictionary g·ªìm key,value v√†o function. V√¨ l√Ω do ƒë√≥ m√† h·ªç g·ªçi n√≥ l√† keywork argument\nB·∫°n c≈©ng c√≥ th·ªÉ vi·∫øt th·∫ø n√†y **data. Mi·ªÖn l√† 2 d·∫•u ** ·ªü tr∆∞·ªõc, m·ªçi ng∆∞·ªùi s·∫Ω hi·ªÉu ƒë√≥ l√† keywork argument, ki·ªÉu dictionary:\ndef intro(**data): for key,value in data.items(): print(key, \u0026#34;: \u0026#34;, value) intro(Firstname=\u0026#34;Sita\u0026#34;, Lastname=\u0026#34;Sharma\u0026#34;, Age=22) # Output:  # Firstname: Sita # Lastname: Sharma # Age: 22 6. Function name with underscore khi ƒë·ªçc code Python b·∫°n c≈©ng s·∫Ω th·∫•y ƒë∆∞·ª£c nhi·ªÅu function/variable c√≥ d·∫•u g·∫°ch d∆∞·ªõi nh∆∞ n√†y __\nclass Foo: __name = \u0026quot;Foo\u0026quot; def __getName(self): print(self.__name) def get(self): self.__getName() v·ªõi 1 d·∫•u g·∫°ch d∆∞·ªõi _, ch√∫ng ta ng·∫ßm hi·ªÉu r·∫±ng bi·∫øn/h√†m ƒë√≥ l√† protected - nghƒ©a l√† ch·ªâ class ƒë√≥ v√† c√°c class con n√™n c√≥ quy·ªÅn truy c·∫≠p\nv·ªõi 2 d·∫•u g·∫°ch d∆∞·ªõi __, ch√∫ng ta ng·∫ßm hi·ªÉu r·∫±ng bi·∫øn/h√†m ƒë√≥ l√† private - nghƒ©a l√† ch·ªâ class ƒë√≥ n√™n c√≥ quy·ªÅn truy c·∫≠p\nT·∫°i sao l·∫°i b√¥i ƒë·∫≠m ch·ªØ n√™n, ƒë√≥ l√† v√¨: ƒê√¢y l√† c√°c quy t·∫Øc ng·∫ßm. N·∫øu b·∫°n v·∫´n c·ªë s·ª≠ d·ª•ng th√¨ c≈©ng s·∫Ω ko c√≥ l·ªói n√†o t·ª´ tr√¨nh bi√™n d·ªãch Python quƒÉng ra c·∫£.\nQuy t·∫Øc n√†y gi√∫p code c·ªßa b·∫°n t∆∞·ªùng minh h∆°n cho ng∆∞·ªùi ƒë·ªçc. Nh∆∞ng ng∆∞·ªùi ko hi·ªÉu s·∫Ω ƒë·∫∑t c√¢u h·ªèi v√¨ sao ph·∫£i ƒë·∫∑t t√™n ki·ªÉu nh∆∞ v·∫≠y? (khi·∫øn h·ªç b·ªëi r·ªëi h∆°n)\nCREDIT https://martinheinz.dev/blog/79\nhttps://www.programiz.com/python-programming/class\nhttps://www.youtube.com/watch?v=6eyOQyG6FpE\u0026amp;ab_channel=PythonforEconometrics\nhttps://www.youtube.com/watch?v=328QCyHrXYA\u0026amp;ab_channel=PythonforEconometrics\nhttps://www.programiz.com/python-programming/args-and-kwargs\nhttps://viblo.asia/p/oop-voi-python-E375zQGblGW\n","href":"/posts/encrypt-python-walrus-lambda-arrow-class-args/","title":"Python: Walrus operator, lambda, arrow, class, (non)keyword arguments"},{"content":"DNS and HTTP V·ªÅ c∆° b·∫£n ƒë·ªÉ t·∫°o LetsEncrypt certificate s·∫Ω c√≥ 2 c√°ch, 1 l√† DNS 2 l√† HTTP\n  DNS: Tr√™n Server ƒë√≥ b·∫≠t terminal, run command certbot gen cert, s·∫Ω nh·∫≠n ƒë∆∞·ª£c 1 token, l·∫•y token ƒë√≥ t·∫°o 1 record TXT tr√™n DNS provider. Quay l·∫°i m√†n h√¨nh terminal, ti·∫øp t·ª•c enter ƒë·ªÉ LetsEncrypt verify r·∫±ng b·∫°n l√† ng∆∞·ªùi s·ªü h·ªØu domain.\n  HTTP: Tr√™n Server ƒë√≥ b·∫≠t terminal, run command certbot gen cert. LetsEncrypt s·∫Ω y√™u c·∫ßu b·∫°n t·∫°o 1 file tr√™n Server ƒë√≥, expose file ƒë√≥ ra port 80 qua http domain. Quay l·∫°i terminal ƒë·ªÉ LetsEncrypt c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c qua http domain, v√† verify r·∫±ng b·∫°n l√† ng∆∞·ªùi s·ªü h·ªØu server v√† domain.\n(Ch√∫ √Ω ƒë·ªÉ expose file ƒë√≥ ra port 80 qua http domain, th√¨ b·∫°n c≈©ng c·∫ßn c√≥ quy·ªÅn v√≤a DNS provider t·∫°o A reccord tr·ªè v·ªÅ public IP c·ªßa Server n·ªØa)\n  Install certbot:\nsudo apt update sudo apt install snapd sudo snap install core sudo snap refresh core sudo apt remove certbot sudo snap install --classic certbot sudo ln -s /snap/bin/certbot /usr/bin/certbot References:\nhttps://www.linode.com/docs/guides/enabling-https-using-certbot-with-nginx-on-ubuntu/\nhttps://www.nginx.com/blog/using-free-ssltls-certificates-from-lets-encrypt-with-nginx/\nGen Letsencrypt cert m√† ko c·∫ßn SSH v√†o Server? Th∆∞·ªùng theo c√°c guide tr√™n m·∫°ng l√† ph·∫£i SSH v√†o server, r·ªìi run certbot ƒë·ªÉ gen cert,\nt·∫•t nhi√™n ph·∫£i c√≥ quy·ªÅn t·∫°o record tr√™n DNS provider, config DNS record tr·ªè v√†o public IP c·ªßa Server.\nTuy nhi√™n, M√¨nh ƒëang t√¨m c√°ch ƒë·ªÉ gen cert m√† ko c·∫ßn SSH v√†o server\nThi b√†i n√†y n√≥i r·∫±ng c√≥ th·ªÉ generate cert m√† ko SSH v√†o server:\nhttps://laracasts.com/discuss/channels/general-discussion/lets-encrypt-is-it-possible-to-generate-a-certificate-without-beeing-on-the-server\nhttps://ongkhaiwei.medium.com/generate-lets-encrypt-certificate-with-dns-challenge-and-namecheap-e5999a040708\nV·ªÅ c∆° b·∫£n th√¨ m√¨nh c√≥ th·ªÉ l√†m th·∫ø, ƒë·∫ßu ti√™n l√† m·ªü terminal:\nsudo certbot certonly --manual --preferred-challenges dns -d \u0026#34;*.DOMAIN\u0026#34; Sau ƒë√≥ n√≥ s·∫Ω tr·∫£ v·ªÅ 1 token, copy token ƒë√≥, l√™n DNS provider, t·∫°o TXT record, value l√† token v·ª´a nh·∫≠n.\nQuay l·∫°i terminal, enter ƒë·ªÉ certbot th·ª±c hi·ªán verification process.\nGen Letsencrypt cert m√† ch·ªâ c√≥ Server, ko c√≥ quy·ªÅn t·∫°o record tr√™n DNS Provider? TH mu·ªën generate cert nh∆∞ng ch∆∞a c√≥ server, c≈©ng ch∆∞a c√≥ quy·ªÅn v√†o DNS ƒë·ªÉ t·∫°o records.\n C√≥ th·ªÉ b·∫°n nghƒ© ƒë·∫øn 1 c√°ch t·∫°o 1 temporary server, t·∫°o cert tr√™n Server ƒë√≥ r·ªìi Sau n√†y mang cert ƒë√≥ sang Server th·∫≠t ƒë·ªÉ install?\n Tr·∫£ l·ªùi:\nIf you generate a LetsEncrypt cert on a ‚Äútemporary server‚Äú and you are choosing HTTP method, then at first you need to expose that server to internet so Letsencrypt can request it via HTTP url. (to do that, you need to create an A record on DNS entries, pointing to temporary server‚Äôs public IP. But you don‚Äôt managed DNS) If you generate a LetsEncrypt cert on a ‚Äútemporary server‚Äú and you are choosing DNS method, then you need to create a TXT record on DNS entries with token given by LetEnscrypt. But you don‚Äôt managed DNS) In both cases, you need at least the ability to manage DNS records. ","href":"/bk/encrypt-note-generate-lets-encrypt-certificate/","title":"Generate Lets Encrypt Certificate"},{"content":"DNS and HTTP V·ªÅ c∆° b·∫£n ƒë·ªÉ t·∫°o LetsEncrypt certificate s·∫Ω c√≥ 2 c√°ch, 1 l√† DNS 2 l√† HTTP\n  DNS: Tr√™n Server ƒë√≥ b·∫≠t terminal, run command certbot gen cert, s·∫Ω nh·∫≠n ƒë∆∞·ª£c 1 token, l·∫•y token ƒë√≥ t·∫°o 1 record TXT tr√™n DNS provider. Quay l·∫°i m√†n h√¨nh terminal, ti·∫øp t·ª•c enter ƒë·ªÉ LetsEncrypt verify r·∫±ng b·∫°n l√† ng∆∞·ªùi s·ªü h·ªØu domain.\n  HTTP: Tr√™n Server ƒë√≥ b·∫≠t terminal, run command certbot gen cert. LetsEncrypt s·∫Ω y√™u c·∫ßu b·∫°n t·∫°o 1 file tr√™n Server ƒë√≥, expose file ƒë√≥ ra port 80 qua http domain. Quay l·∫°i terminal ƒë·ªÉ LetsEncrypt c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c qua http domain, v√† verify r·∫±ng b·∫°n l√† ng∆∞·ªùi s·ªü h·ªØu server v√† domain.\n(Ch√∫ √Ω ƒë·ªÉ expose file ƒë√≥ ra port 80 qua http domain, th√¨ b·∫°n c≈©ng c·∫ßn c√≥ quy·ªÅn v√≤a DNS provider t·∫°o A reccord tr·ªè v·ªÅ public IP c·ªßa Server n·ªØa)\n  Install certbot:\nsudo apt update sudo apt install snapd sudo snap install core sudo snap refresh core sudo apt remove certbot sudo snap install --classic certbot sudo ln -s /snap/bin/certbot /usr/bin/certbot References:\nhttps://www.linode.com/docs/guides/enabling-https-using-certbot-with-nginx-on-ubuntu/\nhttps://www.nginx.com/blog/using-free-ssltls-certificates-from-lets-encrypt-with-nginx/\nGen Letsencrypt cert m√† ko c·∫ßn SSH v√†o Server? Th∆∞·ªùng theo c√°c guide tr√™n m·∫°ng l√† ph·∫£i SSH v√†o server, r·ªìi run certbot ƒë·ªÉ gen cert,\nt·∫•t nhi√™n ph·∫£i c√≥ quy·ªÅn t·∫°o record tr√™n DNS provider, config DNS record tr·ªè v√†o public IP c·ªßa Server.\nTuy nhi√™n, M√¨nh ƒëang t√¨m c√°ch ƒë·ªÉ gen cert m√† ko c·∫ßn SSH v√†o server\nThi b√†i n√†y n√≥i r·∫±ng c√≥ th·ªÉ generate cert m√† ko SSH v√†o server:\nhttps://laracasts.com/discuss/channels/general-discussion/lets-encrypt-is-it-possible-to-generate-a-certificate-without-beeing-on-the-server\nhttps://ongkhaiwei.medium.com/generate-lets-encrypt-certificate-with-dns-challenge-and-namecheap-e5999a040708\nV·ªÅ c∆° b·∫£n th√¨ m√¨nh c√≥ th·ªÉ l√†m th·∫ø, ƒë·∫ßu ti√™n l√† m·ªü terminal:\nsudo certbot certonly --manual --preferred-challenges dns -d \u0026#34;*.DOMAIN\u0026#34; Sau ƒë√≥ n√≥ s·∫Ω tr·∫£ v·ªÅ 1 token, copy token ƒë√≥, l√™n DNS provider, t·∫°o TXT record, value l√† token v·ª´a nh·∫≠n.\nQuay l·∫°i terminal, enter ƒë·ªÉ certbot th·ª±c hi·ªán verification process.\nGen Letsencrypt cert m√† ch·ªâ c√≥ Server, ko c√≥ quy·ªÅn t·∫°o record tr√™n DNS Provider? TH mu·ªën generate cert nh∆∞ng ch∆∞a c√≥ server, c≈©ng ch∆∞a c√≥ quy·ªÅn v√†o DNS ƒë·ªÉ t·∫°o records.\n C√≥ th·ªÉ b·∫°n nghƒ© ƒë·∫øn 1 c√°ch t·∫°o 1 temporary server, t·∫°o cert tr√™n Server ƒë√≥ r·ªìi Sau n√†y mang cert ƒë√≥ sang Server th·∫≠t ƒë·ªÉ install?\n Tr·∫£ l·ªùi:\nIf you generate a LetsEncrypt cert on a ‚Äútemporary server‚Äú and you are choosing HTTP method, then at first you need to expose that server to internet so Letsencrypt can request it via HTTP url. (to do that, you need to create an A record on DNS entries, pointing to temporary server‚Äôs public IP. But you don‚Äôt managed DNS) If you generate a LetsEncrypt cert on a ‚Äútemporary server‚Äú and you are choosing DNS method, then you need to create a TXT record on DNS entries with token given by LetEnscrypt. But you don‚Äôt managed DNS) In both cases, you need at least the ability to manage DNS records. ","href":"/posts/encrypt-note-generate-lets-encrypt-certificate/","title":"Generate Lets Encrypt Certificate"},{"content":"","href":"/tags/letsecrypt/","title":"Letsecrypt"},{"content":"1. Story V√¨ s·ª± kh√°c bi·ªát CPU arhchitecture c√≥ th·ªÉ d·∫´n ƒë·∫øn l·ªói n·∫øu b·∫°n c√†i ƒë·∫∑t ph·∫ßn m·ªÅm ko t∆∞∆°ng th√≠ch.\nM√¨nh c√≥ c∆° h·ªôi s·ª≠ d·ª•ng c·∫£ 2 lo·∫°i amd64 v√† arm64, g·∫∑p nhi·ªÅu l·ªói khi l√†m theo c√°c tut tr√™n m·∫°ng n√™n ph·∫£i vi·∫øt b√†i n√†y.\nCheck CPU architecture c·ªßa m√°y b·∫°n ƒëang s·ª≠ d·ª•ng:\ndpkg --print-architecture k·∫øt qu·∫£ c√≥ th·ªÉ ra arm64 ho·∫∑c amd64, t√πy theo ƒë√≥ m√† ch·ªçn c√°c package ph√π h·ª£p ƒë·ªÉ install\nhttps://computingforgeeks.com/install-kvm-centos-rhel-ubuntu-debian-sles-arch/\nhttps://computingforgeeks.com/how-to-run-minikube-on-kvm/\n2. Steps to install Minikube (on arm64) B√†i n√†y h∆∞·ªõng d·∫´n Install Minikube on Ubuntu18.04 Oracle cloud VM\n2.1. Install Docker sudo apt update sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt update sudo apt upgrade -y sudo apt install docker.io -y sudo systemctl enable docker sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/v2.5.0/docker-compose-linux-aarch64\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version 2.2. Install 1 s·ªë package c·∫ßn thi·∫øt sudo apt update sudo apt install apt-transport-https sudo apt upgrade # install KVM sudo apt update sudo apt -y install qemu-kvm libvirt-dev bridge-utils libvirt-daemon-system libvirt-daemon virtinst bridge-utils libosinfo-bin libguestfs-tools virt-top sudo modprobe vhost_net sudo lsmod | grep vhost echo \u0026#34;vhost_net\u0026#34; | sudo tee -a /etc/modules 2.3. Download minikube (Ch·ªó n√†y v√¨ m√¨nh d√πng Oracle VM c√≥ ki·∫øn tr√∫c CPU l√† arm64 n√™n ph·∫£i downlink arm64, ch·ª© download c√°i amd64 s·∫Ω ko d√πng dc)\nwget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-arm64 chmod +x minikube-linux-arm64 sudo mv minikube-linux-arm64 /usr/local/bin/minikube Check version:\n$ minikube version minikube version: v1.28.0 commit: 986b1ebd987211ed16f8cc10aed7d2c42fc8392f 2.4. Install kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/arm64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version -o json 2.5. Install Docker Machine KVM drive (ƒëang b·ªã l·ªói v√¨ ko t√¨m dc b·∫£n cho arm64) curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 chmod +x docker-machine-driver-kvm2 sudo mv docker-machine-driver-kvm2 /usr/local/bin/ docker-machine-driver-kvm2 version =\u0026gt; ƒëang b·ªã l·ªói v√¨ ko t√¨m dc b·∫£n cho arm64 refer:\nhttps://github.com/kubernetes/minikube/issues/9418\nhttps://github.com/kubernetes/minikube/issues/9593 https://github.com/kubernetes/minikube/issues/9228\n2.6. Add user sudo usermod -aG libvirt $USER newgrp libvirt 2.7. Fix tr∆∞·ªõc 1 s·ªë l·ªói r·ªìi start minikube sudo apt-get install -y conntrack # Download: wget https://go.dev/dl/go1.19.4.linux-arm64.tar.gz # unzip:  tar -zxvf go1.19.4.linux-arm64.tar.gz sudo mv go /usr/local/ cd /usr/local/go/bin sudo cp * /usr/bin/ go version # -\u0026gt; th·∫•y ƒë√∫ng version download v·ªÅ l√† OK git clone https://github.com/Mirantis/cri-dockerd.git cd cri-dockerd mkdir bin go build -o bin/cri-dockerd mkdir -p /usr/local/bin install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd cp -a packaging/systemd/* /etc/systemd/system sed -i -e \u0026#39;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,\u0026#39; /etc/systemd/system/cri-docker.service systemctl daemon-reload systemctl enable cri-docker.service systemctl enable --now cri-docker.socket check version cri-dockerd:\n$ cri-dockerd --version cri-dockerd 0.2.6 (HEAD) VERSION=\u0026#34;v1.25.0\u0026#34; wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/critest-$VERSION-linux-arm64.tar.gz sudo tar zxvf critest-$VERSION-linux-arm64.tar.gz -C /usr/local/bin rm -f critest-$VERSION-linux-arm64.tar.gz Test version crictl:\n$ crictl --version crictl version v1.25.0 sudo chown $USER $HOME/.kube/config \u0026amp;\u0026amp; chmod 600 $HOME/.kube/config sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube $ minikube start --driver=docker * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the docker driver based on user configuration * Using Docker driver with root privileges * Starting control plane node minikube in cluster minikube * Pulling base image ... * Downloading Kubernetes v1.25.3 preload ... \u0026gt; preloaded-images-k8s-v18-v1...: 320.81 MiB / 320.81 MiB 100.00% 133.13 * Creating docker container (CPUs=2, Memory=5900MB) ... * Preparing Kubernetes v1.25.3 on Docker 20.10.20 ... - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image gcr.io/k8s-minikube/storage-provisioner:v5 * Enabled addons: storage-provisioner, default-storageclass * Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default 3. Troubleshooting (arm64) 3.1. L·ªói PROVIDER_KVM2_ERROR $ minikube config set vm-driver kvm2 ! These changes will take effect upon a minikube delete and then a minikube start $ minikube start * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the kvm2 driver based on user configuration X Exiting due to PROVIDER_KVM2_ERROR: /usr/bin/virsh domcapabilities --virttype kvm failed: error: failed to get emulator capabilities error: invalid argument: KVM is not supported by '/usr/bin/kvm' on this host exit status 1 * Suggestion: Follow your Linux distribution instructions for configuring KVM * Documentation: https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ -\u0026gt; Do kvm2 ko kh·∫£ d·ª•ng, n√™n ph·∫£i d√πng minikube start --driver=none, ho·∫∑c minikube start --driver=docker\nreference: https://github.com/kubernetes/minikube/issues/5667\n3.2. L·ªói GUEST_MISSING_CONNTRACK $ minikube config set vm-driver none ! These changes will take effect upon a minikube delete and then a minikube start $ minikube start * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the none driver based on user configuration X Exiting due to GUEST_MISSING_CONNTRACK: Sorry, Kubernetes 1.25.3 requires conntrack to be installed in root's path -\u0026gt; Fix b·∫±ng c√°ch:\nsudo apt-get install -y conntrack 3.3. L·ªói NOT_FOUND_CRI_DOCKERD $ minikube start --driver=none * minikube v1.28.0 on Ubuntu 18.04 (arm64) ! Both driver=none and vm-driver=none have been set. Since vm-driver is deprecated, minikube will default to driver=none. If vm-driver is set in the global config, please run \u0026quot;minikube config unset vm-driver\u0026quot; to resolve this warning. * Using the none driver based on user configuration * Starting control plane node minikube in cluster minikube * Running on localhost (CPUs=4, Memory=23997MB, Disk=148662MB) ... * Exiting due to NOT_FOUND_CRI_DOCKERD: * Suggestion: The none driver with Kubernetes v1.24+ and the docker container-runtime requires cri-dockerd. Please install cri-dockerd using these instructions: https://github.com/Mirantis/cri-dockerd#build-and-install https://github.com/kubernetes/minikube/issues/9593 https://github.com/kubernetes/minikube/issues/5667\nFix b·∫±ng c√°ch c√†i cri-dockerd, tuy nhi√™n ƒë·ªÉ c√†i ƒëc n√≥ th√¨ l·∫°i c·∫ßn c√†i golang tr∆∞·ªõc\nV√†o ƒë√¢y l·∫•y link go latest file gz: https://go.dev/dl/\nT∆∞∆°ng ·ª©ng v·ªõi arm64 linux s·∫Ω t√¨m ƒë∆∞·ª£c link: https://go.dev/dl/go1.19.4.linux-arm64.tar.gz\n# Download: wget https://go.dev/dl/go1.19.4.linux-arm64.tar.gz # unzip:  tar -zxvf go1.19.4.linux-arm64.tar.gz sudo mv go /usr/local/ cd /usr/local/go/bin sudo cp * /usr/bin/ go version # -\u0026gt; th·∫•y ƒë√∫ng version download v·ªÅ l√† OK C√†i cri-dockerd:\nhttps://github.com/Mirantis/cri-dockerd#build-and-install\ngit clone https://github.com/Mirantis/cri-dockerd.git cd cri-dockerd mkdir bin go build -o bin/cri-dockerd mkdir -p /usr/local/bin install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd cp -a packaging/systemd/* /etc/systemd/system sed -i -e \u0026#39;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,\u0026#39; /etc/systemd/system/cri-docker.service systemctl daemon-reload systemctl enable cri-docker.service systemctl enable --now cri-docker.socket check version cri-dockerd:\n$ cri-dockerd --version cri-dockerd 0.2.6 (HEAD) 3.4. L·ªói Exiting due to RUNTIME_ENABLE Exiting due to RUNTIME_ENABLE Temporary Error: sudo crictl version: exit status 1\nDo ch∆∞a c√†i crictl\nC√†i nh∆∞ sau (ch√∫ √Ω m√¨nh ƒëang d√πng arm64): https://github.com/kubernetes-sigs/cri-tools#install-crictl\nVERSION=\u0026#34;v1.25.0\u0026#34; wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/critest-$VERSION-linux-arm64.tar.gz sudo tar zxvf critest-$VERSION-linux-arm64.tar.gz -C /usr/local/bin rm -f critest-$VERSION-linux-arm64.tar.gz Test version crictl:\n$ crictl --version crictl version v1.25.0 3.5. L·ªói HOST_KUBECONFIG_PERMISSION X Exiting due to HOST_KUBECONFIG_PERMISSION: Failed kubeconfig update: Error reading file \u0026quot;/home/ubuntu/.kube/config\u0026quot;: open /home/ubuntu/.kube/config: permission denied * Suggestion: Run: 'sudo chown $USER $HOME/.kube/config \u0026amp;\u0026amp; chmod 600 $HOME/.kube/config' * Related issue: https://github.com/kubernetes/minikube/issues/5714 -\u0026gt; fix b·∫±ng c√°ch:\nsudo chown $USER $HOME/.kube/config \u0026amp;\u0026amp; chmod 600 $HOME/.kube/config 3.6. L·ªói HOST_HOME_PERMISSION X Exiting due to HOST_HOME_PERMISSION: Unable to load config: open /home/ubuntu/.minikube/profiles/minikube/config.json: permission denied * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix * Related issue: https://github.com/kubernetes/minikube/issues/9165 -\u0026gt; fix b·∫±ng c√°ch:\nsudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube 3.7. L·ªói pending c√°c pod coredns v√† storage-provisioner $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-lh9tw 0/1 Pending 0 9m41s kube-system etcd-ubuntu-oc-gp02 1/1 Running 2 (4m30s ago) 9m54s kube-system kube-apiserver-ubuntu-oc-gp02 1/1 Running 2 (4m29s ago) 9m53s kube-system kube-controller-manager-ubuntu-oc-gp02 1/1 Running 2 (4m30s ago) 9m53s kube-system kube-proxy-mjpc6 1/1 Running 2 (4m38s ago) 9m41s kube-system kube-scheduler-ubuntu-oc-gp02 1/1 Running 2 (4m37s ago) 9m53s kube-system storage-provisioner 0/1 Pending 0 4m20s =\u0026gt; Do b·ªã l·ªói node NotReady n√™n m·ªõi c√≥ 2 pod pending\n$ k get nodes NAME STATUS ROLES AGE VERSION ubuntu-oc-gp02 NotReady control-plane 21h v1.25.3 $ k describe node ubuntu-oc-gp02 container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized =\u0026gt; l·ªói ch∆∞a fix dc ho√†n to√†n, d√πng driver=docker ƒë·ªÉ workaround: minikube start --driver=docker\n-\u0026gt; https://github.com/kubernetes/minikube/issues/14924#issuecomment-1241675839\n# x√≥a minikube ƒëi:  minikube delete # run l·∫°i driver=docker minikube start --driver=docker Test get nodes status Ready nh∆∞ n√†y l√† OK:\n$ k get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane 24s v1.25.3 $ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-9pw2q 1/1 Running 0 38s kube-system etcd-minikube 1/1 Running 0 52s kube-system kube-apiserver-minikube 1/1 Running 0 51s kube-system kube-controller-manager-minikube 1/1 Running 0 51s kube-system kube-proxy-45kcb 1/1 Running 0 39s kube-system kube-scheduler-minikube 1/1 Running 0 51s kube-system storage-provisioner 1/1 Running 0 49s $ kubectl cluster-info Kubernetes control plane is running at https://10.0.0.228:8443 CoreDNS is running at https://10.0.0.228:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/ubuntu/.minikube/ca.crt extensions: - extension: last-update: Tue, 13 Dec 2022 10:06:52 UTC provider: minikube.sigs.k8s.io version: v1.28.0 name: cluster_info server: https://10.0.0.228:8443 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Tue, 13 Dec 2022 10:06:52 UTC provider: minikube.sigs.k8s.io version: v1.28.0 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: #NAME? user: client-certificate: /home/ubuntu/.minikube/profiles/minikube/client.crt client-key: /home/ubuntu/.minikube/profiles/minikube/client.key 3.8. L·ªói ko minikube ssh ƒë∆∞·ª£c Do driver m√¨nh ch·ªçn l√† none n√™n ko th·ªÉ ssh dc:\n$ minikube ssh X Exiting due to MK_USAGE: 'none' driver does not support 'minikube ssh' command 3.9. M·ªôt s·ªë command th∆∞·ªùng d√πng List c√°c addon m√† minikube ƒëang enable:\n$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | 3rd party (Ambassador) | | auto-pause | minikube | disabled | Google | | cloud-spanner | minikube | disabled | Google | | csi-hostpath-driver | minikube | disabled | Kubernetes | | dashboard | minikube | disabled | Kubernetes | | default-storageclass | minikube | enabled ‚úÖ | Kubernetes | | efk | minikube | disabled | 3rd party (Elastic) | | freshpod | minikube | disabled | Google | | gcp-auth | minikube | disabled | Google | | gvisor | minikube | disabled | Google | | headlamp | minikube | disabled | 3rd party (kinvolk.io) | | helm-tiller | minikube | disabled | 3rd party (Helm) | | inaccel | minikube | disabled | 3rd party (InAccel | | | | | [info@inaccel.com]) | | ingress | minikube | disabled | Kubernetes | | ingress-dns | minikube | disabled | Google | | istio | minikube | disabled | 3rd party (Istio) | | istio-provisioner | minikube | disabled | 3rd party (Istio) | | kong | minikube | disabled | 3rd party (Kong HQ) | | kubevirt | minikube | disabled | 3rd party (KubeVirt) | | logviewer | minikube | disabled | 3rd party (unknown) | | metallb | minikube | disabled | 3rd party (MetalLB) | | metrics-server | minikube | disabled | Kubernetes | | nvidia-driver-installer | minikube | disabled | Google | | nvidia-gpu-device-plugin | minikube | disabled | 3rd party (Nvidia) | | olm | minikube | disabled | 3rd party (Operator Framework) | | pod-security-policy | minikube | disabled | 3rd party (unknown) | | portainer | minikube | disabled | 3rd party (Portainer.io) | | registry | minikube | disabled | Google | | registry-aliases | minikube | disabled | 3rd party (unknown) | | registry-creds | minikube | disabled | 3rd party (UPMC Enterprises) | | storage-provisioner | minikube | enabled ‚úÖ | Google | | storage-provisioner-gluster | minikube | disabled | 3rd party (Gluster) | | volumesnapshots | minikube | disabled | Kubernetes | |-----------------------------|----------|--------------|--------------------------------| C√°c service c·ªßa kube-system namespace:\n$ kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 10m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 10m Open dashboard:\nminikube dashboard $ minikube dashboard --url http://192.168.39.117:30000 Xem ƒë·ªãa ch·ªâ Ip c·ªßa minikube:\nminikube ip # 192.168.49.2 Clean up:\n$ minikube delete * Deleting \u0026quot;minikube\u0026quot; in docker ... * Deleting container \u0026quot;minikube\u0026quot; ... * Removing /home/ubuntu/.minikube/machines/minikube ... * Removed all traces of the \u0026quot;minikube\u0026quot; cluster. $ minikube delete --all --purge 4. Setup vsftpd FTP server tr√™n Minikube (arm64) Y√™u c·∫ßu l√†:\n Setup vsftpd FTP server. Vi·∫øt trong 1 file yaml. Data ph·∫£i dc persistence. Access ƒëc t·ª´ FileZilla.  Tuy nhi√™n do m·∫°ng nh√† m√¨nh setup minikube th√¨ qu√° ch·∫≠m v√† y·∫øu, n√™n m√¨nh ph·∫£i run minikube tr√™n m√°y ·∫£o Cloud (oracle).\nKhi ƒë√≥, m√¨nh ko th·ªÉ d√πng FileZilla t·ª´ laptop ƒë·ªÉ access ƒë∆∞·ª£c, (ƒë√£ th·ª≠ c√†i Wireguard VPN, Tinyproxy)\nƒë√†nh test b·∫±ng command line ftp.\nB√™n ngo√†i host m√¨nh s·∫Ω t·∫°o 1 folder /opt/devops/ftp-lab/minikube-data-mnt\nƒê·ªÉ mount ƒë∆∞·ª£c folder t·ª´ b√™n ngo√†i host v√†o trong Minikube VM th√¨ c·∫ßn mount ngay t·ª´ khi start nh∆∞ sau:\nminikube start --driver=docker --mount --mount-string /opt/devops/ftp-lab/minikube-data-mnt:/mnt/data\n(/mnt/data s·∫Ω ƒë∆∞·ª£c t·∫°o ra ·ªü b√™n trong Minikube VM)\n$ minikube start --driver=docker --mount --mount-string /opt/devops/ftp-lab/minikube-data-mnt:/mnt/data * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the docker driver based on user configuration * Using Docker driver with root privileges * Starting control plane node minikube in cluster minikube * Pulling base image ... * Downloading Kubernetes v1.25.3 preload ... \u0026gt; preloaded-images-k8s-v18-v1...: 320.81 MiB / 320.81 MiB 100.00% 133.13 * Creating docker container (CPUs=2, Memory=5900MB) ... * Preparing Kubernetes v1.25.3 on Docker 20.10.20 ... - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image gcr.io/k8s-minikube/storage-provisioner:v5 * Enabled addons: storage-provisioner, default-storageclass * Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default $ minikube ip 192.168.49.2 V√¨ docker-vsftpd ko c√≥ image cho arm64 n√™n ph·∫£i t·ª± build ·ªü local,\nC√°c command sau s·∫Ω t·∫°o ra docker image b√™n trong minikube VM (not host VM nh√©)\neval $(minikube docker-env) git clone https://github.com/fauria/docker-vsftpd cd docker-vsftpd docker build -t terabithians/vsftpd:1.0.0 . Image ƒë∆∞·ª£c t·∫°o ra trong minikube VM (N·∫øu x√≥a minikube delete th√¨ image n√†y c≈©ng s·∫Ω m·∫•t theo):\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE terabithians/vsftpd 1.0.0 46f9af7d0ac2 12 seconds ago 1.09GB File ftp-k8s-all.yaml:\nƒê·ªÉ √Ω ph·∫ßn PersistentVolume: s·∫Ω th·∫•y /mnt/data ƒë∆∞·ª£c define ·ªü ƒë√¢y, ch√≠nh l√† ƒë∆∞·ªùng d·∫´n ƒë∆∞·ª£c mount t·ª´ ngo√†i v√†o trong Minikube\nkind: PersistentVolume apiVersion: v1 metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 3Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/mnt/data\u0026#34; --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 1Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: my-ftp-deployment labels: app: my-ftp spec: replicas: 1 selector: matchLabels: app: my-ftp template: metadata: labels: app: my-ftp spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: - name: my-ftp-container # image: fauria/vsftpd image: terabithians/vsftpd:1.0.0 # imagePullPolicy: Never ports: - containerPort: 21 - containerPort: 20 - containerPort: 30020 - containerPort: 30021 volumeMounts: - mountPath: \u0026#34;/home/vsftpd\u0026#34; name: task-pv-storage env: - name: FTP_USER value: \u0026#34;sftpuser\u0026#34; - name: FTP_PASS value: \u0026#34;sftp1234\u0026#34; - name: PASV_ADDRESS value: \u0026#34;192.168.49.2\u0026#34; # minikube: 192.168.49.2 - name: PASV_MIN_PORT value: \u0026#34;30020\u0026#34; - name: PASV_MAX_PORT value: \u0026#34;30021\u0026#34; --- apiVersion: v1 kind: Service metadata: name: my-ftp-service labels: app: my-ftp spec: type: NodePort # LoadBalancer ports: - port: 21 targetPort: 21 nodePort: 30025 protocol: TCP name: ftp21 - port: 20 targetPort: 20 protocol: TCP nodePort: 30026 name: ftp20 - port: 30020 targetPort: 30020 nodePort: 30020 protocol: TCP name: ftp30020 - port: 30021 targetPort: 30021 nodePort: 30021 protocol: TCP name: ftp30021 selector: app: my-ftp kubectl create ns sftp kubectl create -f ftp-k8s-all.yaml -n sftp check:\n$ k get pods,svc,pvc -n sftp NAME READY STATUS RESTARTS AGE pod/my-ftp-deployment-d458df5d8-vlwf5 1/1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-ftp-service NodePort 10.99.37.214 \u0026lt;none\u0026gt; 21:30025/TCP,20:30026/TCP,30020:30020/TCP,30021:30021/TCP 8m37s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/task-pv-claim Bound task-pv-volume 3Gi RWO manual 8m37s $ k logs my-ftp-deployment-d458df5d8-b7gd4 -n sftp ************************************************* * * * Docker image: fauria/vsftpd * * https://github.com/fauria/docker-vsftpd * * * ************************************************* SERVER SETTINGS --------------- ¬∑ FTP User: sftpuser ¬∑ FTP Password: sftp1234 ¬∑ Log file: /var/log/vsftpd/vsftpd.log ¬∑ Redirect vsftpd log to STDOUT: No. $ k describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: sftp/task-pv-claim Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 3Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data HostPathType: Events: \u0026lt;none\u0026gt; test ftp:\n$ ftp 192.168.49.2 30025 Connected to 192.168.49.2. 220 (vsFTPd 3.0.2) Name (192.168.49.2:ubuntu): sftpuser 331 Please specify the password. Password: 230 Login successful. Remote system type is UNIX. Using binary mode to transfer files. ftp\u0026gt; pass Passive mode on. ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. 226 Directory send OK. test th·ª≠ mkdir ƒë·ªÉ t·∫°o folder test-1, test-2.\nftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. 226 Directory send OK. ftp\u0026gt; mkdir test-1 257 \u0026quot;/test-1\u0026quot; created ftp\u0026gt; mkdir test-2 257 \u0026quot;/test-2\u0026quot; created ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,68). 150 Here comes the directory listing. drwx------ 2 ftp ftp 4096 Dec 17 13:30 test-1 drwx------ 2 ftp ftp 4096 Dec 17 13:31 test-2 gi·ªù n·∫øu exit ra ƒë·ªÉ xem folder /opt/devops/ftp-lab/minikube-data-mnt:\nubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ ll total 20 drwxr-xr-x 5 14 staff 4096 Dec 17 13:34 ./ drwxrwxr-x 3 14 staff 4096 Dec 17 13:30 ../ drwx------ 2 14 staff 4096 Dec 17 13:36 test-1/ drwx------ 2 14 staff 4096 Dec 17 13:31 test-2/ -\u0026gt; ch·ª©ng t·ªè data trong FTP server ƒë√£ ƒë∆∞·ª£c mount ra ngo√†i host.\nGi·ªù th·ª≠ chi·ªÅu ng∆∞·ª£c l·∫°i, t·∫°o folder ngo√†i host xem trong FTP server c√≥ hi·ªÉn th·ªã ko?.\nubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ ll total 20 drwxr-xr-x 5 14 staff 4096 Dec 17 13:34 ./ drwxrwxr-x 3 14 staff 4096 Dec 17 13:30 ../ drwx------ 2 14 staff 4096 Dec 17 13:36 test-1/ drwx------ 2 14 staff 4096 Dec 17 13:31 test-2/ ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ mkdir oh-dir ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ ll total 20 drwxr-xr-x 5 14 staff 4096 Dec 17 13:34 ./ drwxrwxr-x 3 14 staff 4096 Dec 17 13:30 ../ drwxr-xr-x 2 root root 4096 Dec 17 13:35 oh-dir/ drwx------ 2 14 staff 4096 Dec 17 13:36 test-1/ drwx------ 2 14 staff 4096 Dec 17 13:31 test-2/ V√†o FTP server xem:\n$ ftp 192.168.49.2 30025 Connected to 192.168.49.2. 220 (vsFTPd 3.0.2) Name (192.168.49.2:ubuntu): sftpuser 331 Please specify the password. Password: 230 Login successful. Remote system type is UNIX. Using binary mode to transfer files. ftp\u0026gt; pass Passive mode on. ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. drwxr-xr-x 2 ftp ftp 4096 Dec 17 13:35 oh-dir -------------\u0026gt; nghƒ©a l√† ƒë√£ OK drwx------ 2 ftp ftp 4096 Dec 17 13:36 test-1 drwx------ 2 ftp ftp 4096 Dec 17 13:31 test-2 226 Directory send OK. -\u0026gt; V·∫≠y nghƒ©a l√† data trong FTP server ƒë√£ ƒë∆∞·ª£c mount ra ngo√†i host OK.\nGi·ªù s·∫Ω test Tr∆∞·ªùng h·ª£p x√≥a minikube ƒëi, t·∫°o l·∫°i. Th√¨ data v·∫´n ph·∫£i ƒë∆∞·ª£c perssistence.\n# X√≥a minikube ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ minikube delete * Deleting \u0026quot;minikube\u0026quot; in docker ... * Deleting container \u0026quot;minikube\u0026quot; ... * Removing /home/ubuntu/.minikube/machines/minikube ... * Removed all traces of the \u0026quot;minikube\u0026quot; cluster. # T·∫°o l·∫°i minikube v·ªõi mount v√†o folder c√≥ s·∫µn data ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ minikube start --driver=docker --mount --mount-string /opt/devops/ftp-lab/minikube-data-mnt:/mnt/data * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the docker driver based on user configuration * Using Docker driver with root privileges * Starting control plane node minikube in cluster minikube * Pulling base image ... * Creating docker container (CPUs=2, Memory=5900MB) ... * Preparing Kubernetes v1.25.3 on Docker 20.10.20 ... - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image gcr.io/k8s-minikube/storage-provisioner:v5 * Enabled addons: default-storageclass, storage-provisioner * Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default # T·∫°o l·∫°i namespace, resource ftp-k8s-all.yaml ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-dxncx 1/1 Running 0 25s kube-system etcd-minikube 1/1 Running 0 38s kube-system kube-apiserver-minikube 1/1 Running 0 38s kube-system kube-controller-manager-minikube 1/1 Running 0 38s kube-system kube-proxy-r74tj 1/1 Running 0 25s kube-system kube-scheduler-minikube 1/1 Running 0 37s kube-system storage-provisioner 1/1 Running 1 (24s ago) 36s ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k create ns sftp namespace/sftp created ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k create -f ftp-k8s-all.yaml -n sftp persistentvolume/task-pv-volume created persistentvolumeclaim/task-pv-claim created deployment.apps/my-ftp-deployment created service/my-ftp-service created ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-dxncx 1/1 Running 0 99s kube-system etcd-minikube 1/1 Running 0 112s kube-system kube-apiserver-minikube 1/1 Running 0 112s kube-system kube-controller-manager-minikube 1/1 Running 0 112s kube-system kube-proxy-r74tj 1/1 Running 0 99s kube-system kube-scheduler-minikube 1/1 Running 0 111s kube-system storage-provisioner 1/1 Running 1 (98s ago) 110s sftp my-ftp-deployment-d458df5d8-clstr 1/1 Running 0 49s # access v√†o FTP server xem c√≥ c√≤n data ko? ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ ftp 192.168.49.2 30025 Connected to 192.168.49.2. 220 (vsFTPd 3.0.2) Name (192.168.49.2:ubuntu): sftpuser 331 Please specify the password. Password: 230 Login successful. Remote system type is UNIX. Using binary mode to transfer files. ftp\u0026gt; ls 500 Illegal PORT command. ftp: bind: Address already in use ftp\u0026gt; pas Passive mode on. ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. drwxr-xr-x 2 ftp ftp 4096 Dec 17 13:35 oh-dir drwx------ 2 ftp ftp 4096 Dec 17 13:36 test-1 drwx------ 2 ftp ftp 4096 Dec 17 13:31 test-2 226 Directory send OK. -\u0026gt; Th·∫•y c√≥ s·∫µn 3 folder tr∆∞·ªõc khi delete minikube, nghƒ©a l√† ƒë√£ Th√†nh C√¥ng üòÅüòÅ\n=\u0026gt; Nh∆∞ v·∫≠y l√† ƒë√£ ho√†n th√†nh ƒë∆∞·ª£c 3/4 y√™u c·∫ßu, ch·ªâ c√≤n c√°i access qua FileZilla ko test ƒë∆∞·ª£c.\nCREDIT:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\n5. M·ªôt s·ªë ch√∫ √Ω sau khi l√†m ph·∫ßn FTP Server 5.1. Kh√°c nhau gi·ªØa c√°c Access Mode c·ªßa PV C√¢u n√†y c√≥ th·ªÉ b·ªã h·ªèi khi ph·ªèng v·∫•n:\nReadWriteOnce - The volume can be mounted as read-write by a single node.\nReadOnlyMany - The volume can be mounted as read-only by many nodes.\nReadWriteMany - The volume can be mounted as read-write by many nodes.\n5.2. What if request storage c·ªßa PVC nh·ªè h∆°n PV ƒê·ªÉ √Ω ·ªü ph·∫ßn t·∫°o PV v√† PVC:\nM√¨nh ƒëang setup PV 3Gi, PVC request 1Gi. Tuy nhi√™n khi describe pv,pvc ra th√¨ t·∫•t c·∫£ ƒë·ªÅu l√† 3Gi. Ko th·∫•y 1Gi ·ªü ƒë√¢u c·∫£. Nh∆∞ v·∫≠y set request 1Gi ·ªü PVC h·∫ßu nh∆∞ v√¥ nghƒ©a. PVC s·∫Ω l·∫•y to√†n b·ªô capacity c·ªßa PV.\nrefer: https://discuss.kubernetes.io/t/when-capacity-of-pvc-is-smaller-than-capacity-of-pc/8572\n5.3. Check Usage storage c·ªßa PV, PVC H·∫ßu nh∆∞ ko c√≥ c√°ch ch√≠nh th·ª©c n√†o ƒë·ªÉ xem usage storage c·ªßa PV,PVC:\nrefer: https://stackoverflow.com/questions/59299503/is-there-an-efficient-way-to-check-the-usage-for-pv-pvc-in-kubernetes c√°i plugin ƒë∆∞·ª£c recommend l√† kubectl-df-pv th√¨ l·∫°i ch·ªâ support GKE. Kh√¥ng support EKS, AKS, minikube.\n1 c√°ch workaround l√† d√πng 1 busybox: https://stackoverflow.com/a/64627103/9922066\nng·∫Øn g·ªçn l√† trong deployment s·∫Ω c√≥ th√™m 1 container run command: du -sh /home/vsftpd li√™n t·ª•c sau 10s.\nR·ªìi khi mu·ªën bi·∫øt hi·ªán t·∫°i usage storage c·ªßa PV,PVC l√† bao nhi√™u th√¨ get log c·ªßa busybox ra:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: my-ftp-deployment labels: app: my-ftp spec: replicas: 1 selector: matchLabels: app: my-ftp template: metadata: labels: app: my-ftp spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: - name: my-ftp-container # image: fauria/vsftpd image: terabithians/vsftpd:1.0.0 # imagePullPolicy: Never ports: - containerPort: 21 - containerPort: 20 - containerPort: 30020 - containerPort: 30021 volumeMounts: - mountPath: \u0026#34;/home/vsftpd\u0026#34; name: task-pv-storage env: - name: FTP_USER value: \u0026#34;sftpuser\u0026#34; - name: FTP_PASS value: \u0026#34;sftp1234\u0026#34; - name: PASV_ADDRESS value: \u0026#34;192.168.49.2\u0026#34; # minikube: 192.168.49.2 - name: PASV_MIN_PORT value: \u0026#34;30020\u0026#34; - name: PASV_MAX_PORT value: \u0026#34;30021\u0026#34; - name: busybox image: busybox command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do du -sh /home/vsftpd; sleep 10;done\u0026#34;] volumeMounts: - mountPath: \u0026#34;/home/vsftpd\u0026#34; name: task-pv-storage k logs my-ftp-deployment-58565df89d-4b7x9 busybox -n sftp 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd -\u0026gt; m√¨nh ƒëang c√≥ 7.2M trong PV, PVC\ncheck l·∫°i ngo√†i host:\nubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt$ sudo du -sh . 7.2M . 6. Troubleshooting (amd64) M√¨nh d√πng Azure VM, OS: Ubuntu 18.04\nC√°i amd64 th√¨ c√≥ nhi·ªÅu b√†i h∆∞·ªõng d·∫´n kh√° ƒë·∫ßy ƒë·ªß v√† d·ªÖ r·ªìi, m√¨nh ko ghi l·∫°i n·ªØa\nSau ƒë√¢y l√† l·ªói m√¨nh g·∫∑p:\n$ minikube delete * Uninstalling Kubernetes v1.25.3 using kubeadm ... E1215 08:36:54.985219 7380 delete.go:516] unpause failed: kubelet start: sudo systemctl start kubelet: exit status 5 stdout: stderr: Failed to start kubelet.service: Unit kubelet.service not found. X error deleting profile \u0026quot;minikube\u0026quot;: failed to delete cluster: /bin/bash -c \u0026quot;sudo env PATH=\u0026quot;/var/lib/minikube/binaries/v1.25.3:$PATH\u0026quot; kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force\u0026quot;: exit status 127 stdout: stderr: env: ‚Äòkubeadm‚Äô: No such file or directory -\u0026gt; C·∫ßn c√†i kubeadm: https://cuongquach.com/cai-dat-kubernetes-cluster-voi-kubeadm.html\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list \u0026gt; deb https://apt.kubernetes.io/ kubernetes-xenial main \u0026gt; EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl Sau ƒë√≥ m·ªõi minikube delete ƒë∆∞·ª£c.\n7. CREDIT https://cuongquach.com/cai-dat-kubernetes-cluster-voi-kubeadm.html\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\nhttps://github.com/fauria/docker-vsftpd\nhttps://github.com/kubernetes/minikube/issues/14924#issuecomment-1241675839\nhttps://computingforgeeks.com/install-kvm-centos-rhel-ubuntu-debian-sles-arch/\nhttps://computingforgeeks.com/how-to-run-minikube-on-kvm/\nhttps://github.com/aledv/kubernetes-ftp\n","href":"/bk/encrypt-install-minikube-on-linux-ubuntu-arm64-not-amd64/","title":"Install Minikube on Linux Ubuntu arm64 (not amd64)"},{"content":"1. Story V√¨ s·ª± kh√°c bi·ªát CPU arhchitecture c√≥ th·ªÉ d·∫´n ƒë·∫øn l·ªói n·∫øu b·∫°n c√†i ƒë·∫∑t ph·∫ßn m·ªÅm ko t∆∞∆°ng th√≠ch.\nM√¨nh c√≥ c∆° h·ªôi s·ª≠ d·ª•ng c·∫£ 2 lo·∫°i amd64 v√† arm64, g·∫∑p nhi·ªÅu l·ªói khi l√†m theo c√°c tut tr√™n m·∫°ng n√™n ph·∫£i vi·∫øt b√†i n√†y.\nCheck CPU architecture c·ªßa m√°y b·∫°n ƒëang s·ª≠ d·ª•ng:\ndpkg --print-architecture k·∫øt qu·∫£ c√≥ th·ªÉ ra arm64 ho·∫∑c amd64, t√πy theo ƒë√≥ m√† ch·ªçn c√°c package ph√π h·ª£p ƒë·ªÉ install\nhttps://computingforgeeks.com/install-kvm-centos-rhel-ubuntu-debian-sles-arch/\nhttps://computingforgeeks.com/how-to-run-minikube-on-kvm/\n2. Steps to install Minikube (on arm64) B√†i n√†y h∆∞·ªõng d·∫´n Install Minikube on Ubuntu18.04 Oracle cloud VM\n2.1. Install Docker sudo apt update sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt update sudo apt upgrade -y sudo apt install docker.io -y sudo systemctl enable docker sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/v2.5.0/docker-compose-linux-aarch64\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version 2.2. Install 1 s·ªë package c·∫ßn thi·∫øt sudo apt update sudo apt install apt-transport-https sudo apt upgrade # install KVM sudo apt update sudo apt -y install qemu-kvm libvirt-dev bridge-utils libvirt-daemon-system libvirt-daemon virtinst bridge-utils libosinfo-bin libguestfs-tools virt-top sudo modprobe vhost_net sudo lsmod | grep vhost echo \u0026#34;vhost_net\u0026#34; | sudo tee -a /etc/modules 2.3. Download minikube (Ch·ªó n√†y v√¨ m√¨nh d√πng Oracle VM c√≥ ki·∫øn tr√∫c CPU l√† arm64 n√™n ph·∫£i downlink arm64, ch·ª© download c√°i amd64 s·∫Ω ko d√πng dc)\nwget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-arm64 chmod +x minikube-linux-arm64 sudo mv minikube-linux-arm64 /usr/local/bin/minikube Check version:\n$ minikube version minikube version: v1.28.0 commit: 986b1ebd987211ed16f8cc10aed7d2c42fc8392f 2.4. Install kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/arm64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/ kubectl version -o json 2.5. Install Docker Machine KVM drive (ƒëang b·ªã l·ªói v√¨ ko t√¨m dc b·∫£n cho arm64) curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 chmod +x docker-machine-driver-kvm2 sudo mv docker-machine-driver-kvm2 /usr/local/bin/ docker-machine-driver-kvm2 version =\u0026gt; ƒëang b·ªã l·ªói v√¨ ko t√¨m dc b·∫£n cho arm64 refer:\nhttps://github.com/kubernetes/minikube/issues/9418\nhttps://github.com/kubernetes/minikube/issues/9593 https://github.com/kubernetes/minikube/issues/9228\n2.6. Add user sudo usermod -aG libvirt $USER newgrp libvirt 2.7. Fix tr∆∞·ªõc 1 s·ªë l·ªói r·ªìi start minikube sudo apt-get install -y conntrack # Download: wget https://go.dev/dl/go1.19.4.linux-arm64.tar.gz # unzip:  tar -zxvf go1.19.4.linux-arm64.tar.gz sudo mv go /usr/local/ cd /usr/local/go/bin sudo cp * /usr/bin/ go version # -\u0026gt; th·∫•y ƒë√∫ng version download v·ªÅ l√† OK git clone https://github.com/Mirantis/cri-dockerd.git cd cri-dockerd mkdir bin go build -o bin/cri-dockerd mkdir -p /usr/local/bin install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd cp -a packaging/systemd/* /etc/systemd/system sed -i -e \u0026#39;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,\u0026#39; /etc/systemd/system/cri-docker.service systemctl daemon-reload systemctl enable cri-docker.service systemctl enable --now cri-docker.socket check version cri-dockerd:\n$ cri-dockerd --version cri-dockerd 0.2.6 (HEAD) VERSION=\u0026#34;v1.25.0\u0026#34; wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/critest-$VERSION-linux-arm64.tar.gz sudo tar zxvf critest-$VERSION-linux-arm64.tar.gz -C /usr/local/bin rm -f critest-$VERSION-linux-arm64.tar.gz Test version crictl:\n$ crictl --version crictl version v1.25.0 sudo chown $USER $HOME/.kube/config \u0026amp;\u0026amp; chmod 600 $HOME/.kube/config sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube $ minikube start --driver=docker * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the docker driver based on user configuration * Using Docker driver with root privileges * Starting control plane node minikube in cluster minikube * Pulling base image ... * Downloading Kubernetes v1.25.3 preload ... \u0026gt; preloaded-images-k8s-v18-v1...: 320.81 MiB / 320.81 MiB 100.00% 133.13 * Creating docker container (CPUs=2, Memory=5900MB) ... * Preparing Kubernetes v1.25.3 on Docker 20.10.20 ... - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image gcr.io/k8s-minikube/storage-provisioner:v5 * Enabled addons: storage-provisioner, default-storageclass * Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default 3. Troubleshooting (arm64) 3.1. L·ªói PROVIDER_KVM2_ERROR $ minikube config set vm-driver kvm2 ! These changes will take effect upon a minikube delete and then a minikube start $ minikube start * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the kvm2 driver based on user configuration X Exiting due to PROVIDER_KVM2_ERROR: /usr/bin/virsh domcapabilities --virttype kvm failed: error: failed to get emulator capabilities error: invalid argument: KVM is not supported by '/usr/bin/kvm' on this host exit status 1 * Suggestion: Follow your Linux distribution instructions for configuring KVM * Documentation: https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ -\u0026gt; Do kvm2 ko kh·∫£ d·ª•ng, n√™n ph·∫£i d√πng minikube start --driver=none, ho·∫∑c minikube start --driver=docker\nreference: https://github.com/kubernetes/minikube/issues/5667\n3.2. L·ªói GUEST_MISSING_CONNTRACK $ minikube config set vm-driver none ! These changes will take effect upon a minikube delete and then a minikube start $ minikube start * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the none driver based on user configuration X Exiting due to GUEST_MISSING_CONNTRACK: Sorry, Kubernetes 1.25.3 requires conntrack to be installed in root's path -\u0026gt; Fix b·∫±ng c√°ch:\nsudo apt-get install -y conntrack 3.3. L·ªói NOT_FOUND_CRI_DOCKERD $ minikube start --driver=none * minikube v1.28.0 on Ubuntu 18.04 (arm64) ! Both driver=none and vm-driver=none have been set. Since vm-driver is deprecated, minikube will default to driver=none. If vm-driver is set in the global config, please run \u0026quot;minikube config unset vm-driver\u0026quot; to resolve this warning. * Using the none driver based on user configuration * Starting control plane node minikube in cluster minikube * Running on localhost (CPUs=4, Memory=23997MB, Disk=148662MB) ... * Exiting due to NOT_FOUND_CRI_DOCKERD: * Suggestion: The none driver with Kubernetes v1.24+ and the docker container-runtime requires cri-dockerd. Please install cri-dockerd using these instructions: https://github.com/Mirantis/cri-dockerd#build-and-install https://github.com/kubernetes/minikube/issues/9593 https://github.com/kubernetes/minikube/issues/5667\nFix b·∫±ng c√°ch c√†i cri-dockerd, tuy nhi√™n ƒë·ªÉ c√†i ƒëc n√≥ th√¨ l·∫°i c·∫ßn c√†i golang tr∆∞·ªõc\nV√†o ƒë√¢y l·∫•y link go latest file gz: https://go.dev/dl/\nT∆∞∆°ng ·ª©ng v·ªõi arm64 linux s·∫Ω t√¨m ƒë∆∞·ª£c link: https://go.dev/dl/go1.19.4.linux-arm64.tar.gz\n# Download: wget https://go.dev/dl/go1.19.4.linux-arm64.tar.gz # unzip:  tar -zxvf go1.19.4.linux-arm64.tar.gz sudo mv go /usr/local/ cd /usr/local/go/bin sudo cp * /usr/bin/ go version # -\u0026gt; th·∫•y ƒë√∫ng version download v·ªÅ l√† OK C√†i cri-dockerd:\nhttps://github.com/Mirantis/cri-dockerd#build-and-install\ngit clone https://github.com/Mirantis/cri-dockerd.git cd cri-dockerd mkdir bin go build -o bin/cri-dockerd mkdir -p /usr/local/bin install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd cp -a packaging/systemd/* /etc/systemd/system sed -i -e \u0026#39;s,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,\u0026#39; /etc/systemd/system/cri-docker.service systemctl daemon-reload systemctl enable cri-docker.service systemctl enable --now cri-docker.socket check version cri-dockerd:\n$ cri-dockerd --version cri-dockerd 0.2.6 (HEAD) 3.4. L·ªói Exiting due to RUNTIME_ENABLE Exiting due to RUNTIME_ENABLE Temporary Error: sudo crictl version: exit status 1\nDo ch∆∞a c√†i crictl\nC√†i nh∆∞ sau (ch√∫ √Ω m√¨nh ƒëang d√πng arm64): https://github.com/kubernetes-sigs/cri-tools#install-crictl\nVERSION=\u0026#34;v1.25.0\u0026#34; wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/critest-$VERSION-linux-arm64.tar.gz sudo tar zxvf critest-$VERSION-linux-arm64.tar.gz -C /usr/local/bin rm -f critest-$VERSION-linux-arm64.tar.gz Test version crictl:\n$ crictl --version crictl version v1.25.0 3.5. L·ªói HOST_KUBECONFIG_PERMISSION X Exiting due to HOST_KUBECONFIG_PERMISSION: Failed kubeconfig update: Error reading file \u0026quot;/home/ubuntu/.kube/config\u0026quot;: open /home/ubuntu/.kube/config: permission denied * Suggestion: Run: 'sudo chown $USER $HOME/.kube/config \u0026amp;\u0026amp; chmod 600 $HOME/.kube/config' * Related issue: https://github.com/kubernetes/minikube/issues/5714 -\u0026gt; fix b·∫±ng c√°ch:\nsudo chown $USER $HOME/.kube/config \u0026amp;\u0026amp; chmod 600 $HOME/.kube/config 3.6. L·ªói HOST_HOME_PERMISSION X Exiting due to HOST_HOME_PERMISSION: Unable to load config: open /home/ubuntu/.minikube/profiles/minikube/config.json: permission denied * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix * Related issue: https://github.com/kubernetes/minikube/issues/9165 -\u0026gt; fix b·∫±ng c√°ch:\nsudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube 3.7. L·ªói pending c√°c pod coredns v√† storage-provisioner $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-lh9tw 0/1 Pending 0 9m41s kube-system etcd-ubuntu-oc-gp02 1/1 Running 2 (4m30s ago) 9m54s kube-system kube-apiserver-ubuntu-oc-gp02 1/1 Running 2 (4m29s ago) 9m53s kube-system kube-controller-manager-ubuntu-oc-gp02 1/1 Running 2 (4m30s ago) 9m53s kube-system kube-proxy-mjpc6 1/1 Running 2 (4m38s ago) 9m41s kube-system kube-scheduler-ubuntu-oc-gp02 1/1 Running 2 (4m37s ago) 9m53s kube-system storage-provisioner 0/1 Pending 0 4m20s =\u0026gt; Do b·ªã l·ªói node NotReady n√™n m·ªõi c√≥ 2 pod pending\n$ k get nodes NAME STATUS ROLES AGE VERSION ubuntu-oc-gp02 NotReady control-plane 21h v1.25.3 $ k describe node ubuntu-oc-gp02 container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized =\u0026gt; l·ªói ch∆∞a fix dc ho√†n to√†n, d√πng driver=docker ƒë·ªÉ workaround: minikube start --driver=docker\n-\u0026gt; https://github.com/kubernetes/minikube/issues/14924#issuecomment-1241675839\n# x√≥a minikube ƒëi:  minikube delete # run l·∫°i driver=docker minikube start --driver=docker Test get nodes status Ready nh∆∞ n√†y l√† OK:\n$ k get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane 24s v1.25.3 $ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-9pw2q 1/1 Running 0 38s kube-system etcd-minikube 1/1 Running 0 52s kube-system kube-apiserver-minikube 1/1 Running 0 51s kube-system kube-controller-manager-minikube 1/1 Running 0 51s kube-system kube-proxy-45kcb 1/1 Running 0 39s kube-system kube-scheduler-minikube 1/1 Running 0 51s kube-system storage-provisioner 1/1 Running 0 49s $ kubectl cluster-info Kubernetes control plane is running at https://10.0.0.228:8443 CoreDNS is running at https://10.0.0.228:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/ubuntu/.minikube/ca.crt extensions: - extension: last-update: Tue, 13 Dec 2022 10:06:52 UTC provider: minikube.sigs.k8s.io version: v1.28.0 name: cluster_info server: https://10.0.0.228:8443 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Tue, 13 Dec 2022 10:06:52 UTC provider: minikube.sigs.k8s.io version: v1.28.0 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: #NAME? user: client-certificate: /home/ubuntu/.minikube/profiles/minikube/client.crt client-key: /home/ubuntu/.minikube/profiles/minikube/client.key 3.8. L·ªói ko minikube ssh ƒë∆∞·ª£c Do driver m√¨nh ch·ªçn l√† none n√™n ko th·ªÉ ssh dc:\n$ minikube ssh X Exiting due to MK_USAGE: 'none' driver does not support 'minikube ssh' command 3.9. M·ªôt s·ªë command th∆∞·ªùng d√πng List c√°c addon m√† minikube ƒëang enable:\n$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | 3rd party (Ambassador) | | auto-pause | minikube | disabled | Google | | cloud-spanner | minikube | disabled | Google | | csi-hostpath-driver | minikube | disabled | Kubernetes | | dashboard | minikube | disabled | Kubernetes | | default-storageclass | minikube | enabled ‚úÖ | Kubernetes | | efk | minikube | disabled | 3rd party (Elastic) | | freshpod | minikube | disabled | Google | | gcp-auth | minikube | disabled | Google | | gvisor | minikube | disabled | Google | | headlamp | minikube | disabled | 3rd party (kinvolk.io) | | helm-tiller | minikube | disabled | 3rd party (Helm) | | inaccel | minikube | disabled | 3rd party (InAccel | | | | | [info@inaccel.com]) | | ingress | minikube | disabled | Kubernetes | | ingress-dns | minikube | disabled | Google | | istio | minikube | disabled | 3rd party (Istio) | | istio-provisioner | minikube | disabled | 3rd party (Istio) | | kong | minikube | disabled | 3rd party (Kong HQ) | | kubevirt | minikube | disabled | 3rd party (KubeVirt) | | logviewer | minikube | disabled | 3rd party (unknown) | | metallb | minikube | disabled | 3rd party (MetalLB) | | metrics-server | minikube | disabled | Kubernetes | | nvidia-driver-installer | minikube | disabled | Google | | nvidia-gpu-device-plugin | minikube | disabled | 3rd party (Nvidia) | | olm | minikube | disabled | 3rd party (Operator Framework) | | pod-security-policy | minikube | disabled | 3rd party (unknown) | | portainer | minikube | disabled | 3rd party (Portainer.io) | | registry | minikube | disabled | Google | | registry-aliases | minikube | disabled | 3rd party (unknown) | | registry-creds | minikube | disabled | 3rd party (UPMC Enterprises) | | storage-provisioner | minikube | enabled ‚úÖ | Google | | storage-provisioner-gluster | minikube | disabled | 3rd party (Gluster) | | volumesnapshots | minikube | disabled | Kubernetes | |-----------------------------|----------|--------------|--------------------------------| C√°c service c·ªßa kube-system namespace:\n$ kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 10m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 10m Open dashboard:\nminikube dashboard $ minikube dashboard --url http://192.168.39.117:30000 Xem ƒë·ªãa ch·ªâ Ip c·ªßa minikube:\nminikube ip # 192.168.49.2 Clean up:\n$ minikube delete * Deleting \u0026quot;minikube\u0026quot; in docker ... * Deleting container \u0026quot;minikube\u0026quot; ... * Removing /home/ubuntu/.minikube/machines/minikube ... * Removed all traces of the \u0026quot;minikube\u0026quot; cluster. $ minikube delete --all --purge 4. Setup vsftpd FTP server tr√™n Minikube (arm64) Y√™u c·∫ßu l√†:\n Setup vsftpd FTP server. Vi·∫øt trong 1 file yaml. Data ph·∫£i dc persistence. Access ƒëc t·ª´ FileZilla.  Tuy nhi√™n do m·∫°ng nh√† m√¨nh setup minikube th√¨ qu√° ch·∫≠m v√† y·∫øu, n√™n m√¨nh ph·∫£i run minikube tr√™n m√°y ·∫£o Cloud (oracle).\nKhi ƒë√≥, m√¨nh ko th·ªÉ d√πng FileZilla t·ª´ laptop ƒë·ªÉ access ƒë∆∞·ª£c, (ƒë√£ th·ª≠ c√†i Wireguard VPN, Tinyproxy)\nƒë√†nh test b·∫±ng command line ftp.\nB√™n ngo√†i host m√¨nh s·∫Ω t·∫°o 1 folder /opt/devops/ftp-lab/minikube-data-mnt\nƒê·ªÉ mount ƒë∆∞·ª£c folder t·ª´ b√™n ngo√†i host v√†o trong Minikube VM th√¨ c·∫ßn mount ngay t·ª´ khi start nh∆∞ sau:\nminikube start --driver=docker --mount --mount-string /opt/devops/ftp-lab/minikube-data-mnt:/mnt/data\n(/mnt/data s·∫Ω ƒë∆∞·ª£c t·∫°o ra ·ªü b√™n trong Minikube VM)\n$ minikube start --driver=docker --mount --mount-string /opt/devops/ftp-lab/minikube-data-mnt:/mnt/data * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the docker driver based on user configuration * Using Docker driver with root privileges * Starting control plane node minikube in cluster minikube * Pulling base image ... * Downloading Kubernetes v1.25.3 preload ... \u0026gt; preloaded-images-k8s-v18-v1...: 320.81 MiB / 320.81 MiB 100.00% 133.13 * Creating docker container (CPUs=2, Memory=5900MB) ... * Preparing Kubernetes v1.25.3 on Docker 20.10.20 ... - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image gcr.io/k8s-minikube/storage-provisioner:v5 * Enabled addons: storage-provisioner, default-storageclass * Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default $ minikube ip 192.168.49.2 V√¨ docker-vsftpd ko c√≥ image cho arm64 n√™n ph·∫£i t·ª± build ·ªü local,\nC√°c command sau s·∫Ω t·∫°o ra docker image b√™n trong minikube VM (not host VM nh√©)\neval $(minikube docker-env) git clone https://github.com/fauria/docker-vsftpd cd docker-vsftpd docker build -t terabithians/vsftpd:1.0.0 . Image ƒë∆∞·ª£c t·∫°o ra trong minikube VM (N·∫øu x√≥a minikube delete th√¨ image n√†y c≈©ng s·∫Ω m·∫•t theo):\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE terabithians/vsftpd 1.0.0 46f9af7d0ac2 12 seconds ago 1.09GB File ftp-k8s-all.yaml:\nƒê·ªÉ √Ω ph·∫ßn PersistentVolume: s·∫Ω th·∫•y /mnt/data ƒë∆∞·ª£c define ·ªü ƒë√¢y, ch√≠nh l√† ƒë∆∞·ªùng d·∫´n ƒë∆∞·ª£c mount t·ª´ ngo√†i v√†o trong Minikube\nkind: PersistentVolume apiVersion: v1 metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 3Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/mnt/data\u0026#34; --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 1Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: my-ftp-deployment labels: app: my-ftp spec: replicas: 1 selector: matchLabels: app: my-ftp template: metadata: labels: app: my-ftp spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: - name: my-ftp-container # image: fauria/vsftpd image: terabithians/vsftpd:1.0.0 # imagePullPolicy: Never ports: - containerPort: 21 - containerPort: 20 - containerPort: 30020 - containerPort: 30021 volumeMounts: - mountPath: \u0026#34;/home/vsftpd\u0026#34; name: task-pv-storage env: - name: FTP_USER value: \u0026#34;sftpuser\u0026#34; - name: FTP_PASS value: \u0026#34;sftp1234\u0026#34; - name: PASV_ADDRESS value: \u0026#34;192.168.49.2\u0026#34; # minikube: 192.168.49.2 - name: PASV_MIN_PORT value: \u0026#34;30020\u0026#34; - name: PASV_MAX_PORT value: \u0026#34;30021\u0026#34; --- apiVersion: v1 kind: Service metadata: name: my-ftp-service labels: app: my-ftp spec: type: NodePort # LoadBalancer ports: - port: 21 targetPort: 21 nodePort: 30025 protocol: TCP name: ftp21 - port: 20 targetPort: 20 protocol: TCP nodePort: 30026 name: ftp20 - port: 30020 targetPort: 30020 nodePort: 30020 protocol: TCP name: ftp30020 - port: 30021 targetPort: 30021 nodePort: 30021 protocol: TCP name: ftp30021 selector: app: my-ftp kubectl create ns sftp kubectl create -f ftp-k8s-all.yaml -n sftp check:\n$ k get pods,svc,pvc -n sftp NAME READY STATUS RESTARTS AGE pod/my-ftp-deployment-d458df5d8-vlwf5 1/1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-ftp-service NodePort 10.99.37.214 \u0026lt;none\u0026gt; 21:30025/TCP,20:30026/TCP,30020:30020/TCP,30021:30021/TCP 8m37s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/task-pv-claim Bound task-pv-volume 3Gi RWO manual 8m37s $ k logs my-ftp-deployment-d458df5d8-b7gd4 -n sftp ************************************************* * * * Docker image: fauria/vsftpd * * https://github.com/fauria/docker-vsftpd * * * ************************************************* SERVER SETTINGS --------------- ¬∑ FTP User: sftpuser ¬∑ FTP Password: sftp1234 ¬∑ Log file: /var/log/vsftpd/vsftpd.log ¬∑ Redirect vsftpd log to STDOUT: No. $ k describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: sftp/task-pv-claim Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 3Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data HostPathType: Events: \u0026lt;none\u0026gt; test ftp:\n$ ftp 192.168.49.2 30025 Connected to 192.168.49.2. 220 (vsFTPd 3.0.2) Name (192.168.49.2:ubuntu): sftpuser 331 Please specify the password. Password: 230 Login successful. Remote system type is UNIX. Using binary mode to transfer files. ftp\u0026gt; pass Passive mode on. ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. 226 Directory send OK. test th·ª≠ mkdir ƒë·ªÉ t·∫°o folder test-1, test-2.\nftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. 226 Directory send OK. ftp\u0026gt; mkdir test-1 257 \u0026quot;/test-1\u0026quot; created ftp\u0026gt; mkdir test-2 257 \u0026quot;/test-2\u0026quot; created ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,68). 150 Here comes the directory listing. drwx------ 2 ftp ftp 4096 Dec 17 13:30 test-1 drwx------ 2 ftp ftp 4096 Dec 17 13:31 test-2 gi·ªù n·∫øu exit ra ƒë·ªÉ xem folder /opt/devops/ftp-lab/minikube-data-mnt:\nubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ ll total 20 drwxr-xr-x 5 14 staff 4096 Dec 17 13:34 ./ drwxrwxr-x 3 14 staff 4096 Dec 17 13:30 ../ drwx------ 2 14 staff 4096 Dec 17 13:36 test-1/ drwx------ 2 14 staff 4096 Dec 17 13:31 test-2/ -\u0026gt; ch·ª©ng t·ªè data trong FTP server ƒë√£ ƒë∆∞·ª£c mount ra ngo√†i host.\nGi·ªù th·ª≠ chi·ªÅu ng∆∞·ª£c l·∫°i, t·∫°o folder ngo√†i host xem trong FTP server c√≥ hi·ªÉn th·ªã ko?.\nubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ ll total 20 drwxr-xr-x 5 14 staff 4096 Dec 17 13:34 ./ drwxrwxr-x 3 14 staff 4096 Dec 17 13:30 ../ drwx------ 2 14 staff 4096 Dec 17 13:36 test-1/ drwx------ 2 14 staff 4096 Dec 17 13:31 test-2/ ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ mkdir oh-dir ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt/sftpuser$ ll total 20 drwxr-xr-x 5 14 staff 4096 Dec 17 13:34 ./ drwxrwxr-x 3 14 staff 4096 Dec 17 13:30 ../ drwxr-xr-x 2 root root 4096 Dec 17 13:35 oh-dir/ drwx------ 2 14 staff 4096 Dec 17 13:36 test-1/ drwx------ 2 14 staff 4096 Dec 17 13:31 test-2/ V√†o FTP server xem:\n$ ftp 192.168.49.2 30025 Connected to 192.168.49.2. 220 (vsFTPd 3.0.2) Name (192.168.49.2:ubuntu): sftpuser 331 Please specify the password. Password: 230 Login successful. Remote system type is UNIX. Using binary mode to transfer files. ftp\u0026gt; pass Passive mode on. ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. drwxr-xr-x 2 ftp ftp 4096 Dec 17 13:35 oh-dir -------------\u0026gt; nghƒ©a l√† ƒë√£ OK drwx------ 2 ftp ftp 4096 Dec 17 13:36 test-1 drwx------ 2 ftp ftp 4096 Dec 17 13:31 test-2 226 Directory send OK. -\u0026gt; V·∫≠y nghƒ©a l√† data trong FTP server ƒë√£ ƒë∆∞·ª£c mount ra ngo√†i host OK.\nGi·ªù s·∫Ω test Tr∆∞·ªùng h·ª£p x√≥a minikube ƒëi, t·∫°o l·∫°i. Th√¨ data v·∫´n ph·∫£i ƒë∆∞·ª£c perssistence.\n# X√≥a minikube ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ minikube delete * Deleting \u0026quot;minikube\u0026quot; in docker ... * Deleting container \u0026quot;minikube\u0026quot; ... * Removing /home/ubuntu/.minikube/machines/minikube ... * Removed all traces of the \u0026quot;minikube\u0026quot; cluster. # T·∫°o l·∫°i minikube v·ªõi mount v√†o folder c√≥ s·∫µn data ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ minikube start --driver=docker --mount --mount-string /opt/devops/ftp-lab/minikube-data-mnt:/mnt/data * minikube v1.28.0 on Ubuntu 18.04 (arm64) * Using the docker driver based on user configuration * Using Docker driver with root privileges * Starting control plane node minikube in cluster minikube * Pulling base image ... * Creating docker container (CPUs=2, Memory=5900MB) ... * Preparing Kubernetes v1.25.3 on Docker 20.10.20 ... - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Verifying Kubernetes components... - Using image gcr.io/k8s-minikube/storage-provisioner:v5 * Enabled addons: default-storageclass, storage-provisioner * Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default # T·∫°o l·∫°i namespace, resource ftp-k8s-all.yaml ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-dxncx 1/1 Running 0 25s kube-system etcd-minikube 1/1 Running 0 38s kube-system kube-apiserver-minikube 1/1 Running 0 38s kube-system kube-controller-manager-minikube 1/1 Running 0 38s kube-system kube-proxy-r74tj 1/1 Running 0 25s kube-system kube-scheduler-minikube 1/1 Running 0 37s kube-system storage-provisioner 1/1 Running 1 (24s ago) 36s ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k create ns sftp namespace/sftp created ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k create -f ftp-k8s-all.yaml -n sftp persistentvolume/task-pv-volume created persistentvolumeclaim/task-pv-claim created deployment.apps/my-ftp-deployment created service/my-ftp-service created ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-565d847f94-dxncx 1/1 Running 0 99s kube-system etcd-minikube 1/1 Running 0 112s kube-system kube-apiserver-minikube 1/1 Running 0 112s kube-system kube-controller-manager-minikube 1/1 Running 0 112s kube-system kube-proxy-r74tj 1/1 Running 0 99s kube-system kube-scheduler-minikube 1/1 Running 0 111s kube-system storage-provisioner 1/1 Running 1 (98s ago) 110s sftp my-ftp-deployment-d458df5d8-clstr 1/1 Running 0 49s # access v√†o FTP server xem c√≥ c√≤n data ko? ubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab$ ftp 192.168.49.2 30025 Connected to 192.168.49.2. 220 (vsFTPd 3.0.2) Name (192.168.49.2:ubuntu): sftpuser 331 Please specify the password. Password: 230 Login successful. Remote system type is UNIX. Using binary mode to transfer files. ftp\u0026gt; ls 500 Illegal PORT command. ftp: bind: Address already in use ftp\u0026gt; pas Passive mode on. ftp\u0026gt; ls 227 Entering Passive Mode (192,168,49,2,117,69). 150 Here comes the directory listing. drwxr-xr-x 2 ftp ftp 4096 Dec 17 13:35 oh-dir drwx------ 2 ftp ftp 4096 Dec 17 13:36 test-1 drwx------ 2 ftp ftp 4096 Dec 17 13:31 test-2 226 Directory send OK. -\u0026gt; Th·∫•y c√≥ s·∫µn 3 folder tr∆∞·ªõc khi delete minikube, nghƒ©a l√† ƒë√£ Th√†nh C√¥ng üòÅüòÅ\n=\u0026gt; Nh∆∞ v·∫≠y l√† ƒë√£ ho√†n th√†nh ƒë∆∞·ª£c 3/4 y√™u c·∫ßu, ch·ªâ c√≤n c√°i access qua FileZilla ko test ƒë∆∞·ª£c.\nCREDIT:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\n5. M·ªôt s·ªë ch√∫ √Ω sau khi l√†m ph·∫ßn FTP Server 5.1. Kh√°c nhau gi·ªØa c√°c Access Mode c·ªßa PV C√¢u n√†y c√≥ th·ªÉ b·ªã h·ªèi khi ph·ªèng v·∫•n:\nReadWriteOnce - The volume can be mounted as read-write by a single node.\nReadOnlyMany - The volume can be mounted as read-only by many nodes.\nReadWriteMany - The volume can be mounted as read-write by many nodes.\n5.2. What if request storage c·ªßa PVC nh·ªè h∆°n PV ƒê·ªÉ √Ω ·ªü ph·∫ßn t·∫°o PV v√† PVC:\nM√¨nh ƒëang setup PV 3Gi, PVC request 1Gi. Tuy nhi√™n khi describe pv,pvc ra th√¨ t·∫•t c·∫£ ƒë·ªÅu l√† 3Gi. Ko th·∫•y 1Gi ·ªü ƒë√¢u c·∫£. Nh∆∞ v·∫≠y set request 1Gi ·ªü PVC h·∫ßu nh∆∞ v√¥ nghƒ©a. PVC s·∫Ω l·∫•y to√†n b·ªô capacity c·ªßa PV.\nrefer: https://discuss.kubernetes.io/t/when-capacity-of-pvc-is-smaller-than-capacity-of-pc/8572\n5.3. Check Usage storage c·ªßa PV, PVC H·∫ßu nh∆∞ ko c√≥ c√°ch ch√≠nh th·ª©c n√†o ƒë·ªÉ xem usage storage c·ªßa PV,PVC:\nrefer: https://stackoverflow.com/questions/59299503/is-there-an-efficient-way-to-check-the-usage-for-pv-pvc-in-kubernetes c√°i plugin ƒë∆∞·ª£c recommend l√† kubectl-df-pv th√¨ l·∫°i ch·ªâ support GKE. Kh√¥ng support EKS, AKS, minikube.\n1 c√°ch workaround l√† d√πng 1 busybox: https://stackoverflow.com/a/64627103/9922066\nng·∫Øn g·ªçn l√† trong deployment s·∫Ω c√≥ th√™m 1 container run command: du -sh /home/vsftpd li√™n t·ª•c sau 10s.\nR·ªìi khi mu·ªën bi·∫øt hi·ªán t·∫°i usage storage c·ªßa PV,PVC l√† bao nhi√™u th√¨ get log c·ªßa busybox ra:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: my-ftp-deployment labels: app: my-ftp spec: replicas: 1 selector: matchLabels: app: my-ftp template: metadata: labels: app: my-ftp spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: - name: my-ftp-container # image: fauria/vsftpd image: terabithians/vsftpd:1.0.0 # imagePullPolicy: Never ports: - containerPort: 21 - containerPort: 20 - containerPort: 30020 - containerPort: 30021 volumeMounts: - mountPath: \u0026#34;/home/vsftpd\u0026#34; name: task-pv-storage env: - name: FTP_USER value: \u0026#34;sftpuser\u0026#34; - name: FTP_PASS value: \u0026#34;sftp1234\u0026#34; - name: PASV_ADDRESS value: \u0026#34;192.168.49.2\u0026#34; # minikube: 192.168.49.2 - name: PASV_MIN_PORT value: \u0026#34;30020\u0026#34; - name: PASV_MAX_PORT value: \u0026#34;30021\u0026#34; - name: busybox image: busybox command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do du -sh /home/vsftpd; sleep 10;done\u0026#34;] volumeMounts: - mountPath: \u0026#34;/home/vsftpd\u0026#34; name: task-pv-storage k logs my-ftp-deployment-58565df89d-4b7x9 busybox -n sftp 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd 7.2M /home/vsftpd -\u0026gt; m√¨nh ƒëang c√≥ 7.2M trong PV, PVC\ncheck l·∫°i ngo√†i host:\nubuntu@ubuntu-oc-gp02:/opt/devops/ftp-lab/minikube-data-mnt$ sudo du -sh . 7.2M . 6. Troubleshooting (amd64) M√¨nh d√πng Azure VM, OS: Ubuntu 18.04\nC√°i amd64 th√¨ c√≥ nhi·ªÅu b√†i h∆∞·ªõng d·∫´n kh√° ƒë·∫ßy ƒë·ªß v√† d·ªÖ r·ªìi, m√¨nh ko ghi l·∫°i n·ªØa\nSau ƒë√¢y l√† l·ªói m√¨nh g·∫∑p:\n$ minikube delete * Uninstalling Kubernetes v1.25.3 using kubeadm ... E1215 08:36:54.985219 7380 delete.go:516] unpause failed: kubelet start: sudo systemctl start kubelet: exit status 5 stdout: stderr: Failed to start kubelet.service: Unit kubelet.service not found. X error deleting profile \u0026quot;minikube\u0026quot;: failed to delete cluster: /bin/bash -c \u0026quot;sudo env PATH=\u0026quot;/var/lib/minikube/binaries/v1.25.3:$PATH\u0026quot; kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force\u0026quot;: exit status 127 stdout: stderr: env: ‚Äòkubeadm‚Äô: No such file or directory -\u0026gt; C·∫ßn c√†i kubeadm: https://cuongquach.com/cai-dat-kubernetes-cluster-voi-kubeadm.html\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list \u0026gt; deb https://apt.kubernetes.io/ kubernetes-xenial main \u0026gt; EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl Sau ƒë√≥ m·ªõi minikube delete ƒë∆∞·ª£c.\n7. CREDIT https://cuongquach.com/cai-dat-kubernetes-cluster-voi-kubeadm.html\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\nhttps://github.com/fauria/docker-vsftpd\nhttps://github.com/kubernetes/minikube/issues/14924#issuecomment-1241675839\nhttps://computingforgeeks.com/install-kvm-centos-rhel-ubuntu-debian-sles-arch/\nhttps://computingforgeeks.com/how-to-run-minikube-on-kvm/\nhttps://github.com/aledv/kubernetes-ftp\n","href":"/posts/encrypt-install-minikube-on-linux-ubuntu-arm64-not-amd64/","title":"Install Minikube on Linux Ubuntu arm64 (not amd64)"},{"content":"","href":"/tags/kubernetes/","title":"Kubernetes"},{"content":"","href":"/tags/proxy/","title":"Proxy"},{"content":"1. Story Tr√™n th·ª±c t·∫ø c√≥ 2 lo·∫°i proxy l√† Reverse Proxy (ex: Nginx) v√† Forward proxy (ex: Tinyproxy)\nReverse Proxy ƒë∆∞·ª£c config ·ªü ph√≠a Server ƒë·ªÉ route c√°c traffic t·ª´ Client ƒë·∫øn backend ph√π h·ª£p. Client s·∫Ω ko bi·∫øt v·ªÅ s·ª± t·ªìn t·∫°i c·ªßa reverse proxy (Nginx).\nForward Proxy ƒë∆∞·ª£c config ·ªü ph√≠a Client ƒë·ªÉ control traffic ƒë·∫øn v√† ƒëi ra kh·ªèi network c·ªßa Client. Server s·∫Ω ko bi·∫øt v·ªÅ s·ª± t·ªìn t·∫°i c·ªßa forward proxy.\nUse cases c·ªßa Forward Proxy:\n  Forward proxy s·∫Ω monitor c√°c traffic incoming v√† outgoing kh·ªèi network c·ªßa Client (ho·∫°t ƒë·ªông nh∆∞ firewall), trong 1 s·ªë c√¥ng ty h·ªç s·∫Ω ch·∫∑n Internet v√† ch·ªâ cho nh√¢n vi√™n l∆∞·ªõt web khi ƒë√£ setup proxy, proxy n√†y s·∫Ω ch·∫∑n 1 s·ªë website nh∆∞ facebook.com ch·∫≥ng h·∫°n.\n  N·∫øu b·∫°n g·ª≠i request li√™n t·ª•c ƒë·∫øn 1 server, kh·∫£ nƒÉng cao IP c·ªßa b·∫°n s·∫Ω b·ªã block v√¨ ph√≠a Server ph√°t hi·ªán b·∫°n ƒëang crawl data c·ªßa h·ªç. Khi ƒë√≥ n·∫øu b·∫°n setup nhi·ªÅu proxy server ·ªü ph√≠a tr∆∞·ªõc IP c·ªßa b·∫°n, th√¨ c√°i m√† Server ph√°t hi·ªán v√† block s·∫Ω l√† c√°c IP c·ªßa Proxy server ch·ª© ko ph·∫£i IP th·∫≠t s·ª± c·ªßa b·∫°n.\n  1 s·ªë trang web s·∫Ω ch·∫∑n c√°c IP n∆∞·ªõc ngo√†i request ƒë·∫øn. V√≠ d·ª• 1 web c·ªßa VN ch·∫∑n c√°c IP t·ª´ n∆∞·ªõc ngo√†i. V·∫≠y th√¨ c√°c AWS Lambda c·ªßa b·∫°n call t·ª´ US, Singapore, \u0026hellip; s·∫Ω b·ªã ch·∫∑n h·∫øt. Khi ƒë√≥ b·∫°n c·∫ßn setup 1 proxy server c√≥ IP ·ªü VN, r·ªìi cho Lambda call qua c√°c proxy server ƒë√≥. Nh∆∞ v·∫≠y website s·∫Ω coi c√°c request t·ª´ Lambda dc g·ª≠i t·ªõi t·ª´ VN.\n  Use cases c·ªßa Reverse Proxy:\n  ƒê·ªÉ l√†m cache content qua ƒë√≥ s·∫Ω reduce load traffic on Server qua ƒë√≥ tƒÉng tr·∫£i nghi·ªám ng∆∞·ªùi d√πng v√¨ request nhanh h∆°n.\n  NgƒÉn ch·∫∑n data breach, b·∫±ng c√°ch encrypt transmitted data in network traffic, reverse proxy s·∫Ω gi√∫p web server b·∫£o v·ªá d·ªØ li·ªáu ng d√πng.\n  Load balancing, reverse proxy gi√∫p c√¢n b·∫±ng t·∫£i cho c√°c server\n   Reverse proxy is for server end and something client doesn\u0026rsquo;t really see or think about. It\u0026rsquo;s to retrieve content from the backend servers and hand to the client.\n  Forward proxy is something the client sets up in order to connect to rest of the internet. In turn, the server may potentially know nothing about your forward proxy.\n  Nginx is originally designed to be a reverse proxy, and not a forward proxy. But it can still be used as a forward one. That\u0026rsquo;s why you probably couldn\u0026rsquo;t find much configuration for it.\n  This is more a theory answer as I\u0026rsquo;ve never done this myself, but a configuration like following should work.\n server { listen 8888; location / { resolver 8.8.8.8; # may or may not be necessary. proxy_pass http://$http_host$uri$is_args$args; } }  This is just the important bits, you\u0026rsquo;ll need to configure the rest.\n  The idea is that the proxy_pass will pass to a variable host rather than a predefined one. So if you request http://example.com/foo?bar, your http header will include host of example.com. This will make your proxy_pass retrieve data from http://example.com/foo?bar.\n  The document that you linked is using it as a reverse proxy. It would be equivalent to\n proxy_pass http://localhost:80; 2. Practice with Forward Proxy ƒê·∫ßu ti√™n l√† setup 1 Forward proxy server (m√¨nh ch·ªçn tinyproxy):\nmkdir tinyproxy \u0026amp; mkdir tinyproxy/data t·∫°o file tinyproxy/Dockerfile:\n# # Dockerfile for tinyproxy # FROM alpine:3 MAINTAINER EasyPi Software Foundation RUN set -xe \\ \u0026amp;\u0026amp; apk add --no-cache tinyproxy \\ \u0026amp;\u0026amp; sed -i -e \u0026#39;/^Allow /s/^/#/\u0026#39; \\ -e \u0026#39;/^ConnectPort /s/^/#/\u0026#39; \\ -e \u0026#39;/^#DisableViaHeader /s/^#//\u0026#39; \\ /etc/tinyproxy/tinyproxy.conf VOLUME /etc/tinyproxy EXPOSE 8888 CMD [\u0026#34;tinyproxy\u0026#34;, \u0026#34;-d\u0026#34;] build image tinyproxy:\ndocker build -t tinyproxy . T·∫°o file tinyproxy/data/tinyproxy.conf:\nCopy t·ª´ ƒë√¢y: https://github.com/vimagick/dockerfiles/blob/master/tinyproxy/data/tinyproxy.conf\nUncomment d√≤ng #BasicAuth user password, s·ª≠a th√†nh user/pass m√† b·∫°n mong mu·ªën,\nch√∫ √Ω ch·ªâ d√πng c√°c k√Ω t·ª± sau cho password -a-z0-9._:\nBasicAuth youruser1 yourpassword123 T·∫°o file tinyproxy/docker-compose.yml:\nversion: \u0026#34;3.8\u0026#34; services: tinyproxy: container_name: tinyproxy image: tinyproxy ports: - \u0026#34;8888:8888\u0026#34; volumes: - ./data:/etc/tinyproxy restart: unless-stopped Run:\ncd tinyproxy/ docker-compose up -d Test:\ncurl -x youruser:yourpassword@127.0.0.1:8888 https://ifconfig.co Gi·ªù B·∫±ng 1 c√°ch n√†o ƒë√≥, h√£y open port 8888 ƒë·ªÉ c√°c request t·ª´ internet c√≥ th·ªÉ ƒë·∫øn ƒëc con server ƒëang c√†i tinyproxy c·ªßa b·∫°n.ü§£\nƒêo·∫°n code sau tr√™n AWS Lambda s·∫Ω gi√∫p b·∫°n send request th√¥ng qua Proxy tr√™n:\n(Ch√∫ √Ω Python c·ªßa m√¨nh 3.9, requests layer d√πng l·∫°i t·ª´ 3.6)\n(M√¨nh ƒë√£ set bi·∫øn m√¥i tr∆∞·ªùng proxy = http://youruser1:yourpassword123@133.215.12.13:8888)\nimport os, requests from requests.auth import HTTPProxyAuth def lambda_handler(event, context): # Get Lambda environment variables proxy_string = os.environ.get(\u0026#39;proxy\u0026#39;) s = requests.Session() url = \u0026#39;https://ifconfig.co/json\u0026#39; response = s.get(url, proxies={\u0026#34;http\u0026#34;: proxy_string, \u0026#34;https\u0026#34;: proxy_string}) ip = response.json()[\u0026#39;ip\u0026#39;] print(\u0026#39;Your IP is {}\u0026#39;.format(ip)) print(response.status_code) Log:\nSTART RequestId: d344caxxxxxxxxxxxxxxxxx814ea Version: $LATEST Your IP is 133.215.12.13 200 END RequestId: d344caxxxxxxxxxxxxxxxxx814ea REPORT RequestId: d344caxxxxxxxxxxxxxxxxx814ea\tDuration: 679.03 ms\tBilled Duration: 680 ms\tMemory Size: 128 MB\tMax Memory Used: 51 MB\tInit Duration: 412.09 ms Nh∆∞ v·∫≠y c√°c request t·ª´ Lambda ƒë·∫øn 1 endpoint n√†o ƒë√≥, s·∫Ω ƒëi qua tinyproxy c·ªßa m√¨nh. ƒêi·ªÅu n√†y s·∫Ω c√≥ √≠ch n·∫øu b·∫°n mu·ªën crawl data t·ª´ d√¢u ƒë√≥.\nTr√™n m·∫°ng c√≥ 1 s·ªë ch·ªó free proxy IP nh∆∞ng ko ch·∫Øc c√≥ n√™n d√πng ko:\nhttps://pastebin.com/raw/VJwVkqRT\nhttps://proxyscrape.com/free-proxy-list\nhttps://www.freeproxylists.net/\n3. CREDIT gi·∫£i th√≠ch forward proxy and reverse proxy:\nhttps://stackoverflow.com/a/46083709/9922066\nhttps://research.aimultiple.com/forward-vs-reverse-proxy/\nso s√°nh tinyproxy v·ªõi nginx v·ªÅ vi·ªác forward proxy:\nhttps://serverfault.com/questions/168151/multi-threaded-alternative-to-tinyproxy\nnginx proxy with Basic Auth:\nhttps://serverfault.com/a/230754/589181\nNginx ƒëc design cho reverse proxy, ko officially support forward proxy:\nhttps://superuser.com/a/604353\n1 c√°ch ƒë·ªÉ d√πng nginx ƒë·ªÉ l√†m forward proxy l√† d√πng: ngx_http_proxy_connect_module, nh∆∞ng c·∫ßn build l·∫°i, ko ch·∫Øc d√πng dc v·ªõi swag:\nhttps://superuser.com/a/1270790,\nhttps://github.com/reiz/nginx_proxy\nhttps://stackoverflow.com/questions/34025964/python-requests-api-using-proxy-for-https-request-get-407-proxy-authentication-r\nhttps://github.com/vimagick/dockerfiles/blob/master/tinyproxy\n","href":"/bk/encrypt-reverse-proxy-forward-proxy/","title":"Reverse proxy and Forward proxy"},{"content":"1. Story Tr√™n th·ª±c t·∫ø c√≥ 2 lo·∫°i proxy l√† Reverse Proxy (ex: Nginx) v√† Forward proxy (ex: Tinyproxy)\nReverse Proxy ƒë∆∞·ª£c config ·ªü ph√≠a Server ƒë·ªÉ route c√°c traffic t·ª´ Client ƒë·∫øn backend ph√π h·ª£p. Client s·∫Ω ko bi·∫øt v·ªÅ s·ª± t·ªìn t·∫°i c·ªßa reverse proxy (Nginx).\nForward Proxy ƒë∆∞·ª£c config ·ªü ph√≠a Client ƒë·ªÉ control traffic ƒë·∫øn v√† ƒëi ra kh·ªèi network c·ªßa Client. Server s·∫Ω ko bi·∫øt v·ªÅ s·ª± t·ªìn t·∫°i c·ªßa forward proxy.\nUse cases c·ªßa Forward Proxy:\n  Forward proxy s·∫Ω monitor c√°c traffic incoming v√† outgoing kh·ªèi network c·ªßa Client (ho·∫°t ƒë·ªông nh∆∞ firewall), trong 1 s·ªë c√¥ng ty h·ªç s·∫Ω ch·∫∑n Internet v√† ch·ªâ cho nh√¢n vi√™n l∆∞·ªõt web khi ƒë√£ setup proxy, proxy n√†y s·∫Ω ch·∫∑n 1 s·ªë website nh∆∞ facebook.com ch·∫≥ng h·∫°n.\n  N·∫øu b·∫°n g·ª≠i request li√™n t·ª•c ƒë·∫øn 1 server, kh·∫£ nƒÉng cao IP c·ªßa b·∫°n s·∫Ω b·ªã block v√¨ ph√≠a Server ph√°t hi·ªán b·∫°n ƒëang crawl data c·ªßa h·ªç. Khi ƒë√≥ n·∫øu b·∫°n setup nhi·ªÅu proxy server ·ªü ph√≠a tr∆∞·ªõc IP c·ªßa b·∫°n, th√¨ c√°i m√† Server ph√°t hi·ªán v√† block s·∫Ω l√† c√°c IP c·ªßa Proxy server ch·ª© ko ph·∫£i IP th·∫≠t s·ª± c·ªßa b·∫°n.\n  1 s·ªë trang web s·∫Ω ch·∫∑n c√°c IP n∆∞·ªõc ngo√†i request ƒë·∫øn. V√≠ d·ª• 1 web c·ªßa VN ch·∫∑n c√°c IP t·ª´ n∆∞·ªõc ngo√†i. V·∫≠y th√¨ c√°c AWS Lambda c·ªßa b·∫°n call t·ª´ US, Singapore, \u0026hellip; s·∫Ω b·ªã ch·∫∑n h·∫øt. Khi ƒë√≥ b·∫°n c·∫ßn setup 1 proxy server c√≥ IP ·ªü VN, r·ªìi cho Lambda call qua c√°c proxy server ƒë√≥. Nh∆∞ v·∫≠y website s·∫Ω coi c√°c request t·ª´ Lambda dc g·ª≠i t·ªõi t·ª´ VN.\n  Use cases c·ªßa Reverse Proxy:\n  ƒê·ªÉ l√†m cache content qua ƒë√≥ s·∫Ω reduce load traffic on Server qua ƒë√≥ tƒÉng tr·∫£i nghi·ªám ng∆∞·ªùi d√πng v√¨ request nhanh h∆°n.\n  NgƒÉn ch·∫∑n data breach, b·∫±ng c√°ch encrypt transmitted data in network traffic, reverse proxy s·∫Ω gi√∫p web server b·∫£o v·ªá d·ªØ li·ªáu ng d√πng.\n  Load balancing, reverse proxy gi√∫p c√¢n b·∫±ng t·∫£i cho c√°c server\n   Reverse proxy is for server end and something client doesn\u0026rsquo;t really see or think about. It\u0026rsquo;s to retrieve content from the backend servers and hand to the client.\n  Forward proxy is something the client sets up in order to connect to rest of the internet. In turn, the server may potentially know nothing about your forward proxy.\n  Nginx is originally designed to be a reverse proxy, and not a forward proxy. But it can still be used as a forward one. That\u0026rsquo;s why you probably couldn\u0026rsquo;t find much configuration for it.\n  This is more a theory answer as I\u0026rsquo;ve never done this myself, but a configuration like following should work.\n server { listen 8888; location / { resolver 8.8.8.8; # may or may not be necessary. proxy_pass http://$http_host$uri$is_args$args; } }  This is just the important bits, you\u0026rsquo;ll need to configure the rest.\n  The idea is that the proxy_pass will pass to a variable host rather than a predefined one. So if you request http://example.com/foo?bar, your http header will include host of example.com. This will make your proxy_pass retrieve data from http://example.com/foo?bar.\n  The document that you linked is using it as a reverse proxy. It would be equivalent to\n proxy_pass http://localhost:80; 2. Practice with Forward Proxy ƒê·∫ßu ti√™n l√† setup 1 Forward proxy server (m√¨nh ch·ªçn tinyproxy):\nmkdir tinyproxy \u0026amp; mkdir tinyproxy/data t·∫°o file tinyproxy/Dockerfile:\n# # Dockerfile for tinyproxy # FROM alpine:3 MAINTAINER EasyPi Software Foundation RUN set -xe \\ \u0026amp;\u0026amp; apk add --no-cache tinyproxy \\ \u0026amp;\u0026amp; sed -i -e \u0026#39;/^Allow /s/^/#/\u0026#39; \\ -e \u0026#39;/^ConnectPort /s/^/#/\u0026#39; \\ -e \u0026#39;/^#DisableViaHeader /s/^#//\u0026#39; \\ /etc/tinyproxy/tinyproxy.conf VOLUME /etc/tinyproxy EXPOSE 8888 CMD [\u0026#34;tinyproxy\u0026#34;, \u0026#34;-d\u0026#34;] build image tinyproxy:\ndocker build -t tinyproxy . T·∫°o file tinyproxy/data/tinyproxy.conf:\nCopy t·ª´ ƒë√¢y: https://github.com/vimagick/dockerfiles/blob/master/tinyproxy/data/tinyproxy.conf\nUncomment d√≤ng #BasicAuth user password, s·ª≠a th√†nh user/pass m√† b·∫°n mong mu·ªën,\nch√∫ √Ω ch·ªâ d√πng c√°c k√Ω t·ª± sau cho password -a-z0-9._:\nBasicAuth youruser1 yourpassword123 T·∫°o file tinyproxy/docker-compose.yml:\nversion: \u0026#34;3.8\u0026#34; services: tinyproxy: container_name: tinyproxy image: tinyproxy ports: - \u0026#34;8888:8888\u0026#34; volumes: - ./data:/etc/tinyproxy restart: unless-stopped Run:\ncd tinyproxy/ docker-compose up -d Test:\ncurl -x youruser:yourpassword@127.0.0.1:8888 https://ifconfig.co Gi·ªù B·∫±ng 1 c√°ch n√†o ƒë√≥, h√£y open port 8888 ƒë·ªÉ c√°c request t·ª´ internet c√≥ th·ªÉ ƒë·∫øn ƒëc con server ƒëang c√†i tinyproxy c·ªßa b·∫°n.ü§£\nƒêo·∫°n code sau tr√™n AWS Lambda s·∫Ω gi√∫p b·∫°n send request th√¥ng qua Proxy tr√™n:\n(Ch√∫ √Ω Python c·ªßa m√¨nh 3.9, requests layer d√πng l·∫°i t·ª´ 3.6)\n(M√¨nh ƒë√£ set bi·∫øn m√¥i tr∆∞·ªùng proxy = http://youruser1:yourpassword123@133.215.12.13:8888)\nimport os, requests from requests.auth import HTTPProxyAuth def lambda_handler(event, context): # Get Lambda environment variables proxy_string = os.environ.get(\u0026#39;proxy\u0026#39;) s = requests.Session() url = \u0026#39;https://ifconfig.co/json\u0026#39; response = s.get(url, proxies={\u0026#34;http\u0026#34;: proxy_string, \u0026#34;https\u0026#34;: proxy_string}) ip = response.json()[\u0026#39;ip\u0026#39;] print(\u0026#39;Your IP is {}\u0026#39;.format(ip)) print(response.status_code) Log:\nSTART RequestId: d344caxxxxxxxxxxxxxxxxx814ea Version: $LATEST Your IP is 133.215.12.13 200 END RequestId: d344caxxxxxxxxxxxxxxxxx814ea REPORT RequestId: d344caxxxxxxxxxxxxxxxxx814ea\tDuration: 679.03 ms\tBilled Duration: 680 ms\tMemory Size: 128 MB\tMax Memory Used: 51 MB\tInit Duration: 412.09 ms Nh∆∞ v·∫≠y c√°c request t·ª´ Lambda ƒë·∫øn 1 endpoint n√†o ƒë√≥, s·∫Ω ƒëi qua tinyproxy c·ªßa m√¨nh. ƒêi·ªÅu n√†y s·∫Ω c√≥ √≠ch n·∫øu b·∫°n mu·ªën crawl data t·ª´ d√¢u ƒë√≥.\nTr√™n m·∫°ng c√≥ 1 s·ªë ch·ªó free proxy IP nh∆∞ng ko ch·∫Øc c√≥ n√™n d√πng ko:\nhttps://pastebin.com/raw/VJwVkqRT\nhttps://proxyscrape.com/free-proxy-list\nhttps://www.freeproxylists.net/\n3. CREDIT gi·∫£i th√≠ch forward proxy and reverse proxy:\nhttps://stackoverflow.com/a/46083709/9922066\nhttps://research.aimultiple.com/forward-vs-reverse-proxy/\nso s√°nh tinyproxy v·ªõi nginx v·ªÅ vi·ªác forward proxy:\nhttps://serverfault.com/questions/168151/multi-threaded-alternative-to-tinyproxy\nnginx proxy with Basic Auth:\nhttps://serverfault.com/a/230754/589181\nNginx ƒëc design cho reverse proxy, ko officially support forward proxy:\nhttps://superuser.com/a/604353\n1 c√°ch ƒë·ªÉ d√πng nginx ƒë·ªÉ l√†m forward proxy l√† d√πng: ngx_http_proxy_connect_module, nh∆∞ng c·∫ßn build l·∫°i, ko ch·∫Øc d√πng dc v·ªõi swag:\nhttps://superuser.com/a/1270790,\nhttps://github.com/reiz/nginx_proxy\nhttps://stackoverflow.com/questions/34025964/python-requests-api-using-proxy-for-https-request-get-407-proxy-authentication-r\nhttps://github.com/vimagick/dockerfiles/blob/master/tinyproxy\n","href":"/posts/encrypt-reverse-proxy-forward-proxy/","title":"Reverse proxy and Forward proxy"},{"content":"1. Setup Alexa connect to Home Assistant L√†m theo h∆∞·ªõng d·∫´n ·ªü ƒë√¢y:\nhttps://www.home-assistant.io/integrations/alexa.smart_home/\n1.1. Setup Tr√™n AWS Console create Role for Lambda:\nTr√™n Alexa developer page, create skill:\nTr√™n AWS Console Lambda, t·∫°o function, n√™n ch·ªçn us-east-1 v√¨ c√°c region kh√°c d·ªÖ b·ªã l·ªói l·∫Øm:\nSet trigger to Alexa:\nUpload code from Github:\nSetup Environment variables:\nUpdate home assistant file configuration.yml:\nCreate long live token ƒë·ªÉ test t√Ω n·ªØa:\nTest Lambda:\nCopy Lambda ARN:\nTr√™n Alexa Developer Page, paste function ARN:\nTr√™n Alexa Developer Page, setup Link account:\nCh·ªó Your Client ID, n√™n ch·ªçn https://pintagui.amazon.com/: Gi·ªù login v√†o app Alexa tr√™n ƒëi·ªán tho·∫°i (N·∫øu b·∫°n dung Iphone, c√≥ th·ªÉ b·∫°n s·∫Ω ph·∫£i chuy·ªÉn v√πng US ƒë·ªÉ t·∫£i app, d√πng Android s·∫Ω c·∫ßn down file APK):\nV√†o tab Skill \u0026amp; Game, Your Skill s·∫Ω th·∫•y Skill c·ªßa b·∫°n v·ª´a t·∫°o:\nLogin ƒë·ªÉ link account th√†nh c√¥ng nh√©, n·∫øu d√πng wifi ko login dc th√¨ h√£y d√πng 4G\nSau khi login th√¨ App s·∫Ω t·ª± ƒë·ªông discovery devices\n1.2. Troubeshooting B·∫°n c√≥ th·ªÉ d√πng web sau ƒë·ªÉ thay cho app tr√™n ƒëi·ªán tho·∫°i:\nhttps://alexa.amazon.com\n  L·ªói v√¨ ch·ªçn Singapore, 1 region kh√¥ng ƒë∆∞·ª£c ch·∫•p nh·∫≠n:\n  L·ªói v√¨ ch·ªçn Trigger nh·∫ßm skill set m√† ko ch·ªçn Smart Home:\n  L·ªói v√¨ m√¨nh ch·ªçn Your Client ID l√† c√°i link ...jp/ ph·∫£i d√πng https://pintagui.amazon.com/:\n  N·∫øu b·ªã l·ªói ko discovery ƒë∆∞·ª£c device sau khi linked account, solution: ƒê·ªïi h·∫øt stack sang English (US), lambda sang N.Verginia region: https://community.home-assistant.io/t/alexa-integration-does-not-find-devices/193732/5\n  2. Setup Youtube Skill for Alexa L√†m theo h∆∞·ªõng d·∫´n ·ªü ƒë√¢y:\nhttps://github.com/hoangmnsd/YouTubeForAlexa\n2.1. Setup tr√™n GCP v√†o ƒë√¢y t·∫°o project:\nhttps://console.developers.google.com/project\nv√†o ƒë√¢y:\nhttps://console.developers.google.com/apis/library?project=tester-api-key\nsearch youtube data API v3. Enable it:\nClick on \u0026ldquo;Create Credentials\u0026rdquo;\nSet like this:\n ‚ÄúWhich API are you using?‚Äù: YouTube Data API v3 ‚ÄúFrom where you call the API?‚Äù: Server web (ex. node.js, Tomcat) ‚ÄúWhich data you use?‚Äù: Public Data Click on ‚ÄúWhich credentials i need?‚Äù  After some seconds you will see under ‚ÄúGet your credentials‚Äù the key that wee need.\nCOPY and SAVE the key in the notepad.\n2.2. Setup tr√™n AWS Create lambda function on us-east-1:\nAdd trigger for Lambda, note that Skill ID verification:\nUpload zip file from https://github.com/wes1993/YouTubeForAlexa/blob/master/YouTubeForAlexaLambda.zip\nSet Environment variable for Lambda, m√¨nh ch·ªçn pytube ƒë·ªÉ test th√¥i, ng∆∞·ªùi ta khuy√™n d√πng youtube_dl v√¨ n√≥ ·ªïn ƒë·ªãnh h∆°n tuy h∆°i ch·∫≠m:\nSetup timeout to 30s, Memory to 512 Mb:\n2.3. Setup tr√™n Alexa Developer Page Go to the Alexa Console (https://developer.amazon.com/alexa/console/ask):\nInvocation name ch·ªçn my youtube, ko n√™n ch·ªçn you tube v√¨ d·ªÖ ph√°t √¢m sai:\nUpdate JSON Editor:\nUpdate Interface:\nUpdate Endpoints:\nUpdate permission tab:\nclick Build Model and wait for success\nEnable Test development:\n2.4. Test Youtube skill Alexa launch youtube: sau command n√†y s·∫Ω th·∫•y trong list c·ªßa Alexa app c√≥ list Youtube Favorite, Youtube:\nhttps://alexa.amazon.com/\nUpdate Youtube Alexa list, M√¨nh s·ª≠a l·∫°i c√°c playlist public Youtube: Gi·∫£i th√≠ch:\nMy News playlist: tin m·ªõi c·ªßa VTC NOW\nMy Vietnam TV playlist: chuy·ªÉn ƒë·ªông 24h VTV\nMy Black playlist: ƒêen v√¢u official\nThat song I like: Show c·ªßa ƒêen\nMy Dragon playlist: Imagine Dragon\nMy Taylor playlist: Taylor Swift\nMy Super awsome playlist: Phan M·∫°nh Qu·ª≥nh\nMy Social playlist: Ki·∫øn th·ª©c th√∫ v·ªã playlist\nMy Billionaire playlist: t·ªï bu√¥n 247 tu·∫•n ti·ªÅn t·ªâ\nCommand to use:\nAlexa launch youtube: command n√†y s·∫Ω kh·ªüi t·∫°o b·∫°n ƒë·∫ßu c√°c Favorite Youtube list cho b·∫°n\nAlexa stop / next / previous / shuffle: ph·∫£i stop tr∆∞·ªõc.\nAlexa ask my youtube to play [My Dragon playlist]: ch√∫ √Ω ph√°t √¢m my youtube\n2.5. Optional steps C√≥ 3 l·ªói m√† m√¨nh g·∫∑p ph·∫£i trong qu√° tr√¨nh d√πng skill n√†y:\n   L·ªói Playback failed, \u0026quot;Device playback error\u0026quot;, MEDIA ERROR\nC√≥ nh·ªØng playlist l·ªói ngay c·∫£ khi d√πng ho·∫∑c ko d√πng proxy nh∆∞ 2 playlist n√†y:\nTaylor playlist | https://www.youtube.com/playlist?list=PLMEZyDHJojxNYSVgRCPt589DI5H7WT1ZK\nRed playlist | https://www.youtube.com/playlist?list=PLvaeEf26a-mB6RsQ1UjLfAhGypELXulZE\nSolution: ch∆∞a t√¨m ra..    L·ªói N·∫øu trong playlist c√≥ c√°c video vi ph·∫°m ch√≠nh s√°ch Youtube b·ªã ·∫©n ƒëi th√¨ Alexa s·∫Ω stop lu√¥n playlist:\nm√¨nh ƒë√£ log issue: https://github.com/wes1993/YouTubeForAlexa/issues/23\nSolution: ch∆∞a t√¨m ra..    L·ªói V√¨ Lambda call ƒë·∫øn Youtube t·ª´ IP c·ªßa Lambda region (US) n√™n s·∫Ω c√≥ 1 s·ªë video ch·ªâ kh·∫£ d·ª•ng ·ªü Vietnam s·∫Ω ko th·ªÉ play ƒë∆∞·ª£c.\nSolution: D√πng forward proxy tiny proxy (https://github.com/hoangmnsd/YouTubeForAlexa#extra-step-optional)    D∆∞·ªõi ƒë√¢y m√¨nh vi·∫øt v·ªÅ vi·ªác m√¨nh l√†m ƒë·ªÉ ch·∫°y tinyproxy:\n C√†i tinyproxy ƒë·ªÉ l√†m forward proxy. (ƒê·ªÉ hi·ªÉu forward proxy v√† reserve proxy kh√°c nh∆∞ n√†o th√¨ xem b√†i n√†y nh√©: Reverse proxy and Forward proxy)\ntheo h∆∞·ªõng d·∫´n n√†y: https://github.com/hoangmnsd/YouTubeForAlexa#extra-step-optional\nN√≥i n√¥m na th√¨ ta s·∫Ω setup ƒë·ªÉ Lambda ko call tr·ª±c ti·∫øp ƒë·∫øn Youtube m√† ƒëi qua tinyproxy r·ªìi m·ªõi ƒë·∫øn Youtube, nh∆∞ v·∫≠y Youtube s·∫Ω hi·ªÉu c√°c request l√† ƒë·∫øn t·ª´ 1 IP c·ªë ƒë·ªãnh c·ªßa nh√† m√¨nh, ch·ª© ko ph·∫£i ƒë·∫øn t·ª´ Lambda\nM√¨nh ƒë√£ th·ª≠ run docker tr√™n RPi nh∆∞ng b·ªã l·ªói (c√≥ l·∫Ω do Rpi l√† arm64 n√™n ko d√πng dc image c·ªßa vimagick tr√™n Docker Hub). N√™n ph·∫£i build l·∫°i image tr√™n RPi ƒë·ªÉ ch·∫°y theo Dockerfile ·ªü ƒë√¢y: https://github.com/vimagick/dockerfiles/tree/master/tinyproxy\nTest trong local th√¨ ok nh∆∞ng khi test Lambda ko connect dc ƒë·∫øn tinyproxy trong m·∫°ng nh√† m√¨nh. C√≥ th·ªÉ v√¨ m√¨nh ko th·ªÉ expose port 8888 ra ƒë∆∞·ª£c, m√† m√¨nh c≈©ng ko mu·ªën expose nhi·ªÅu port ra.\n-\u0026gt; M√¨nh chuy·ªÉn qua c√†i tinyproxy tr√™n VM Oracle Cloud c·ªßa m√¨nh. S·∫Ω c·∫ßn m·ªü all port 8888.\nDo VM Oracle c·ªßa m√¨nh l√† ip UK n√™n s·∫Ω c√≥ 1 s·ªë playlist ko th·ªÉ play ƒë∆∞·ª£c, v√≠ d·ª• nh∆∞ c√°c playlist c·ªßa VTC, VTV24 (Ch·∫Øc VTV24 ch·ªâ publish video cho khu v·ª±c Vietnam xem)\nM√¨nh ƒëang t√≠nh thu√™ VPS c·ªßa Vietnam nh∆∞ hostingviet.vn 130k/th√°ng, ƒë·ªÉ c√†i tinyproxy\nnh∆∞ng k·ªÉ c·∫£ c√†i proxy th√¨ ch·ªâ c√≥ th·ªÉ fix ƒë∆∞·ª£c l·ªói s·ªë 3 th√¥i, ko fix ƒë∆∞·ª£c l·ªói 1 \u0026amp; 2 üò™üò™\nUpdate 2023/Jan/07:\nSau khi m√¨nh thu√™ 1 VPS c·ªßa hostingviet.vn v·ªõi gi√° 135k/th√°ng th√¨ v·∫´n ko play ƒë∆∞·ª£c video latest trong 1 channel\nN·∫øu d√πng proxy c·ªßa Hostingviet, m√¨nh set 1 HA script TTS cho Alexa r·∫±ng \u0026ldquo;ask My Youtube to play channel VTV1 VTV Go\u0026rdquo; th√¨:\n l·ªói 1 l√† n√≥ ko play clip latest c·ªßa Channel ƒë√≥, l·ªói 2 l√† ƒë∆∞·ª£c 1 l√∫c th√¨ th∆∞·ªùng xuy√™n tr·∫£ v·ªÅ l·ªói \u0026quot;The channel XXX hasn't worked, shall I try the next one?\u0026quot;  ƒê√£ th·ª≠ chuy·ªÉn bi·∫øn m√¥i tr∆∞·ªùng c·ªßa AWS Lambda qua l·∫°i pytube, youtube-dl nh∆∞ng v·∫´n ko fix ƒë∆∞·ª£c l·ªói 1, l·ªói 2.\nNh∆∞ng n·∫øu chuy·ªÉn proxy sang d√πng VM c·ªßa Oracle th√¨ ko b·ªã l·ªói s·ªë 2, tuy nhi√™n v·∫´n ko fix ƒë∆∞·ª£c l·ªói 1 (m√† play 1 clip t·ª´ 14/06/2022.)\nC√≤n n·∫øu set c·ª©ng Youtube playlist URL tr√™n Alexa app th√¨ g·∫∑p ph·∫£i l·ªói: Do Admin k√™nh VTV1 VTV Go ƒëang s·∫Øp x·∫øp th·ª© t·ª± t·ª´ oldest ƒë·∫øn latest, n√™n khi play th√¨ lu√¥n b·∫Øt ƒë·∫ßu t·ª´ clip c≈© nh·∫•t, ph·∫£i \u0026ldquo;next\u0026rdquo; m√£i m·ªõi ƒë·∫øn clip latest. Ho·∫∑c l√† c·ª© ra l·ªánh \u0026ldquo;shuffle\u0026rdquo; r·ªìi next m√£i ƒë·ªÉ may m·∫Øn ƒë·∫øn clip latest.üòÉü§£\n2.6. Update 2023.02 using Youtube Stream Repeater Th√°ng 2.2023, wes1993 ƒë√£ release 1 b·∫£n update m·ªõi v·ªõi c·∫≠p nh·∫≠t l√† s·ª≠ d·ª•ng YouTube-Stream-Repeater ƒë·ªÉ stream Youtube video/audio, c√≥ th·ªÉ coi l√† ·ªïn ƒë·ªãnh h∆°n pytube v√† youtube-dl. Th·ª±c s·ª± l√† version c≈© nhi·ªÅu l·ªói qu√°, c·ª© 1 th√°ng d√πng kho·∫£ng 10 15 ng√†y l√† l·∫°i l·ªói. M√† l·ªói r·∫•t kh√≥ debug lu√¥n, ch·∫£ hi·ªÉu nguy√™n nh√¢n g√¨ m√† t·ª± nhi√™n b·ªã l·ªói, th∆∞·ªùng Alexa s·∫Ω ko ph√°t Youtube video v√† n√≥i \u0026ldquo;Video hasn\u0026rsquo;t work\u0026rdquo;. Hy v·ªçng l√† l·∫ßn update n√†y s·∫Ω kh√° h∆°n.\nTuy nhi√™n th√¨ Stephano (aka wes1993) ƒë√£ t·∫°o ra 1 Home Assistant Addon ƒë·ªÉ integrate.\nV·ªõi m√¨nh c√°ch n√†y ko d√πng ƒë∆∞·ª£c, v√¨ m√¨nh ƒëang s·ª≠ d·ª•ng Home Assistant Container, ko c√≥ ch·ª©c nƒÉng Addon.\nV√†, m√¨nh c≈©ng ko th√≠ch d√πng Addon v√¨ n√≥ ƒë·∫∑c th√π cho HomeAssistant qu√°.\nL·∫ßn m√≤ khi ƒë·ªçc comment n√†y th√¨ m√¨nh th·∫•y c√°ch c·ªßa DavidBerdik ph√π h·ª£p v·ªõi m√¨nh h∆°n.\nM√¨nh mu·ªën l√†m th·ª≠ ki·ªÉu n√†y: (diagram)\nV√¨ m√¨nh c√≥ 1 Oracle VM (4CPU/24GB RAM) ƒëang ch∆∞a t·∫≠n d·ª•ng h·∫øt, s·∫Ω run YouTube-Stream-Repeater container, expose ra HTTPS url.\nAWS Lambda s·∫Ω request ƒë·∫øn YouTube-Stream-Repeater, v√† Alexa s·∫Ω serve audio t·ª´ ƒë√≥.\nM√¨nh ko run YouTube-Stream-Repeater tr√™n RPi ·ªü trong m·∫°ng local c√πng v·ªõi HA v√¨:\n Con RPi ƒëang d√πng Wifi 2.4Ghz, k·∫øt n·ªëi internet c·ªßa n√≥ kh√° ch·∫≠m, RPi c≈©ng g·∫ßn full b·ªô nh·ªõ r·ªìi (80%). Vi·ªác expose m·∫°ng gia ƒë√¨nh th√™m 1 app tr√™n port 4000 n·ªØa l√†m m√¨nh th·∫•y lo l·∫Øng. M√¨nh mu·ªën expose c√†ng √≠t c√†ng t·ªët (Hi·ªán t·∫°i ƒëang expose HomeAssistant r·ªìi)  D∆∞·ªõi ƒë√¢y l√† c√°c step khi m√¨nh ƒë√£ l√†m:\n2.6.1. Expose VM Oracle to HTTPs with swag T·∫°o 1 subdomain \u0026lt;REDACTED\u0026gt;.duckdns.org tr√™n https://duckdns.org (c√≥ th·ªÉ ph·∫£i d√πng VPN ƒë·ªÉ v√†o ƒë∆∞·ª£c)\nTr√™n vm Oracle cloud, s·ª≠a Security Group: expose port 80,443\nC√†i swag theo b√†i n√†y:\nFile /opt/devops/docker-compose.yml:\nversion: '3.0' services: swag: image: lscr.io/linuxserver/swag:1.30.0 container_name: swag cap_add: - NET_ADMIN environment: - PUID=1000 - PGID=1000 - TZ=Asia/Ho_Chi_Minh - URL=\u0026lt;REDACTED\u0026gt;.duckdns.org - VALIDATION=duckdns - SUBDOMAINS=wildcard, #optional - CERTPROVIDER= #optional - DNSPLUGIN=cloudflare #optional - PROPAGATION= #optional - DUCKDNSTOKEN= #optional - EMAIL= #optional - ONLY_SUBDOMAINS=false #optional - EXTRA_DOMAINS= #optional - STAGING=true #optional volumes: - /opt/devops/swag/config:/config ports: - 443:443 - 80:80 #optional restart: unless-stopped logging: driver: \u0026quot;json-file\u0026quot; options: max-size: \u0026quot;10m\u0026quot; max-file: \u0026quot;10\u0026quot; Ch√∫ √Ω m√¨nh ƒëang ch·ªâ ƒë·ªãnh STAGING=true ƒë·ªÉ test vi·ªác request staging tr∆∞·ªõc\nT·∫°o s·∫µn folder /opt/devops/swag/config\nRun:\ncd /opt/devops/ docker-compose up -d Check log nh∆∞ n√†y l√† ok:\n$ docker logs swag ... Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/\u0026lt;REDACTED\u0026gt;.duckdns.org/fullchain.pem Key is saved at: /etc/letsencrypt/live/\u0026lt;REDACTED\u0026gt;.duckdns.org/privkey.pem This certificate expires on 2023-06-18. These files will be updated when the certificate renews. NEXT STEPS: - The certificate will need to be renewed before it expires. Certbot can automatically renew the certificate in the background, but you may need to take steps to enable that functionality. See https://certbot.org/renewal-setup for instructions. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - If you like Certbot, please consider supporting our work by: * Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate * Donating to EFF: https://eff.org/donate-le - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - New certificate generated; starting nginx cont-init: info: /etc/cont-init.d/50-certbot exited 0 cont-init: info: running /etc/cont-init.d/55-permissions cont-init: info: /etc/cont-init.d/55-permissions exited 0 cont-init: info: running /etc/cont-init.d/60-renew The cert does not expire within the next day. Letting the cron script handle the renewal attempts overnight (2:08am). cont-init: info: /etc/cont-init.d/60-renew exited 0 cont-init: info: running /etc/cont-init.d/70-outdated cont-init: info: /etc/cont-init.d/70-outdated exited 0 cont-init: info: running /etc/cont-init.d/85-version-checks cont-init: info: /etc/cont-init.d/85-version-checks exited 0 cont-init: info: running /etc/cont-init.d/99-custom-files [custom-init] No custom files found, skipping... cont-init: info: /etc/cont-init.d/99-custom-files exited 0 s6-rc: info: service legacy-cont-init successfully started s6-rc: info: service init-mods: starting s6-rc: info: service init-mods successfully started s6-rc: info: service init-mods-package-install: starting s6-rc: info: service init-mods-package-install successfully started s6-rc: info: service init-mods-end: starting s6-rc: info: service init-mods-end successfully started s6-rc: info: service init-services: starting s6-rc: info: service init-services successfully started s6-rc: info: service legacy-services: starting services-up: info: copying legacy longrun cron (no readiness notification) services-up: info: copying legacy longrun fail2ban (no readiness notification) services-up: info: copying legacy longrun nginx (no readiness notification) services-up: info: copying legacy longrun php-fpm (no readiness notification) s6-rc: info: service legacy-services successfully started s6-rc: info: service 99-ci-service-check: starting [ls.io-init] done. s6-rc: info: service 99-ci-service-check successfully started Server ready Tr√™n browser access v√†o \u0026lt;REDACTED\u0026gt;.duckdns.org, N·∫øu browser b√°o unsecure, ph·∫£i ·∫•n v√†o \u0026ldquo;wish to continue\u0026rdquo; th√¨ l√† b√¨nh th∆∞·ªùng v√¨ m√¨nh ƒëang d√πng STAGING=true m√†, th·∫•y giao di·ªán \u0026ldquo;Welcome to your SWAG instance\u0026rdquo; l√† OK.\nS·ª≠a l·∫°i file: docker-compose.yml:\nSTAGING=false Xong ch·∫°y l·∫°i:\ncd /opt/devops/ docker-compose up -d Check docker logs swag ko c√≥ l·ªói g√¨ v√† N·∫øu hi·ªán Server ready l√† OK.\nV√†o 1 Browser ·∫©n danh kh√°c check: \u0026lt;REDACTED\u0026gt;.duckdns.org, ko th·∫•y tr√¨nh duy·ªát b√°o unsecure, ko c·∫ßn ph·∫£i ·∫•n v√†o \u0026ldquo;wish to continue\u0026rdquo;, th·∫•y giao di·ªán \u0026ldquo;Welcome to your SWAG instance\u0026rdquo; l√† OK\n2.6.2. Setup backend l√† YouTube-Stream-Repeater container Gi·ªù run YouTube-Stream-Repeater:\ncd /opt/devops/ git clone https://github.com/DavidBerdik/YouTube-Stream-Repeater cd YouTube-Stream-Repeater docker-compose up -d S·∫Ω th·∫•y 1 container run tr√™n port 4000\nTest b·∫±ng c√°ch: curl http://localhost:4000/meta/FBjVss96C0E\nN·∫øu tr·∫£ v·ªÅ 1 chu·ªói json l√† OK\nGi·ªù stop n√≥ ƒëi:\ncd YouTube-Stream-Repeater docker-compose stop Check docker images s·∫Ω th·∫•y ƒë√£ c√≥ images:\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE youtube-stream-repeater_server latest f18dd1edca15 8 hours ago 575MB S·ª≠a /opt/devops/docker-compose.yml ban ƒë·∫ßu, th√™m youtubestreamrepeater v√†o:\nversion: '3.0' services: ... youtubestreamrepeater: image: youtube-stream-repeater_server:latest container_name: youtubestreamrepeater restart: unless-stopped ports: - \u0026quot;4000:4000\u0026quot; logging: driver: \u0026quot;json-file\u0026quot; options: max-size: \u0026quot;10m\u0026quot; max-file: \u0026quot;10\u0026quot; 2.6.3. Setup Nginx ƒë·ªÉ point v√†o backend YouTube-Stream-Repeater container V√†o folder /opt/devops/swag/config/nginx/proxy-confs ƒë√£ ƒë∆∞·ª£c mount:\nS·ª≠a file youtube-dl-server.subdomain.conf.sample, rename th√†nh youtube-dl-server.subdomain.conf:\n# ch·ªó 1. server_name \u0026lt;REDACTED\u0026gt;.duckdns.org; # ch·ªó 2. set $upstream_app youtubestreamrepeater; # \u0026lt;======== container name set $upstream_port 4000; Run 1 th·ªÉ:\ncd /opt/devops/ docker-compose up -d V√†o Browser check l·∫°i \u0026lt;REDACTED\u0026gt;.duckdns.org n·∫øu tr·∫£ v·ªÅ {\u0026quot;detail\u0026quot;:\u0026quot;Not Found\u0026quot;} l√† OK.\nN·∫øu tr·∫£ v·ªÅ l·ªói 502 Bad gateway l√† l·ªói nha, c·∫ßn l√†m ƒë√∫ng step, trong file /opt/devops/docker-compose.yml c·∫ßn c√≥ c·∫£ swag v√† youtubestreamrepeater.\nTh·ª≠ link n√†y: https://\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E, N·∫øu tr·∫£ v·ªÅ 1 chu·ªói JSON nghƒ©a l√† OK\n2.6.4. Setup Nginx Basic Authentication Hi·ªán t·∫°i th√¨ ƒëang expose \u0026lt;REDACTED\u0026gt;.duckdns.org ra public, ai c≈©ng d√πng ƒë∆∞·ª£c.\nGi·ªù mu·ªën setup Authentication cho n√≥ ƒë·ªÉ h·∫°n ch·∫ø ng∆∞·ªùi l·∫° v√†o d√πng ch√πa üòÅ\nTa t·∫°o password cho user name ytalexa:\nsudo apt install apache2-utils cd /opt/devops/swag/config/nginx htpasswd -c /opt/devops/swag/config/nginx/.htpasswd ytalexa # Nh·∫≠p password v√†o, ko n√™n c√≥ m·∫•y k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ n√™n d√πng ch·ªØ/s·ªë/in hoa Command tr√™n l∆∞u user name v√† password v√†o 1 file /opt/devops/swag/config/nginx/.htpasswd\nS·ª≠a file: /opt/devops/swag/config/nginx/proxy-confs/youtube-dl-server.subdomain.conf, uncomment m·∫•y d√≤ng n√†y:\n... location / { # enable the next two lines for http auth auth_basic \u0026quot;Restricted\u0026quot;; auth_basic_user_file /config/nginx/.htpasswd; ... restart swag container\nTest l·∫°i t·ª´ ngo√†i v√†o n·∫øu ko c√≥ user/password th√¨ s·∫Ω nh∆∞ n√†y:\n$ curl https://\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;401 Authorization Required\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;401 Authorization Required\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; N·∫øu c√≥ user/password ƒë√∫ng format n√†y s·∫Ω tr·∫£ v·ªÅ chu·ªói JSON k·∫øt qu·∫£:\ncurl -u \u0026lt;YOUR USER NAME\u0026gt;:\u0026lt;YOUR PASSWORD\u0026gt; https://\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E ho·∫∑c: curl https://\u0026lt;YOUR USER NAME\u0026gt;:\u0026lt;YOUR PASSWORD\u0026gt;@\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E 2.6.5. Update AWS Lambda function Update Lambda function b·∫±ng file zip ·ªü ƒë√¢y: https://github.com/wes1993/YouTubeForAlexa/releases/download/09.02.2023/YouTubeForAlexaLambda.zip\nV√†o Lambda -\u0026gt; Configuration -\u0026gt; Environment variables, s·ª≠a:\nKey: get_url_service Value: youtubestream Key: ytstreamurl Value: \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;REDACTED\u0026gt;.duckdns.org Do ch√∫ng ta ƒëang s·ª≠ d·ª•ng YouTube-Stream-Repeater c·ªßa DavidBerdik ch·ª© ko ph·∫£i Home Assistant Addon c·ªßa Stephano (wes1993) n√™n C√≥ 1 ch√∫t c·∫ßn s·ª≠a n·ªØa ƒë·ªÉ m·ªçi th·ª© ch·∫°y ƒë∆∞·ª£c, l√† lambda_function.py:\nLine 1074: url = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/api/meta/\u0026quot; + id s·ª≠a th√†nh: url = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/meta/\u0026quot; + id Line 1083: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/api/dl/\u0026quot; + id + \u0026quot;?f=bestvideo\u0026quot; s·ª≠a th√†nh: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/dl/\u0026quot; + id + \u0026quot;?f=bestvideo\u0026quot; Line 1085: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/api/dl/\u0026quot; + id + \u0026quot;?f=bestaudio\u0026quot; s·ª≠a th√†nh: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/dl/\u0026quot; + id + \u0026quot;?f=bestaudio\u0026quot; Save function Lambda l·∫°i.\nGi·ªù test m·ªçi th·ª© s·∫Ω OK, th·ª≠ Alexa, ask My Youtube to play a video by Taylor Swift\nLog Cloudwatch:\nINIT_START Runtime Version: python:3.7.v23 Runtime Version ARN: arn:aws:lambda:us-east-1::runtime:4xxxxxxxxxxxxxxxxxc START RequestId: 2bxxxxxxxxxxxxxxxbd Version: $LATEST [INFO] 2023-03-21T07:22:39.754Z 2b5xxxabbd 400 [INFO] 2023-03-21T07:22:39.754Z 2b5xxxabbd {'message': 'List name already exists', 'type': 'NameConflict'} [INFO] 2023-03-21T07:22:39.972Z 2b5xxxabbd 400 [INFO] 2023-03-21T07:22:39.972Z 2b5xxxabbd {'message': 'List name already exists', 'type': 'NameConflict'} END RequestId: 2b5xxxabbd REPORT RequestId: 2b5xxxabbd Duration: 326.93 ms Billed Duration: 327 ms Memory Size: 512 MB Max Memory Used: 51 MB Init Duration: 550.63 ms START RequestId: 0yyyyyyyyyyyyye Version: $LATEST [INFO] 2023-03-21T07:23:06.701Z 0yyyyyyyyyyyyye {'version': '1.0', 'session': {'new': False, 'sessionId': 'amzn1.echo-api.session.b5xxxxxxxxxxxxxxbc4a8', ' [INFO] 2023-03-21T07:23:06.701Z 0yyyyyyyyyyyyye Looking for: video by Taylor swift [INFO] 2023-03-21T07:23:06.701Z 0yyyyyyyyyyyyye checking for faves [INFO] 2023-03-21T07:23:06.867Z 0yyyyyyyyyyyyye checking Add shortcuts to your favorite videos or playlists like this: [INFO] 2023-03-21T07:23:07.370Z 0yyyyyyyyyyyyye Getting YouTubeStream url for https://www.youtube.com/watch?v=h8DLofLM7No [INFO] 2023-03-21T07:23:07.370Z 0yyyyyyyyyyyyye Appending ?f=bestaudio [INFO] 2023-03-21T07:23:08.866Z 0yyyyyyyyyyyyye Sending song: Taylor Swift - Lavender Haze (Official Music Video) to Alexa END RequestId: 0yyyyyyyyyyyyye REPORT RequestId: 0yyyyyyyyyyyyye Duration: 2167.59 ms Billed Duration: 2168 ms Memory Size: 512 MB Max Memory Used: 51 MB Check log c·ªßa YouTube-Stream-Repeater nh∆∞ n√†y l√† ok, ko c√≥ l·ªói g√¨:\n$ docker logs youtubestreamrepeater INFO: Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit) INFO: Started parent process [1] INFO: Started server process [9] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [11] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [14] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [7] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [13] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [8] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [12] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [10] INFO: Waiting for application startup. INFO: Application startup complete. [QV3jQQ9_TUw]: requested with format bestaudio and subs None, configuring... [QV3jQQ9_TUw]: sending stream [QV3jQQ9_TUw]: stream will be sent as is (bestaudio) [QV3jQQ9_TUw]: download type: video/webm (.webm) INFO: 172.18.0.3:39584 - \u0026quot;GET /dl/QV3jQQ9_TUw?f=bestaudio HTTP/1.1\u0026quot; 200 OK INFO: 172.18.0.3:39156 - \u0026quot;GET /meta/XcQWfs90lFU HTTP/1.1\u0026quot; 200 OK [XcQWfs90lFU]: requested with format bestaudio and subs None, configuring... [XcQWfs90lFU]: sending stream [XcQWfs90lFU]: stream will be sent as is (bestaudio) [XcQWfs90lFU]: download type: video/webm (.webm) INFO: 172.18.0.3:55760 - \u0026quot;GET /dl/XcQWfs90lFU?f=bestaudio HTTP/1.1\u0026quot; 200 OK [QV3jQQ9_TUw]: end of data (no error reported) cleanup[QV3jQQ9_TUw]: killing downloader process (PID: 499) INFO: 172.18.0.3:35288 - \u0026quot;GET /meta/wyqwYhDXkNQ HTTP/1.1\u0026quot; 200 OK [wyqwYhDXkNQ]: requested with format bestaudio and subs None, configuring... [wyqwYhDXkNQ]: sending stream [wyqwYhDXkNQ]: stream will be sent as is (bestaudio) [wyqwYhDXkNQ]: download type: video/webm (.webm) INFO: 172.18.0.3:35290 - \u0026quot;GET /dl/wyqwYhDXkNQ?f=bestaudio HTTP/1.1\u0026quot; 200 OK Done! D√πng c√°ch n√†y m√¨nh th·∫•y ngon l√†nh h∆°n h·∫≥n. Special thanks to Stephano (wes1993) and DavidBerdik ü•∞\n3. Setup other skills Search 2 skill sau: VOV, NhacCuaTui\n3.1. VOV VOV Command to use:\nAlexa open VOV: open VOV.\nAlexa stop: stop, mu·ªën chuy·ªÉn channel kh√°c th√¨ ph·∫£i stop tr∆∞·ªõc.\nAlexa next: ƒë·ªÉ chuy·ªÉn channel 1, 2, 3, vov giao th√¥ng.\n3.2. NhacCuaTui Tr∆∞·ªõc ti√™n c·∫ßn ƒëƒÉng k√Ω account NCT, free c≈©ng dc ko c·∫ßn VIP\nSkill n√†y n√≥ ko hi·ªÉu t√™n c·ªßa Playlist, m√† mu·ªën playlist th√¨ c·∫ßn bi·∫øt playlist ƒë√≥ c√≥ s·ªë th·ª© t·ª± l√† s·ªë m·∫•y.\nV√≠ d·ª• trong account NCT m√¨nh c√≥ kho·∫£ng 9,10 playlist:\nplaylist 2: Vietnamese 2\nplaylist 3: My Music\nplaylist 4: Foreign 2\nplaylist 5: Vietnamese 1\nplaylist 6: Foreign 1\nplaylist 7: 2NE1 Parkbom\nplaylist 8: B·ª©c t∆∞·ªùng\nplaylist 9: So deep\nplaylist 10: billboard 100 (1958-2015)\nCommand to use:\nAlexa, play playlist [3] from NCT: Mu·ªën play playlist My Music t∆∞∆°ng ·ª©ng v·ªõi s·ªë th·ª© t·ª± l√† 3.\nAlexa, stop / next: mu·ªën next b√†i ti·∫øp theo c·∫ßn stop tr∆∞·ªõc.\nAlexa play Vpop from NCT: Play nh·∫°c Vi·ªát tr√™n NCT. Alexa what's hot today from NCT: play l·∫ßn l∆∞·ª£t c√°c b√†i ƒëang TOP BXH nh·∫°c Vi·ªát.\n4. Troubleshooting   N·∫øu b·∫°n mu·ªën Xem l·∫°i c√°c command m√† Alexa ƒë√£ record l·∫°i th√¨ v√†o ƒë√¢y:\nhttps://www.amazon.com/alexa-privacy/apd/rvh\nNh·ªù c√°i trang tr√™n m√† m√¨nh bi·∫øt m√¨nh ph√°t √¢m l√∫c th√¨ youtube l√∫c th√¨ you tube. L√∫c th√¨ c√≥ my news playlist, l√∫c th√¨ news playlist.\nTh·∫ø n√™n m√¨nh t√≥m l·∫°i n√™n ƒë·ªÉ invocation name l√† my youtube ch·ª© ko n√™n ƒë·ªÉ you tube. C√°c playlist th√¨ c≈©ng ƒë·ªìng b·ªô 1 ch√∫t, ƒë·ªÅu c√≥ ch·ªØ my ·ªü ƒë·∫ßu. ƒê·ªçc sai 1 ch·ªØ l√† n√≥ ko hi·ªÉu ngay, do con Youtube skill n√†y n√≥ l·ªüm.\n  N·∫øu b·ªã l·ªói ko discovery ƒë∆∞·ª£c device sau khi linked account, solution: ƒê·ªïi h·∫øt stack sang English (US), lambda sang N.Verginia region:\nhttps://community.home-assistant.io/t/alexa-integration-does-not-find-devices/193732/5\n  B·∫°n c√≥ th·ªÉ d√πng web sau ƒë·ªÉ thay cho app tr√™n ƒëi·ªán tho·∫°i:\nhttps://alexa.amazon.com\n  5. Integrate Alexa Media Player on HACS c√°i n√†y l√† ƒë·ªÉ c√≥ th·ªÉ control Alexa qua giao di·ªán Home Assistant ho·∫∑c qua programming\n5.1. Setup restart HASS\nSetting -\u0026gt; Integration -\u0026gt; Add integration:\nm√†n h√¨nh Alexa Integration Configuration:\nƒê·ª´ng v·ªôi ƒëi·ªÅn Built-in 2FA App Key v√¨ ch√∫ng ta s·∫Ω l·∫•y n√≥ b·∫±ng c√°c step sau:\nLogin v√†o https://amazon.com r·ªìi v√†o Your Account:\nV√†o ph·∫ßn 2 step verfication:\n·∫§n v√†o Add new app:\nƒê·ª´ng scan QR code, h√£y ·∫•n v√†o Cant scan barcode ƒë·ªÉ COPY l·∫•y code:\nPaste code v·ª´a l·∫•y ƒë∆∞·ª£c v√†o m√†n h√¨nh Alexa Integration Configuration:\nS·∫Ω xu·∫•t hi·ªán OTP Code 6 k√Ω t·ª± ·ªü ƒë√¢y, Copy ti·∫øp:\nPaste OTP 6 k√Ω t·ª± v√†o ƒë√¢y r·ªìi ·∫•n verify:\nQuay l·∫°i m√†n h√¨nh HASS, click check box v√† Submit:\nN√≥ s·∫Ω hi·ªán 1 web ƒë·ªÉ login b·∫±ng account Amazon:\nm√†n h√¨nh Loading:\nM√†n h√¨nh t√¨m ƒëc device echo tr√™n HASS:\nTab Setting - Integration s·∫Ω xu·∫•t hi·ªán Echo device:\nƒê·∫øn ƒë√¢y b·∫°n c√≥ th·ªÉ D√πng HASS ƒëi·ªÅu khi·ªÉn c√°c entity c·ªßa Alexa:\n5.2. Use case 5.2.1. Text to speech Gi·∫£ s·ª≠ m√¨nh mu·ªën add 1 card v√†o lovelace:\ntitle: \u0026#34;Alexa Notes\u0026#34; path: \u0026#34;alexa\u0026#34; cards: - type: \u0026#39;custom:mini-media-player\u0026#39; entity: media_player.hoang_s_echo_dot icon: \u0026#39;mdi:amazon\u0026#39; tts: platform: alexa entity_id: media_player.hoang_s_echo_dot B·∫°n c√≥ th·ªÉ ƒë√°nh text v√†o text box, sau ƒë√≥ Alexa s·∫Ω ƒë·ªçc nh·ªØng g√¨ b·∫°n vi·∫øt (Alexa ch·ªâ hi·ªÉu English)\nƒê·∫øn ƒë√¢y m√¨nh nghƒ© ƒë·∫øn 1 use case:\n M√¨nh ·ªü cty g√µ text v√†o text to speech: How are you?. Alexa ·ªü nh√† s·∫Ω ph√°t ra ti·∫øng \u0026ldquo;How are you\u0026rdquo; ƒë·∫øn v·ª£ m√¨nh. V·ª£ m√¨nh ·ªü nh√† nghe th·∫•y, ra l·ªánh cho Alexa: \u0026ldquo;Alexa, ask Reply Now say I am fine\u0026rdquo;. ALexa s·∫Ω g·ª≠i message v√†o group chat gia ƒë√¨nh \u0026ldquo;I am fine\u0026rdquo;. M√¨nh nh·∫≠n dc tin nh·∫Øn qua Telegram.  ƒê·ªÉ l√†m ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥ th√¨ c·∫ßn vi·∫øt 1 skill ƒë·ªÉ ra l·ªánh cho Alexa send message ƒë·∫øn Telegram group chat. Kh√° th√∫ v·ªã ƒë·∫•y ch·ª©. S·∫Ω l√†m ngay ·ªü ph·∫ßn 6.\n5.2.2. Command to Alexa by text Tr∆∞·ªõc gi·ªù l√† ch√∫ng ta ra l·ªánh cho Alexa qua gi·ªçng n√≥i th·ª±c s·ª±. Gi·ªù n·∫øu b·∫°n ko mu·ªën n√≥i v√¨ l∆∞·ªùi, b·∫°n mu·ªën call 1 HA script, script ƒë√≥ s·∫Ω g·ª≠i text ƒë·∫øn Alexa, Alexa nh·∫≠n ƒëo·∫°n text ƒë√≥, hi·ªÉu v√† th·ª±c hi·ªán th√¨ l√†m th·∫ø n√†o? N·∫øu l√†m ƒë∆∞·ª£c ƒëi·ªÅu n√†y s·∫Ω r·∫•t ti·ªán l·ª£i cho c√°c automation c·ªßa ch√∫ng ta v·ªõi Alexa.\nUse case d·ªÖ nh·∫•t v·ªõi m√¨nh l√†, ƒê·ªÉ ra l·ªánh Alexa m·ªü My Youtube skill v√† b·∫≠t playlist My Vietnam TV playlist, m√¨nh s·∫Ω ph·∫£i n√≥i:\n Alexa ask My Youtube to play My Vietnam TV playlist\n Nh∆∞ng l·∫ßn n√†o c≈©ng ph·∫£i n√≥i th·∫ø th√¨ h∆°i d√†i üôÑ. Ho·∫∑c l√† v·ªõi nh·ªØng channel ti·∫øng Vi·ªát th√¨ b·∫°n n√≥i ch·∫Øc ch·∫Øn Alexa s·∫Ω ko hi·ªÉu. M√¨nh mu·ªën 1 trong 3 c√°ch sau:\n ·∫§n 1 button tr√™n giao di·ªán Home Assistant, n√≥ s·∫Ω trigger 1 HA script, Alexa nghe l·ªánh l√†m theo. D√πng ƒëi·ªán tho·∫°i qu∆° nh·∫π v√†o c√°i NFC tag g·∫Øn tr√™n b√†n ƒÉn, n√≥ s·∫Ω trigger 1 HA script, Alexa nghe l·ªánh l√†m theo. D√πng t√≠nh nƒÉng Routine c·ªßa Alexa app, ch·ªâ c·∫ßn n√≥i \u0026ldquo;Alexa, TV\u0026rdquo;, n√≥ s·∫Ω trigger 1 HA script, Alexa nghe l·ªánh l√†m theo.  C·∫£ 3 c√°ch tr√™n ƒë·ªÅu kh·∫£ thi 1 khi b·∫°n ƒë√£ integrate ƒë∆∞·ª£c Alexa Media player.\nV·ªÅ c∆° b·∫£n th√¨ script trong file scripts.yaml:\n# command to alexa ask My Youtube to play my Vietnam TV playlist alexa_youtube_cd24: sequence: - service: media_player.play_media target: entity_id: media_player.hoang_s_echo_dot data: media_content_type: custom media_content_id: \u0026#39;ask My Youtube to play my Vietnam TV playlist\u0026#39; C√≤n ·ªü dashboard th√¨ call script qua button nh∆∞ n√†y:\n... - type: glance title: Alexa command entities: - entity: script.alexa_youtube_cd24 icon: \u0026#39;mdi:youtube-tv\u0026#39; name: Cƒê24h show_state: false tap_action: # confirmation: # text: Are you sure to change state of this device? action: call-service service: script.alexa_youtube_cd24 service_data: entity_id: script.alexa_youtube_cd24 V·∫≠y l√† b·∫°n ch·ªâ c·∫ßn ·∫•n button Cƒê24h l√† xong, magic?ü§£\n1 khi ƒë√£ t·∫°o ƒë∆∞·ª£c script r·ªìi th√¨ vi·ªác c√≤n l·∫°i ch·ªâ l√† setting NFC tag ho·∫∑c Routine tr√™n Alexa app n·ªØa th√¥i, c√°i ·∫•y t√πy b·∫°n ch·ªçn.\n5.2.3. D√πng service TTS trong automaion ƒê√¢y l√† 1 v√≠ d·ª• v·ªÅ vi·ªác s·ª≠ d·ª•ng service: notify.YOUR_ECHO v√† tts ƒë·ªÉ trigger Alexa n√≥i 1 random phrase n√†o ƒë√≥:\n- id: \u0026#39;welcome-home-dvfhsfef\u0026#39; alias: \u0026#34;Welcome home\u0026#34; description: Alexa say welcome home mode: restart trigger: - entity_id: binary_sensor.front_door_sensor_contact platform: state to: \u0026#39;on\u0026#39; condition: \u0026#34;{{ is_state(\u0026#39;input_boolean.nobodyhome_mode\u0026#39;, \u0026#39;on\u0026#39;) }}\u0026#34; action: - delay: 00:00:03 - data: {} service_template: \u0026#34;script.striplight_power\u0026#34; - service: notify.alexa_media_hoang_s_echo_dot data: data: type: tts message: \u0026#39;{{ [ \u0026#34;Welcome home! \u0026#34;, \u0026#34;Wow! You are home. \u0026#34;, \u0026#34;Finally! You are home. \u0026#34;, \u0026#34;Home sweet home! \u0026#34;, ]| random + [ \u0026#34;I am really very happy you here. \u0026#34;, \u0026#34;You belong here. \u0026#34;, \u0026#34;Hope you have enjoyed your day. \u0026#34;, \u0026#34;I have been waiting for you all day long. \u0026#34;, \u0026#34;Hope you had a busy day. \u0026#34;, \u0026#34;Its great to have you back. \u0026#34;, \u0026#34;Would you like some music? say, Alexa, make some noise. \u0026#34;, ]| random }}\u0026#39; 6. Create My Alexa skill that send Telegram message Nh·∫Øc l·∫°i use case:\n M√¨nh ·ªü cty g√µ text v√†o text to speech: How are you?. Alexa ·ªü nh√† s·∫Ω ph√°t ra ti·∫øng \u0026ldquo;How are you\u0026rdquo; ƒë·∫øn v·ª£ m√¨nh. V·ª£ m√¨nh ·ªü nh√† nghe th·∫•y, ra l·ªánh cho Alexa: \u0026ldquo;Alexa, ask Reply Now say I am fine\u0026rdquo;. ALexa s·∫Ω g·ª≠i message v√†o group chat gia ƒë√¨nh \u0026ldquo;I am fine\u0026rdquo;. M√¨nh nh·∫≠n dc tin nh·∫Øn qua Telegram.  ƒê·ªÉ l√†m ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥ th√¨ c·∫ßn vi·∫øt 1 skill ƒë·ªÉ ra l·ªánh cho Alexa send message ƒë·∫øn Telegram group chat. Kh√° th√∫ v·ªã ƒë·∫•y ch·ª©. S·∫Ω l√†m ngay sau ƒë√¢y\u0026hellip;\n6.1. Setup Telegram bot \u0026hellip; m√¨nh ko vi·∫øt l·∫°i ph·∫ßn n√†y v√¨ c√≥ th·ªÉ s·ª≠ d·ª•ng l·∫°i b√†i tr∆∞·ªõc: Lambda + API Gateway, Telegram Bot and Serverless Webapp\nL·∫•y ƒë∆∞·ª£c TELEGRAM_TOKEN ƒë·ªÉ s·ª≠ d·ª•ng.\nNgo√†i ra c·∫ßn l·∫•y dc CHAT_ID c·ªßa group chat m√† con bot ƒë√£ ƒë∆∞·ª£c add v√†o.\n6.2. Setup tr√™n Alexa Developer console V√†o ƒë√¢y: https://developer.amazon.com/alexa/console/ask\nT·∫°o Skill Reply Now:\nCh√∫ √Ω, ch·ªçn Model -\u0026gt; Custom, Hosting services -\u0026gt; Provision your own:\nCh·ªçn template -\u0026gt; Start from Scratch:\nJson content:\n{ \u0026#34;interactionModel\u0026#34;: { \u0026#34;languageModel\u0026#34;: { \u0026#34;invocationName\u0026#34;: \u0026#34;reply now\u0026#34;, \u0026#34;intents\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.CancelIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.HelpIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.StopIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;HelloWorldIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;SendMessageIntent\u0026#34;, \u0026#34;slots\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;AMAZON.SearchQuery\u0026#34; } ], \u0026#34;samples\u0026#34;: [ \u0026#34;send {query}\u0026#34;, \u0026#34;say {query}\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.NavigateHomeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.FallbackIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.PauseIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.ResumeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] } ], \u0026#34;types\u0026#34;: [] } } } Save Model, Build Model\nSave Endpoint, Save Model, Build Model\nCh·ªù Build Model success\n6.3. Setup tr√™n AWS T·∫°o function telegram-alexa, d√πng runtime python 3.9\nT·∫°o v√† add c√°c layer v√†o (c√≥ th·ªÉ b·∫°n s·∫Ω c·∫ßn t√¨m l·∫°i b√†i discord ƒë·ªÉ bi·∫øt c√°ch t·∫°o layer):\nL·∫•y code ·ªü ƒë√¢y paste v√†o: https://github.com/alexa-samples/skill-sample-python-helloworld-classes/tree/master/lambda/py\nCh√∫ √Ω d√≤ng cu·ªëi c√πng ph·∫£i s·ª≠a th√†nh lambda_handler = sb.lambda_handler() th√¨ m·ªõi ch·∫°y dc\nCh√∫ √Ω set c√°c environment variable: CHAT_ID, TELEGRAM_TOKEN\nCode Lambda c·∫ßn s·ª≠a l·∫°i nh∆∞ sau, v·ªÅ c∆° b·∫£n m√¨nh ch·ªâ th√™m class SendMessageIntentHandler, function send_telegram_msg(msg, chat_id):\n# -*- coding: utf-8 -*- # This sample demonstrates handling intents from an Alexa skill using the Alexa Skills Kit SDK for Python. # Please visit https://alexa.design/cookbook for additional examples on implementing slots, dialog management, # session persistence, api calls, and more. # This sample is built using the handler classes approach in skill builder. import logging, os import gettext import requests import json from botocore.exceptions import ClientError from ask_sdk_core.skill_builder import SkillBuilder from ask_sdk_core.dispatch_components import ( AbstractRequestHandler, AbstractRequestInterceptor, AbstractExceptionHandler) import ask_sdk_core.utils as ask_utils from ask_sdk_core.handler_input import HandlerInput from ask_sdk_model import Response from alexa import data logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) # Get Lambda environment variables TOKEN = os.environ.get(\u0026#39;TELEGRAM_TOKEN\u0026#39;) chat_id = os.environ.get(\u0026#39;CHAT_ID\u0026#39;) ... def send_telegram_msg(msg, chat_id): \u0026#34;\u0026#34;\u0026#34; Tries to Send Telegram message. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: BASE_URL = \u0026#34;https://api.telegram.org/bot{}\u0026#34;.format(TOKEN) url = BASE_URL + \u0026#34;/sendMessage\u0026#34; data = {\u0026#34;text\u0026#34;: msg.encode(\u0026#34;utf8\u0026#34;), \u0026#34;chat_id\u0026#34;: chat_id} requests.post(url, data) except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info( \u0026#34;Send message to Telegram command throttled, automatically retrying...\u0026#34;) send_telegram_msg(msg, chat_id) else: logger.error( \u0026#34;Send message to Telegram command Failed!\\n%s\u0026#34;, str(err)) return False except: raise class SendMessageIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for SendMessage Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;SendMessageIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG slots = handler_input.request_envelope.request.intent.slots query = slots[\u0026#34;query\u0026#34;].value msg = \u0026#34;\\\u0026#34;\u0026#34; + query + \u0026#34;\\\u0026#34;\u0026#34; + \u0026#34; - someone said.\u0026#34; # send telegram message logger.info(\u0026#34;Your messsage: \u0026#34; + query) send_telegram_msg(msg, chat_id) logger.info(\u0026#34;Your messsage to Telegram was sent\u0026#34;) speak_output = \u0026#34;Sent it\u0026#34; return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) ... sb = SkillBuilder() sb.add_request_handler(LaunchRequestHandler()) sb.add_request_handler(HelloWorldIntentHandler()) sb.add_request_handler(SendMessageIntentHandler()) sb.add_request_handler(HelpIntentHandler()) sb.add_request_handler(CancelOrStopIntentHandler()) sb.add_request_handler(FallbackIntentHandler()) sb.add_request_handler(SessionEndedRequestHandler()) # make sure IntentReflectorHandler is last so it doesn\u0026#39;t override your custom intent handlers sb.add_request_handler(IntentReflectorHandler()) sb.add_global_request_interceptor(LocalizationInterceptor()) sb.add_exception_handler(CatchAllExceptionHandler()) lambda_handler = sb.lambda_handler() Add trigger cho Lambda function, ch√∫ √Ω ch·ªçn Enable skill verification:\n6.4. Test Alexa skill G√µ v√†o ho·∫∑c n√≥i v√†o mic, n·∫øu tr·∫£ v·ªÅ k·∫øt qu·∫£ sent it l√† OK:\nCheck k·∫øt qu·∫£ tr√™n telegram chat s·∫Ω c√≥ tin nh·∫Øn con bot g·ª≠i ƒë·∫øn\nB·∫±ng c√°ch n√†y b·∫°n c√≥ th·ªÉ v√†o Cloudwatch ƒë·ªÉ xem log:\n7. Create My Alexa skill that return Lunar Calendar M√¨nh s·∫Ω h·ªèi Alexa ki·ªÉu nh∆∞:\n Me: \u0026ldquo;Alexa lunar calendar?\u0026rdquo; Alexa: \u0026ldquo;today? yesterday? tomorrow?\u0026rdquo; Me: \u0026ldquo;today\u0026rdquo; Alexa: \u0026ldquo;In Vietnam Lunar calendar, today is day: 3, month: 12\u0026rdquo; -\u0026gt; nghƒ©a l√† m√πng 3 th√°ng ch·∫°p.  ƒê·ªÉ l√†m skill n√†y v·ªÅ c∆° b·∫£n l√† d·ªÖ, quan tr·ªçng l√† google ƒëc c√°ch t√≠nh Lunar calendar m√† th√¥i\nC√°c b∆∞·ªõc t·∫°o skill gi·ªëng nh∆∞ ƒë√£ l√†m v·ªõi Reply Now\nChu·ªói JSON trong ph·∫ßn model s·∫Ω ki·ªÉu nh∆∞ n√†y:\n{ \u0026#34;interactionModel\u0026#34;: { \u0026#34;languageModel\u0026#34;: { \u0026#34;invocationName\u0026#34;: \u0026#34;lunar calendar\u0026#34;, \u0026#34;intents\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.CancelIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.HelpIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.StopIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;HelloWorldIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;LunarCalendarTodayIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [ \u0026#34;tell me lunar calendar\u0026#34;, \u0026#34;today\u0026#34;, \u0026#34;lunar calendar today\u0026#34;, \u0026#34;lunar calendar date\u0026#34;, \u0026#34;lunar date today\u0026#34;, \u0026#34;tell me lunar date\u0026#34;, \u0026#34;lunar calendar today\u0026#34;, \u0026#34;today in lunar calendar\u0026#34;, \u0026#34;what is today in lunar calendar\u0026#34;, \u0026#34;what is today in lunar date\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;LunarCalendarYesterdayIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [ \u0026#34;yesterday\u0026#34;, \u0026#34;lunar calendar yesterday\u0026#34;, \u0026#34;the day before\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;LunarCalendarTomorrowIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [ \u0026#34;tomorrow\u0026#34;, \u0026#34;lunar calendar tomorrow\u0026#34;, \u0026#34;the day after\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.NavigateHomeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.FallbackIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.PauseIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.ResumeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] } ], \u0026#34;types\u0026#34;: [] } } } Step t·∫°o Lambda c≈©ng t∆∞∆°ng t·ª±, nh∆∞ng trong folder /data t·∫°o file AL.py content l·∫•y t·ª´ link n√†y\nCode Lambda s·∫Ω c√≥ nh·ªØng ph·∫ßn n√†y:\n# -*- coding: utf-8 -*- # This sample demonstrates handling intents from an Alexa skill using the Alexa Skills Kit SDK for Python. # Please visit https://alexa.design/cookbook for additional examples on implementing slots, dialog management, # session persistence, api calls, and more. # This sample is built using the handler classes approach in skill builder. import logging, os import gettext import requests import json from botocore.exceptions import ClientError from ask_sdk_core.skill_builder import SkillBuilder from ask_sdk_core.dispatch_components import ( AbstractRequestHandler, AbstractRequestInterceptor, AbstractExceptionHandler) import ask_sdk_core.utils as ask_utils from ask_sdk_core.handler_input import HandlerInput from ask_sdk_model import Response from alexa import data from alexa import AL from datetime import datetime from dateutil.relativedelta import relativedelta currentDT = datetime.now() print(\u0026#39;------------------\u0026#39;) print(\u0026#34;Run at: %s\u0026#34; % currentDT) # Get current datetime this_year = datetime.today().strftime(\u0026#39;%Y\u0026#39;) this_month = datetime.today().strftime(\u0026#39;%m\u0026#39;) current_day = currentDT.strftime(\u0026#39;%d\u0026#39;) print(\u0026#34;This year: %s. This month: %s. Current day: %s\u0026#34; % (this_year, this_month, current_day)) # Get yesterday datetime yesterday_date = relativedelta(days=-1) + currentDT yesterday_day = yesterday_date.strftime(\u0026#39;%d\u0026#39;) yesterday_month = yesterday_date.strftime(\u0026#39;%m\u0026#39;) yesterday_year = yesterday_date.strftime(\u0026#39;%Y\u0026#39;) print(\u0026#34;Yesterday year: %s. Yesterday month: %s. Yesterday day: %s\u0026#34; % (yesterday_year, yesterday_month, yesterday_day)) # Get tomorrow datetime tomorrow_date = relativedelta(days=1) + currentDT tomorrow_day = tomorrow_date.strftime(\u0026#39;%d\u0026#39;) tomorrow_month = tomorrow_date.strftime(\u0026#39;%m\u0026#39;) tomorrow_year = tomorrow_date.strftime(\u0026#39;%Y\u0026#39;) print(\u0026#34;Tomorrow year: %s. Tomorrow month: %s. Tomorrow day: %s\u0026#34; % (tomorrow_year, tomorrow_month, tomorrow_day)) logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) .... class LunarCalendarTodayIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for LunarCalendarTodayIntent Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;LunarCalendarTodayIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG lunar_date = AL.S2L(int(current_day), int(this_month), int(this_year), timeZone = 7) print(lunar_date) # DEBUG lunar_day = lunar_date[0] lunar_month = lunar_date[1] speak_output = \u0026#34;In Vietnam Lunar calendar, today is... Day: %d. Month: %d.\u0026#34; % (lunar_day, lunar_month) return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) class LunarCalendarYesterdayIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for LunarCalendarYesterdayIntent Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;LunarCalendarYesterdayIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG lunar_date = AL.S2L(int(yesterday_day), int(yesterday_month), int(yesterday_year), timeZone = 7) print(lunar_date) # DEBUG lunar_day = lunar_date[0] lunar_month = lunar_date[1] speak_output = \u0026#34;In Vietnam Lunar calendar, yesterday is... Day: %d. Month: %d.\u0026#34; % (lunar_day, lunar_month) return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) class LunarCalendarTomorrowIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for LunarCalendarTomorrowIntent Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;LunarCalendarTomorrowIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG lunar_date = AL.S2L(int(tomorrow_day), int(tomorrow_month), int(tomorrow_year), timeZone = 7) print(lunar_date) # DEBUG lunar_day = lunar_date[0] lunar_month = lunar_date[1] speak_output = \u0026#34;In Vietnam Lunar calendar, tomorrow is... Day: %d. Month: %d.\u0026#34; % (lunar_day, lunar_month) return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) ... sb = SkillBuilder() sb.add_request_handler(LaunchRequestHandler()) sb.add_request_handler(HelloWorldIntentHandler()) sb.add_request_handler(LunarCalendarTodayIntentHandler()) sb.add_request_handler(LunarCalendarYesterdayIntentHandler()) sb.add_request_handler(LunarCalendarTomorrowIntentHandler()) sb.add_request_handler(HelpIntentHandler()) sb.add_request_handler(CancelOrStopIntentHandler()) sb.add_request_handler(FallbackIntentHandler()) sb.add_request_handler(SessionEndedRequestHandler()) # make sure IntentReflectorHandler is last so it doesn\u0026#39;t override your custom intent handlers sb.add_request_handler(IntentReflectorHandler()) sb.add_global_request_interceptor(LocalizationInterceptor()) sb.add_exception_handler(CatchAllExceptionHandler()) lambda_handler = sb.lambda_handler() Test l·∫°i Alexa skill gi·ªëng nh∆∞ ph·∫ßn 6.4\n8. CREDIT Document Official: https://www.home-assistant.io/integrations/alexa.smart_home/#test-the-lambda-function\nEverythingSmartHome:\nAlexa with Home Assistant Local for FREE Without Subscription:\nhttps://www.youtube.com/watch?v=Ww2LI59IQ0A\u0026amp;ab_channel=EverythingSmartHome\nSauber-LabUK:\nLet\u0026rsquo;s install Amazon Alexa in our Home Assistant ‚Äì Local Setup:\nhttps://www.youtube.com/watch?v=5G733Lv-PhY\u0026amp;ab_channel=Sauber-LabUK\nK√™nh T√°y M√°y - Alexa setup: https://www.youtube.com/watch?v=sBaeXxKnSMg\u0026amp;ab_channel=K%C3%AAnhT%C3%A1yM%C3%A1y\nAlexa call script:\nhttps://youtu.be/0ElXDPw5c1Q?t=1136 Alexa include exclude entity:\nhttps://youtu.be/PhWpnc-Pvko?t=223\nMark Watt Tech:\nInstall Alexa Media player qua HACS ƒë·ªÉ control Echo th√¥ng qua HASS:\nhttps://www.youtube.com/watch?v=UsnhL2z_UUY\u0026amp;ab_channel=MarkWattTech\nScripts + Automations, Idea v·ªÅ setup cho Alexa run script c·ªßa HASS: https://www.youtube.com/watch?v=0ElXDPw5c1Q\u0026amp;ab_channel=MarkWattTech\nALEXA ACTIONABLE NOTIFICATIONS (Home Assistant + Alexa Skill):\nhttps://www.youtube.com/watch?v=uoifhNyEErE\u0026amp;ab_channel=MarkWattTech\nPaulHibbert - Alexa play Youtube music, Setup alexa skill ri√™ng c·ªßa m√¨nh v√† AWS lambda ri√™ng: post t·ª´ 2019 2020, c√≥ th·ªÉ ƒë√£ outdated:\nhttps://www.youtube.com/watch?v=mluD8kQ06NM\u0026amp;ab_channel=PaulHibbert\nhttps://www.youtube.com/watch?v=5k9OGbeek28\u0026amp;ab_channel=PaulHibbert\nGithub alexa-youtube m√† t√°c gi·∫£ ndg63276 s·∫Ω cung c·∫•p Lambda ARN url cho m·ªói ng∆∞·ªùi gi√° 3$/month: https://github.com/ndg63276/alexa-youtube\nc√°c forks tr∆∞·ªõc khi ndg63276 make private: https://github.com/cipi1965/alexa-youtube-it update t·ª´ 2018 https://github.com/FedericoHeichou/alexa-youtube : N√äN FORK V·ªÄ\nGithub alexa-youtube-skill, t√°c gi·∫£ ƒë√£ archive: https://github.com/dmhacker/alexa-youtube-skill https://imgur.com/a/H5R7L\nhttps://github.com/mbpictures/alexa-youtube-skill -\u0026gt; update t·ª´ 2019 https://github.com/waiwong614/alexa-tube -\u0026gt; update t·ª´ 2019\nGithub alexa-youtube well-maintained: https://github.com/wes1993/YouTubeForAlexa: N√äN FORK V·ªÄ file zip ƒë∆∞·ª£c extract ra repo n√†y ƒë·ªÉ xem:\nhttps://github.com/DavidBerdik/YouTubeForAlexa-Lambda\nC√°c c√°ch nghe Youtube audio tr√™n Alexa Echo dot:\nhttps://diysmarthomeplanet.com/echo-dot-youtube/\n Unofficial YouTube Skill on Github Workaround via Fire TV Pairing with the smartphone  AndrewTech k√™nh l√†m c√°c demo li√™n quan ƒë·∫øn Youtube v·ªõi Alexa:\nhttps://www.youtube.com/@Andrewstech/videos\nGithub c·ªßa K√™nh AndrewTech l√†m Youtube for Alexa:\nhttps://github.com/Imihaljko/youtube-for-alexa\nwell-maintained, web base run tr√™n Docker ƒë·ªÉ download Youtube t·ª± host: N√äN FORK V·ªÄ https://github.com/xxcodianxx/youtube-dl-web\nwell-maintained, CLI, 1 library python c√≥ 2.9k forks, 36k stars: N√äN FORK V·ªÄ https://github.com/yt-dlp/yt-dlp\nwell-maintained, stream b·∫±ng t·ª± d·ª±ng server Youtube audio server:\nhttps://github.com/codealchemist/youtube-audio-server\n1 s·ªë tip khi s·ª≠ d·ª•ng Alexa: https://www.youtube.com/watch?v=Zey1P1ZEyn4\u0026amp;ab_channel=Pocket-lint\nLunarCalendar:\nhttps://github.com/quangvinh86/SolarLunarCalendar/blob/master/LunarSolar.py\nhttps://www.informatik.uni-leipzig.de/~duc/amlich/AL.py\nRandom phrase with Alexa:\nhttps://youtu.be/3MdnRfCQcVE?t=154\nhttps://community.home-assistant.io/t/how-to-create-multiple-phrases-to-send-at-random-to-tts/19807/43\n","href":"/bk/encrypt-connect-home-assistant-to-alexa-echo-dot4/","title":"Connect Home Assistant to Amazon Alexa Echo Dot4"},{"content":"1. Setup Alexa connect to Home Assistant L√†m theo h∆∞·ªõng d·∫´n ·ªü ƒë√¢y:\nhttps://www.home-assistant.io/integrations/alexa.smart_home/\n1.1. Setup Tr√™n AWS Console create Role for Lambda:\nTr√™n Alexa developer page, create skill:\nTr√™n AWS Console Lambda, t·∫°o function, n√™n ch·ªçn us-east-1 v√¨ c√°c region kh√°c d·ªÖ b·ªã l·ªói l·∫Øm:\nSet trigger to Alexa:\nUpload code from Github:\nSetup Environment variables:\nUpdate home assistant file configuration.yml:\nCreate long live token ƒë·ªÉ test t√Ω n·ªØa:\nTest Lambda:\nCopy Lambda ARN:\nTr√™n Alexa Developer Page, paste function ARN:\nTr√™n Alexa Developer Page, setup Link account:\nCh·ªó Your Client ID, n√™n ch·ªçn https://pintagui.amazon.com/: Gi·ªù login v√†o app Alexa tr√™n ƒëi·ªán tho·∫°i (N·∫øu b·∫°n dung Iphone, c√≥ th·ªÉ b·∫°n s·∫Ω ph·∫£i chuy·ªÉn v√πng US ƒë·ªÉ t·∫£i app, d√πng Android s·∫Ω c·∫ßn down file APK):\nV√†o tab Skill \u0026amp; Game, Your Skill s·∫Ω th·∫•y Skill c·ªßa b·∫°n v·ª´a t·∫°o:\nLogin ƒë·ªÉ link account th√†nh c√¥ng nh√©, n·∫øu d√πng wifi ko login dc th√¨ h√£y d√πng 4G\nSau khi login th√¨ App s·∫Ω t·ª± ƒë·ªông discovery devices\n1.2. Troubeshooting B·∫°n c√≥ th·ªÉ d√πng web sau ƒë·ªÉ thay cho app tr√™n ƒëi·ªán tho·∫°i:\nhttps://alexa.amazon.com\n  L·ªói v√¨ ch·ªçn Singapore, 1 region kh√¥ng ƒë∆∞·ª£c ch·∫•p nh·∫≠n:\n  L·ªói v√¨ ch·ªçn Trigger nh·∫ßm skill set m√† ko ch·ªçn Smart Home:\n  L·ªói v√¨ m√¨nh ch·ªçn Your Client ID l√† c√°i link ...jp/ ph·∫£i d√πng https://pintagui.amazon.com/:\n  N·∫øu b·ªã l·ªói ko discovery ƒë∆∞·ª£c device sau khi linked account, solution: ƒê·ªïi h·∫øt stack sang English (US), lambda sang N.Verginia region: https://community.home-assistant.io/t/alexa-integration-does-not-find-devices/193732/5\n  2. Setup Youtube Skill for Alexa L√†m theo h∆∞·ªõng d·∫´n ·ªü ƒë√¢y:\nhttps://github.com/hoangmnsd/YouTubeForAlexa\n2.1. Setup tr√™n GCP v√†o ƒë√¢y t·∫°o project:\nhttps://console.developers.google.com/project\nv√†o ƒë√¢y:\nhttps://console.developers.google.com/apis/library?project=tester-api-key\nsearch youtube data API v3. Enable it:\nClick on \u0026ldquo;Create Credentials\u0026rdquo;\nSet like this:\n ‚ÄúWhich API are you using?‚Äù: YouTube Data API v3 ‚ÄúFrom where you call the API?‚Äù: Server web (ex. node.js, Tomcat) ‚ÄúWhich data you use?‚Äù: Public Data Click on ‚ÄúWhich credentials i need?‚Äù  After some seconds you will see under ‚ÄúGet your credentials‚Äù the key that wee need.\nCOPY and SAVE the key in the notepad.\n2.2. Setup tr√™n AWS Create lambda function on us-east-1:\nAdd trigger for Lambda, note that Skill ID verification:\nUpload zip file from https://github.com/wes1993/YouTubeForAlexa/blob/master/YouTubeForAlexaLambda.zip\nSet Environment variable for Lambda, m√¨nh ch·ªçn pytube ƒë·ªÉ test th√¥i, ng∆∞·ªùi ta khuy√™n d√πng youtube_dl v√¨ n√≥ ·ªïn ƒë·ªãnh h∆°n tuy h∆°i ch·∫≠m:\nSetup timeout to 30s, Memory to 512 Mb:\n2.3. Setup tr√™n Alexa Developer Page Go to the Alexa Console (https://developer.amazon.com/alexa/console/ask):\nInvocation name ch·ªçn my youtube, ko n√™n ch·ªçn you tube v√¨ d·ªÖ ph√°t √¢m sai:\nUpdate JSON Editor:\nUpdate Interface:\nUpdate Endpoints:\nUpdate permission tab:\nclick Build Model and wait for success\nEnable Test development:\n2.4. Test Youtube skill Alexa launch youtube: sau command n√†y s·∫Ω th·∫•y trong list c·ªßa Alexa app c√≥ list Youtube Favorite, Youtube:\nhttps://alexa.amazon.com/\nUpdate Youtube Alexa list, M√¨nh s·ª≠a l·∫°i c√°c playlist public Youtube: Gi·∫£i th√≠ch:\nMy News playlist: tin m·ªõi c·ªßa VTC NOW\nMy Vietnam TV playlist: chuy·ªÉn ƒë·ªông 24h VTV\nMy Black playlist: ƒêen v√¢u official\nThat song I like: Show c·ªßa ƒêen\nMy Dragon playlist: Imagine Dragon\nMy Taylor playlist: Taylor Swift\nMy Super awsome playlist: Phan M·∫°nh Qu·ª≥nh\nMy Social playlist: Ki·∫øn th·ª©c th√∫ v·ªã playlist\nMy Billionaire playlist: t·ªï bu√¥n 247 tu·∫•n ti·ªÅn t·ªâ\nCommand to use:\nAlexa launch youtube: command n√†y s·∫Ω kh·ªüi t·∫°o b·∫°n ƒë·∫ßu c√°c Favorite Youtube list cho b·∫°n\nAlexa stop / next / previous / shuffle: ph·∫£i stop tr∆∞·ªõc.\nAlexa ask my youtube to play [My Dragon playlist]: ch√∫ √Ω ph√°t √¢m my youtube\n2.5. Optional steps C√≥ 3 l·ªói m√† m√¨nh g·∫∑p ph·∫£i trong qu√° tr√¨nh d√πng skill n√†y:\n   L·ªói Playback failed, \u0026quot;Device playback error\u0026quot;, MEDIA ERROR\nC√≥ nh·ªØng playlist l·ªói ngay c·∫£ khi d√πng ho·∫∑c ko d√πng proxy nh∆∞ 2 playlist n√†y:\nTaylor playlist | https://www.youtube.com/playlist?list=PLMEZyDHJojxNYSVgRCPt589DI5H7WT1ZK\nRed playlist | https://www.youtube.com/playlist?list=PLvaeEf26a-mB6RsQ1UjLfAhGypELXulZE\nSolution: ch∆∞a t√¨m ra..    L·ªói N·∫øu trong playlist c√≥ c√°c video vi ph·∫°m ch√≠nh s√°ch Youtube b·ªã ·∫©n ƒëi th√¨ Alexa s·∫Ω stop lu√¥n playlist:\nm√¨nh ƒë√£ log issue: https://github.com/wes1993/YouTubeForAlexa/issues/23\nSolution: ch∆∞a t√¨m ra..    L·ªói V√¨ Lambda call ƒë·∫øn Youtube t·ª´ IP c·ªßa Lambda region (US) n√™n s·∫Ω c√≥ 1 s·ªë video ch·ªâ kh·∫£ d·ª•ng ·ªü Vietnam s·∫Ω ko th·ªÉ play ƒë∆∞·ª£c.\nSolution: D√πng forward proxy tiny proxy (https://github.com/hoangmnsd/YouTubeForAlexa#extra-step-optional)    D∆∞·ªõi ƒë√¢y m√¨nh vi·∫øt v·ªÅ vi·ªác m√¨nh l√†m ƒë·ªÉ ch·∫°y tinyproxy:\n C√†i tinyproxy ƒë·ªÉ l√†m forward proxy. (ƒê·ªÉ hi·ªÉu forward proxy v√† reserve proxy kh√°c nh∆∞ n√†o th√¨ xem b√†i n√†y nh√©: Reverse proxy and Forward proxy)\ntheo h∆∞·ªõng d·∫´n n√†y: https://github.com/hoangmnsd/YouTubeForAlexa#extra-step-optional\nN√≥i n√¥m na th√¨ ta s·∫Ω setup ƒë·ªÉ Lambda ko call tr·ª±c ti·∫øp ƒë·∫øn Youtube m√† ƒëi qua tinyproxy r·ªìi m·ªõi ƒë·∫øn Youtube, nh∆∞ v·∫≠y Youtube s·∫Ω hi·ªÉu c√°c request l√† ƒë·∫øn t·ª´ 1 IP c·ªë ƒë·ªãnh c·ªßa nh√† m√¨nh, ch·ª© ko ph·∫£i ƒë·∫øn t·ª´ Lambda\nM√¨nh ƒë√£ th·ª≠ run docker tr√™n RPi nh∆∞ng b·ªã l·ªói (c√≥ l·∫Ω do Rpi l√† arm64 n√™n ko d√πng dc image c·ªßa vimagick tr√™n Docker Hub). N√™n ph·∫£i build l·∫°i image tr√™n RPi ƒë·ªÉ ch·∫°y theo Dockerfile ·ªü ƒë√¢y: https://github.com/vimagick/dockerfiles/tree/master/tinyproxy\nTest trong local th√¨ ok nh∆∞ng khi test Lambda ko connect dc ƒë·∫øn tinyproxy trong m·∫°ng nh√† m√¨nh. C√≥ th·ªÉ v√¨ m√¨nh ko th·ªÉ expose port 8888 ra ƒë∆∞·ª£c, m√† m√¨nh c≈©ng ko mu·ªën expose nhi·ªÅu port ra.\n-\u0026gt; M√¨nh chuy·ªÉn qua c√†i tinyproxy tr√™n VM Oracle Cloud c·ªßa m√¨nh. S·∫Ω c·∫ßn m·ªü all port 8888.\nDo VM Oracle c·ªßa m√¨nh l√† ip UK n√™n s·∫Ω c√≥ 1 s·ªë playlist ko th·ªÉ play ƒë∆∞·ª£c, v√≠ d·ª• nh∆∞ c√°c playlist c·ªßa VTC, VTV24 (Ch·∫Øc VTV24 ch·ªâ publish video cho khu v·ª±c Vietnam xem)\nM√¨nh ƒëang t√≠nh thu√™ VPS c·ªßa Vietnam nh∆∞ hostingviet.vn 130k/th√°ng, ƒë·ªÉ c√†i tinyproxy\nnh∆∞ng k·ªÉ c·∫£ c√†i proxy th√¨ ch·ªâ c√≥ th·ªÉ fix ƒë∆∞·ª£c l·ªói s·ªë 3 th√¥i, ko fix ƒë∆∞·ª£c l·ªói 1 \u0026amp; 2 üò™üò™\nUpdate 2023/Jan/07:\nSau khi m√¨nh thu√™ 1 VPS c·ªßa hostingviet.vn v·ªõi gi√° 135k/th√°ng th√¨ v·∫´n ko play ƒë∆∞·ª£c video latest trong 1 channel\nN·∫øu d√πng proxy c·ªßa Hostingviet, m√¨nh set 1 HA script TTS cho Alexa r·∫±ng \u0026ldquo;ask My Youtube to play channel VTV1 VTV Go\u0026rdquo; th√¨:\n l·ªói 1 l√† n√≥ ko play clip latest c·ªßa Channel ƒë√≥, l·ªói 2 l√† ƒë∆∞·ª£c 1 l√∫c th√¨ th∆∞·ªùng xuy√™n tr·∫£ v·ªÅ l·ªói \u0026quot;The channel XXX hasn't worked, shall I try the next one?\u0026quot;  ƒê√£ th·ª≠ chuy·ªÉn bi·∫øn m√¥i tr∆∞·ªùng c·ªßa AWS Lambda qua l·∫°i pytube, youtube-dl nh∆∞ng v·∫´n ko fix ƒë∆∞·ª£c l·ªói 1, l·ªói 2.\nNh∆∞ng n·∫øu chuy·ªÉn proxy sang d√πng VM c·ªßa Oracle th√¨ ko b·ªã l·ªói s·ªë 2, tuy nhi√™n v·∫´n ko fix ƒë∆∞·ª£c l·ªói 1 (m√† play 1 clip t·ª´ 14/06/2022.)\nC√≤n n·∫øu set c·ª©ng Youtube playlist URL tr√™n Alexa app th√¨ g·∫∑p ph·∫£i l·ªói: Do Admin k√™nh VTV1 VTV Go ƒëang s·∫Øp x·∫øp th·ª© t·ª± t·ª´ oldest ƒë·∫øn latest, n√™n khi play th√¨ lu√¥n b·∫Øt ƒë·∫ßu t·ª´ clip c≈© nh·∫•t, ph·∫£i \u0026ldquo;next\u0026rdquo; m√£i m·ªõi ƒë·∫øn clip latest. Ho·∫∑c l√† c·ª© ra l·ªánh \u0026ldquo;shuffle\u0026rdquo; r·ªìi next m√£i ƒë·ªÉ may m·∫Øn ƒë·∫øn clip latest.üòÉü§£\n2.6. Update 2023.02 using Youtube Stream Repeater Th√°ng 2.2023, wes1993 ƒë√£ release 1 b·∫£n update m·ªõi v·ªõi c·∫≠p nh·∫≠t l√† s·ª≠ d·ª•ng YouTube-Stream-Repeater ƒë·ªÉ stream Youtube video/audio, c√≥ th·ªÉ coi l√† ·ªïn ƒë·ªãnh h∆°n pytube v√† youtube-dl. Th·ª±c s·ª± l√† version c≈© nhi·ªÅu l·ªói qu√°, c·ª© 1 th√°ng d√πng kho·∫£ng 10 15 ng√†y l√† l·∫°i l·ªói. M√† l·ªói r·∫•t kh√≥ debug lu√¥n, ch·∫£ hi·ªÉu nguy√™n nh√¢n g√¨ m√† t·ª± nhi√™n b·ªã l·ªói, th∆∞·ªùng Alexa s·∫Ω ko ph√°t Youtube video v√† n√≥i \u0026ldquo;Video hasn\u0026rsquo;t work\u0026rdquo;. Hy v·ªçng l√† l·∫ßn update n√†y s·∫Ω kh√° h∆°n.\nTuy nhi√™n th√¨ Stephano (aka wes1993) ƒë√£ t·∫°o ra 1 Home Assistant Addon ƒë·ªÉ integrate.\nV·ªõi m√¨nh c√°ch n√†y ko d√πng ƒë∆∞·ª£c, v√¨ m√¨nh ƒëang s·ª≠ d·ª•ng Home Assistant Container, ko c√≥ ch·ª©c nƒÉng Addon.\nV√†, m√¨nh c≈©ng ko th√≠ch d√πng Addon v√¨ n√≥ ƒë·∫∑c th√π cho HomeAssistant qu√°.\nL·∫ßn m√≤ khi ƒë·ªçc comment n√†y th√¨ m√¨nh th·∫•y c√°ch c·ªßa DavidBerdik ph√π h·ª£p v·ªõi m√¨nh h∆°n.\nM√¨nh mu·ªën l√†m th·ª≠ ki·ªÉu n√†y: (diagram)\nV√¨ m√¨nh c√≥ 1 Oracle VM (4CPU/24GB RAM) ƒëang ch∆∞a t·∫≠n d·ª•ng h·∫øt, s·∫Ω run YouTube-Stream-Repeater container, expose ra HTTPS url.\nAWS Lambda s·∫Ω request ƒë·∫øn YouTube-Stream-Repeater, v√† Alexa s·∫Ω serve audio t·ª´ ƒë√≥.\nM√¨nh ko run YouTube-Stream-Repeater tr√™n RPi ·ªü trong m·∫°ng local c√πng v·ªõi HA v√¨:\n Con RPi ƒëang d√πng Wifi 2.4Ghz, k·∫øt n·ªëi internet c·ªßa n√≥ kh√° ch·∫≠m, RPi c≈©ng g·∫ßn full b·ªô nh·ªõ r·ªìi (80%). Vi·ªác expose m·∫°ng gia ƒë√¨nh th√™m 1 app tr√™n port 4000 n·ªØa l√†m m√¨nh th·∫•y lo l·∫Øng. M√¨nh mu·ªën expose c√†ng √≠t c√†ng t·ªët (Hi·ªán t·∫°i ƒëang expose HomeAssistant r·ªìi)  D∆∞·ªõi ƒë√¢y l√† c√°c step khi m√¨nh ƒë√£ l√†m:\n2.6.1. Expose VM Oracle to HTTPs with swag T·∫°o 1 subdomain \u0026lt;REDACTED\u0026gt;.duckdns.org tr√™n https://duckdns.org (c√≥ th·ªÉ ph·∫£i d√πng VPN ƒë·ªÉ v√†o ƒë∆∞·ª£c)\nTr√™n vm Oracle cloud, s·ª≠a Security Group: expose port 80,443\nC√†i swag theo b√†i n√†y:\nFile /opt/devops/docker-compose.yml:\nversion: '3.0' services: swag: image: lscr.io/linuxserver/swag:1.30.0 container_name: swag cap_add: - NET_ADMIN environment: - PUID=1000 - PGID=1000 - TZ=Asia/Ho_Chi_Minh - URL=\u0026lt;REDACTED\u0026gt;.duckdns.org - VALIDATION=duckdns - SUBDOMAINS=wildcard, #optional - CERTPROVIDER= #optional - DNSPLUGIN=cloudflare #optional - PROPAGATION= #optional - DUCKDNSTOKEN= #optional - EMAIL= #optional - ONLY_SUBDOMAINS=false #optional - EXTRA_DOMAINS= #optional - STAGING=true #optional volumes: - /opt/devops/swag/config:/config ports: - 443:443 - 80:80 #optional restart: unless-stopped logging: driver: \u0026quot;json-file\u0026quot; options: max-size: \u0026quot;10m\u0026quot; max-file: \u0026quot;10\u0026quot; Ch√∫ √Ω m√¨nh ƒëang ch·ªâ ƒë·ªãnh STAGING=true ƒë·ªÉ test vi·ªác request staging tr∆∞·ªõc\nT·∫°o s·∫µn folder /opt/devops/swag/config\nRun:\ncd /opt/devops/ docker-compose up -d Check log nh∆∞ n√†y l√† ok:\n$ docker logs swag ... Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/\u0026lt;REDACTED\u0026gt;.duckdns.org/fullchain.pem Key is saved at: /etc/letsencrypt/live/\u0026lt;REDACTED\u0026gt;.duckdns.org/privkey.pem This certificate expires on 2023-06-18. These files will be updated when the certificate renews. NEXT STEPS: - The certificate will need to be renewed before it expires. Certbot can automatically renew the certificate in the background, but you may need to take steps to enable that functionality. See https://certbot.org/renewal-setup for instructions. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - If you like Certbot, please consider supporting our work by: * Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate * Donating to EFF: https://eff.org/donate-le - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - New certificate generated; starting nginx cont-init: info: /etc/cont-init.d/50-certbot exited 0 cont-init: info: running /etc/cont-init.d/55-permissions cont-init: info: /etc/cont-init.d/55-permissions exited 0 cont-init: info: running /etc/cont-init.d/60-renew The cert does not expire within the next day. Letting the cron script handle the renewal attempts overnight (2:08am). cont-init: info: /etc/cont-init.d/60-renew exited 0 cont-init: info: running /etc/cont-init.d/70-outdated cont-init: info: /etc/cont-init.d/70-outdated exited 0 cont-init: info: running /etc/cont-init.d/85-version-checks cont-init: info: /etc/cont-init.d/85-version-checks exited 0 cont-init: info: running /etc/cont-init.d/99-custom-files [custom-init] No custom files found, skipping... cont-init: info: /etc/cont-init.d/99-custom-files exited 0 s6-rc: info: service legacy-cont-init successfully started s6-rc: info: service init-mods: starting s6-rc: info: service init-mods successfully started s6-rc: info: service init-mods-package-install: starting s6-rc: info: service init-mods-package-install successfully started s6-rc: info: service init-mods-end: starting s6-rc: info: service init-mods-end successfully started s6-rc: info: service init-services: starting s6-rc: info: service init-services successfully started s6-rc: info: service legacy-services: starting services-up: info: copying legacy longrun cron (no readiness notification) services-up: info: copying legacy longrun fail2ban (no readiness notification) services-up: info: copying legacy longrun nginx (no readiness notification) services-up: info: copying legacy longrun php-fpm (no readiness notification) s6-rc: info: service legacy-services successfully started s6-rc: info: service 99-ci-service-check: starting [ls.io-init] done. s6-rc: info: service 99-ci-service-check successfully started Server ready Tr√™n browser access v√†o \u0026lt;REDACTED\u0026gt;.duckdns.org, N·∫øu browser b√°o unsecure, ph·∫£i ·∫•n v√†o \u0026ldquo;wish to continue\u0026rdquo; th√¨ l√† b√¨nh th∆∞·ªùng v√¨ m√¨nh ƒëang d√πng STAGING=true m√†, th·∫•y giao di·ªán \u0026ldquo;Welcome to your SWAG instance\u0026rdquo; l√† OK.\nS·ª≠a l·∫°i file: docker-compose.yml:\nSTAGING=false Xong ch·∫°y l·∫°i:\ncd /opt/devops/ docker-compose up -d Check docker logs swag ko c√≥ l·ªói g√¨ v√† N·∫øu hi·ªán Server ready l√† OK.\nV√†o 1 Browser ·∫©n danh kh√°c check: \u0026lt;REDACTED\u0026gt;.duckdns.org, ko th·∫•y tr√¨nh duy·ªát b√°o unsecure, ko c·∫ßn ph·∫£i ·∫•n v√†o \u0026ldquo;wish to continue\u0026rdquo;, th·∫•y giao di·ªán \u0026ldquo;Welcome to your SWAG instance\u0026rdquo; l√† OK\n2.6.2. Setup backend l√† YouTube-Stream-Repeater container Gi·ªù run YouTube-Stream-Repeater:\ncd /opt/devops/ git clone https://github.com/DavidBerdik/YouTube-Stream-Repeater cd YouTube-Stream-Repeater docker-compose up -d S·∫Ω th·∫•y 1 container run tr√™n port 4000\nTest b·∫±ng c√°ch: curl http://localhost:4000/meta/FBjVss96C0E\nN·∫øu tr·∫£ v·ªÅ 1 chu·ªói json l√† OK\nGi·ªù stop n√≥ ƒëi:\ncd YouTube-Stream-Repeater docker-compose stop Check docker images s·∫Ω th·∫•y ƒë√£ c√≥ images:\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE youtube-stream-repeater_server latest f18dd1edca15 8 hours ago 575MB S·ª≠a /opt/devops/docker-compose.yml ban ƒë·∫ßu, th√™m youtubestreamrepeater v√†o:\nversion: '3.0' services: ... youtubestreamrepeater: image: youtube-stream-repeater_server:latest container_name: youtubestreamrepeater restart: unless-stopped ports: - \u0026quot;4000:4000\u0026quot; logging: driver: \u0026quot;json-file\u0026quot; options: max-size: \u0026quot;10m\u0026quot; max-file: \u0026quot;10\u0026quot; 2.6.3. Setup Nginx ƒë·ªÉ point v√†o backend YouTube-Stream-Repeater container V√†o folder /opt/devops/swag/config/nginx/proxy-confs ƒë√£ ƒë∆∞·ª£c mount:\nS·ª≠a file youtube-dl-server.subdomain.conf.sample, rename th√†nh youtube-dl-server.subdomain.conf:\n# ch·ªó 1. server_name \u0026lt;REDACTED\u0026gt;.duckdns.org; # ch·ªó 2. set $upstream_app youtubestreamrepeater; # \u0026lt;======== container name set $upstream_port 4000; Run 1 th·ªÉ:\ncd /opt/devops/ docker-compose up -d V√†o Browser check l·∫°i \u0026lt;REDACTED\u0026gt;.duckdns.org n·∫øu tr·∫£ v·ªÅ {\u0026quot;detail\u0026quot;:\u0026quot;Not Found\u0026quot;} l√† OK.\nN·∫øu tr·∫£ v·ªÅ l·ªói 502 Bad gateway l√† l·ªói nha, c·∫ßn l√†m ƒë√∫ng step, trong file /opt/devops/docker-compose.yml c·∫ßn c√≥ c·∫£ swag v√† youtubestreamrepeater.\nTh·ª≠ link n√†y: https://\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E, N·∫øu tr·∫£ v·ªÅ 1 chu·ªói JSON nghƒ©a l√† OK\n2.6.4. Setup Nginx Basic Authentication Hi·ªán t·∫°i th√¨ ƒëang expose \u0026lt;REDACTED\u0026gt;.duckdns.org ra public, ai c≈©ng d√πng ƒë∆∞·ª£c.\nGi·ªù mu·ªën setup Authentication cho n√≥ ƒë·ªÉ h·∫°n ch·∫ø ng∆∞·ªùi l·∫° v√†o d√πng ch√πa üòÅ\nTa t·∫°o password cho user name ytalexa:\nsudo apt install apache2-utils cd /opt/devops/swag/config/nginx htpasswd -c /opt/devops/swag/config/nginx/.htpasswd ytalexa # Nh·∫≠p password v√†o, ko n√™n c√≥ m·∫•y k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ n√™n d√πng ch·ªØ/s·ªë/in hoa Command tr√™n l∆∞u user name v√† password v√†o 1 file /opt/devops/swag/config/nginx/.htpasswd\nS·ª≠a file: /opt/devops/swag/config/nginx/proxy-confs/youtube-dl-server.subdomain.conf, uncomment m·∫•y d√≤ng n√†y:\n... location / { # enable the next two lines for http auth auth_basic \u0026quot;Restricted\u0026quot;; auth_basic_user_file /config/nginx/.htpasswd; ... restart swag container\nTest l·∫°i t·ª´ ngo√†i v√†o n·∫øu ko c√≥ user/password th√¨ s·∫Ω nh∆∞ n√†y:\n$ curl https://\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;401 Authorization Required\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;401 Authorization Required\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; N·∫øu c√≥ user/password ƒë√∫ng format n√†y s·∫Ω tr·∫£ v·ªÅ chu·ªói JSON k·∫øt qu·∫£:\ncurl -u \u0026lt;YOUR USER NAME\u0026gt;:\u0026lt;YOUR PASSWORD\u0026gt; https://\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E ho·∫∑c: curl https://\u0026lt;YOUR USER NAME\u0026gt;:\u0026lt;YOUR PASSWORD\u0026gt;@\u0026lt;REDACTED\u0026gt;.duckdns.org/meta/FBjVss96C0E 2.6.5. Update AWS Lambda function Update Lambda function b·∫±ng file zip ·ªü ƒë√¢y: https://github.com/wes1993/YouTubeForAlexa/releases/download/09.02.2023/YouTubeForAlexaLambda.zip\nV√†o Lambda -\u0026gt; Configuration -\u0026gt; Environment variables, s·ª≠a:\nKey: get_url_service Value: youtubestream Key: ytstreamurl Value: \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;REDACTED\u0026gt;.duckdns.org Do ch√∫ng ta ƒëang s·ª≠ d·ª•ng YouTube-Stream-Repeater c·ªßa DavidBerdik ch·ª© ko ph·∫£i Home Assistant Addon c·ªßa Stephano (wes1993) n√™n C√≥ 1 ch√∫t c·∫ßn s·ª≠a n·ªØa ƒë·ªÉ m·ªçi th·ª© ch·∫°y ƒë∆∞·ª£c, l√† lambda_function.py:\nLine 1074: url = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/api/meta/\u0026quot; + id s·ª≠a th√†nh: url = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/meta/\u0026quot; + id Line 1083: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/api/dl/\u0026quot; + id + \u0026quot;?f=bestvideo\u0026quot; s·ª≠a th√†nh: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/dl/\u0026quot; + id + \u0026quot;?f=bestvideo\u0026quot; Line 1085: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/api/dl/\u0026quot; + id + \u0026quot;?f=bestaudio\u0026quot; s·ª≠a th√†nh: stream_ext = \u0026quot;https://\u0026quot; + environ['ytstreamurl'] + \u0026quot;/dl/\u0026quot; + id + \u0026quot;?f=bestaudio\u0026quot; Save function Lambda l·∫°i.\nGi·ªù test m·ªçi th·ª© s·∫Ω OK, th·ª≠ Alexa, ask My Youtube to play a video by Taylor Swift\nLog Cloudwatch:\nINIT_START Runtime Version: python:3.7.v23 Runtime Version ARN: arn:aws:lambda:us-east-1::runtime:4xxxxxxxxxxxxxxxxxc START RequestId: 2bxxxxxxxxxxxxxxxbd Version: $LATEST [INFO] 2023-03-21T07:22:39.754Z 2b5xxxabbd 400 [INFO] 2023-03-21T07:22:39.754Z 2b5xxxabbd {'message': 'List name already exists', 'type': 'NameConflict'} [INFO] 2023-03-21T07:22:39.972Z 2b5xxxabbd 400 [INFO] 2023-03-21T07:22:39.972Z 2b5xxxabbd {'message': 'List name already exists', 'type': 'NameConflict'} END RequestId: 2b5xxxabbd REPORT RequestId: 2b5xxxabbd Duration: 326.93 ms Billed Duration: 327 ms Memory Size: 512 MB Max Memory Used: 51 MB Init Duration: 550.63 ms START RequestId: 0yyyyyyyyyyyyye Version: $LATEST [INFO] 2023-03-21T07:23:06.701Z 0yyyyyyyyyyyyye {'version': '1.0', 'session': {'new': False, 'sessionId': 'amzn1.echo-api.session.b5xxxxxxxxxxxxxxbc4a8', ' [INFO] 2023-03-21T07:23:06.701Z 0yyyyyyyyyyyyye Looking for: video by Taylor swift [INFO] 2023-03-21T07:23:06.701Z 0yyyyyyyyyyyyye checking for faves [INFO] 2023-03-21T07:23:06.867Z 0yyyyyyyyyyyyye checking Add shortcuts to your favorite videos or playlists like this: [INFO] 2023-03-21T07:23:07.370Z 0yyyyyyyyyyyyye Getting YouTubeStream url for https://www.youtube.com/watch?v=h8DLofLM7No [INFO] 2023-03-21T07:23:07.370Z 0yyyyyyyyyyyyye Appending ?f=bestaudio [INFO] 2023-03-21T07:23:08.866Z 0yyyyyyyyyyyyye Sending song: Taylor Swift - Lavender Haze (Official Music Video) to Alexa END RequestId: 0yyyyyyyyyyyyye REPORT RequestId: 0yyyyyyyyyyyyye Duration: 2167.59 ms Billed Duration: 2168 ms Memory Size: 512 MB Max Memory Used: 51 MB Check log c·ªßa YouTube-Stream-Repeater nh∆∞ n√†y l√† ok, ko c√≥ l·ªói g√¨:\n$ docker logs youtubestreamrepeater INFO: Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit) INFO: Started parent process [1] INFO: Started server process [9] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [11] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [14] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [7] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [13] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [8] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [12] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Started server process [10] INFO: Waiting for application startup. INFO: Application startup complete. [QV3jQQ9_TUw]: requested with format bestaudio and subs None, configuring... [QV3jQQ9_TUw]: sending stream [QV3jQQ9_TUw]: stream will be sent as is (bestaudio) [QV3jQQ9_TUw]: download type: video/webm (.webm) INFO: 172.18.0.3:39584 - \u0026quot;GET /dl/QV3jQQ9_TUw?f=bestaudio HTTP/1.1\u0026quot; 200 OK INFO: 172.18.0.3:39156 - \u0026quot;GET /meta/XcQWfs90lFU HTTP/1.1\u0026quot; 200 OK [XcQWfs90lFU]: requested with format bestaudio and subs None, configuring... [XcQWfs90lFU]: sending stream [XcQWfs90lFU]: stream will be sent as is (bestaudio) [XcQWfs90lFU]: download type: video/webm (.webm) INFO: 172.18.0.3:55760 - \u0026quot;GET /dl/XcQWfs90lFU?f=bestaudio HTTP/1.1\u0026quot; 200 OK [QV3jQQ9_TUw]: end of data (no error reported) cleanup[QV3jQQ9_TUw]: killing downloader process (PID: 499) INFO: 172.18.0.3:35288 - \u0026quot;GET /meta/wyqwYhDXkNQ HTTP/1.1\u0026quot; 200 OK [wyqwYhDXkNQ]: requested with format bestaudio and subs None, configuring... [wyqwYhDXkNQ]: sending stream [wyqwYhDXkNQ]: stream will be sent as is (bestaudio) [wyqwYhDXkNQ]: download type: video/webm (.webm) INFO: 172.18.0.3:35290 - \u0026quot;GET /dl/wyqwYhDXkNQ?f=bestaudio HTTP/1.1\u0026quot; 200 OK Done! D√πng c√°ch n√†y m√¨nh th·∫•y ngon l√†nh h∆°n h·∫≥n. Special thanks to Stephano (wes1993) and DavidBerdik ü•∞\n3. Setup other skills Search 2 skill sau: VOV, NhacCuaTui\n3.1. VOV VOV Command to use:\nAlexa open VOV: open VOV.\nAlexa stop: stop, mu·ªën chuy·ªÉn channel kh√°c th√¨ ph·∫£i stop tr∆∞·ªõc.\nAlexa next: ƒë·ªÉ chuy·ªÉn channel 1, 2, 3, vov giao th√¥ng.\n3.2. NhacCuaTui Tr∆∞·ªõc ti√™n c·∫ßn ƒëƒÉng k√Ω account NCT, free c≈©ng dc ko c·∫ßn VIP\nSkill n√†y n√≥ ko hi·ªÉu t√™n c·ªßa Playlist, m√† mu·ªën playlist th√¨ c·∫ßn bi·∫øt playlist ƒë√≥ c√≥ s·ªë th·ª© t·ª± l√† s·ªë m·∫•y.\nV√≠ d·ª• trong account NCT m√¨nh c√≥ kho·∫£ng 9,10 playlist:\nplaylist 2: Vietnamese 2\nplaylist 3: My Music\nplaylist 4: Foreign 2\nplaylist 5: Vietnamese 1\nplaylist 6: Foreign 1\nplaylist 7: 2NE1 Parkbom\nplaylist 8: B·ª©c t∆∞·ªùng\nplaylist 9: So deep\nplaylist 10: billboard 100 (1958-2015)\nCommand to use:\nAlexa, play playlist [3] from NCT: Mu·ªën play playlist My Music t∆∞∆°ng ·ª©ng v·ªõi s·ªë th·ª© t·ª± l√† 3.\nAlexa, stop / next: mu·ªën next b√†i ti·∫øp theo c·∫ßn stop tr∆∞·ªõc.\nAlexa play Vpop from NCT: Play nh·∫°c Vi·ªát tr√™n NCT. Alexa what's hot today from NCT: play l·∫ßn l∆∞·ª£t c√°c b√†i ƒëang TOP BXH nh·∫°c Vi·ªát.\n4. Troubleshooting   N·∫øu b·∫°n mu·ªën Xem l·∫°i c√°c command m√† Alexa ƒë√£ record l·∫°i th√¨ v√†o ƒë√¢y:\nhttps://www.amazon.com/alexa-privacy/apd/rvh\nNh·ªù c√°i trang tr√™n m√† m√¨nh bi·∫øt m√¨nh ph√°t √¢m l√∫c th√¨ youtube l√∫c th√¨ you tube. L√∫c th√¨ c√≥ my news playlist, l√∫c th√¨ news playlist.\nTh·∫ø n√™n m√¨nh t√≥m l·∫°i n√™n ƒë·ªÉ invocation name l√† my youtube ch·ª© ko n√™n ƒë·ªÉ you tube. C√°c playlist th√¨ c≈©ng ƒë·ªìng b·ªô 1 ch√∫t, ƒë·ªÅu c√≥ ch·ªØ my ·ªü ƒë·∫ßu. ƒê·ªçc sai 1 ch·ªØ l√† n√≥ ko hi·ªÉu ngay, do con Youtube skill n√†y n√≥ l·ªüm.\n  N·∫øu b·ªã l·ªói ko discovery ƒë∆∞·ª£c device sau khi linked account, solution: ƒê·ªïi h·∫øt stack sang English (US), lambda sang N.Verginia region:\nhttps://community.home-assistant.io/t/alexa-integration-does-not-find-devices/193732/5\n  B·∫°n c√≥ th·ªÉ d√πng web sau ƒë·ªÉ thay cho app tr√™n ƒëi·ªán tho·∫°i:\nhttps://alexa.amazon.com\n  5. Integrate Alexa Media Player on HACS c√°i n√†y l√† ƒë·ªÉ c√≥ th·ªÉ control Alexa qua giao di·ªán Home Assistant ho·∫∑c qua programming\n5.1. Setup restart HASS\nSetting -\u0026gt; Integration -\u0026gt; Add integration:\nm√†n h√¨nh Alexa Integration Configuration:\nƒê·ª´ng v·ªôi ƒëi·ªÅn Built-in 2FA App Key v√¨ ch√∫ng ta s·∫Ω l·∫•y n√≥ b·∫±ng c√°c step sau:\nLogin v√†o https://amazon.com r·ªìi v√†o Your Account:\nV√†o ph·∫ßn 2 step verfication:\n·∫§n v√†o Add new app:\nƒê·ª´ng scan QR code, h√£y ·∫•n v√†o Cant scan barcode ƒë·ªÉ COPY l·∫•y code:\nPaste code v·ª´a l·∫•y ƒë∆∞·ª£c v√†o m√†n h√¨nh Alexa Integration Configuration:\nS·∫Ω xu·∫•t hi·ªán OTP Code 6 k√Ω t·ª± ·ªü ƒë√¢y, Copy ti·∫øp:\nPaste OTP 6 k√Ω t·ª± v√†o ƒë√¢y r·ªìi ·∫•n verify:\nQuay l·∫°i m√†n h√¨nh HASS, click check box v√† Submit:\nN√≥ s·∫Ω hi·ªán 1 web ƒë·ªÉ login b·∫±ng account Amazon:\nm√†n h√¨nh Loading:\nM√†n h√¨nh t√¨m ƒëc device echo tr√™n HASS:\nTab Setting - Integration s·∫Ω xu·∫•t hi·ªán Echo device:\nƒê·∫øn ƒë√¢y b·∫°n c√≥ th·ªÉ D√πng HASS ƒëi·ªÅu khi·ªÉn c√°c entity c·ªßa Alexa:\n5.2. Use case 5.2.1. Text to speech Gi·∫£ s·ª≠ m√¨nh mu·ªën add 1 card v√†o lovelace:\ntitle: \u0026#34;Alexa Notes\u0026#34; path: \u0026#34;alexa\u0026#34; cards: - type: \u0026#39;custom:mini-media-player\u0026#39; entity: media_player.hoang_s_echo_dot icon: \u0026#39;mdi:amazon\u0026#39; tts: platform: alexa entity_id: media_player.hoang_s_echo_dot B·∫°n c√≥ th·ªÉ ƒë√°nh text v√†o text box, sau ƒë√≥ Alexa s·∫Ω ƒë·ªçc nh·ªØng g√¨ b·∫°n vi·∫øt (Alexa ch·ªâ hi·ªÉu English)\nƒê·∫øn ƒë√¢y m√¨nh nghƒ© ƒë·∫øn 1 use case:\n M√¨nh ·ªü cty g√µ text v√†o text to speech: How are you?. Alexa ·ªü nh√† s·∫Ω ph√°t ra ti·∫øng \u0026ldquo;How are you\u0026rdquo; ƒë·∫øn v·ª£ m√¨nh. V·ª£ m√¨nh ·ªü nh√† nghe th·∫•y, ra l·ªánh cho Alexa: \u0026ldquo;Alexa, ask Reply Now say I am fine\u0026rdquo;. ALexa s·∫Ω g·ª≠i message v√†o group chat gia ƒë√¨nh \u0026ldquo;I am fine\u0026rdquo;. M√¨nh nh·∫≠n dc tin nh·∫Øn qua Telegram.  ƒê·ªÉ l√†m ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥ th√¨ c·∫ßn vi·∫øt 1 skill ƒë·ªÉ ra l·ªánh cho Alexa send message ƒë·∫øn Telegram group chat. Kh√° th√∫ v·ªã ƒë·∫•y ch·ª©. S·∫Ω l√†m ngay ·ªü ph·∫ßn 6.\n5.2.2. Command to Alexa by text Tr∆∞·ªõc gi·ªù l√† ch√∫ng ta ra l·ªánh cho Alexa qua gi·ªçng n√≥i th·ª±c s·ª±. Gi·ªù n·∫øu b·∫°n ko mu·ªën n√≥i v√¨ l∆∞·ªùi, b·∫°n mu·ªën call 1 HA script, script ƒë√≥ s·∫Ω g·ª≠i text ƒë·∫øn Alexa, Alexa nh·∫≠n ƒëo·∫°n text ƒë√≥, hi·ªÉu v√† th·ª±c hi·ªán th√¨ l√†m th·∫ø n√†o? N·∫øu l√†m ƒë∆∞·ª£c ƒëi·ªÅu n√†y s·∫Ω r·∫•t ti·ªán l·ª£i cho c√°c automation c·ªßa ch√∫ng ta v·ªõi Alexa.\nUse case d·ªÖ nh·∫•t v·ªõi m√¨nh l√†, ƒê·ªÉ ra l·ªánh Alexa m·ªü My Youtube skill v√† b·∫≠t playlist My Vietnam TV playlist, m√¨nh s·∫Ω ph·∫£i n√≥i:\n Alexa ask My Youtube to play My Vietnam TV playlist\n Nh∆∞ng l·∫ßn n√†o c≈©ng ph·∫£i n√≥i th·∫ø th√¨ h∆°i d√†i üôÑ. Ho·∫∑c l√† v·ªõi nh·ªØng channel ti·∫øng Vi·ªát th√¨ b·∫°n n√≥i ch·∫Øc ch·∫Øn Alexa s·∫Ω ko hi·ªÉu. M√¨nh mu·ªën 1 trong 3 c√°ch sau:\n ·∫§n 1 button tr√™n giao di·ªán Home Assistant, n√≥ s·∫Ω trigger 1 HA script, Alexa nghe l·ªánh l√†m theo. D√πng ƒëi·ªán tho·∫°i qu∆° nh·∫π v√†o c√°i NFC tag g·∫Øn tr√™n b√†n ƒÉn, n√≥ s·∫Ω trigger 1 HA script, Alexa nghe l·ªánh l√†m theo. D√πng t√≠nh nƒÉng Routine c·ªßa Alexa app, ch·ªâ c·∫ßn n√≥i \u0026ldquo;Alexa, TV\u0026rdquo;, n√≥ s·∫Ω trigger 1 HA script, Alexa nghe l·ªánh l√†m theo.  C·∫£ 3 c√°ch tr√™n ƒë·ªÅu kh·∫£ thi 1 khi b·∫°n ƒë√£ integrate ƒë∆∞·ª£c Alexa Media player.\nV·ªÅ c∆° b·∫£n th√¨ script trong file scripts.yaml:\n# command to alexa ask My Youtube to play my Vietnam TV playlist alexa_youtube_cd24: sequence: - service: media_player.play_media target: entity_id: media_player.hoang_s_echo_dot data: media_content_type: custom media_content_id: \u0026#39;ask My Youtube to play my Vietnam TV playlist\u0026#39; C√≤n ·ªü dashboard th√¨ call script qua button nh∆∞ n√†y:\n... - type: glance title: Alexa command entities: - entity: script.alexa_youtube_cd24 icon: \u0026#39;mdi:youtube-tv\u0026#39; name: Cƒê24h show_state: false tap_action: # confirmation: # text: Are you sure to change state of this device? action: call-service service: script.alexa_youtube_cd24 service_data: entity_id: script.alexa_youtube_cd24 V·∫≠y l√† b·∫°n ch·ªâ c·∫ßn ·∫•n button Cƒê24h l√† xong, magic?ü§£\n1 khi ƒë√£ t·∫°o ƒë∆∞·ª£c script r·ªìi th√¨ vi·ªác c√≤n l·∫°i ch·ªâ l√† setting NFC tag ho·∫∑c Routine tr√™n Alexa app n·ªØa th√¥i, c√°i ·∫•y t√πy b·∫°n ch·ªçn.\n5.2.3. D√πng service TTS trong automaion ƒê√¢y l√† 1 v√≠ d·ª• v·ªÅ vi·ªác s·ª≠ d·ª•ng service: notify.YOUR_ECHO v√† tts ƒë·ªÉ trigger Alexa n√≥i 1 random phrase n√†o ƒë√≥:\n- id: \u0026#39;welcome-home-dvfhsfef\u0026#39; alias: \u0026#34;Welcome home\u0026#34; description: Alexa say welcome home mode: restart trigger: - entity_id: binary_sensor.front_door_sensor_contact platform: state to: \u0026#39;on\u0026#39; condition: \u0026#34;{{ is_state(\u0026#39;input_boolean.nobodyhome_mode\u0026#39;, \u0026#39;on\u0026#39;) }}\u0026#34; action: - delay: 00:00:03 - data: {} service_template: \u0026#34;script.striplight_power\u0026#34; - service: notify.alexa_media_hoang_s_echo_dot data: data: type: tts message: \u0026#39;{{ [ \u0026#34;Welcome home! \u0026#34;, \u0026#34;Wow! You are home. \u0026#34;, \u0026#34;Finally! You are home. \u0026#34;, \u0026#34;Home sweet home! \u0026#34;, ]| random + [ \u0026#34;I am really very happy you here. \u0026#34;, \u0026#34;You belong here. \u0026#34;, \u0026#34;Hope you have enjoyed your day. \u0026#34;, \u0026#34;I have been waiting for you all day long. \u0026#34;, \u0026#34;Hope you had a busy day. \u0026#34;, \u0026#34;Its great to have you back. \u0026#34;, \u0026#34;Would you like some music? say, Alexa, make some noise. \u0026#34;, ]| random }}\u0026#39; 6. Create My Alexa skill that send Telegram message Nh·∫Øc l·∫°i use case:\n M√¨nh ·ªü cty g√µ text v√†o text to speech: How are you?. Alexa ·ªü nh√† s·∫Ω ph√°t ra ti·∫øng \u0026ldquo;How are you\u0026rdquo; ƒë·∫øn v·ª£ m√¨nh. V·ª£ m√¨nh ·ªü nh√† nghe th·∫•y, ra l·ªánh cho Alexa: \u0026ldquo;Alexa, ask Reply Now say I am fine\u0026rdquo;. ALexa s·∫Ω g·ª≠i message v√†o group chat gia ƒë√¨nh \u0026ldquo;I am fine\u0026rdquo;. M√¨nh nh·∫≠n dc tin nh·∫Øn qua Telegram.  ƒê·ªÉ l√†m ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥ th√¨ c·∫ßn vi·∫øt 1 skill ƒë·ªÉ ra l·ªánh cho Alexa send message ƒë·∫øn Telegram group chat. Kh√° th√∫ v·ªã ƒë·∫•y ch·ª©. S·∫Ω l√†m ngay sau ƒë√¢y\u0026hellip;\n6.1. Setup Telegram bot \u0026hellip; m√¨nh ko vi·∫øt l·∫°i ph·∫ßn n√†y v√¨ c√≥ th·ªÉ s·ª≠ d·ª•ng l·∫°i b√†i tr∆∞·ªõc: Lambda + API Gateway, Telegram Bot and Serverless Webapp\nL·∫•y ƒë∆∞·ª£c TELEGRAM_TOKEN ƒë·ªÉ s·ª≠ d·ª•ng.\nNgo√†i ra c·∫ßn l·∫•y dc CHAT_ID c·ªßa group chat m√† con bot ƒë√£ ƒë∆∞·ª£c add v√†o.\n6.2. Setup tr√™n Alexa Developer console V√†o ƒë√¢y: https://developer.amazon.com/alexa/console/ask\nT·∫°o Skill Reply Now:\nCh√∫ √Ω, ch·ªçn Model -\u0026gt; Custom, Hosting services -\u0026gt; Provision your own:\nCh·ªçn template -\u0026gt; Start from Scratch:\nJson content:\n{ \u0026#34;interactionModel\u0026#34;: { \u0026#34;languageModel\u0026#34;: { \u0026#34;invocationName\u0026#34;: \u0026#34;reply now\u0026#34;, \u0026#34;intents\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.CancelIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.HelpIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.StopIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;HelloWorldIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;SendMessageIntent\u0026#34;, \u0026#34;slots\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;AMAZON.SearchQuery\u0026#34; } ], \u0026#34;samples\u0026#34;: [ \u0026#34;send {query}\u0026#34;, \u0026#34;say {query}\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.NavigateHomeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.FallbackIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.PauseIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.ResumeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] } ], \u0026#34;types\u0026#34;: [] } } } Save Model, Build Model\nSave Endpoint, Save Model, Build Model\nCh·ªù Build Model success\n6.3. Setup tr√™n AWS T·∫°o function telegram-alexa, d√πng runtime python 3.9\nT·∫°o v√† add c√°c layer v√†o (c√≥ th·ªÉ b·∫°n s·∫Ω c·∫ßn t√¨m l·∫°i b√†i discord ƒë·ªÉ bi·∫øt c√°ch t·∫°o layer):\nL·∫•y code ·ªü ƒë√¢y paste v√†o: https://github.com/alexa-samples/skill-sample-python-helloworld-classes/tree/master/lambda/py\nCh√∫ √Ω d√≤ng cu·ªëi c√πng ph·∫£i s·ª≠a th√†nh lambda_handler = sb.lambda_handler() th√¨ m·ªõi ch·∫°y dc\nCh√∫ √Ω set c√°c environment variable: CHAT_ID, TELEGRAM_TOKEN\nCode Lambda c·∫ßn s·ª≠a l·∫°i nh∆∞ sau, v·ªÅ c∆° b·∫£n m√¨nh ch·ªâ th√™m class SendMessageIntentHandler, function send_telegram_msg(msg, chat_id):\n# -*- coding: utf-8 -*- # This sample demonstrates handling intents from an Alexa skill using the Alexa Skills Kit SDK for Python. # Please visit https://alexa.design/cookbook for additional examples on implementing slots, dialog management, # session persistence, api calls, and more. # This sample is built using the handler classes approach in skill builder. import logging, os import gettext import requests import json from botocore.exceptions import ClientError from ask_sdk_core.skill_builder import SkillBuilder from ask_sdk_core.dispatch_components import ( AbstractRequestHandler, AbstractRequestInterceptor, AbstractExceptionHandler) import ask_sdk_core.utils as ask_utils from ask_sdk_core.handler_input import HandlerInput from ask_sdk_model import Response from alexa import data logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) # Get Lambda environment variables TOKEN = os.environ.get(\u0026#39;TELEGRAM_TOKEN\u0026#39;) chat_id = os.environ.get(\u0026#39;CHAT_ID\u0026#39;) ... def send_telegram_msg(msg, chat_id): \u0026#34;\u0026#34;\u0026#34; Tries to Send Telegram message. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: BASE_URL = \u0026#34;https://api.telegram.org/bot{}\u0026#34;.format(TOKEN) url = BASE_URL + \u0026#34;/sendMessage\u0026#34; data = {\u0026#34;text\u0026#34;: msg.encode(\u0026#34;utf8\u0026#34;), \u0026#34;chat_id\u0026#34;: chat_id} requests.post(url, data) except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info( \u0026#34;Send message to Telegram command throttled, automatically retrying...\u0026#34;) send_telegram_msg(msg, chat_id) else: logger.error( \u0026#34;Send message to Telegram command Failed!\\n%s\u0026#34;, str(err)) return False except: raise class SendMessageIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for SendMessage Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;SendMessageIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG slots = handler_input.request_envelope.request.intent.slots query = slots[\u0026#34;query\u0026#34;].value msg = \u0026#34;\\\u0026#34;\u0026#34; + query + \u0026#34;\\\u0026#34;\u0026#34; + \u0026#34; - someone said.\u0026#34; # send telegram message logger.info(\u0026#34;Your messsage: \u0026#34; + query) send_telegram_msg(msg, chat_id) logger.info(\u0026#34;Your messsage to Telegram was sent\u0026#34;) speak_output = \u0026#34;Sent it\u0026#34; return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) ... sb = SkillBuilder() sb.add_request_handler(LaunchRequestHandler()) sb.add_request_handler(HelloWorldIntentHandler()) sb.add_request_handler(SendMessageIntentHandler()) sb.add_request_handler(HelpIntentHandler()) sb.add_request_handler(CancelOrStopIntentHandler()) sb.add_request_handler(FallbackIntentHandler()) sb.add_request_handler(SessionEndedRequestHandler()) # make sure IntentReflectorHandler is last so it doesn\u0026#39;t override your custom intent handlers sb.add_request_handler(IntentReflectorHandler()) sb.add_global_request_interceptor(LocalizationInterceptor()) sb.add_exception_handler(CatchAllExceptionHandler()) lambda_handler = sb.lambda_handler() Add trigger cho Lambda function, ch√∫ √Ω ch·ªçn Enable skill verification:\n6.4. Test Alexa skill G√µ v√†o ho·∫∑c n√≥i v√†o mic, n·∫øu tr·∫£ v·ªÅ k·∫øt qu·∫£ sent it l√† OK:\nCheck k·∫øt qu·∫£ tr√™n telegram chat s·∫Ω c√≥ tin nh·∫Øn con bot g·ª≠i ƒë·∫øn\nB·∫±ng c√°ch n√†y b·∫°n c√≥ th·ªÉ v√†o Cloudwatch ƒë·ªÉ xem log:\n7. Create My Alexa skill that return Lunar Calendar M√¨nh s·∫Ω h·ªèi Alexa ki·ªÉu nh∆∞:\n Me: \u0026ldquo;Alexa lunar calendar?\u0026rdquo; Alexa: \u0026ldquo;today? yesterday? tomorrow?\u0026rdquo; Me: \u0026ldquo;today\u0026rdquo; Alexa: \u0026ldquo;In Vietnam Lunar calendar, today is day: 3, month: 12\u0026rdquo; -\u0026gt; nghƒ©a l√† m√πng 3 th√°ng ch·∫°p.  ƒê·ªÉ l√†m skill n√†y v·ªÅ c∆° b·∫£n l√† d·ªÖ, quan tr·ªçng l√† google ƒëc c√°ch t√≠nh Lunar calendar m√† th√¥i\nC√°c b∆∞·ªõc t·∫°o skill gi·ªëng nh∆∞ ƒë√£ l√†m v·ªõi Reply Now\nChu·ªói JSON trong ph·∫ßn model s·∫Ω ki·ªÉu nh∆∞ n√†y:\n{ \u0026#34;interactionModel\u0026#34;: { \u0026#34;languageModel\u0026#34;: { \u0026#34;invocationName\u0026#34;: \u0026#34;lunar calendar\u0026#34;, \u0026#34;intents\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.CancelIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.HelpIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.StopIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;HelloWorldIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;LunarCalendarTodayIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [ \u0026#34;tell me lunar calendar\u0026#34;, \u0026#34;today\u0026#34;, \u0026#34;lunar calendar today\u0026#34;, \u0026#34;lunar calendar date\u0026#34;, \u0026#34;lunar date today\u0026#34;, \u0026#34;tell me lunar date\u0026#34;, \u0026#34;lunar calendar today\u0026#34;, \u0026#34;today in lunar calendar\u0026#34;, \u0026#34;what is today in lunar calendar\u0026#34;, \u0026#34;what is today in lunar date\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;LunarCalendarYesterdayIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [ \u0026#34;yesterday\u0026#34;, \u0026#34;lunar calendar yesterday\u0026#34;, \u0026#34;the day before\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;LunarCalendarTomorrowIntent\u0026#34;, \u0026#34;slots\u0026#34;: [], \u0026#34;samples\u0026#34;: [ \u0026#34;tomorrow\u0026#34;, \u0026#34;lunar calendar tomorrow\u0026#34;, \u0026#34;the day after\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.NavigateHomeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.FallbackIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.PauseIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;AMAZON.ResumeIntent\u0026#34;, \u0026#34;samples\u0026#34;: [] } ], \u0026#34;types\u0026#34;: [] } } } Step t·∫°o Lambda c≈©ng t∆∞∆°ng t·ª±, nh∆∞ng trong folder /data t·∫°o file AL.py content l·∫•y t·ª´ link n√†y\nCode Lambda s·∫Ω c√≥ nh·ªØng ph·∫ßn n√†y:\n# -*- coding: utf-8 -*- # This sample demonstrates handling intents from an Alexa skill using the Alexa Skills Kit SDK for Python. # Please visit https://alexa.design/cookbook for additional examples on implementing slots, dialog management, # session persistence, api calls, and more. # This sample is built using the handler classes approach in skill builder. import logging, os import gettext import requests import json from botocore.exceptions import ClientError from ask_sdk_core.skill_builder import SkillBuilder from ask_sdk_core.dispatch_components import ( AbstractRequestHandler, AbstractRequestInterceptor, AbstractExceptionHandler) import ask_sdk_core.utils as ask_utils from ask_sdk_core.handler_input import HandlerInput from ask_sdk_model import Response from alexa import data from alexa import AL from datetime import datetime from dateutil.relativedelta import relativedelta currentDT = datetime.now() print(\u0026#39;------------------\u0026#39;) print(\u0026#34;Run at: %s\u0026#34; % currentDT) # Get current datetime this_year = datetime.today().strftime(\u0026#39;%Y\u0026#39;) this_month = datetime.today().strftime(\u0026#39;%m\u0026#39;) current_day = currentDT.strftime(\u0026#39;%d\u0026#39;) print(\u0026#34;This year: %s. This month: %s. Current day: %s\u0026#34; % (this_year, this_month, current_day)) # Get yesterday datetime yesterday_date = relativedelta(days=-1) + currentDT yesterday_day = yesterday_date.strftime(\u0026#39;%d\u0026#39;) yesterday_month = yesterday_date.strftime(\u0026#39;%m\u0026#39;) yesterday_year = yesterday_date.strftime(\u0026#39;%Y\u0026#39;) print(\u0026#34;Yesterday year: %s. Yesterday month: %s. Yesterday day: %s\u0026#34; % (yesterday_year, yesterday_month, yesterday_day)) # Get tomorrow datetime tomorrow_date = relativedelta(days=1) + currentDT tomorrow_day = tomorrow_date.strftime(\u0026#39;%d\u0026#39;) tomorrow_month = tomorrow_date.strftime(\u0026#39;%m\u0026#39;) tomorrow_year = tomorrow_date.strftime(\u0026#39;%Y\u0026#39;) print(\u0026#34;Tomorrow year: %s. Tomorrow month: %s. Tomorrow day: %s\u0026#34; % (tomorrow_year, tomorrow_month, tomorrow_day)) logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) .... class LunarCalendarTodayIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for LunarCalendarTodayIntent Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;LunarCalendarTodayIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG lunar_date = AL.S2L(int(current_day), int(this_month), int(this_year), timeZone = 7) print(lunar_date) # DEBUG lunar_day = lunar_date[0] lunar_month = lunar_date[1] speak_output = \u0026#34;In Vietnam Lunar calendar, today is... Day: %d. Month: %d.\u0026#34; % (lunar_day, lunar_month) return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) class LunarCalendarYesterdayIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for LunarCalendarYesterdayIntent Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;LunarCalendarYesterdayIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG lunar_date = AL.S2L(int(yesterday_day), int(yesterday_month), int(yesterday_year), timeZone = 7) print(lunar_date) # DEBUG lunar_day = lunar_date[0] lunar_month = lunar_date[1] speak_output = \u0026#34;In Vietnam Lunar calendar, yesterday is... Day: %d. Month: %d.\u0026#34; % (lunar_day, lunar_month) return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) class LunarCalendarTomorrowIntentHandler(AbstractRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler for LunarCalendarTomorrowIntent Intent.\u0026#34;\u0026#34;\u0026#34; def can_handle(self, handler_input): # type: (HandlerInput) -\u0026gt; bool return ask_utils.is_intent_name(\u0026#34;LunarCalendarTomorrowIntent\u0026#34;)(handler_input) def handle(self, handler_input): # envelope = handler_input.request_envelope # DEBUG # logger.info(\u0026#34;Envelope Attr: {}\u0026#34;.format(envelope)) # DEBUG lunar_date = AL.S2L(int(tomorrow_day), int(tomorrow_month), int(tomorrow_year), timeZone = 7) print(lunar_date) # DEBUG lunar_day = lunar_date[0] lunar_month = lunar_date[1] speak_output = \u0026#34;In Vietnam Lunar calendar, tomorrow is... Day: %d. Month: %d.\u0026#34; % (lunar_day, lunar_month) return ( handler_input.response_builder .speak(speak_output) # .ask(\u0026#34;add a reprompt if you want to keep the session open for the user to respond\u0026#34;) .response ) ... sb = SkillBuilder() sb.add_request_handler(LaunchRequestHandler()) sb.add_request_handler(HelloWorldIntentHandler()) sb.add_request_handler(LunarCalendarTodayIntentHandler()) sb.add_request_handler(LunarCalendarYesterdayIntentHandler()) sb.add_request_handler(LunarCalendarTomorrowIntentHandler()) sb.add_request_handler(HelpIntentHandler()) sb.add_request_handler(CancelOrStopIntentHandler()) sb.add_request_handler(FallbackIntentHandler()) sb.add_request_handler(SessionEndedRequestHandler()) # make sure IntentReflectorHandler is last so it doesn\u0026#39;t override your custom intent handlers sb.add_request_handler(IntentReflectorHandler()) sb.add_global_request_interceptor(LocalizationInterceptor()) sb.add_exception_handler(CatchAllExceptionHandler()) lambda_handler = sb.lambda_handler() Test l·∫°i Alexa skill gi·ªëng nh∆∞ ph·∫ßn 6.4\n8. CREDIT Document Official: https://www.home-assistant.io/integrations/alexa.smart_home/#test-the-lambda-function\nEverythingSmartHome:\nAlexa with Home Assistant Local for FREE Without Subscription:\nhttps://www.youtube.com/watch?v=Ww2LI59IQ0A\u0026amp;ab_channel=EverythingSmartHome\nSauber-LabUK:\nLet\u0026rsquo;s install Amazon Alexa in our Home Assistant ‚Äì Local Setup:\nhttps://www.youtube.com/watch?v=5G733Lv-PhY\u0026amp;ab_channel=Sauber-LabUK\nK√™nh T√°y M√°y - Alexa setup: https://www.youtube.com/watch?v=sBaeXxKnSMg\u0026amp;ab_channel=K%C3%AAnhT%C3%A1yM%C3%A1y\nAlexa call script:\nhttps://youtu.be/0ElXDPw5c1Q?t=1136 Alexa include exclude entity:\nhttps://youtu.be/PhWpnc-Pvko?t=223\nMark Watt Tech:\nInstall Alexa Media player qua HACS ƒë·ªÉ control Echo th√¥ng qua HASS:\nhttps://www.youtube.com/watch?v=UsnhL2z_UUY\u0026amp;ab_channel=MarkWattTech\nScripts + Automations, Idea v·ªÅ setup cho Alexa run script c·ªßa HASS: https://www.youtube.com/watch?v=0ElXDPw5c1Q\u0026amp;ab_channel=MarkWattTech\nALEXA ACTIONABLE NOTIFICATIONS (Home Assistant + Alexa Skill):\nhttps://www.youtube.com/watch?v=uoifhNyEErE\u0026amp;ab_channel=MarkWattTech\nPaulHibbert - Alexa play Youtube music, Setup alexa skill ri√™ng c·ªßa m√¨nh v√† AWS lambda ri√™ng: post t·ª´ 2019 2020, c√≥ th·ªÉ ƒë√£ outdated:\nhttps://www.youtube.com/watch?v=mluD8kQ06NM\u0026amp;ab_channel=PaulHibbert\nhttps://www.youtube.com/watch?v=5k9OGbeek28\u0026amp;ab_channel=PaulHibbert\nGithub alexa-youtube m√† t√°c gi·∫£ ndg63276 s·∫Ω cung c·∫•p Lambda ARN url cho m·ªói ng∆∞·ªùi gi√° 3$/month: https://github.com/ndg63276/alexa-youtube\nc√°c forks tr∆∞·ªõc khi ndg63276 make private: https://github.com/cipi1965/alexa-youtube-it update t·ª´ 2018 https://github.com/FedericoHeichou/alexa-youtube : N√äN FORK V·ªÄ\nGithub alexa-youtube-skill, t√°c gi·∫£ ƒë√£ archive: https://github.com/dmhacker/alexa-youtube-skill https://imgur.com/a/H5R7L\nhttps://github.com/mbpictures/alexa-youtube-skill -\u0026gt; update t·ª´ 2019 https://github.com/waiwong614/alexa-tube -\u0026gt; update t·ª´ 2019\nGithub alexa-youtube well-maintained: https://github.com/wes1993/YouTubeForAlexa: N√äN FORK V·ªÄ file zip ƒë∆∞·ª£c extract ra repo n√†y ƒë·ªÉ xem:\nhttps://github.com/DavidBerdik/YouTubeForAlexa-Lambda\nC√°c c√°ch nghe Youtube audio tr√™n Alexa Echo dot:\nhttps://diysmarthomeplanet.com/echo-dot-youtube/\n Unofficial YouTube Skill on Github Workaround via Fire TV Pairing with the smartphone  AndrewTech k√™nh l√†m c√°c demo li√™n quan ƒë·∫øn Youtube v·ªõi Alexa:\nhttps://www.youtube.com/@Andrewstech/videos\nGithub c·ªßa K√™nh AndrewTech l√†m Youtube for Alexa:\nhttps://github.com/Imihaljko/youtube-for-alexa\nwell-maintained, web base run tr√™n Docker ƒë·ªÉ download Youtube t·ª± host: N√äN FORK V·ªÄ https://github.com/xxcodianxx/youtube-dl-web\nwell-maintained, CLI, 1 library python c√≥ 2.9k forks, 36k stars: N√äN FORK V·ªÄ https://github.com/yt-dlp/yt-dlp\nwell-maintained, stream b·∫±ng t·ª± d·ª±ng server Youtube audio server:\nhttps://github.com/codealchemist/youtube-audio-server\n1 s·ªë tip khi s·ª≠ d·ª•ng Alexa: https://www.youtube.com/watch?v=Zey1P1ZEyn4\u0026amp;ab_channel=Pocket-lint\nLunarCalendar:\nhttps://github.com/quangvinh86/SolarLunarCalendar/blob/master/LunarSolar.py\nhttps://www.informatik.uni-leipzig.de/~duc/amlich/AL.py\nRandom phrase with Alexa:\nhttps://youtu.be/3MdnRfCQcVE?t=154\nhttps://community.home-assistant.io/t/how-to-create-multiple-phrases-to-send-at-random-to-tts/19807/43\n","href":"/posts/encrypt-connect-home-assistant-to-alexa-echo-dot4/","title":"Connect Home Assistant to Amazon Alexa Echo Dot4"},{"content":"","href":"/tags/homeassistant/","title":"HomeAssistant"},{"content":"","href":"/tags/openssl/","title":"OpenSSL"},{"content":"Story B√†i to√°n l√†: vi·∫øt 2 script (Bashell v√† python3.9) ƒë·ªÉ encrypt string theo AES-256 sao cho k·∫øt qu·∫£ ph·∫£i gi·ªëng nhau gi·ªØa Bashell v√† Python3.9\nKhi ch·∫°y bashell th√¨ ƒë∆°n gi·∫£n, ch√∫ng ta c·∫ßn d√πng openssl version 1.1.1:\n$ echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -nosalt -md md5 -p *** WARNING : deprecated key derivation used. Using -iter or -pbkdf2 would be better. key=D8578EDF8458CE06FBC5BB76A58C5CA452E5CFB01236818B586F3F17BEF92D59 iv =7CE322DF4C30C37DDD74C05B170A74E6 wMdoe1EZD4tVU9Fm/AWbXg== -\u0026gt; Command tr√™n:\n Encrpyt string Hello v·ªõi password ƒë·ªÉ encrypt l√† qwerty. Output ra d∆∞·ªõi d·∫°ng base64. Ch·ªçn option -nosalt ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ execute bao nhi√™u l·∫ßn c≈©ng ra gi·ªëng nhau. (c√°i n√†y ko ƒëc recommend l·∫Øm, nh∆∞ng b√†i to√°n c·ªßa m√¨nh c·∫ßn k·∫øt qu·∫£ gi·ªëng nhau th√¨ ho·∫∑c l√† ch·ªçn nosalt ho·∫∑c l√† ch·ªâ ƒë·ªãnh r√µ gi√° tr·ªã c·ªßa Salt. ·ªû ƒë√¢y minh ch·ªçn nosalt). Ch·ªçn thu·∫≠t to√°n ƒë·ªÉ encrypt l√† md5, b·∫°n c√≥ th·ªÉ ch·ªçn SHA256 n·ªØa. Nh∆∞ng ch√∫ √Ω ch·ªâ ƒë·ªãnh r√µ d√πng c√°i n√†o nh√©. Ch·ªçn -p ƒë·ªÉ n√≥ output ra key v√† iv. key v√† iv l√† 2 chu·ªói string ƒë∆∞·ª£c t·∫°o ra b·ªüi password m√† ch√∫ng ta cung c·∫•p, openssl s·∫Ω d√πng key v√† iv ƒë·ªÉ encrypt string Hello.  C√≤n ƒë√¢y l√† k·∫øt qu·∫£ c·ªßa function python3:\n$ python3 test-aes-ok.py salt= key=D8578EDF8458CE06FBC5BB76A58C5CA452E5CFB01236818B586F3F17BEF92D59 iv=7CE322DF4C30C37DDD74C05B170A74E6 Cipher (CBC) - Base64 without salt: wMdoe1EZD4tVU9Fm/AWbXg== -\u0026gt; C√≥ th·ªÉ th·∫•y key,iv,cipher base64 qu·∫£ ƒë·ªÅu gi·ªëng nhau -\u0026gt; OK\nSau ƒë√¢y l√† function python3.9 m√¨nh ƒë√£ vi·∫øt sau khi c√≥p nh·∫∑t tr√™n internet: (C√°c b·∫°n c√≥ th·ªÉ test c·∫£ t√≠nh nƒÉng ch·ªâ ƒë·ªãnh gi√° tr·ªã cho Salt. K·∫øt qu·∫£ v·∫´n s·∫Ω ra gi·ªëng nhau)\n# https://asecuritysite.com/encryption/aes_python from Crypto.Cipher import AES import base64 import Padding import binascii \u0026#39;\u0026#39;\u0026#39; USAGE example: python3 test-aes-ok.py Install python3.9 and library: sudo add-apt-repository ppa:deadsnakes/ppa sudo apt update sudo apt install python3.9 sudo apt-get install python3-pip sudo apt install python3.9-distutils pip3 install --upgrade setuptools pip3 install --upgrade pip pip3 install --upgrade distlib pip3 install pycryptodome pip3 install Padding Description: This script try to get same result of bash shell commands: echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -S 241fa86763b85341 -md md5 -p echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -nosalt -md md5 -p \u0026#39;\u0026#39;\u0026#39; def get_key_and_iv(password, salt, klen=32, ilen=16, msgdgst=\u0026#39;md5\u0026#39;): mdf = getattr(__import__(\u0026#39;hashlib\u0026#39;, fromlist=[msgdgst]), msgdgst) password = password.encode(\u0026#39;ascii\u0026#39;, \u0026#39;ignore\u0026#39;) # convert to ASCII salt = bytearray.fromhex(salt) # convert to ASCII try: maxlen = klen + ilen keyiv = mdf((password + salt)).digest() tmp = [keyiv] while len(tmp) \u0026lt; maxlen: tmp.append( mdf(tmp[-1] + password + salt).digest() ) keyiv += tmp[-1] # append the last byte key = keyiv[:klen] iv = keyiv[klen:klen+ilen] print(\u0026#39;\\nsalt=\u0026#39; + binascii.hexlify(salt).decode(\u0026#39;ascii\u0026#39;).upper()) print(\u0026#39;key=\u0026#39; + binascii.hexlify(key).decode(\u0026#39;ascii\u0026#39;).upper()) print(\u0026#39;iv=\u0026#39; + binascii.hexlify(iv).decode(\u0026#39;ascii\u0026#39;).upper()) return key, iv except UnicodeDecodeError: return None, None def encrypt(plaintext,key, mode,salt): key,iv=get_key_and_iv(key,salt) encobj = AES.new(key,mode,iv) return(encobj.encrypt(plaintext.encode())) # \u0026#39;\u0026#39;\u0026#39; # # Test #1 with salt, same ressult to command: # # echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -S 241fa86763b85341 -md md5 -p # \u0026#39;\u0026#39;\u0026#39; # plaintext_1=\u0026#39;Hello\u0026#39; # pwd_1=\u0026#39;qwerty\u0026#39; # salt_1=\u0026#39;241fa86763b85341\u0026#39; # plaintext = Padding.appendPadding(plaintext_1,mode=\u0026#39;CMS\u0026#39;) # ciphertext = encrypt(plaintext,pwd_1,AES.MODE_CBC,salt_1) # ctext = b\u0026#39;Salted__\u0026#39; + bytearray.fromhex(salt_1) + ciphertext # print (\u0026#34;\\nCipher (CBC) - Base64 with salt:\\t\u0026#34;,base64.b64encode(bytearray(ctext)).decode()) \u0026#39;\u0026#39;\u0026#39; # Test #2 with nosalt, same ressult to command: # echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -nosalt -md md5 -p \u0026#39;\u0026#39;\u0026#39; plaintext_2=\u0026#39;Hello\u0026#39; pwd_2=\u0026#39;qwerty\u0026#39; salt_2=\u0026#39;\u0026#39; plaintext = Padding.appendPadding(plaintext_2,mode=\u0026#39;CMS\u0026#39;) ciphertext = encrypt(plaintext,pwd_2,AES.MODE_CBC,salt_2) print (\u0026#34;\\nCipher (CBC) - Base64 without salt:\\t\u0026#34;,base64.b64encode(bytearray(ciphertext)).decode()) # \u0026#39;\u0026#39;\u0026#39; # # Test #3 with nosalt, same ressult to command: # # echo -n bbbb | openssl enc -aes-256-cbc -pass pass:\u0026#34;XXXXX\u0026#34; -e -base64 -nosalt -md md5 -p # \u0026#39;\u0026#39;\u0026#39; # plaintext_3=\u0026#39;bbbb\u0026#39; # pwd_3=\u0026#39;XXXXX\u0026#39; # salt_3=\u0026#39;\u0026#39; # plaintext = Padding.appendPadding(plaintext_3,mode=\u0026#39;CMS\u0026#39;) # ciphertext = encrypt(plaintext,pwd_3,AES.MODE_CBC,salt_3) # print (\u0026#34;\\nCipher (CBC) - Base64 without salt:\\t\u0026#34;,base64.b64encode(bytearray(ciphertext)).decode()) Qu·∫£n l√Ω multi-version cho python tr√™n ubuntu18.04 https://hackersandslackers.com/multiple-python-versions-ubuntu-20-04/\nsudo add-apt-repository ppa:deadsnakes/ppa sudo apt update sudo apt install python3.9 ƒê·ªÉ xem tr√™n m√°y hi·ªán t·∫°i c√≥ nh·ªØng version python3 n√†o:\nls /usr/bin/python3* /usr/bin/python3 /usr/bin/python3-jsonpointer /usr/bin/python3.6m /usr/bin/python3m-config /usr/bin/python3-config /usr/bin/python3-jsonschema /usr/bin/python3.6m-config /usr/bin/python3-jsondiff /usr/bin/python3.6 /usr/bin/python3.9 /usr/bin/python3-jsonpatch /usr/bin/python3.6-config /usr/bin/python3m -\u0026gt; c√≥ 2 version 3.6 v√† 3.9\nSet alternative versions for Python:\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 $ update-alternatives --list python3 /usr/bin/python3.8 /usr/bin/python3.9 Gi·ªù n·∫øu mu·ªën swap gi·ªØa c√°c version python:\n$ update-alternatives --config python3 There are 2 choices for the alternative python3 (providing /usr/bin/python3). Selection Path Priority Status ------------------------------------------------------------ * 0 /usr/bin/python3.9 2 auto mode 1 /usr/bin/python3.6 1 manual mode 2 /usr/bin/python3.9 2 manual mode Press \u0026lt;enter\u0026gt; to keep the current choice[*], or type selection number: CREDIT https://asecuritysite.com/encryption/aes_python\n","href":"/bk/encrypt-openssl-aes256-python-bash/","title":"OpenSSL with Python and Bash"},{"content":"Story B√†i to√°n l√†: vi·∫øt 2 script (Bashell v√† python3.9) ƒë·ªÉ encrypt string theo AES-256 sao cho k·∫øt qu·∫£ ph·∫£i gi·ªëng nhau gi·ªØa Bashell v√† Python3.9\nKhi ch·∫°y bashell th√¨ ƒë∆°n gi·∫£n, ch√∫ng ta c·∫ßn d√πng openssl version 1.1.1:\n$ echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -nosalt -md md5 -p *** WARNING : deprecated key derivation used. Using -iter or -pbkdf2 would be better. key=D8578EDF8458CE06FBC5BB76A58C5CA452E5CFB01236818B586F3F17BEF92D59 iv =7CE322DF4C30C37DDD74C05B170A74E6 wMdoe1EZD4tVU9Fm/AWbXg== -\u0026gt; Command tr√™n:\n Encrpyt string Hello v·ªõi password ƒë·ªÉ encrypt l√† qwerty. Output ra d∆∞·ªõi d·∫°ng base64. Ch·ªçn option -nosalt ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ execute bao nhi√™u l·∫ßn c≈©ng ra gi·ªëng nhau. (c√°i n√†y ko ƒëc recommend l·∫Øm, nh∆∞ng b√†i to√°n c·ªßa m√¨nh c·∫ßn k·∫øt qu·∫£ gi·ªëng nhau th√¨ ho·∫∑c l√† ch·ªçn nosalt ho·∫∑c l√† ch·ªâ ƒë·ªãnh r√µ gi√° tr·ªã c·ªßa Salt. ·ªû ƒë√¢y minh ch·ªçn nosalt). Ch·ªçn thu·∫≠t to√°n ƒë·ªÉ encrypt l√† md5, b·∫°n c√≥ th·ªÉ ch·ªçn SHA256 n·ªØa. Nh∆∞ng ch√∫ √Ω ch·ªâ ƒë·ªãnh r√µ d√πng c√°i n√†o nh√©. Ch·ªçn -p ƒë·ªÉ n√≥ output ra key v√† iv. key v√† iv l√† 2 chu·ªói string ƒë∆∞·ª£c t·∫°o ra b·ªüi password m√† ch√∫ng ta cung c·∫•p, openssl s·∫Ω d√πng key v√† iv ƒë·ªÉ encrypt string Hello.  C√≤n ƒë√¢y l√† k·∫øt qu·∫£ c·ªßa function python3:\n$ python3 test-aes-ok.py salt= key=D8578EDF8458CE06FBC5BB76A58C5CA452E5CFB01236818B586F3F17BEF92D59 iv=7CE322DF4C30C37DDD74C05B170A74E6 Cipher (CBC) - Base64 without salt: wMdoe1EZD4tVU9Fm/AWbXg== -\u0026gt; C√≥ th·ªÉ th·∫•y key,iv,cipher base64 qu·∫£ ƒë·ªÅu gi·ªëng nhau -\u0026gt; OK\nSau ƒë√¢y l√† function python3.9 m√¨nh ƒë√£ vi·∫øt sau khi c√≥p nh·∫∑t tr√™n internet: (C√°c b·∫°n c√≥ th·ªÉ test c·∫£ t√≠nh nƒÉng ch·ªâ ƒë·ªãnh gi√° tr·ªã cho Salt. K·∫øt qu·∫£ v·∫´n s·∫Ω ra gi·ªëng nhau)\n# https://asecuritysite.com/encryption/aes_python from Crypto.Cipher import AES import base64 import Padding import binascii \u0026#39;\u0026#39;\u0026#39; USAGE example: python3 test-aes-ok.py Install python3.9 and library: sudo add-apt-repository ppa:deadsnakes/ppa sudo apt update sudo apt install python3.9 sudo apt-get install python3-pip sudo apt install python3.9-distutils pip3 install --upgrade setuptools pip3 install --upgrade pip pip3 install --upgrade distlib pip3 install pycryptodome pip3 install Padding Description: This script try to get same result of bash shell commands: echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -S 241fa86763b85341 -md md5 -p echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -nosalt -md md5 -p \u0026#39;\u0026#39;\u0026#39; def get_key_and_iv(password, salt, klen=32, ilen=16, msgdgst=\u0026#39;md5\u0026#39;): mdf = getattr(__import__(\u0026#39;hashlib\u0026#39;, fromlist=[msgdgst]), msgdgst) password = password.encode(\u0026#39;ascii\u0026#39;, \u0026#39;ignore\u0026#39;) # convert to ASCII salt = bytearray.fromhex(salt) # convert to ASCII try: maxlen = klen + ilen keyiv = mdf((password + salt)).digest() tmp = [keyiv] while len(tmp) \u0026lt; maxlen: tmp.append( mdf(tmp[-1] + password + salt).digest() ) keyiv += tmp[-1] # append the last byte key = keyiv[:klen] iv = keyiv[klen:klen+ilen] print(\u0026#39;\\nsalt=\u0026#39; + binascii.hexlify(salt).decode(\u0026#39;ascii\u0026#39;).upper()) print(\u0026#39;key=\u0026#39; + binascii.hexlify(key).decode(\u0026#39;ascii\u0026#39;).upper()) print(\u0026#39;iv=\u0026#39; + binascii.hexlify(iv).decode(\u0026#39;ascii\u0026#39;).upper()) return key, iv except UnicodeDecodeError: return None, None def encrypt(plaintext,key, mode,salt): key,iv=get_key_and_iv(key,salt) encobj = AES.new(key,mode,iv) return(encobj.encrypt(plaintext.encode())) # \u0026#39;\u0026#39;\u0026#39; # # Test #1 with salt, same ressult to command: # # echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -S 241fa86763b85341 -md md5 -p # \u0026#39;\u0026#39;\u0026#39; # plaintext_1=\u0026#39;Hello\u0026#39; # pwd_1=\u0026#39;qwerty\u0026#39; # salt_1=\u0026#39;241fa86763b85341\u0026#39; # plaintext = Padding.appendPadding(plaintext_1,mode=\u0026#39;CMS\u0026#39;) # ciphertext = encrypt(plaintext,pwd_1,AES.MODE_CBC,salt_1) # ctext = b\u0026#39;Salted__\u0026#39; + bytearray.fromhex(salt_1) + ciphertext # print (\u0026#34;\\nCipher (CBC) - Base64 with salt:\\t\u0026#34;,base64.b64encode(bytearray(ctext)).decode()) \u0026#39;\u0026#39;\u0026#39; # Test #2 with nosalt, same ressult to command: # echo -n Hello | openssl enc -aes-256-cbc -pass pass:\u0026#34;qwerty\u0026#34; -e -base64 -nosalt -md md5 -p \u0026#39;\u0026#39;\u0026#39; plaintext_2=\u0026#39;Hello\u0026#39; pwd_2=\u0026#39;qwerty\u0026#39; salt_2=\u0026#39;\u0026#39; plaintext = Padding.appendPadding(plaintext_2,mode=\u0026#39;CMS\u0026#39;) ciphertext = encrypt(plaintext,pwd_2,AES.MODE_CBC,salt_2) print (\u0026#34;\\nCipher (CBC) - Base64 without salt:\\t\u0026#34;,base64.b64encode(bytearray(ciphertext)).decode()) # \u0026#39;\u0026#39;\u0026#39; # # Test #3 with nosalt, same ressult to command: # # echo -n bbbb | openssl enc -aes-256-cbc -pass pass:\u0026#34;XXXXX\u0026#34; -e -base64 -nosalt -md md5 -p # \u0026#39;\u0026#39;\u0026#39; # plaintext_3=\u0026#39;bbbb\u0026#39; # pwd_3=\u0026#39;XXXXX\u0026#39; # salt_3=\u0026#39;\u0026#39; # plaintext = Padding.appendPadding(plaintext_3,mode=\u0026#39;CMS\u0026#39;) # ciphertext = encrypt(plaintext,pwd_3,AES.MODE_CBC,salt_3) # print (\u0026#34;\\nCipher (CBC) - Base64 without salt:\\t\u0026#34;,base64.b64encode(bytearray(ciphertext)).decode()) Qu·∫£n l√Ω multi-version cho python tr√™n ubuntu18.04 https://hackersandslackers.com/multiple-python-versions-ubuntu-20-04/\nsudo add-apt-repository ppa:deadsnakes/ppa sudo apt update sudo apt install python3.9 ƒê·ªÉ xem tr√™n m√°y hi·ªán t·∫°i c√≥ nh·ªØng version python3 n√†o:\nls /usr/bin/python3* /usr/bin/python3 /usr/bin/python3-jsonpointer /usr/bin/python3.6m /usr/bin/python3m-config /usr/bin/python3-config /usr/bin/python3-jsonschema /usr/bin/python3.6m-config /usr/bin/python3-jsondiff /usr/bin/python3.6 /usr/bin/python3.9 /usr/bin/python3-jsonpatch /usr/bin/python3.6-config /usr/bin/python3m -\u0026gt; c√≥ 2 version 3.6 v√† 3.9\nSet alternative versions for Python:\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 $ update-alternatives --list python3 /usr/bin/python3.8 /usr/bin/python3.9 Gi·ªù n·∫øu mu·ªën swap gi·ªØa c√°c version python:\n$ update-alternatives --config python3 There are 2 choices for the alternative python3 (providing /usr/bin/python3). Selection Path Priority Status ------------------------------------------------------------ * 0 /usr/bin/python3.9 2 auto mode 1 /usr/bin/python3.6 1 manual mode 2 /usr/bin/python3.9 2 manual mode Press \u0026lt;enter\u0026gt; to keep the current choice[*], or type selection number: CREDIT https://asecuritysite.com/encryption/aes_python\n","href":"/posts/encrypt-openssl-aes256-python-bash/","title":"OpenSSL with Python and Bash"},{"content":"Xu√¢n ph√¢n, Thu ph√¢n, H·∫° ch√≠, ƒê√¥ng ch√≠ Xu√¢n ph√¢n, Thu ph√¢n ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n th·ªùi ƒëi·ªÉm m·∫∑t tr·ªùi g·∫ßn x√≠ch ƒë·∫°o nh·∫•t.\nƒê√¥ng ch√≠, H·∫° ch√≠ ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n th·ªùi ƒëi·ªÉm m·∫∑t tr·ªùi xa x√≠ch ƒë·∫°o nh·∫•t.\nXu√¢n ph√¢n th∆∞·ªùng r∆°i v√†o 20/3, ƒë·ªÉ bi·∫øt gi·ªù ch√≠nh x√°c khi x√≠ch ƒë·∫°o g·∫ßn m·∫∑t tr·ªùi nh·∫•t th√¨ search google ƒëi·ªÉm ph√¢n.\nThu ph√¢n th∆∞·ªùng r∆°i v√†o 22,23/9.\nH·∫° ch√≠ th∆∞·ªùng r∆°i v√†o 20,21/6, ƒë·ªÉ bi·∫øt gi·ªù ch√≠nh x√°c khi x√≠ch ƒë·∫°o xa m·∫∑t tr·ªùi nh·∫•t th√¨ search google ƒëi·ªÉm ch√≠.\nƒê√¥ng ch√≠ th∆∞·ªùng r∆°i v√†o 21,22/12.\nD√π ·ªü v·ªã tr√≠ n√†o tr√™n tr√°i ƒë·∫•t th√¨, M·∫∑t tr·ªùi m·ªçc ch√≠nh ƒë√¥ng l·∫∑n ch√≠nh t√¢y v√†o Xu√¢n ph√¢n, Thu ph√¢n.\n4 m√πa   C√≥ 4 m√πa l√† v√¨ tr√°i ƒë·∫•t nghi√™ng, nh∆∞ d·∫•u \u0026ldquo;\\\u0026rdquo;. V√† h∆∞·ªõng nghi√™ng n√†y l√† ko ƒë·ªïi d√π ·ªü b·∫•t k√¨ v·ªã tr√≠ n√†o trong qu·ªπ ƒë·∫°o.\n  Khi v√†o H·∫° ch√≠, ch√≠ tuy·∫øn b·∫Øc ·ªü g·∫ßn m·∫∑t tr·ªùi nh·∫•t -\u0026gt; b√°n c·∫ßu b·∫Øc (VN) l√† m√πa h√® v√¨ th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng nhi·ªÅu n√™n n√≥ng l√™n.\nH√¨nh tr√™n gi·∫£i th√≠ch cho c√¢u \u0026ldquo;ƒê√™m th√°ng 5 ch∆∞a n·∫±m ƒë√£ s√°ng\u0026rdquo;: Khi m√† l√∫c ƒë√≥, n·ª≠a b√°n c·∫ßu B·∫Øc th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng l√¢u.\n  Khi v√†o ƒê√¥ng ch√≠, ch√≠ tuy·∫øn b·∫Øc ·ªü g·∫ßn xa tr·ªùi nh·∫•t -\u0026gt; b√°n c·∫ßu b·∫Øc (VN) l√† m√πa ƒë√¥ng v√¨ th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng √≠t n√™n l·∫°nh ƒëi.\nH√¨nh tr√™n gi·∫£i th√≠ch cho c√¢u \u0026ldquo;Ng√†y th√°ng 10 ch∆∞a c∆∞·ªùi ƒë√£ t·ªëi\u0026rdquo;: Khi m√† l√∫c ƒë√≥, n·ª≠a b√°n c·∫ßu B·∫Øc th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng √≠t.\n  Xu√¢n ph√¢n v√† Thu ph√¢n, x√≠ch ƒë·∫°o g·∫ßn m·∫∑t tr·ªùi nh·∫•t -\u0026gt; m√πa xu√¢n/thu.\n  Quan tr·ªçng l√† tr·ª•c nghi√™ng c·ªßa Tƒê ko ƒë·ªïi ·ªü b·∫•t k·ª≥ v·ªã tr√≠ n√†o. N√™n l·∫ßn l∆∞·ª£t c√°c n·ª≠a b√°n c·∫ßu s·∫Ω thay nhau c√≥ kho·∫£ng tg g·∫ßn m·∫∑t tr·ªùi nh·∫•t.\n  Tr√™n to√†n th·∫ø gi·ªõi, Kh√°i ni·ªám xu√¢n-h·∫°-thu-ƒë√¥ng s·∫Ω ph·ª• thu·ªôc v√†o th·ªùi ti·∫øt ch·ª© ko ph·ª• thu·ªôc v√†o th·ªùi gian. M√πa ƒë√¥ng (winter) l√† m√πa l·∫°nh.\n  ·ªü VN (Ch√≠ tuy·∫øn b·∫Øc) Kho·∫£ng th·ªùi gian l·∫°nh nh·∫•t l√† th√°ng 12 -\u0026gt; s·∫Ω l√† m√πa ƒë√¥ng.\n  ·ªü Australia (Ch√≠ tuy·∫øn nam) Kho·∫£ng th·ªùi gian l·∫°nh nh·∫•t l√† th√°ng 6 -\u0026gt; s·∫Ω l√† m√πa ƒë√¥ng.\n  ·ªü Indonesia (X√≠ch ƒë·∫°o) kh√≠ h·∫≠u ·ªïn ƒë·ªãnh, v√¨ ngay c·∫£ khi xa X√≠ch ƒë·∫°o nh·∫•t, n√≥ v·∫´n ko xa b·∫±ng c√°c n∆∞·ªõc Ch√≠ tuy·∫øn nam/b·∫Øc. Khi g·∫ßn Xƒê nh·∫•t th√¨ c√≥ th·ªÉ s·∫Ω n√≥ng nh∆∞ng ko l√¢u.\n  H∆∞·ªõng m·∫∑t tr·ªùi m·ªçc/l·∫∑n theo m√πa ·ªû VN, V√†o m√πa ƒê√¥ng, m·∫∑t tr·ªùi m·ªçc ·ªü ƒê√¥ng Nam, l·∫∑n T√¢y Nam.\nV√†o m√πa H√®, m·∫∑t tr·ªùi m·ªçc ƒê√¥ng B·∫Øc, l·∫∑n T√¢y B·∫Øc.\nV√†o m√πa Xu√¢n/Thu, m·∫∑t tr·ªùi m·ªçc ƒê√¥ng, l·∫∑n T√¢y.\nThi√™n ƒë·ªânh  V·∫≠y l√∫c n√†o m·∫∑t tr·ªùi ·ªü thi√™n ƒë·ªânh - th·∫≥ng tr√™n ƒë·∫ßu ch√∫ng ta v√† c√°i b√≥ng c·ªßa ch√∫ng ta kh√¥ng b·ªã ng·∫£ sang b√™n n√†o?\nHo√†ng tr·∫£ l·ªùi:\n-\u0026gt; V√¨ tr√°i ƒë·∫•t nghi√™ng n√™n ch·ªâ khi n√†o n∆°i ch√∫ng ta ƒë·ª©ng g·∫ßn m·∫∑t tr·ªùi nh·∫•t th√¨ m·ªõi c√≥ thi√™n ƒë·ªânh.\n-\u0026gt; Ch√∫ng ta (VN) ·ªü ch√≠ tuy·∫øn b·∫Øc n√™n v√†o H·∫° ch√≠ th√¨ m·ªõi ·ªü g·∫ßn m·∫∑t tr·ªùi nh·∫•t.\n-\u0026gt; H·∫° ch√≠ r∆°i v√†o 20,21/6.\n Tuy nhi√™n website Thi√™n vƒÉn VN n√≥i:\n T·∫°i H√† N·ªôi, hai th·ªùi ƒëi·ªÉm M·∫∑t Tr·ªùi ƒëi qua thi√™n ƒë·ªânh h√†ng nƒÉm l√† kho·∫£ng t·ª´ 26 ƒë·∫øn 29 th√°ng 5 v√† t·ª´ 15 ƒë·∫øn 18 th√°ng 7. T·∫°i TP. H·ªì Ch√≠ Minh, hai th·ªùi ƒëi·ªÉm n√†y l√† t·ª´ 15 ƒë·∫øn 16 th√°ng 4 v√† t·ª´ 27 ƒë·∫øn 29 th√°ng 8. C√°c ƒë·ªãa ph∆∞∆°ng c√≥ vƒ© ƒë·ªô kh√°c th√¨ th·ªùi ƒëi·ªÉm ƒë√≥ r∆°i v√†o nh·ªØng ng√†y kh√°c.\n CREDIT https://www.youtube.com/watch?v=gfnuZjAcx00\u0026amp;ab_channel=Deutsch-AkademiemitUlli\nhttps://www.earthspacelab.com/app/seasons/ =\u0026gt; m√¥ h√¨nh qu·ªπ ƒë·∫°o tr√°i ƒë·∫•t 3D\nhttps://thienvanvietnam.org/index.php?option=com_content\u0026amp;view=article\u0026amp;id=1894:mat-troi-moc-o-chinh-dong-va-lan-o-chinh-tay-vao-ngay-nao\u0026amp;catid=23\u0026amp;Itemid=147\n","href":"/bk/encrypt-earth-4-seasons/","title":"Earth 4 Seasons"},{"content":"Xu√¢n ph√¢n, Thu ph√¢n, H·∫° ch√≠, ƒê√¥ng ch√≠ Xu√¢n ph√¢n, Thu ph√¢n ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n th·ªùi ƒëi·ªÉm m·∫∑t tr·ªùi g·∫ßn x√≠ch ƒë·∫°o nh·∫•t.\nƒê√¥ng ch√≠, H·∫° ch√≠ ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n th·ªùi ƒëi·ªÉm m·∫∑t tr·ªùi xa x√≠ch ƒë·∫°o nh·∫•t.\nXu√¢n ph√¢n th∆∞·ªùng r∆°i v√†o 20/3, ƒë·ªÉ bi·∫øt gi·ªù ch√≠nh x√°c khi x√≠ch ƒë·∫°o g·∫ßn m·∫∑t tr·ªùi nh·∫•t th√¨ search google ƒëi·ªÉm ph√¢n.\nThu ph√¢n th∆∞·ªùng r∆°i v√†o 22,23/9.\nH·∫° ch√≠ th∆∞·ªùng r∆°i v√†o 20,21/6, ƒë·ªÉ bi·∫øt gi·ªù ch√≠nh x√°c khi x√≠ch ƒë·∫°o xa m·∫∑t tr·ªùi nh·∫•t th√¨ search google ƒëi·ªÉm ch√≠.\nƒê√¥ng ch√≠ th∆∞·ªùng r∆°i v√†o 21,22/12.\nD√π ·ªü v·ªã tr√≠ n√†o tr√™n tr√°i ƒë·∫•t th√¨, M·∫∑t tr·ªùi m·ªçc ch√≠nh ƒë√¥ng l·∫∑n ch√≠nh t√¢y v√†o Xu√¢n ph√¢n, Thu ph√¢n.\n4 m√πa   C√≥ 4 m√πa l√† v√¨ tr√°i ƒë·∫•t nghi√™ng, nh∆∞ d·∫•u \u0026ldquo;\\\u0026rdquo;. V√† h∆∞·ªõng nghi√™ng n√†y l√† ko ƒë·ªïi d√π ·ªü b·∫•t k√¨ v·ªã tr√≠ n√†o trong qu·ªπ ƒë·∫°o.\n  Khi v√†o H·∫° ch√≠, ch√≠ tuy·∫øn b·∫Øc ·ªü g·∫ßn m·∫∑t tr·ªùi nh·∫•t -\u0026gt; b√°n c·∫ßu b·∫Øc (VN) l√† m√πa h√® v√¨ th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng nhi·ªÅu n√™n n√≥ng l√™n.\nH√¨nh tr√™n gi·∫£i th√≠ch cho c√¢u \u0026ldquo;ƒê√™m th√°ng 5 ch∆∞a n·∫±m ƒë√£ s√°ng\u0026rdquo;: Khi m√† l√∫c ƒë√≥, n·ª≠a b√°n c·∫ßu B·∫Øc th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng l√¢u.\n  Khi v√†o ƒê√¥ng ch√≠, ch√≠ tuy·∫øn b·∫Øc ·ªü g·∫ßn xa tr·ªùi nh·∫•t -\u0026gt; b√°n c·∫ßu b·∫Øc (VN) l√† m√πa ƒë√¥ng v√¨ th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng √≠t n√™n l·∫°nh ƒëi.\nH√¨nh tr√™n gi·∫£i th√≠ch cho c√¢u \u0026ldquo;Ng√†y th√°ng 10 ch∆∞a c∆∞·ªùi ƒë√£ t·ªëi\u0026rdquo;: Khi m√† l√∫c ƒë√≥, n·ª≠a b√°n c·∫ßu B·∫Øc th·ªùi gian m·∫∑t tr·ªùi chi·∫øu s√°ng √≠t.\n  Xu√¢n ph√¢n v√† Thu ph√¢n, x√≠ch ƒë·∫°o g·∫ßn m·∫∑t tr·ªùi nh·∫•t -\u0026gt; m√πa xu√¢n/thu.\n  Quan tr·ªçng l√† tr·ª•c nghi√™ng c·ªßa Tƒê ko ƒë·ªïi ·ªü b·∫•t k·ª≥ v·ªã tr√≠ n√†o. N√™n l·∫ßn l∆∞·ª£t c√°c n·ª≠a b√°n c·∫ßu s·∫Ω thay nhau c√≥ kho·∫£ng tg g·∫ßn m·∫∑t tr·ªùi nh·∫•t.\n  Tr√™n to√†n th·∫ø gi·ªõi, Kh√°i ni·ªám xu√¢n-h·∫°-thu-ƒë√¥ng s·∫Ω ph·ª• thu·ªôc v√†o th·ªùi ti·∫øt ch·ª© ko ph·ª• thu·ªôc v√†o th·ªùi gian. M√πa ƒë√¥ng (winter) l√† m√πa l·∫°nh.\n  ·ªü VN (Ch√≠ tuy·∫øn b·∫Øc) Kho·∫£ng th·ªùi gian l·∫°nh nh·∫•t l√† th√°ng 12 -\u0026gt; s·∫Ω l√† m√πa ƒë√¥ng.\n  ·ªü Australia (Ch√≠ tuy·∫øn nam) Kho·∫£ng th·ªùi gian l·∫°nh nh·∫•t l√† th√°ng 6 -\u0026gt; s·∫Ω l√† m√πa ƒë√¥ng.\n  ·ªü Indonesia (X√≠ch ƒë·∫°o) kh√≠ h·∫≠u ·ªïn ƒë·ªãnh, v√¨ ngay c·∫£ khi xa X√≠ch ƒë·∫°o nh·∫•t, n√≥ v·∫´n ko xa b·∫±ng c√°c n∆∞·ªõc Ch√≠ tuy·∫øn nam/b·∫Øc. Khi g·∫ßn Xƒê nh·∫•t th√¨ c√≥ th·ªÉ s·∫Ω n√≥ng nh∆∞ng ko l√¢u.\n  H∆∞·ªõng m·∫∑t tr·ªùi m·ªçc/l·∫∑n theo m√πa ·ªû VN, V√†o m√πa ƒê√¥ng, m·∫∑t tr·ªùi m·ªçc ·ªü ƒê√¥ng Nam, l·∫∑n T√¢y Nam.\nV√†o m√πa H√®, m·∫∑t tr·ªùi m·ªçc ƒê√¥ng B·∫Øc, l·∫∑n T√¢y B·∫Øc.\nV√†o m√πa Xu√¢n/Thu, m·∫∑t tr·ªùi m·ªçc ƒê√¥ng, l·∫∑n T√¢y.\nThi√™n ƒë·ªânh  V·∫≠y l√∫c n√†o m·∫∑t tr·ªùi ·ªü thi√™n ƒë·ªânh - th·∫≥ng tr√™n ƒë·∫ßu ch√∫ng ta v√† c√°i b√≥ng c·ªßa ch√∫ng ta kh√¥ng b·ªã ng·∫£ sang b√™n n√†o?\nHo√†ng tr·∫£ l·ªùi:\n-\u0026gt; V√¨ tr√°i ƒë·∫•t nghi√™ng n√™n ch·ªâ khi n√†o n∆°i ch√∫ng ta ƒë·ª©ng g·∫ßn m·∫∑t tr·ªùi nh·∫•t th√¨ m·ªõi c√≥ thi√™n ƒë·ªânh.\n-\u0026gt; Ch√∫ng ta (VN) ·ªü ch√≠ tuy·∫øn b·∫Øc n√™n v√†o H·∫° ch√≠ th√¨ m·ªõi ·ªü g·∫ßn m·∫∑t tr·ªùi nh·∫•t.\n-\u0026gt; H·∫° ch√≠ r∆°i v√†o 20,21/6.\n Tuy nhi√™n website Thi√™n vƒÉn VN n√≥i:\n T·∫°i H√† N·ªôi, hai th·ªùi ƒëi·ªÉm M·∫∑t Tr·ªùi ƒëi qua thi√™n ƒë·ªânh h√†ng nƒÉm l√† kho·∫£ng t·ª´ 26 ƒë·∫øn 29 th√°ng 5 v√† t·ª´ 15 ƒë·∫øn 18 th√°ng 7. T·∫°i TP. H·ªì Ch√≠ Minh, hai th·ªùi ƒëi·ªÉm n√†y l√† t·ª´ 15 ƒë·∫øn 16 th√°ng 4 v√† t·ª´ 27 ƒë·∫øn 29 th√°ng 8. C√°c ƒë·ªãa ph∆∞∆°ng c√≥ vƒ© ƒë·ªô kh√°c th√¨ th·ªùi ƒëi·ªÉm ƒë√≥ r∆°i v√†o nh·ªØng ng√†y kh√°c.\n CREDIT https://www.youtube.com/watch?v=gfnuZjAcx00\u0026amp;ab_channel=Deutsch-AkademiemitUlli\nhttps://www.earthspacelab.com/app/seasons/ =\u0026gt; m√¥ h√¨nh qu·ªπ ƒë·∫°o tr√°i ƒë·∫•t 3D\nhttps://thienvanvietnam.org/index.php?option=com_content\u0026amp;view=article\u0026amp;id=1894:mat-troi-moc-o-chinh-dong-va-lan-o-chinh-tay-vao-ngay-nao\u0026amp;catid=23\u0026amp;Itemid=147\n","href":"/posts/encrypt-earth-4-seasons/","title":"Earth 4 Seasons"},{"content":"","href":"/tags/geography/","title":"Geography"},{"content":"","href":"/categories/non-tech/","title":"Non-Tech"},{"content":"","href":"/tags/docker/","title":"Docker"},{"content":"1. Story M√¨nh c√≥ 1 static site t·∫°o ra b·∫±ng GatsbyJS (1 framework base tr√™n ReactJS) ƒë√£ l√¢u, c√°ch ƒë√¢y kho·∫£ng 1 nƒÉm:\nhttps://github.com/hoangmnsd/hoangmnsd-the404blog-theme.git\nGi·ªù ƒë·ªïi m√°y mu·ªën ti·∫øp t·ª•c quay l·∫°i ph√°t tri·ªÉn app ƒë√≥. Nh∆∞ng laptop y·∫øu qu√° ko th·ªÉ npm install n·ªïi.\n-\u0026gt; M√¨nh chuy·ªÉn h∆∞·ªõng sang deploy app ƒë√≥ tr√™n 1 server ri√™ng nh∆∞ RasberryPi, nh∆∞ng l·∫°i d√≠nh ƒë·∫øn network bandwidth qu√° ch·∫≠m, npm install r·∫•t t·ªën th·ªùi gian.\n-\u0026gt; M√¨nh chuy·ªÉn ti·∫øp sang h∆∞·ªõng deploy app ƒë√≥ tr√™n 1 VM ƒë·ªÉ t·∫≠n d·ª•ng network bandwidth si√™u nhanh c·ªßa h·ªç.\nM√¥i tr∆∞·ªùng m√¨nh d√πng l√†: Canonical-Ubuntu-18.04-aarch64-2022.04.24-0.\nBan ƒë·∫ßu ch·ªâ ƒë·ªãnh install nodejs v√† npm tr√™n VM r·ªìi build \u0026amp; run lu√¥n, nh∆∞ng th·∫•y nhi·ªÅu l·ªói li√™n quan ƒë·∫øn OS qu√°, fix v√† ƒë·ªïi version nodejs ƒë·ªÉ test li√™n t·ª•c kh√° m·ªát, nhi·ªÅu khi ko bi·∫øt fix ƒë∆∞·ª£c l·ªói l√† do command n√†o\u0026hellip;\n-\u0026gt; M√¨nh quy·∫øt ƒë·ªãnh chuy·ªÉn sang Dockerize lu√¥n tr√™n VM, ƒë·ªÉ sau n√†y khi ƒë·ªïi server s·∫Ω ti·ªán t·∫°o l·∫°i m√¥i tr∆∞·ªùng. √ù t∆∞·ªüng s·∫Ω l√†:\n Synchronize code t·ª´ local l√™n VM b·∫±ng WinSCP, t·∫•t nhi√™n ch·ªâ ph·∫ßn code m√¨nh s·ª≠a th√¥i, ignore node_modules, App run b·∫±ng Docker Compose v√† ƒë∆∞·ª£c mount source code dirs t·ª´ VM v√†o Container M·ªói khi s·ª≠a code tr√™n local, s·∫Ω ƒë∆∞·ª£c sync l√™n VM qua WinSCP, t·ª´ ƒë√≥ mount t·ª± ƒë·ªông v√†o Container v√† ph·∫£n √°nh l√™n public IP/port c·ªßa VM.  Ch√∫ √Ω: Vi·ªác build ƒëi build l·∫°i Dockerfile c≈©ng s·∫Ω r·∫•t t·ªën dung l∆∞·ª£ng (c√≥ v√†i ng√†y m√† m√¨nh t·ªën 50GB r√°c) n√™n ch√∫ √Ω khi ch·ªçn server n√™n ch·ªçn ·ªï disk dung l∆∞·ª£ng l·ªõn.\nƒê√¢y l√† file package.json c·ªßa repo:\n$ cat package.json { \u0026quot;name\u0026quot;: \u0026quot;gatsby-starter-default\u0026quot;, \u0026quot;private\u0026quot;: true, \u0026quot;description\u0026quot;: \u0026quot;A simple starter to get up and developing quickly with Gatsby\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;Kyle Mathews \u0026lt;mathews.kyle@gmail.com\u0026gt;\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;@gatsby-contrib/gatsby-plugin-elasticlunr-search\u0026quot;: \u0026quot;^2.3.0\u0026quot;, \u0026quot;@hoangmnsd/gatsby-theme-amplify-cognito\u0026quot;: \u0026quot;^1.1.5\u0026quot;, \u0026quot;bootstrap\u0026quot;: \u0026quot;^4.3.1\u0026quot;, \u0026quot;disqus-react\u0026quot;: \u0026quot;^1.0.6\u0026quot;, \u0026quot;gatsby-cli\u0026quot;: \u0026quot;^3.14.2\u0026quot;, \u0026quot;gatsby\u0026quot;: \u0026quot;^2.13.13\u0026quot;, \u0026quot;gatsby-image\u0026quot;: \u0026quot;^2.2.4\u0026quot;, \u0026quot;gatsby-plugin-disqus\u0026quot;: \u0026quot;^1.1.2\u0026quot;, \u0026quot;gatsby-plugin-manifest\u0026quot;: \u0026quot;^2.2.1\u0026quot;, \u0026quot;gatsby-plugin-offline\u0026quot;: \u0026quot;^2.2.1\u0026quot;, \u0026quot;gatsby-plugin-react-helmet\u0026quot;: \u0026quot;^3.1.0\u0026quot;, \u0026quot;gatsby-plugin-sharp\u0026quot;: \u0026quot;^2.2.3\u0026quot;, \u0026quot;gatsby-plugin-transition-link\u0026quot;: \u0026quot;^1.20.5\u0026quot;, \u0026quot;gatsby-remark-copy-linked-files\u0026quot;: \u0026quot;^2.1.3\u0026quot;, \u0026quot;gatsby-remark-images\u0026quot;: \u0026quot;^3.1.4\u0026quot;, \u0026quot;gatsby-remark-prismjs\u0026quot;: \u0026quot;^3.3.2\u0026quot;, \u0026quot;gatsby-remark-responsive-iframe\u0026quot;: \u0026quot;^2.2.3\u0026quot;, \u0026quot;gatsby-remark-smartypants\u0026quot;: \u0026quot;^2.1.2\u0026quot;, \u0026quot;gatsby-source-filesystem\u0026quot;: \u0026quot;^2.1.3\u0026quot;, \u0026quot;gatsby-transformer-remark\u0026quot;: \u0026quot;^2.6.3\u0026quot;, \u0026quot;gatsby-transformer-sharp\u0026quot;: \u0026quot;^2.2.1\u0026quot;, \u0026quot;gsap\u0026quot;: \u0026quot;^3.7.1\u0026quot;, \u0026quot;jquery\u0026quot;: \u0026quot;^3.4.1\u0026quot;, \u0026quot;popper.js\u0026quot;: \u0026quot;^1.15.0\u0026quot;, \u0026quot;prismjs\u0026quot;: \u0026quot;^1.16.0\u0026quot;, \u0026quot;prop-types\u0026quot;: \u0026quot;^15.7.2\u0026quot;, \u0026quot;react\u0026quot;: \u0026quot;^16.8.6\u0026quot;, \u0026quot;react-bootstrap\u0026quot;: \u0026quot;^1.6.1\u0026quot;, \u0026quot;react-dom\u0026quot;: \u0026quot;^16.8.6\u0026quot;, \u0026quot;react-helmet\u0026quot;: \u0026quot;^5.2.1\u0026quot;, \u0026quot;react-hook-form\u0026quot;: \u0026quot;^7.12.1\u0026quot;, \u0026quot;react-social-icons\u0026quot;: \u0026quot;^4.1.0\u0026quot;, \u0026quot;sharp\u0026quot;: \u0026quot;^0.27.2\u0026quot; }, \u0026quot;devDependencies\u0026quot;: { \u0026quot;prettier\u0026quot;: \u0026quot;^1.18.2\u0026quot; }, \u0026quot;keywords\u0026quot;: [ \u0026quot;gatsby\u0026quot; ], \u0026quot;license\u0026quot;: \u0026quot;MIT\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;build\u0026quot;: \u0026quot;gatsby build\u0026quot;, \u0026quot;develop\u0026quot;: \u0026quot;gatsby develop\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;prettier --write src/**/*.{js,jsx}\u0026quot;, \u0026quot;start\u0026quot;: \u0026quot;npm run develop\u0026quot;, \u0026quot;serve\u0026quot;: \u0026quot;gatsby serve\u0026quot;, \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Write tests! -\u0026gt; https://gatsby.dev/unit-testing\\\u0026quot;\u0026quot; }, \u0026quot;repository\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;git\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://github.com/gatsbyjs/gatsby-starter-default\u0026quot; }, \u0026quot;bugs\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;https://github.com/gatsbyjs/gatsby/issues\u0026quot; } } 2. M·ªôt s·ªë l·ªói v√† command c·∫ßn l∆∞u √Ω khi c·ªë g·∫Øng npm install tr√™n VM ‚õî L·ªói Failed at the sharp@0.27.2 install script.\nƒê·ªÉ fix th√¨ m√¨nh ph·∫£i ƒë·ªïi version nodejs ƒë·ªÉ test:\nnpm install sharp@0.27.2 --save  gatsby-cli c≈©ng c√≥ nhi·ªÅu version 2/3/4 n√™n ch√∫ √Ω ch·ªçn 1 specific version:  npm i gatsby@2 $ gatsby -v Gatsby CLI version: 2.19.3 ‚õî Nhi·ªÅu l√∫c npm install th√¨ l·ªói nh∆∞ng npm install -g l·∫°i OK\n‚õî C√≥ th·ªÉ g·∫∑p l·ªói v·ªÅ permision\nSolution: https://stackoverflow.com/questions/51811564/sh-1-node-permission-denied\nnpm config set user 0 npm config set unsafe-perm true üÜó C√°ch ƒë·ªÉ x√°c ƒë·ªãnh OS architect ƒëang s·ª≠ d·ª•ng:\n$ node Welcome to Node.js v14.20.1. Type \u0026#34;.help\u0026#34; for more information. \u0026gt; process.arch \u0026#39;arm64\u0026#39; \u0026gt; process.platform \u0026#39;linux\u0026#39; \u0026gt; process.versions { node: \u0026#39;14.20.1\u0026#39;, v8: \u0026#39;8.4.371.23-node.87\u0026#39;, uv: \u0026#39;1.42.0\u0026#39;, zlib: \u0026#39;1.2.11\u0026#39;, brotli: \u0026#39;1.0.9\u0026#39;, ares: \u0026#39;1.18.1\u0026#39;, modules: \u0026#39;83\u0026#39;, nghttp2: \u0026#39;1.42.0\u0026#39;, napi: \u0026#39;8\u0026#39;, llhttp: \u0026#39;2.1.6\u0026#39;, openssl: \u0026#39;1.1.1q\u0026#39;, cldr: \u0026#39;40.0\u0026#39;, icu: \u0026#39;70.1\u0026#39;, tz: \u0026#39;2021a3\u0026#39;, unicode: \u0026#39;14.0\u0026#39; } üÜó C√°ch ƒë·ªÉ check c√°c package ph√π h·ª£p v√† latest c·ªßa npm:\nhttps://stackoverflow.com/a/71980468/9922066\n$ npm outdated Package Current Wanted Latest Location sharp MISSING 0.22.1 0.31.1 gatsby-starter-default @gatsby-contrib/gatsby-plugin-elasticlunr-search 2.4.2 2.4.2 3.0.2 gatsby-starter-default bootstrap 4.6.2 4.6.2 5.2.2 gatsby-starter-default gatsby 2.32.13 2.32.13 4.24.5 gatsby-starter-default gatsby-image 2.11.0 2.11.0 3.11.0 gatsby-starter-default gatsby-plugin-manifest 2.12.1 2.12.1 4.24.0 gatsby-starter-default gatsby-plugin-offline 2.2.10 2.2.10 5.24.0 gatsby-starter-default gatsby-plugin-react-helmet 3.10.0 3.10.0 5.24.0 gatsby-starter-default gatsby-plugin-sharp 2.14.4 2.14.4 4.24.0 gatsby-starter-default gatsby-remark-copy-linked-files 2.10.0 2.10.0 5.24.0 gatsby-starter-default gatsby-remark-images 3.11.1 3.11.1 6.24.0 gatsby-starter-default gatsby-remark-prismjs 3.13.0 3.13.0 6.24.0 gatsby-starter-default gatsby-remark-responsive-iframe 2.11.0 2.11.0 5.24.0 gatsby-starter-default gatsby-remark-smartypants 2.10.0 2.10.0 5.24.0 gatsby-starter-default gatsby-source-filesystem 2.11.1 2.11.1 4.24.0 gatsby-starter-default gatsby-transformer-remark 2.16.1 2.16.1 5.24.0 gatsby-starter-default gatsby-transformer-sharp 2.12.1 2.12.1 4.24.0 gatsby-starter-default react 16.14.0 16.14.0 18.2.0 gatsby-starter-default react-bootstrap 1.6.6 1.6.6 2.5.0 gatsby-starter-default react-dom 16.14.0 16.14.0 18.2.0 gatsby-starter-default react-helmet 5.2.1 5.2.1 6.1.0 gatsby-starter-default react-social-icons 4.1.0 4.1.0 5.15.0 gatsby-starter-default =\u0026gt; npm install sharp@0.22.1\nüÜó C√≥ th·ªÉ b·∫°n s·∫Ω nghƒ© r·∫±ng:\n N·∫øu ko th·ªÉ install https://www.npmjs.com/package/sharp/v/0.27.2 dc th√¨ h√£y chuy·ªÉn sang s·ª≠ d·ª•ng c√°i n√†o ph√π h·ª£p vs OS c·ªßa b·∫°n. ·ªû ƒë√¢y l√† ubuntu oracle cloud vm ch·ªâ c√≥ sharp@0.22.1 l√† install ƒë∆∞·ª£c th√¨ chuy·ªÉn th√¥i.\n Tuy nhi√™n, mu·ªën s·ª≠ d·ª•ng sharp@0.22.1 th√¨ c·∫ßn thay ƒë·ªïi version c·ªßa: gatsby-transformer-sharp v√† gatsby-plugin-sharp n·ªØa v√¨ ch√∫ng ƒëang point ƒë·∫øn sharp@0.27.2.\nV√¨ ko bi·∫øt version n√†o c·ªßa gatsby-transformer-sharp v√† gatsby-plugin-sharp s·∫Ω ph√π h·ª£p v·ªõi sharp@0.22.1.\n-\u0026gt; n√™n m√¨nh b·ªè cu·ªôc, quay l·∫°i t√¨m c√°ch ƒë·ªÉ install ƒë∆∞·ª£c sharp@0.27.2.\nüÜó Khi npm install -g v·∫´n l·ªói thi·∫øu sharp@0.27.2 th√¨ th·ª≠ search xem package n√†o c·∫ßn sharp@0.27.2:\nsudo grep -ril \u0026quot;0.27.2\u0026quot; üÜó C√≥ th·ªÉ vi·ªác install v·ªõi npm install sharp@0.27.2 --ignore-scripts s·∫Ω ƒë∆∞·ª£c,\nnh∆∞ng m√¨nh khuy√™n ko n√™n v√¨ install ƒë∆∞·ª£c sharp@0.27.2 theo c√°ch ƒë√≥ r·ªìi s·∫Ω l·∫°i l·ªói ·ªü ch·ªó kh√°c th√¥i\n‚õî Tr√™n server m√† run Docker, c√≥ th·ªÉ b·ªã l·ªói n√†y khi check log container:\nhttps://stackoverflow.com/questions/55763428/react-native-error-enospc-system-limit-for-number-of-file-watchers-reached\nSolution:\n# insert the new value into the system config echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf \u0026amp;\u0026amp; sudo sysctl -p # check that the new value was applied cat /proc/sys/fs/inotify/max_user_watches üÜó Check c√°c LTS version c·ªßa node, t·∫°i th·ªùi ƒëi·ªÉm n√†y th√¨ c√°c version sau l√† LTS latest t∆∞∆°ng ·ª©ng v·ªõi Node 12/14/16:\n$ nvm ls-remote v12.22.12 v14.20.1 v16.18.0 Ch√∫ng ta s·∫Ω th·ª≠ l·∫ßn l∆∞·ª£t c√°c nodejs version 12,14,16 ƒë·ªÉ test.\nN√™n d√πng c√°c LTS version c·ªßa nodejs ƒë·ªÉ s·ª≠ d·ª•ng.\n‚õî 1 s·ªë command install m√† ch·ªâ ƒë·ªãnh arch v√† platform:\nnpm cache clean --force npm install --arch=x64 --platform=linux --legacy-peer-deps nh∆∞ng l√∫c run npm start th√¨ l·∫°i l·ªói:\nError in \u0026quot;/opt/hoangmnsd-the404blog-theme/node_modules/gatsby-transformer-sharp/gatsby-node.js\u0026quot;: 'linux-x64' binaries cannot be used on the 'linux-arm64v8' platform. Please remove the 'node_modules/sharp' directory and run 'npm install' on the 'linux-arm64v8' platform. Error: 'linux-x64' binaries cannot be used on the 'linux-arm64v8' platform. Please remove the 'node_modules/sharp' directory and run 'npm ins tall' on the 'linux-arm64v8' platform. Run l·∫°i:\nnpm cache clean --force npm install --arch=arm64v8 --platform=linux --legacy-peer-deps -\u0026gt; V·∫´n l·ªói nh∆∞ng l√† l·ªói kh√°c üò≠\n-\u0026gt; T√∫m l·∫°i, cu·ªëi c√πng qu√° n·∫£n n√™n m√¨nh chuy·ªÉn sang c·ªë g·∫Øng Dockerize, t·∫°o Dockerfile v√† build image\n3. M·ªôt s·ªë l·ªói v√† command c·∫ßn l∆∞u √Ω khi Dockerize GatsbyJS app C√≥ 2 lo·∫°i image l√† Debian slim image v√† Alpine image th∆∞·ªùng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p (https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/)\nTh∆∞·ªùng th·∫•y m·ªçi ng∆∞·ªùi hay ch·ªçn Alpine image v√¨ n√≥ nh·∫π, nh∆∞ng theo b√†i vi·∫øt tr√™n th√¨ ho recommend n√™n ch·ªçn Debian slim image v·ªõi LTS nodejs version th√¨ t·ªët h∆°n.\nM√¨nh s·∫Ω th·ª≠ d√πng Alpine image tr∆∞·ªõc sau ƒë√≥ l√† Debian slim image.\nD∆∞·ªõi ƒë√¢y l√† 1 s·ªë l·ªói khi m√¨nh c·ªë g·∫Øng build image t·ª´ Dockerfile.\n3.1. L·ªói g·∫∑p khi build t·ª´ Alpine image ‚õî L·ªói ko th·ªÉ install dc sharp@0.27.2: L·ªói n√†y g·∫∑p khi d√πng FROM node:12.18.0-alpine (m√¨nh ch·ªçn v√¨ 12.18.0 l√† version m√† Netlify ƒëang ch·∫°y OK)\n\u0026gt; sharp@0.27.2 install /app/node_modules/gatsby-plugin-manifest/node_modules/sharp \u0026gt; (node install/libvips \u0026amp;\u0026amp; node install/dll-copy \u0026amp;\u0026amp; prebuild-install) || (node-gyp rebuild \u0026amp;\u0026amp; node install/dll-copy) info sharp Downloading https://github.com/lovell/sharp-libvips/releases/download/v8.10.5/libvips-8.10.5-linuxmusl-arm64v8.tar.br ERR! sharp Prebuilt libvips 8.10.5 binaries are not yet available for linuxmusl-arm64v8 info sharp Attempting to build from source via node-gyp but this may fail due to the above error info sharp Please see https://sharp.pixelplumbing.com/install for required dependencies make: Entering directory '/app/node_modules/gatsby-plugin-manifest/node_modules/sharp/build' CC(target) Release/obj.target/nothing/../../../node-addon-api/nothing.o AR(target) Release/obj.target/../../../node-addon-api/nothing.a COPY Release/nothing.a TOUCH Release/obj.target/libvips-cpp.stamp CXX(target) Release/obj.target/sharp/src/common.o In file included from ../src/common.cc:24: /usr/include/vips/vips8:35:10: fatal error: glib-object.h: No such file or directory 35 | #include \u0026lt;glib-object.h\u0026gt; | ^~~~~~~~~~~~~~~ compilation terminated. make: Leaving directory '/app/node_modules/gatsby-plugin-manifest/node_modules/sharp/build' make: *** [sharp.target.mk:139: Release/obj.target/sharp/src/common.o] Error 1 gyp ERR! build error gyp ERR! stack Error: `make` failed with exit code: 2 gyp ERR! stack at ChildProcess.onExit (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:194:23) gyp ERR! stack at ChildProcess.emit (events.js:315:20) gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12) gyp ERR! System Linux 5.4.0-1083-oracle gyp ERR! command \u0026quot;/usr/local/bin/node\u0026quot; \u0026quot;/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\u0026quot; \u0026quot;rebuild\u0026quot; gyp ERR! cwd /app/node_modules/gatsby-plugin-manifest/node_modules/sharp gyp ERR! node -v v12.18.0 gyp ERR! node-gyp -v v5.1.0 -\u0026gt; Ch·ªãu ko fix ƒë∆∞·ª£c, n√™n ƒë·ªïi version nodejs kh√°c:\n FROM node:12.22.12-alpine OK, FROM node:14.18.2-alpine3.15 OK, FROM node:16.18.0-alpine OK.  ‚õî L·ªói python2 (no such package): L·ªói n√†y g·∫∑p khi d√πng FROM node:14.20.1-alpine\nERROR: unable to select packages: python2 (no such package): required by: world[python2] The command '/bin/sh -c apk add --no-cache python2 make gcc g++ autoconf automake libtool \u0026amp;\u0026amp; apk add vips-dev fftw-dev --update-cache \u0026amp;\u0026amp; rm -fR /var/cache/apk/*' returned a non-zero code: 1 Nguy√™n nh√¢n: do b·∫°n ƒëang d√πng python2\nSolution: trong Dockerfile ph·∫£i d√πng python3\n‚õî L·ªói pngquant-bin: L·ªói n√†y g·∫∑p khi d√πng FROM node:14.20.1-alpine\n\u0026gt; pngquant-bin@6.0.1 postinstall /app/node_modules/pngquant-bin \u0026gt; node lib/install.js spawn Unknown system error -8 pngquant pre-build test failed compiling from source Error: pngquant failed to build, make sure that libpng-dev is installed at /app/node_modules/bin-build/node_modules/execa/index.js:231:11 at runMicrotasks (\u0026lt;anonymous\u0026gt;) at processTicksAndRejections (internal/process/task_queues.js:95:5) at async Promise.all (index 0) Nguy√™n nh√¢n: https://github.com/imagemin/pngquant-bin/issues/36#issuecomment-381587705\nSolution: trong Dockerfile add th√™m bash zlib-dev libpng-dev\n‚õî L·ªói Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: g·∫∑p ph·∫£i ·ªü c√°c image nh∆∞:\n FROM node:14.18.2-alpine3.15, FROM node:12.22.12-alpine  internal/modules/cjs/loader.js:1102 throw new ERR_REQUIRE_ESM(filename, parentPath, packageJsonPath); ^ Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: /usr/local/lib/node_modules/gatsby/node_modules/remark-mdx/index.js require() of ES modules is not supported. require() of /usr/local/lib/node_modules/gatsby/node_modules/remark-mdx/index.js from /usr/local/lib/node_modules/gatsby/node_modules/gatsby-recipes/dist/graphq l-server/server.js is an ES module file as it is a .js file whose nearest parent package.json contains \u0026quot;type\u0026quot;: \u0026quot;module\u0026quot; which defines all .js files in that package scope as ES modules. Instead rename index.js to end in .cjs, change the requiring code to use import(), or remove \u0026quot;type\u0026quot;: \u0026quot;module\u0026quot; from /usr/local/lib/node_modules/gatsby/node_modules/remark-mdx/package.json. -\u0026gt; https://github.com/gatsbyjs/gatsby/issues/33713\nSau khi check file package-lock.json:\nNguy√™n nh√¢n: Do gatsby-cli@2.19.3 ƒëang c·∫ßn gatsby-recipes@0.9.3.\nM√† gatsby-recipes@0.9.3 th√¨ l·∫°i c·∫ßn remark-mdx@2.0.0-next.4 v√† remark-mdxjs@2.0.0-next.4.\nSolution: n√™n tƒÉng version gatsby-cli l√™n v3:\nS·ª≠a Dockerfile th√†nh RUN npm install -g gatsby@3\nTuy nhi√™n ƒë·ª´ng nh·∫ßm l·∫´n m√¨nh ƒëang d√πng gatsby ver 3, trong container v·∫´n s·∫Ω th·∫•y gatsby ƒëang ·ªü ver 2:\nbash-5.1$ gatsby -v Gatsby CLI version: 3.14.2 Gatsby version: 2.32.13 3.2. Dockerfile cho Alpine image ƒê√¢y l√† Dockerfile ho√†n ch·ªânh m√† m√¨nh ƒë√£ d√πng ƒë·ªÉ build ƒë∆∞·ª£c image t·ª´ base image l√† NodeJS Alpine:\n#FROM node:12.22.12-alpine #FROM node:16.18.0-alpine FROM node:14.20.1-alpine RUN apk --no-cache update \\  \u0026amp;\u0026amp; apk --no-cache add python3 make gcc g++ autoconf automake libtool \\  \u0026amp;\u0026amp; apk add vips-dev fftw-dev zlib-dev libpng-dev bash --update-cache \\  \u0026amp;\u0026amp; rm -fR /var/cache/apk/* WORKDIR /app COPY ./package*.json /app/ RUN npm cache clean --force RUN npm install -g gatsby@3 RUN npm ci --only=production --no-optional # below command work with Node v14.20.1 but not Node v16.18.0 (remember to remove package-lock.json first) # i use npm install when I want to regenerate package-lock.json # RUN npm install --no-optional # fix error https://stackoverflow.com/q/67639482/9922066 RUN mkdir -p /app/.cache \u0026amp;\u0026amp; chmod -R 777 /app/.cache RUN mkdir -p /app/public \u0026amp;\u0026amp; chmod -R 777 /app/public COPY . /app RUN chown -R node:node /app USER node EXPOSE 8000 CMD [\u0026#34;gatsby\u0026#34;, \u0026#34;develop\u0026#34;, \u0026#34;-H\u0026#34;, \u0026#34;0.0.0.0\u0026#34; ] 3.3. L·ªói g·∫∑p khi build t·ª´ Debian slim image M√¨nh s·∫Ω s·ª≠ d·ª•ng image n√†y: FROM node:16.18.0-bullseye-slim\n‚õî L·ªói Error: pngquant failed to build, make sure that libpng-dev is installed\nnpm ERR! code 1 npm ERR! path /app/node_modules/pngquant-bin npm ERR! command failed npm ERR! command sh -c -- node lib/install.js npm ERR! compiling from source npm ERR! Command failed: /app/node_modules/pngquant-bin/vendor/pngquant --version npm ERR! /app/node_modules/pngquant-bin/vendor/pngquant: 1:ELF: not found npm ERR! /app/node_modules/pngquant-bin/vendor/pngquant: 1: G: not found npm ERR! npm ERR! npm ERR! pngquant pre-build test failed npm ERR! Error: pngquant failed to build, make sure that libpng-dev is installed npm ERR! at /app/node_modules/bin-build/node_modules/execa/index.js:231:11 npm ERR! at processTicksAndRejections (node:internal/process/task_queues:96:5) npm ERR! at async Promise.all (index 0) Nguy√™n nh√¢n: Dockerfile thi·∫øu libpng-dev\nSolution: trong Dockerfile add th√™m libpng-dev\n‚õî L·ªói Command failed: /app/node_modules/mozjpeg/vendor/cjpeg -version\nnpm ERR! code 1 npm ERR! path /app/node_modules/mozjpeg npm ERR! command failed npm ERR! command sh -c -- node lib/install.js npm ERR! compiling from source npm ERR! Command failed: /app/node_modules/mozjpeg/vendor/cjpeg -version npm ERR! /app/node_modules/mozjpeg/vendor/cjpeg: 1: Syntax error: word unexpected (expecting \u0026quot;)\u0026quot;) npm ERR! npm ERR! npm ERR! mozjpeg pre-build test failed npm ERR! Error: Command failed: /bin/sh -c ./configure --enable-static --disable-shared --disable-dependency-tracking --with-jpeg8 --prefix=\u0026quot;/app/node_modules/mozjpeg/vendor\u0026quot; --bindir=\u0026quot;/app/node_modules/mozjpeg/vendor\u0026quot; --libdir=\u0026quot;/app/node_modules/mozjpeg/vendor\u0026quot; npm ERR! ./configure: line 13631: PKG_PROG_PKG_CONFIG: command not found npm ERR! ./configure: line 13810: syntax error near unexpected token `libpng,' npm ERR! ./configure: line 13810: `PKG_CHECK_MODULES(libpng, libpng, HAVE_LIBPNG=1,' npm ERR! Nguy√™n nh√¢n: https://stackoverflow.com/a/64927666/9922066\nSolution: trong Dockerfile add th√™m automake autoconf libtool dpkg pkg-config libpng-dev g++\n‚õî L·ªói Can't find Python executable \u0026quot;python\u0026quot;, you can set the PYTHON env variable.. G·∫∑p khi d√πng c√°c image:\n FROM node:16.18.0-bullseye-slim, FROM node:16.18.0-alpine  Ch√∫ √Ω: N·∫øu m√¨nh gi·ªØ nguy√™n package-lock.json ƒëang tr·ªè ƒë·∫øn \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot;, package.json c≈©ng ƒëang tr·ªè ƒë·∫øn \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot;, run npm ci th√¨ s·∫Ω ko c√≥ l·ªói n√†y.\nNh∆∞ng\u0026hellip;\nQu√° tr√¨nh l√†m m√¨nh th·∫•y:\n package.json c√≥ \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot; l√† ko √Ω nghƒ©a l·∫Øm, v√¨ c√°c package gatsby-transformer-sharp v√† gatsby-plugin-sharp v·∫´n ƒëang tr·ªè ƒë·∫øn sharp@0.27.2. M√¨nh th·ª≠ x√≥a package-lock.json, S·ª≠a package.json t·ª´ \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot; th√†nh \u0026quot;sharp\u0026quot;: \u0026quot;^0.27.2\u0026quot;. V√† run RUN npm install ƒë·ªÉ generate l·∫°i package-lock.json.  S·∫Ω g·∫∑p l·ªói n√†y:\nnpm ERR! code 2 npm ERR! path /app/node_modules/leveldown npm ERR! command failed npm ERR! command sh -c -- prebuild --install npm ERR! prebuild info begin Prebuild version 4.5.0 npm ERR! prebuild ERR! configure error npm ERR! prebuild ERR! stack Error: Can't find Python executable \u0026quot;python\u0026quot;, you can set the PYTHON env variable. npm ERR! prebuild ERR! stack at PythonFinder.failNoPython (/app/node_modules/node-gyp/lib/configure.js:484:19) npm ERR! prebuild ERR! stack at PythonFinder.\u0026lt;anonymous\u0026gt; (/app/node_modules/node-gyp/lib/configure.js:406:16) npm ERR! prebuild ERR! stack at F (/app/node_modules/which/which.js:68:16) npm ERR! prebuild ERR! stack at E (/app/node_modules/which/which.js:80:29) npm ERR! prebuild ERR! stack at /app/node_modules/which/which.js:89:16 npm ERR! prebuild ERR! stack at /app/node_modules/isexe/index.js:42:5 npm ERR! prebuild ERR! stack at /app/node_modules/isexe/mode.js:8:5 npm ERR! prebuild ERR! stack at FSReqCallback.oncomplete (node:fs:202:21) K·ªÉ c·∫£ khi Dockerfile c√≥ ENV PYTHON=/usr/bin/python3 th√¨ l·∫°i s·∫Ω g·∫∑p l·ªói ti·∫øp theo SyntaxError: invalid syntax:\nnpm ERR! code 2 npm ERR! path /app/node_modules/leveldown npm ERR! command failed npm ERR! command sh -c -- prebuild --install npm ERR! prebuild info begin Prebuild version 4.5.0 npm ERR! prebuild ERR! configure error npm ERR! prebuild ERR! stack Error: Command failed: /usr/bin/python3 -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 npm ERR! prebuild ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! stack ^ npm ERR! prebuild ERR! stack SyntaxError: invalid syntax npm ERR! prebuild ERR! stack npm ERR! prebuild ERR! stack at ChildProcess.exithandler (node:child_process:402:12) npm ERR! prebuild ERR! stack at ChildProcess.emit (node:events:513:28) npm ERR! prebuild ERR! stack at maybeClose (node:internal/child_process:1100:16) npm ERR! prebuild ERR! stack at Process.ChildProcess._handle.onexit (node:internal/child_process:304:5) npm ERR! prebuild ERR! not ok npm ERR! prebuild ERR! build Error: Command failed: /usr/bin/python3 -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! build File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 npm ERR! prebuild ERR! build import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! build ^ npm ERR! prebuild ERR! build SyntaxError: invalid syntax npm ERR! prebuild ERR! build npm ERR! prebuild ERR! build at ChildProcess.exithandler (node:child_process:402:12) ƒê·∫øn l·ªói n√†y th√¨ h∆°i n·∫£n, c√≥ l·∫Ω l√† do python3 ko ph√π h·ª£p, ph·∫£i d√πng python2 chƒÉng?\nTh·ª≠ s·ª≠a l·∫°i Dockerfile RUN apt-get install python2 v√† ENV PYTHON=/usr/bin/python2 th√¨ l·∫°i g·∫∑p l·ªói d·ªã h∆°n:\nnpm ERR! code 2 npm ERR! path /app/node_modules/leveldown npm ERR! command failed npm ERR! command sh -c -- prebuild --install npm ERR! make: Entering directory '/app/node_modules/leveldown/build' npm ERR! CXX(target) Release/obj.target/leveldb/deps/leveldb/leveldb-1.18.0/db/builder.o npm ERR! make: Leaving directory '/app/node_modules/leveldown/build' npm ERR! prebuild info begin Prebuild version 4.5.0 npm ERR! prebuild http GET https://nodejs.org/download/release/v16.18.0/node-v16.18.0-headers.tar.gz npm ERR! prebuild http 200 https://nodejs.org/download/release/v16.18.0/node-v16.18.0-headers.tar.gz npm ERR! prebuild http GET https://nodejs.org/download/release/v16.18.0/SHASUMS256.txt npm ERR! prebuild http 200 https://nodejs.org/download/release/v16.18.0/SHASUMS256.txt npm ERR! (node:27) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only. npm ERR! (Use `node --trace-deprecation ...` to show where the warning was created) npm ERR! prebuild info spawn /usr/bin/python2 npm ERR! prebuild info spawn args [ npm ERR! prebuild info spawn args '/app/node_modules/node-gyp/gyp/gyp_main.py', npm ERR! prebuild info spawn args 'binding.gyp', npm ERR! prebuild info spawn args '-f', npm ERR! prebuild info spawn args 'make', npm ERR! prebuild info spawn args '-I', npm ERR! prebuild info spawn args '/app/node_modules/leveldown/build/config.gypi', npm ERR! prebuild info spawn args '-I', npm ERR! prebuild info spawn args '/app/node_modules/node-gyp/addon.gypi', npm ERR! prebuild info spawn args '-I', npm ERR! prebuild info spawn args '/app/node_modules/leveldown/16.18.0/include/node/common.gypi', npm ERR! prebuild info spawn args '-Dlibrary=shared_library', npm ERR! prebuild info spawn args '-Dvisibility=default', npm ERR! prebuild info spawn args '-Dnode_root_dir=/app/node_modules/leveldown/16.18.0', npm ERR! prebuild info spawn args '-Dnode_gyp_dir=/app/node_modules/node-gyp', npm ERR! prebuild info spawn args '-Dnode_lib_file=/app/node_modules/leveldown/16.18.0/\u0026lt;(target_arch)/node.lib', npm ERR! prebuild info spawn args '-Dmodule_root_dir=/app/node_modules/leveldown', npm ERR! prebuild info spawn args '-Dnode_engine=v8', npm ERR! prebuild info spawn args '--depth=.', npm ERR! prebuild info spawn args '--no-parallel', npm ERR! prebuild info spawn args '--generator-output', npm ERR! prebuild info spawn args 'build', npm ERR! prebuild info spawn args '-Goutput_dir=.' npm ERR! prebuild info spawn args ] npm ERR! prebuild info spawn make npm ERR! prebuild info spawn args [ 'BUILDTYPE=Release', '-C', 'build' ] npm ERR! In file included from ../deps/leveldb/leveldb-1.18.0/port/port_posix.h:47, npm ERR! from ../deps/leveldb/leveldb-1.18.0/port/port.h:16, npm ERR! from ../deps/leveldb/leveldb-1.18.0/db/filename.h:14, npm ERR! from ../deps/leveldb/leveldb-1.18.0/db/builder.cc:7: npm ERR! ../deps/leveldb/leveldb-1.18.0/port/atomic_pointer.h:211:2: error: #error Please implement AtomicPointer for this platform. npm ERR! 211 | #error Please implement AtomicPointer for this platform. npm ERR! | ^~~~~ npm ERR! make: *** [deps/leveldb/leveldb.target.mk:162: Release/obj.target/leveldb/deps/leveldb/leveldb-1.18.0/db/builder.o] Error 1 npm ERR! prebuild ERR! build error L·ªói n√†y l·∫°i ch·ªãu ko th·ªÉ hi·ªÉu, coi nh∆∞ ko fix ƒë∆∞·ª£c.\nSolution: M√¨nh chuy·ªÉn sang h∆∞·ªõng kh√°c, s·ª≠ d·ª•ng NodeJS 14:\n FROM node:14.20.1-alpine v√† FROM node:14.20.1-bullseye-slim\nth√¨ l·∫°i OK ko c√≥ l·ªói g√¨, trong container khi start l√™n m√¨nh s·∫Ω l·∫•y dc package-lock.json ra OK. ü§£  Nh∆∞ v·∫≠y d∆∞·ªùng nh∆∞ version NodeJS ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn c√¢u l·ªánh npm install.\n-\u0026gt; K·∫øt lu·∫≠n l·∫°i l√† m√¨nh n√™n s·ª≠ d·ª•ng NodeJS v14.20.1.\n3.4. Dockerfile cho Debian slim image ƒê√¢y l√† Dockerfile ho√†n ch·ªânh m√† m√¨nh ƒë√£ d√πng ƒë·ªÉ build ƒë∆∞·ª£c image t·ª´ base image l√† NodeJS Deiam slim:\n# FROM node:12.22.12-bullseye-slim # FROM node:16.18.0-bullseye-slim FROM node:14.20.1-bullseye-slim RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\  automake \\  autoconf \\  build-essential \\  software-properties-common \\  libtool \\  dpkg \\  pkg-config \\  libpng-dev \\  g++ \\  bash \\  python3 \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY ./package*.json /app/ RUN npm cache clean --force RUN npm install -g gatsby@3 RUN npm ci --only=production --no-optional # below command work with Node v14.20.1 but not Node v16.18.0 (remember to remove package-lock.json first) # i use npm install when I want to regenerate package-lock.json # RUN npm install --no-optional # fix error https://stackoverflow.com/q/67639482/9922066 RUN mkdir -p /app/.cache \u0026amp;\u0026amp; chmod -R 777 /app/.cache RUN mkdir -p /app/public \u0026amp;\u0026amp; chmod -R 777 /app/public COPY . /app RUN chown -R node:node /app USER node EXPOSE 8000 CMD [\u0026#34;gatsby\u0026#34;, \u0026#34;develop\u0026#34;, \u0026#34;-H\u0026#34;, \u0026#34;0.0.0.0\u0026#34; ] 4. Docker Compose file Sau khi ƒë√£ c√≥ Dockerfile th√¨ c√≥ th·ªÉ build dc image:\ncd /hoangmnsd-the404blog-theme/ docker build -t gatsby-app -f Dockerfile.dev . Image ƒë∆∞·ª£c build kh√° n·∫∑ng ~3.6GB:\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE gatsby-app latest ddd14693d9f3 34 minutes ago 3.62GB node 16.18.0-bullseye-slim 1e696a126824 3 days ago 185MB node 16.18.0-alpine e20db79e2fb9 3 days ago 114MB T·∫°o file docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: web: image: gatsby-app:latest container_name: gatsby-app ports: - \u0026#34;8000:8000\u0026#34; volumes: # map all dir but exclude `node_modules, .cache, public` dir - /app/node_modules - /app/.cache/ - /app/public/ - .:/app environment: - NODE_ENV=development Run docker-compose up -d v√† check logs c·ªßa container docker logs gatsby-app ko c√≥ l·ªói g√¨ ƒë·∫∑c bi·ªát l√† OK\nTruy c·∫≠p v√†o http://VM_PUBLIC_IP:8000 th√¨ b·∫°n s·∫Ω th·∫•y m√¥i tr∆∞·ªùng develop ƒë√£ ok\nB√¢y gi·ªù d√πng WinSCP sync folder t·ª´ local l√™n VM:\n Ch·ªçn ch·∫ø ƒë·ªô \u0026ldquo;Keeping remote directory up to date \u0026hellip;\u0026rdquo; Ch·ªçn Synchronize options: Delete files, exclude c√°c folder sau: .git, node_modules, .cache, public  Gi·ªù m·ªü VS code, edit c√°c file trong /content/, /src/, bao g·ªìm c·∫£ c√°c file nh∆∞ /gatsby-config.js xem sao nh√©.\nM·ªçi th·ª© s·∫Ω ƒë∆∞·ª£c sync t·ª± ƒë·ªông v√†o c√°c volume c·ªßa gatsby container, sau ƒë√≥ ph·∫£n √°nh ngay l√™n website c·ªßa b·∫°n ·ªü ƒë∆∞·ªùng d·∫´n http://VM_PUBLIC_IP:8000.\nC·∫ßn ch√∫ √Ω ƒë√¢y l√† qu√° tr√¨nh Develop th√¥i nh√©. Trong m√¥i tr∆∞·ªùng Production, ·ªü file Dockerfile - sau khi build ra ƒë∆∞·ª£c public folder, b·∫°n n√™n ƒë∆∞a artifacts trong public v√†o Nginx image v√† ch·ªâ release Container Nginx ƒë√≥ th√¨ s·∫Ω nh·∫π h∆°n.\n5. Use multi-stage to cache node_modules Khi build c√°c ·ª©ng d·ª•ng NodeJS lu√¥n ph·∫£i ƒë·ªÉ √Ω ƒë·∫øn node_modules, ƒë√¢y l√† folder s·∫Ω t·ªën nhi·ªÅu th·ªùi gian ƒë·ªÉ install v√† n√≥ c≈©ng kh√° n·∫∑ng, l√™n t·ªõi h∆°n 1 Gb.\nS·ª≠ d·ª•ng multi-stage trong Dockerfile gi√∫p ch√∫ng ta khi build l·∫°i c√°c image m·ªõi s·∫Ω ƒë·ª° t·ªën th·ªùi gian h∆°n.\nV·ªõi Dockerfile nh∆∞ ·ªü ph·∫ßn tr∆∞·ªõc th√¨ m·ªói khi ph·∫£i build l·∫°i image ta s·∫Ω t·ªën 120s ƒë·∫øn 200s cho ph·∫ßn run npm ci.\nM·∫∑c d√π npm ci ƒë∆∞·ª£c recommend s·ª≠ d·ª•ng h∆°n so v·ªõi npm install nh∆∞ng nh∆∞·ª£c ƒëi·ªÉm c·ªßa npm ci l·∫°i l√† n√≥ lu√¥n x√≥a node_modules tr∆∞·ªõc khi install packages.\nV√¨ th·∫ø trong Dockerfile m·ªõi s·∫Øp ƒë∆∞·ª£c vi·∫øt s·∫Ω ko d√πng npm ci n·ªØa m√† quay l·∫°i v·ªõi npm install.\nTuy nhi√™n ta bi·∫øt npm install c√≥ 1 khuy·∫øt ƒëi·ªÉm l√† n√≥ s·∫Ω t·ª± ƒë·ªông update package.json v√† package-lock.json ƒë√¢y l√† ƒëi·ªÅu m√¨nh ko mong mu·ªën.\nƒê·ªÉ kh·∫Øc ƒëi·ªÉm khuy·∫øt ƒëi·ªÉm n√†y th√¨ B·∫°n c√≥ th·ªÉ specific version c·ªßa c√°c package trong package.json b·∫±ng c√°ch ki·ªÉu nh∆∞ n√†y:\n\u0026#34;react\u0026#34;: \u0026#34;^16.0.0\u0026#34; // carat: allow 16.1.0 \u0026#34;react\u0026#34;: \u0026#34;~16.0.0\u0026#34; // tilde: allow 16.0.1 \u0026#34;react\u0026#34;: \u0026#34;16.0.0\u0026#34; // exact: only 16.0.0 Trong project n√†y c·ªßa m√¨nh th√¨ ko c·∫ßn n√™n m√¨nh s·∫Ω ƒë·ªÉ nguy√™n package.json nh∆∞ n√≥ hi·ªán t·∫°i.\nM√¨nh s·∫Ω t√°ch Dockerfile th√†nh 2 file ri√™ng:\n File ƒë·∫ßu ti√™n l√† Dockerfile.dev-bullseye-node-cache ƒë·ªÉ build node-cache ch·ª©a node_modules.\nDockerfile.dev-bullseye-node-cache:  FROM node:14.20.1-bullseye-slim as build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ automake \\ autoconf \\ build-essential \\ software-properties-common \\ libtool \\ dpkg \\ pkg-config \\ libpng-dev \\ g++ \\ bash \\ python3 \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY ./package*.json /app/ RUN npm install -g gatsby@3.14.2 --no-optional --no-audit --progress=false RUN npm install --no-optional --no-audit --progress=false FROM alpine as release COPY --from=build /app/node_modules ./node_modules # because gatsby@3.14.2 is global package, it's node_modules store in `/usr/local/lib/node_modules`, i need to cache it to reuse later COPY --from=build /usr/local/lib/node_modules /usr/local/lib/global_node_modules  File th·ª© hai l√† Dockerfile.dev-bullseye-use-cache ƒë·ªÉ build image s·ª≠ d·ª•ng:  FROM node-cache:latest as cache FROM node:14.20.1-bullseye-slim as build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ automake \\ autoconf \\ build-essential \\ software-properties-common \\ libtool \\ dpkg \\ pkg-config \\ libpng-dev \\ g++ \\ bash \\ python3 \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY ./package*.json /app/ COPY --from=cache /node_modules /app/node_modules # because gatsby@3.14.2 is global package, i need to bring it from node-cache to global package path: /usr/local/lib/node_modules COPY --from=cache /usr/local/lib/global_node_modules /usr/local/lib/node_modules # gatsby@3.14.2 work as a binary at /usr/local/bin/gatsby - a symlink from /usr/local/lib/node_modules/gatsby/cli.js RUN ln -s /usr/local/lib/node_modules/gatsby/cli.js /usr/local/bin/gatsby RUN npm install --no-optional --no-audit --progress=false --prefer-offline # fix error https://stackoverflow.com/q/67639482/9922066 RUN mkdir -p /app/.cache \u0026amp;\u0026amp; chmod -R 777 /app/.cache RUN mkdir -p /app/public \u0026amp;\u0026amp; chmod -R 777 /app/public COPY . /app RUN chown -R node:node /app USER node EXPOSE 8000 CMD [\u0026quot;gatsby\u0026quot;, \u0026quot;develop\u0026quot;, \u0026quot;-H\u0026quot;, \u0026quot;0.0.0.0\u0026quot; ] ƒê·ªÉ s·ª≠ d·ª•ng, ƒë·∫ßu ti√™n c·∫ßn build images:\ncd /hoangmnsd-the404blog-theme/ DOCKER_BUILDKIT=1 docker build -t node-cache -f Dockerfile.dev-bullseye-node-cache . # now you have image `node-cache:latest` for re-using in below build step # it may take 382.1s in total to FINISHED, but you only need to run it once docker build -t gatsby-app -f Dockerfile.dev-bullseye-use-cache . # at first time, it may take 120s in `npm install` step # now you have image `gatsby-app:latest` If you change something in package.json, just rebuild the gatsby-app image:\ndocker build -t gatsby-app -f Dockerfile.dev-bullseye-use-cache . # from second time, it may take 25s in `npm install` step Run Docker Compose:\ncd /hoangmnsd-the404blog-theme/ docker-compose up -d # Your app is running on http://localhost:8000  Now, if you change something in ./src/* or ./content/* or ./gatsby-*, docker-compose automaticly mount them to container, no need action on re-build/ re-run container.\n‚õî L·ªói: Tr√™n 1 s·ªë OS (d√πng ki·∫øn tr√∫c arm64), c√≥ v·∫ª Docker BuildKit ko nh·∫≠n ra local image: https://github.com/docker/cli/issues/3286.\nD·∫´n ƒë·∫øn l·ªói n√†y:\n$ DOCKER_BUILDKIT=1 docker build -t gatsby-app -f Dockerfile.dev-bullseye-use-cache . [+] Building 0.9s (5/5) FINISHED =\u0026gt; [internal] load build definition from Dockerfile.dev-bullseye-use-cache 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 61B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 34B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/node:14.20.1-bullseye-slim 0.4s =\u0026gt; ERROR [internal] load metadata for docker.io/library/node-cache:latest 0.8s =\u0026gt; [auth] library/node-cache:pull token for registry-1.docker.io 0.0s ------ \u0026gt; [internal] load metadata for docker.io/library/node-cache:latest: ------ failed to solve with frontend dockerfile.v0: failed to create LLB definition: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed N√≥ c·ª© chƒÉm chƒÉm t√¨m image docker.io/library/node-cache:latest tuy nhi√™n ƒë√≥ l√† 1 image local c·ªßa m√¨nh.\nC√≥ 1 c√°ch workaround: https://github.com/docker/cli/issues/3286#issuecomment-1168394369.\nHi·ªán m√¨nh ch∆∞a th·∫•y c√°ch fix ph√π h·ª£p v√¨ m√¨nh ko mu·ªën d√πng --platform=linux/arm64 trong c√°c command build c≈©ng nh∆∞ trong Dockerfile.\n‚õî L·ªói: exec user process caused: exec format error -\u0026gt; C√πng l√† Ubuntu18.04 OS, nh∆∞ng n·∫øu CPU kh√°c nhau th√¨ c√πng 1 command v·∫´n c√≥ th·ªÉ x·∫£y ra t√¨nh tr·∫°ng: \u0026ldquo;M√°y n√†y l·ªói nh∆∞ng m√°y kia OK\u0026rdquo;.\nL√† do CPU c·ªßa b·∫°n ƒëang d√πng arm64 architecture. Trong khi ph·∫ßn l·ªõn docker image dc thi·∫øt k·∫ø cho amd64 architecture.\nhttps://forums.docker.com/t/daemon-error-responses-exec-format-error-and-container-is-restarting-wait-until-the-container-is-running/110385/2\n6. Ph√¢n bi·ªát CMD v√† ENTRYPOINT trong Dockerfile 1 b√†i vi·∫øt d·ªÖ hi·ªÉu: https://phoenixnap.com/kb/docker-cmd-vs-entrypoint\nT√≥m t·∫Øt:\n CMD th∆∞·ªùng ƒë·ªÉ cu·ªëi c√πng trong Dockerfile, d√πng ƒë·ªÉ run c√°c c√¢u l·ªánh default khi container ƒë∆∞·ª£c run, c√≥ th·ªÉ b·ªã ghi ƒë√® b·ªüi docker run. ENTRYPOINT c≈©ng th∆∞·ªùng ƒë·ªÉ cu·ªëi c√πng trong Dockerfile, d√πng ƒë·ªÉ run c√°c c√¢u l·ªánh b·∫Øt bu·ªôc ph·∫£i ch·∫°y khi container ƒë∆∞·ª£c run.  Ex:\nFROM ubuntu MAINTAINER sofija RUN apt-get update ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;Hello\u0026quot;] CMD [\u0026quot;World1234\u0026quot;] Test:\nsudo docker build . # run `docker run` sudo docker run [container_name] # output: Hello World1234 # run `docker run` with command sudo docker run [container_name] Hoangmnsd # output: Hello Hoangmnsd # -\u0026gt; b·ªüi v√¨ Hoangmnsd ƒë√£ ghi ƒë√® l√™n CMD, nh∆∞ng n√≥ ko th·ªÉ ghi ƒë√® l√™n ENTRYPOINT, n√™n v·∫´n c√≥ ch·ªØ Hello 7. Run on Gitlab CI [PENDING] Ph·∫ßn ti·∫øp theo m√¨nh s·∫Ω c·ªë g·∫Øng ƒë∆∞a ·ª©ng d·ª•ng n√†y l√™n Gitlab CI. √ù t∆∞·ªüng s·∫Ω l√†:\n D√πng VM l√†m Gitlab Runner Vi·∫øt th√™m v√†o Dockerfile ph·∫ßn build ra public folder v√† s·ª≠ d·ª•ng n√≥. Push code l√™n Gitlab, file .gitlab-ci s·∫Ω trigger Runner build ra image v√† push image ƒë√≥ l√™n 1 Registry n√†o ƒë√≥ (C√≥ th·ªÉ ACR). ·ªû d∆∞·ªõi local c√≥ th·ªÉ pull image v·ª´a build v·ªÅ v√† s·ª≠ d·ª•ng.\nƒê√¢y l√† 1 lu·ªìng c∆° b·∫£n c·ªßa CI.  \u0026hellip;\n8. Best practices 8.1. D√πng npm ci thay cho npm install N√™n xem x√©t vi·ªác trong Dockerfile th√™m package-lock.json v√†o r·ªìi m·ªõi install node_modules. ƒêi·ªÅu ƒë√≥ gi√∫p c√°c version ƒë∆∞·ª£c lock l·∫°i. Theo practices th√¨ n√™n s·ª≠ d·ª•ng RUN npm ci --only=production thay cho npm install. ( Adding --only=prod or --production would not install devDependencies and just install dependencies.)\n8.2. D√πng Debian slim image thay v√¨ Alpine image ∆Øu ti√™n c√°c image Debian slim image h∆°n Alpine image v√¨:\n It uses the bullseye image variant which is the current stable Debian 11 version with a far enough end-of-life date. And finally it uses the slim image variant to specify a smaller software footprint of the operating system which results in less than 200MB of image size, including the Node.js runtime and tooling.\n  Node.js Alpine is an unofficial Docker container image build that is maintained by the Node.js Docker team. The Node.js image bundles the Alpine operating system which is powered by the minimal busybox software tooling and the musl C library implementation. These two Node.js Alpine image characteristics contribute to the Docker image being unofficially supported by the Node.js team. Furthermore, many security vulnerabilities scanners can‚Äôt easily detect software artifacts or runtimes on Node.js Alpine images, which is counterproductive to efforts to secure your container images\n V·ªõi Alpine image th√¨ s·∫Ω install c√°c packge b·∫±ng apk command ki·ªÉu n√†y:\nFROM node:14.20.1-alpine RUN apk --no-cache update V·ªõi Debian slim image th√¨ s·∫Ω install c√°c package b·∫±ng apt-get command ki·ªÉu n√†y:\nFROM node:16.17.0-bullseye-slim RUN apt-get update -\u0026gt; C√≥ th·ªÉ th·∫•y apt-get command s·∫Ω g·∫ßn g≈©i th√¢n thi·ªán v·ªõi Ubuntu/Linux h∆°n apk command.\n8.3. Ch·ªâ ƒë·ªãnh user node thay v√¨ root Kh√¥ng run container b·∫±ng root user, trong Dockerfile h√£y d√πng USER node ƒë·ªÉ run c√°c app nodejs.\n8.4. Nh·ªõ run apt-get update H√£y run RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y trong Dockerfile ƒë·ªÉ fix Docker image vulnerabilities.\n8.5. N√™n d√πng file .dockerignore D√πng .dockerignore file ƒë·ªÉ ko ƒë∆∞a c√°c file ko c·∫ßn thi·∫øt v√†o khi build image.\n8.6. D√πng c√°c tools linter v√† dive https://github.com/hadolint/hadolint -\u0026gt; tool ƒë·ªÉ check convention cho Dockerfile.\nhttps://github.com/wagoodman/dive -\u0026gt; tool ƒë·ªÉ check size c·ªßa c√°c layer, t·ª´ ƒë√≥ gi√∫p b·∫°n debug ƒë∆∞·ª£c command n√†o t·ªën nhi·ªÅu dung l∆∞·ª£ng c·ªßa image.\n8.7. COPY v√† chown/chmod tr√™n c√πng 1 command N·∫øu d√πng dive ƒë·ªÉ ph√¢n t√≠ch image t·∫°o b·ªüi Dockerfile nh∆∞ n√†y:\n... COPY . /app RUN chown -R node:node /app ... C√≥ th·ªÉ b·∫°n s·∫Ω th·∫•y vi·ªác command RUN chown -R node:node /app s·∫Ω ti√™u th·ª• 1 l∆∞·ª£ng dung l∆∞·ª£ng nh·∫•t ƒë·ªãnh. ƒê√≥ l√† do vi·ªác change owner c·∫ßn n√≥ ph·∫£i copy file ra 1 read-only layer v√† paste l·∫°i v√†o 1 layer m·ªõi, v√¥ t√¨nh l√†m t·ªën th·ªùi gian v√† duplicate dung l∆∞·ª£ng image l√™n\nN√™n s·ª≠a l·∫°i th√†nh:\n... COPY --chown=node:node . /app ... Khi ƒë√≥ s·∫Ω gi·∫£m ƒë∆∞·ª£c th·ªùi gian v√† dung l∆∞·ª£ng image kh√° nhi·ªÅu. Tr∆∞·ªùng h·ª£p c·ªßa m√¨nh l√† t·ª´ 2.93 GB xu·ªëng 1.85 GB. üòç\n8.8. D√πng multi-stage build ƒë·ªÉ prevent secret leak (?) https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/#\nD√πng MULTI-STAGE BUILDS ƒë·ªÉ ngƒÉn ch·∫∑n vi·ªác leak c√°c sensitive secret v√†o trong image.\nN·∫øu b·∫°n ƒëang build Docker cho c√¥ng vi·ªác, th√¨ kh·∫£ nƒÉng cao b·∫°n s·∫Ω d√πng 1 private NPM registry, trong tr∆∞·ªùng h·ª£p ƒë√≥ b·∫°n s·∫Ω c·∫ßn truy·ªÅn NPM_TOKEN v√†o ƒë·ªÉ run npm install\nV√≠ d·ª•:\nFROM node:16.17.0-bullseye-slim RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dumb-init ENV NODE_ENV production ENV NPM_TOKEN 1234 WORKDIR /usr/src/app COPY --chown=node:node . . #RUN npm ci --only=production RUN echo \u0026quot;//registry.npmjs.org/:_authToken=$NPM_TOKEN\u0026quot; \u0026gt; .npmrc \u0026amp;\u0026amp; \\ npm ci --only=production USER node CMD [\u0026quot;dumb-init\u0026quot;, \u0026quot;node\u0026quot;, \u0026quot;server.js\u0026quot;] -\u0026gt; Vi·∫øt nh∆∞ tr√™n l√† NOT GOOD V√¨ ƒë∆∞a TOKEN v√†o .npmrc file n√™n b·∫°n s·∫Ω c·∫ßn x√≥a file ƒë√≥ ƒëi:\nRUN echo \u0026quot;//registry.npmjs.org/:_authToken=$NPM_TOKEN\u0026quot; \u0026gt; .npmrc \u0026amp;\u0026amp; \\ npm ci --only=production; \\ rm -rf .npmrc Khi build b·∫°n s·∫Ω truy·ªÅn NPM_TOKEN v√†o qu√° bi·∫øn m√¥i tr∆∞·ªùng:\ndocker build . -t nodejs-tutorial --build-arg NPM_TOKEN=1234 -\u0026gt; Nh∆∞ng nh∆∞ th·∫ø v·∫´n NOT GOOD v√¨ khi run docker history IMAGE_NAME s·∫Ω th·∫•y ƒë∆∞·ª£c history v√† gi√° tr·ªã c·ªßa NPM_TOKEN:\nIMAGE CREATED CREATED BY SIZE COMMENT b4c2c78acaba About a minute ago CMD [\u0026quot;dumb-init\u0026quot; \u0026quot;node\u0026quot; \u0026quot;server.js\u0026quot;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago USER node 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago RUN |1 NPM_TOKEN=1234 /bin/sh -c echo \u0026quot;//reg‚Ä¶ 5.71MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago ARG NPM_TOKEN 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago COPY . . # buildkit 15.3kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago WORKDIR /usr/src/app 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago ENV NODE_ENV=production 0B buildkit.dockerfile.v0 ·ªû ƒë√¢y ng∆∞·ªùi ta h∆∞·ªõng d·∫´n chia l√†m 2 stage:\n# --------------\u0026gt; The build image FROM node:latest AS build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dumb-init ARG NPM_TOKEN WORKDIR /usr/src/app COPY package*.json /usr/src/app/ RUN echo \u0026quot;//registry.npmjs.org/:_authToken=$NPM_TOKEN\u0026quot; \u0026gt; .npmrc \u0026amp;\u0026amp; \\ npm ci --only=production \u0026amp;\u0026amp; \\ rm -f .npmrc # --------------\u0026gt; The production image FROM node:16.17.0-bullseye-slim ENV NODE_ENV production COPY --from=build /usr/bin/dumb-init /usr/bin/dumb-init USER node WORKDIR /usr/src/app COPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules COPY --chown=node:node . /usr/src/app CMD [\u0026quot;dumb-init\u0026quot;, \u0026quot;node\u0026quot;, \u0026quot;server.js\u0026quot;] Stage build ƒë∆∞·ª£c s·ª≠ d·ª•ng NPM_TOKEN v√† sau ƒë√≥ s·∫Ω l√† input c·ªßa stage cu·ªëi c√πng.\nKhi ƒë√≥ docker history IMAGE_NAME s·∫Ω KH√îNG th·∫•y ƒë∆∞·ª£c history v√† gi√° tr·ªã c·ªßa NPM_TOKEN\nTuy nhi√™n d√πng c√°ch n√†y b·∫°n s·∫Ω c·∫ßn ch·∫Øc ch·∫Øn file .npmrc ko ·ªü trong file .dockerignore, v√¨ ch√∫ng ta c·∫ßn n√≥ khi RUN npm ci m√†. V√† l·∫°i khi run docker build, secret v·∫´n ·ªü dang plain-text tr√™n command.\nƒêi·ªÅu n√†y g√¢y ra 1 ch√∫t kh√≥ ch·ªãu v√¨ ko ƒë·∫£m b·∫£o security l·∫Øm. H√£y t√¨m hi·ªÉu c√°ch ti·∫øp theo.\n8.9. D√πng mount=type=secret ƒë·ªÉ prevent secret leak V·ªõi c√°ch n√†y b·∫°n c√≥ th·ªÉ ƒë∆∞a file .npmrc v√†o .dockerignore ngay t·ª´ ƒë·∫ßu:\nfile .dockerignore:\n.dockerignore node_modules npm-debug.log Dockerfile .git .gitignore .npmrc Dockerfile:\n# --------------\u0026gt; The build image FROM node:latest AS build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dumb-init WORKDIR /usr/src/app COPY package*.json /usr/src/app/ RUN --mount=type=secret,mode=0644,id=npmrc,target=/usr/src/app/.npmrc npm ci --only=production # --------------\u0026gt; The production image FROM node:16.17.0-bullseye-slim ENV NODE_ENV production COPY --from=build /usr/bin/dumb-init /usr/bin/dumb-init USER node WORKDIR /usr/src/app COPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules COPY --chown=node:node . /usr/src/app CMD [\u0026quot;dumb-init\u0026quot;, \u0026quot;node\u0026quot;, \u0026quot;server.js\u0026quot;] -\u0026gt; B·∫°n s·∫Ω th·∫•y c√¢u l·ªánh RUN --mount=type=secret,mode=0644,id=npmrc,target=/usr/src/app/.npmrc npm ci --only=production.\nN√≥ ƒë√£ mount c√°i id id=npmrc v√†o trong container ·ªü path /usr/src/app/.npmrc\nC√¢u l·ªánh doker build s·∫Ω nh∆∞ n√†y:\ndocker build . -t nodejs-tutorial --secret id=npmrc,src=.npmrc -\u0026gt; Kh√¥ng c√≤n secret d∆∞·ªõi d·∫°ng plain-text n·ªØa. B·∫°n ch·ªâ ƒë·ªãnh 1 secret v·ªõi id=npmrc, source file ·ªü path .npmrc\nC√°ch n√†y c√≥ th·ªÉ n√≥i l√† chuy√™n nghi·ªáp h∆°n c√°ch tr√™n kh√° nhi·ªÅu ƒë·∫•y.\nT∆∞∆°ng t·ª± nh∆∞ file .npmrc, c√°c b·∫°n c√≥ th·ªÉ √°p d·ª•ng cho tr∆∞·ªùng h·ª£p git clone t·ª´ 1 private git repo v·ªÅ.\nKhi ƒë√≥ ch·∫Øc h·∫≥n c√°c b·∫°n c≈©ng c·∫ßn ƒë∆∞a PRIVATE_KEY v√†o builder ƒë·ªÉ s·ª≠ d·ª•ng, ko mu·ªën cho PRIVATE_KEY v√†o image th√¨ h√£y d√πng c√°ch mount=type=secret n√†y nh√©.\n8.10. D√πng mount=type=ssh ƒë·ªÉ prevent SSH key leak http://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/\n ƒê·ªÉ test th√¨ c·∫ßn t·ª± generate SSH keypair tr√™n m√°y n√†o c≈©ng ƒë∆∞·ª£c, m√¨nh t·∫°o tr√™n Laptop:  ssh-keygen # B√†i n√†y n√≥i r·∫±ng c√¢u l·ªánh tr√™n generate ra 1 b·ªô key y·∫øu v√† d·ªÖ b·ªã bruteforce:  # Source: https://dev.to/levivm/how-to-use-ssh-and-ssh-agent-forwarding-more-secure-ssh-2c32 # N·∫øu c√≥ th·ªÉ h√£y d√πng command sau:  # ssh-keygen -o -a 100 -t ed25519 b·∫°n s·∫Ω ƒë∆∞·ª£c 1 b·ªô key:\ngitlab_com_rsa gitlab_com_rsa.pub  T·∫°o 1 private project tr√™n Gitlab.\nC√°i n√†y t√πy √Ω project n√†o c≈©ng dc, m√¨nh t·∫°o project git@gitlab.com:inmessionante/spring-maven-postgres-docker-k8s.git\n  Add public SSH key gitlab_com_rsa.pub v√†o Gitlab b·∫±ng c√°ch:\nhttps://gitlab.com/-/profile -\u0026gt; SSH Keys Tab -\u0026gt; Add c√°i n·ªôi dung c·ªßa gitlab_com_rsa.pub v√†o\n  Tr√™n VM Run Docker, t·∫°o folder ƒë·ªÉ test:\n  mkdir /opt/devops/docker-ssh-agent-lab Copy file SSH private key gitlab_com_rsa l√™n /home/ubuntu/.ssh (ch·ªó n√†y t√πy b·∫°n, ƒë·ªÉ ƒë√¢u cho secure c≈©ng ƒë∆∞·ª£c)\nFile /opt/devops/docker-ssh-agent-lab/Dockerfile:\n# syntax=docker/dockerfile:1.0.0-experimental FROM alpine RUN apk add --update git openssh # This is necessary to prevent the \u0026quot;git clone\u0026quot; operation from failing # with an \u0026quot;unknown host key\u0026quot; error. RUN mkdir -m 700 /root/.ssh; \\ touch -m 600 /root/.ssh/known_hosts; \\ ssh-keyscan gitlab.com \u0026gt; /root/.ssh/known_hosts # This command has access to the \u0026quot;github\u0026quot; key RUN --mount=type=ssh,id=gitlab git clone git@gitlab.com:inmessionante/spring-maven-postgres-docker-k8s.git Run Docker Build  cd /opt/devops/docker-ssh-agent-lab DOCKER_BUILDKIT=1 docker build --ssh gitlab=/home/ubuntu/.ssh/gitlab_com_rsa -t buildtest . Confirm ƒë√£ git clone ƒë∆∞·ª£c project v√†o trong image  $ docker run buildtest ls -lsa total 80 4 drwxr-xr-x 1 root root 4096 Nov 6 07:11 . 4 drwxr-xr-x 1 root root 4096 Nov 6 07:11 .. 0 -rwxr-xr-x 1 root root 0 Nov 6 07:11 .dockerenv 0 -rw-r--r-- 1 root root 0 Nov 6 07:08 600 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 bin 0 drwxr-xr-x 5 root root 340 Nov 6 07:11 dev 4 drwxr-xr-x 1 root root 4096 Nov 6 07:11 etc 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 home 8 drwxr-xr-x 1 root root 4096 Aug 9 08:49 lib 4 drwxr-xr-x 5 root root 4096 Aug 9 08:49 media 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 mnt 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 opt 0 dr-xr-xr-x 287 root root 0 Nov 6 07:11 proc 4 drwx------ 1 root root 4096 Nov 6 07:08 root 4 drwxr-xr-x 1 root root 4096 Nov 6 07:08 run 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 sbin 4 drwxr-xr-x 7 root root 4096 Nov 6 07:08 spring-maven-postgres-docker-k8s 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 srv 0 dr-xr-xr-x 13 root root 0 Nov 6 07:11 sys 4 drwxrwxrwt 2 root root 4096 Aug 9 08:49 tmp 8 drwxr-xr-x 1 root root 4096 Nov 6 07:08 usr 8 drwxr-xr-x 1 root root 4096 Nov 6 07:08 var OK?\nD√πng ssh-agent v√† $SSH_AUTH_SOCK  Sau b∆∞·ªõc 6 c√≥ v·∫ª ƒë√£ OK? ·ªû b∆∞·ªõc s·ªë 5 ch√∫ng ta ph·∫£i ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c·ªßa private key, m√¨nh ko th√≠ch ƒëi·ªÅu n√†y l·∫Øm.\nN√™n m√¨nh s·∫Ω d√πng command n√†y:\ncd /opt/devops/docker-ssh-agent-lab DOCKER_BUILDKIT=1 docker build --ssh gitlab=$SSH_AUTH_SOCK -t buildtest . Tuy nhi√™n b·∫°n s·∫Ω b·ªã l·ªói sau n·∫øu ko chu·∫©n b·ªã tr∆∞·ªõc:\n$ DOCKER_BUILDKIT=1 docker build --ssh gitlab=$SSH_AUTH_SOCK -t buildtest . could not parse ssh: [gitlab=]: invalid empty ssh agent socket, make sure SSH_AUTH_SOCK is set B·∫°n c·∫ßn ƒë∆∞a private key v√†o ssh-agent tr∆∞·ªõc cho n√≥ qu·∫£n l√Ω:\n# start the ssh-agent in the background $ eval \u0026#34;$(ssh-agent -s)\u0026#34; Agent pid 9501 # Add your SSH private key to the ssh-agent $ ssh-add ~/.ssh/gitlab_com_rsa Identity added: /home/ubuntu/.ssh/gitlab_com_rsa # List identities that ssh-agent is managing $ ssh-add -L ssh-rsa AAAA...B3NzaC1yc2EAqClFkhB9mUfuVGPMe+upN4AV7LS6toRuwHdTJoUfbcjTc9zttxtoKogMXJu/rrf6Pv7I7ksLOL26PyI3vPacnw/VcrVplNw8367TMtap4K9MoWqup2UGyozm6ycd/inMiyeIy...9s= T·∫°i th·ªùi ƒëi·ªÉm n√†y bi·∫øn m√¥i tr∆∞·ªùng $SSH_AUTH_SOCK ƒë√£ ƒë∆∞·ª£c set, b·∫°n c√≥ th·ªÉ check b·∫±ng command:\necho $SSH_AUTH_SOCK Gi·ªù ch·∫°y l·∫°i command docker build:\n$ DOCKER_BUILDKIT=1 docker build --ssh gitlab=$SSH_AUTH_SOCK -t buildtest . [+] Building 8.5s (14/14) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 38B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; resolve image config for docker.io/docker/dockerfile:1.0.0-experimental 0.9s =\u0026gt; [auth] docker/dockerfile:pull token for registry-1.docker.io 0.0s =\u0026gt; CACHED docker-image://docker.io/docker/dockerfile:1.0.0-experimental@sha256:d2d402b6fa1dae752f 0.0s =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 38B 0.0s =\u0026gt; [internal] load .dockerignore 0.1s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.6s =\u0026gt; [auth] library/alpine:pull token for registry-1.docker.io 0.0s =\u0026gt; [1/4] FROM docker.io/library/alpine@sha256:bc41182d7ef5ffc53a40b044e725193bc10142a1243f395ee85 0.0s =\u0026gt; CACHED [2/4] RUN apk add --update git openssh 0.0s =\u0026gt; CACHED [3/4] RUN mkdir -m 700 /root/.ssh; touch -m 600 /root/.ssh/known_hosts; ssh-keyscan 0.0s =\u0026gt; CACHED [4/4] RUN --mount=type=ssh,id=gitlab git clone git@gitlab.com:inmessionante/spring-mave 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:7d93dafe042c123483ffeafee2b63be48bcf3d30b425b47847b2988399cebc65 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/buildtest Sau step 7, Tr√™n m√¥i tr∆∞·ªùng Develop b√¨nh th∆∞·ªùng th√¨ ƒë·∫øn ƒë√¢y m·ªçi th·ª© c√≥ v·∫ª ƒë√£ ·ªïn üòÇ.  Nh∆∞ng tr√™n CI th√¨ kh√°c, VM n√†y s·∫Ω l√† Builder c·ªßa ch√∫ng ta, vi·ªác Builder ch·ª©a private key nghe c√≥ v·∫ª s·∫Ω kh√¥ng secure cho l·∫Øm.\nTrong tr∆∞·ªùng h·ª£p Builder n√†y b·ªã hack, ho·∫∑c nhi·ªÅu ng∆∞·ªùi c√≥ quy·ªÅn truy c·∫≠p, ch√∫ng ta ho√†n to√†n c√≥ th·ªÉ b·ªã leak c√°i private key ·∫•y ra ngo√†i.\nV·∫≠y n·∫øu ch√∫ng ta mu·ªën Builder c≈©ng kh√¥ng th·ªÉ xem ƒë∆∞·ª£c key th√¨ sao. Ta gi·∫£ s·ª≠ m√¨nh ƒëang d√πng Gitlab CI nh√©.\nModify your .gitlab-ci.yml with a before_script action. In the following example, a Debian based image is assumed. Edit to your needs:\nbefore_script: ## ## Install ssh-agent if not already installed, it is required by Docker. ## (change apt-get to yum if you use an RPM-based image) ## - \u0026#39;command -v ssh-agent \u0026gt;/dev/null || ( apt-get update -y \u0026amp;\u0026amp; apt-get install openssh-client -y )\u0026#39; ## ## Run ssh-agent (inside the build environment) ## - eval $(ssh-agent -s) ## ## Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store ## We\u0026#39;re using tr to fix line endings which makes ed25519 keys work ## without extra base64 encoding. ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556 ## - echo \u0026#34;$SSH_PRIVATE_KEY\u0026#34; | tr -d \u0026#39;\\r\u0026#39; | ssh-add - ## ## Create the SSH directory and give it the right permissions ## - mkdir -p ~/.ssh - chmod 700 ~/.ssh ## ## Optionally, if you will be using any Git commands, set the user name and ## and email. ## # - git config --global user.email \u0026#34;user@example.com\u0026#34; # - git config --global user.name \u0026#34;User name\u0026#34; Nh∆∞ v·∫≠y l√† ch·ªâ c·∫ßn truy·ªÅn bi·∫øn m√¥i tr∆∞·ªùng tr√™n Gitlab CI l√† ok, Builder s·∫Ω kh√¥ng l∆∞u c√°c private key n·ªØa.\nƒê·∫øn ƒë√¢y ch√∫ng ta th·∫•y ƒë∆∞·ª£c c√°c t√≠nh nƒÉng h·ªØu √≠ch c·ªßa SSH Agent üòç n√™n m√¨nh quy·∫øt ƒë·ªãnh xa r·ªùi ch·ªß ƒë·ªÅ ch√≠nh c·ªßa b√†i n√†y ƒë·ªÉ take notes v·ªÅ SSH Agent th√™m 1 ch√∫t:\n8.11. SSH Agent Forwarding https://dev.to/levivm/how-to-use-ssh-and-ssh-agent-forwarding-more-secure-ssh-2c32\nGi·∫£ s·ª≠ b·∫°n l√†m vi·ªác tr√™n Laptop (local).\nB·∫°n c√≥ 1 VM Ubuntu tr√™n Cloud. VM n√†y c√≥ th·ªÉ share cho nhi·ªÅu teammate c√πng nhau s·ª≠ d·ª•ng. VM n√†y c·∫ßn pull source t·ª´ 1 private repository tr√™n Gitlab.com.\nB·∫°n th∆∞·ªùng pull source v·ªÅ b·∫±ng 1 SSH private key (m√† b·∫°n ƒë√£ add public key c·ªßa n√≥ v√†o Gitlab r·ªìi).\nC√°i private key ƒë√≥ b·∫°n ƒëang l∆∞u tr√™n VM.\nGi·ªù b·∫°n ko mu·ªën VM ƒë√≥ l∆∞u private key n·ªØa (V√¨ ng∆∞·ªùi kh√°c/admin) c√≥ th·ªÉ v√†o v√† l·∫•y key ƒë√≥.\n-\u0026gt; ƒê√¢y l√† l√∫c c·∫ßn s·ª≠ d·ª•ng SSH Agent Forwarding.\nNg·∫Øn g·ªçn: B·∫°n forward SSH Agent session t·ª´ Laptop l√™n VM. Khi·∫øn n√≥ c√≥ th·ªÉ pull source code v·ªÅ m√† ko c·∫ßn l∆∞u c√°i private key.\nC·∫ßn m√¥ t·∫£ r√µ, tr√™n Laptop c·ªßa b·∫°n c√≥ 2 private key:\n VM_PRIVATE_KEY: d√πng ƒë·ªÉ SSH t·ª´ Laptop l√™n VM. GITLAB_PRIVATE_KEY: d√πng ƒë·ªÉ pull source t·ª´ Gitlab.com v·ªÅ. Tr∆∞·ªõc ƒë√¢y b·∫°n l∆∞u n√≥ tr√™n VM, gi·ªù ko mu·ªën n·ªØa. ubuntu@112.222.54.31: user v√† IP c·ªßa VM.  b√†i n√†y m√¨nh d√πng GitBash tr√™n Windows:\n# start the ssh-agent in the background $ eval \u0026#34;$(ssh-agent -s)\u0026#34; Agent pid 9501 # Add your SSH private key to the ssh-agent $ ssh-add ./GITLAB_PRIVATE_KEY Identity added: ... # T·∫°i th·ªùi ƒëi·ªÉm n√†y: ssh-agent ƒë√£ s·∫µn s√†ng cho b·∫°n forward  # SSH from Laptop to VM and forward agent.  # -A option enables forwarding of the authentication agent connection.  $ ssh -A -i VM_PRIVATE_KEY ubuntu@112.222.54.31 # T·∫°i th·ªùi ƒëi·ªÉm n√†y b·∫°n ƒë√£ login v√†o VM,  # b·∫°n s·∫Ω th·∫•y $SSH_AUTH_SOCK c√≥ gi√° tr·ªã  $ echo $SSH_AUTH_SOCK # output: /tmp/ssh-baaawdgssv/agent.12322 # Test connection $ ssh -T git@gitlab.com # H√£y th·ª≠ pull source t·ª´ 1 private repository c·ªßa b·∫°n tr√™n Gitlab.com v·ªÅ xem sao.  # ch·∫Øc ch·∫Øn ph·∫£i pull ƒë∆∞·ª£c m·ªõi OK nh√©.  Ph·∫£i n√≥i c√°ch n√†y kh√° hay v√† secure, tuy nhi√™n vi·ªác forward ssh-agent l√™n VM c≈©ng ch·∫£ kh√°c n√†o ƒë∆∞a key c·ªßa b·∫°n l√™n VM, h√£y ch·∫Øc ch·∫Øn x√≥a session tr∆∞·ªõc khi b·∫°n logout kh·ªèi con VM d√πng chung n√†y. C√≥ 1 v√†i c√°ch ƒë·ªÉ t·ª± ƒë·ªông chuy·ªán clear ssh-agent l√† b√†i n√†y: https://rabexc.org/posts/pitfalls-of-ssh-agents\nNgo√†i ra, tr∆∞·ªùng h·ª£p Gitlab self-hosted server ƒë·ª©ng sau VPN th√¨ m√¨nh ch∆∞a th·ª≠.\nCREDIT https://walterteng.com/gatsby-docker\nhttps://dev.to/stoutlabs/my-docker-setup-for-gatsbyjs-and-nextjs-5gao\nk·∫øt h·ª£p nginx ƒë·ªÉ serve public dir https://valenciandigital.com/insights/why-containerize-your-gatsby-application\nbest practices: https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/ best practices: https://github.com/nodejs/docker-node/blob/main/docs/BestPractices.md\nbest practices: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nbest practices: https://adambrodziak.pl/dockerfile-good-practices-for-node-and-npm\nhttps://itnext.io/how-to-speed-up-node-js-modules-installation-in-ci-cd-pipeline-as-of-2020-4865d77c0eb7\nhttps://forum.gitlab.com/t/how-to-cache-node-modules-globally-for-all-pipelines-for-a-project/49169\nso s√°nh CMD vs ENTRYPOINT trong Dockerfile: https://phoenixnap.com/kb/docker-cmd-vs-entrypoint\nhttps://remelehane.dev/posts/diy-node-cache-for-docker-ci/\nhttps://blog.saeloun.com/2022/07/12/docker-cache.html\nhttps://stackoverflow.com/questions/54574821/docker-build-not-using-cache-when-copying-gemfile-while-using-cache-from#comment98285860_54574821\nhttps://youtu.be/r_UBWjMUd-0\nhttp://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/\nhttps://docs.gitlab.com/ee/ci/ssh_keys/#ssh-keys-when-using-the-docker-executor\n","href":"/bk/encrypt-dockerize-reactjs-gatsby-app-good-practices/","title":"Dockerize ReactJS/GatsbyJS app and Run it in Dev environment + good practices"},{"content":"1. Story M√¨nh c√≥ 1 static site t·∫°o ra b·∫±ng GatsbyJS (1 framework base tr√™n ReactJS) ƒë√£ l√¢u, c√°ch ƒë√¢y kho·∫£ng 1 nƒÉm:\nhttps://github.com/hoangmnsd/hoangmnsd-the404blog-theme.git\nGi·ªù ƒë·ªïi m√°y mu·ªën ti·∫øp t·ª•c quay l·∫°i ph√°t tri·ªÉn app ƒë√≥. Nh∆∞ng laptop y·∫øu qu√° ko th·ªÉ npm install n·ªïi.\n-\u0026gt; M√¨nh chuy·ªÉn h∆∞·ªõng sang deploy app ƒë√≥ tr√™n 1 server ri√™ng nh∆∞ RasberryPi, nh∆∞ng l·∫°i d√≠nh ƒë·∫øn network bandwidth qu√° ch·∫≠m, npm install r·∫•t t·ªën th·ªùi gian.\n-\u0026gt; M√¨nh chuy·ªÉn ti·∫øp sang h∆∞·ªõng deploy app ƒë√≥ tr√™n 1 VM ƒë·ªÉ t·∫≠n d·ª•ng network bandwidth si√™u nhanh c·ªßa h·ªç.\nM√¥i tr∆∞·ªùng m√¨nh d√πng l√†: Canonical-Ubuntu-18.04-aarch64-2022.04.24-0.\nBan ƒë·∫ßu ch·ªâ ƒë·ªãnh install nodejs v√† npm tr√™n VM r·ªìi build \u0026amp; run lu√¥n, nh∆∞ng th·∫•y nhi·ªÅu l·ªói li√™n quan ƒë·∫øn OS qu√°, fix v√† ƒë·ªïi version nodejs ƒë·ªÉ test li√™n t·ª•c kh√° m·ªát, nhi·ªÅu khi ko bi·∫øt fix ƒë∆∞·ª£c l·ªói l√† do command n√†o\u0026hellip;\n-\u0026gt; M√¨nh quy·∫øt ƒë·ªãnh chuy·ªÉn sang Dockerize lu√¥n tr√™n VM, ƒë·ªÉ sau n√†y khi ƒë·ªïi server s·∫Ω ti·ªán t·∫°o l·∫°i m√¥i tr∆∞·ªùng. √ù t∆∞·ªüng s·∫Ω l√†:\n Synchronize code t·ª´ local l√™n VM b·∫±ng WinSCP, t·∫•t nhi√™n ch·ªâ ph·∫ßn code m√¨nh s·ª≠a th√¥i, ignore node_modules, App run b·∫±ng Docker Compose v√† ƒë∆∞·ª£c mount source code dirs t·ª´ VM v√†o Container M·ªói khi s·ª≠a code tr√™n local, s·∫Ω ƒë∆∞·ª£c sync l√™n VM qua WinSCP, t·ª´ ƒë√≥ mount t·ª± ƒë·ªông v√†o Container v√† ph·∫£n √°nh l√™n public IP/port c·ªßa VM.  Ch√∫ √Ω: Vi·ªác build ƒëi build l·∫°i Dockerfile c≈©ng s·∫Ω r·∫•t t·ªën dung l∆∞·ª£ng (c√≥ v√†i ng√†y m√† m√¨nh t·ªën 50GB r√°c) n√™n ch√∫ √Ω khi ch·ªçn server n√™n ch·ªçn ·ªï disk dung l∆∞·ª£ng l·ªõn.\nƒê√¢y l√† file package.json c·ªßa repo:\n$ cat package.json { \u0026quot;name\u0026quot;: \u0026quot;gatsby-starter-default\u0026quot;, \u0026quot;private\u0026quot;: true, \u0026quot;description\u0026quot;: \u0026quot;A simple starter to get up and developing quickly with Gatsby\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;Kyle Mathews \u0026lt;mathews.kyle@gmail.com\u0026gt;\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;@gatsby-contrib/gatsby-plugin-elasticlunr-search\u0026quot;: \u0026quot;^2.3.0\u0026quot;, \u0026quot;@hoangmnsd/gatsby-theme-amplify-cognito\u0026quot;: \u0026quot;^1.1.5\u0026quot;, \u0026quot;bootstrap\u0026quot;: \u0026quot;^4.3.1\u0026quot;, \u0026quot;disqus-react\u0026quot;: \u0026quot;^1.0.6\u0026quot;, \u0026quot;gatsby-cli\u0026quot;: \u0026quot;^3.14.2\u0026quot;, \u0026quot;gatsby\u0026quot;: \u0026quot;^2.13.13\u0026quot;, \u0026quot;gatsby-image\u0026quot;: \u0026quot;^2.2.4\u0026quot;, \u0026quot;gatsby-plugin-disqus\u0026quot;: \u0026quot;^1.1.2\u0026quot;, \u0026quot;gatsby-plugin-manifest\u0026quot;: \u0026quot;^2.2.1\u0026quot;, \u0026quot;gatsby-plugin-offline\u0026quot;: \u0026quot;^2.2.1\u0026quot;, \u0026quot;gatsby-plugin-react-helmet\u0026quot;: \u0026quot;^3.1.0\u0026quot;, \u0026quot;gatsby-plugin-sharp\u0026quot;: \u0026quot;^2.2.3\u0026quot;, \u0026quot;gatsby-plugin-transition-link\u0026quot;: \u0026quot;^1.20.5\u0026quot;, \u0026quot;gatsby-remark-copy-linked-files\u0026quot;: \u0026quot;^2.1.3\u0026quot;, \u0026quot;gatsby-remark-images\u0026quot;: \u0026quot;^3.1.4\u0026quot;, \u0026quot;gatsby-remark-prismjs\u0026quot;: \u0026quot;^3.3.2\u0026quot;, \u0026quot;gatsby-remark-responsive-iframe\u0026quot;: \u0026quot;^2.2.3\u0026quot;, \u0026quot;gatsby-remark-smartypants\u0026quot;: \u0026quot;^2.1.2\u0026quot;, \u0026quot;gatsby-source-filesystem\u0026quot;: \u0026quot;^2.1.3\u0026quot;, \u0026quot;gatsby-transformer-remark\u0026quot;: \u0026quot;^2.6.3\u0026quot;, \u0026quot;gatsby-transformer-sharp\u0026quot;: \u0026quot;^2.2.1\u0026quot;, \u0026quot;gsap\u0026quot;: \u0026quot;^3.7.1\u0026quot;, \u0026quot;jquery\u0026quot;: \u0026quot;^3.4.1\u0026quot;, \u0026quot;popper.js\u0026quot;: \u0026quot;^1.15.0\u0026quot;, \u0026quot;prismjs\u0026quot;: \u0026quot;^1.16.0\u0026quot;, \u0026quot;prop-types\u0026quot;: \u0026quot;^15.7.2\u0026quot;, \u0026quot;react\u0026quot;: \u0026quot;^16.8.6\u0026quot;, \u0026quot;react-bootstrap\u0026quot;: \u0026quot;^1.6.1\u0026quot;, \u0026quot;react-dom\u0026quot;: \u0026quot;^16.8.6\u0026quot;, \u0026quot;react-helmet\u0026quot;: \u0026quot;^5.2.1\u0026quot;, \u0026quot;react-hook-form\u0026quot;: \u0026quot;^7.12.1\u0026quot;, \u0026quot;react-social-icons\u0026quot;: \u0026quot;^4.1.0\u0026quot;, \u0026quot;sharp\u0026quot;: \u0026quot;^0.27.2\u0026quot; }, \u0026quot;devDependencies\u0026quot;: { \u0026quot;prettier\u0026quot;: \u0026quot;^1.18.2\u0026quot; }, \u0026quot;keywords\u0026quot;: [ \u0026quot;gatsby\u0026quot; ], \u0026quot;license\u0026quot;: \u0026quot;MIT\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;build\u0026quot;: \u0026quot;gatsby build\u0026quot;, \u0026quot;develop\u0026quot;: \u0026quot;gatsby develop\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;prettier --write src/**/*.{js,jsx}\u0026quot;, \u0026quot;start\u0026quot;: \u0026quot;npm run develop\u0026quot;, \u0026quot;serve\u0026quot;: \u0026quot;gatsby serve\u0026quot;, \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Write tests! -\u0026gt; https://gatsby.dev/unit-testing\\\u0026quot;\u0026quot; }, \u0026quot;repository\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;git\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://github.com/gatsbyjs/gatsby-starter-default\u0026quot; }, \u0026quot;bugs\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;https://github.com/gatsbyjs/gatsby/issues\u0026quot; } } 2. M·ªôt s·ªë l·ªói v√† command c·∫ßn l∆∞u √Ω khi c·ªë g·∫Øng npm install tr√™n VM ‚õî L·ªói Failed at the sharp@0.27.2 install script.\nƒê·ªÉ fix th√¨ m√¨nh ph·∫£i ƒë·ªïi version nodejs ƒë·ªÉ test:\nnpm install sharp@0.27.2 --save  gatsby-cli c≈©ng c√≥ nhi·ªÅu version 2/3/4 n√™n ch√∫ √Ω ch·ªçn 1 specific version:  npm i gatsby@2 $ gatsby -v Gatsby CLI version: 2.19.3 ‚õî Nhi·ªÅu l√∫c npm install th√¨ l·ªói nh∆∞ng npm install -g l·∫°i OK\n‚õî C√≥ th·ªÉ g·∫∑p l·ªói v·ªÅ permision\nSolution: https://stackoverflow.com/questions/51811564/sh-1-node-permission-denied\nnpm config set user 0 npm config set unsafe-perm true üÜó C√°ch ƒë·ªÉ x√°c ƒë·ªãnh OS architect ƒëang s·ª≠ d·ª•ng:\n$ node Welcome to Node.js v14.20.1. Type \u0026#34;.help\u0026#34; for more information. \u0026gt; process.arch \u0026#39;arm64\u0026#39; \u0026gt; process.platform \u0026#39;linux\u0026#39; \u0026gt; process.versions { node: \u0026#39;14.20.1\u0026#39;, v8: \u0026#39;8.4.371.23-node.87\u0026#39;, uv: \u0026#39;1.42.0\u0026#39;, zlib: \u0026#39;1.2.11\u0026#39;, brotli: \u0026#39;1.0.9\u0026#39;, ares: \u0026#39;1.18.1\u0026#39;, modules: \u0026#39;83\u0026#39;, nghttp2: \u0026#39;1.42.0\u0026#39;, napi: \u0026#39;8\u0026#39;, llhttp: \u0026#39;2.1.6\u0026#39;, openssl: \u0026#39;1.1.1q\u0026#39;, cldr: \u0026#39;40.0\u0026#39;, icu: \u0026#39;70.1\u0026#39;, tz: \u0026#39;2021a3\u0026#39;, unicode: \u0026#39;14.0\u0026#39; } üÜó C√°ch ƒë·ªÉ check c√°c package ph√π h·ª£p v√† latest c·ªßa npm:\nhttps://stackoverflow.com/a/71980468/9922066\n$ npm outdated Package Current Wanted Latest Location sharp MISSING 0.22.1 0.31.1 gatsby-starter-default @gatsby-contrib/gatsby-plugin-elasticlunr-search 2.4.2 2.4.2 3.0.2 gatsby-starter-default bootstrap 4.6.2 4.6.2 5.2.2 gatsby-starter-default gatsby 2.32.13 2.32.13 4.24.5 gatsby-starter-default gatsby-image 2.11.0 2.11.0 3.11.0 gatsby-starter-default gatsby-plugin-manifest 2.12.1 2.12.1 4.24.0 gatsby-starter-default gatsby-plugin-offline 2.2.10 2.2.10 5.24.0 gatsby-starter-default gatsby-plugin-react-helmet 3.10.0 3.10.0 5.24.0 gatsby-starter-default gatsby-plugin-sharp 2.14.4 2.14.4 4.24.0 gatsby-starter-default gatsby-remark-copy-linked-files 2.10.0 2.10.0 5.24.0 gatsby-starter-default gatsby-remark-images 3.11.1 3.11.1 6.24.0 gatsby-starter-default gatsby-remark-prismjs 3.13.0 3.13.0 6.24.0 gatsby-starter-default gatsby-remark-responsive-iframe 2.11.0 2.11.0 5.24.0 gatsby-starter-default gatsby-remark-smartypants 2.10.0 2.10.0 5.24.0 gatsby-starter-default gatsby-source-filesystem 2.11.1 2.11.1 4.24.0 gatsby-starter-default gatsby-transformer-remark 2.16.1 2.16.1 5.24.0 gatsby-starter-default gatsby-transformer-sharp 2.12.1 2.12.1 4.24.0 gatsby-starter-default react 16.14.0 16.14.0 18.2.0 gatsby-starter-default react-bootstrap 1.6.6 1.6.6 2.5.0 gatsby-starter-default react-dom 16.14.0 16.14.0 18.2.0 gatsby-starter-default react-helmet 5.2.1 5.2.1 6.1.0 gatsby-starter-default react-social-icons 4.1.0 4.1.0 5.15.0 gatsby-starter-default =\u0026gt; npm install sharp@0.22.1\nüÜó C√≥ th·ªÉ b·∫°n s·∫Ω nghƒ© r·∫±ng:\n N·∫øu ko th·ªÉ install https://www.npmjs.com/package/sharp/v/0.27.2 dc th√¨ h√£y chuy·ªÉn sang s·ª≠ d·ª•ng c√°i n√†o ph√π h·ª£p vs OS c·ªßa b·∫°n. ·ªû ƒë√¢y l√† ubuntu oracle cloud vm ch·ªâ c√≥ sharp@0.22.1 l√† install ƒë∆∞·ª£c th√¨ chuy·ªÉn th√¥i.\n Tuy nhi√™n, mu·ªën s·ª≠ d·ª•ng sharp@0.22.1 th√¨ c·∫ßn thay ƒë·ªïi version c·ªßa: gatsby-transformer-sharp v√† gatsby-plugin-sharp n·ªØa v√¨ ch√∫ng ƒëang point ƒë·∫øn sharp@0.27.2.\nV√¨ ko bi·∫øt version n√†o c·ªßa gatsby-transformer-sharp v√† gatsby-plugin-sharp s·∫Ω ph√π h·ª£p v·ªõi sharp@0.22.1.\n-\u0026gt; n√™n m√¨nh b·ªè cu·ªôc, quay l·∫°i t√¨m c√°ch ƒë·ªÉ install ƒë∆∞·ª£c sharp@0.27.2.\nüÜó Khi npm install -g v·∫´n l·ªói thi·∫øu sharp@0.27.2 th√¨ th·ª≠ search xem package n√†o c·∫ßn sharp@0.27.2:\nsudo grep -ril \u0026quot;0.27.2\u0026quot; üÜó C√≥ th·ªÉ vi·ªác install v·ªõi npm install sharp@0.27.2 --ignore-scripts s·∫Ω ƒë∆∞·ª£c,\nnh∆∞ng m√¨nh khuy√™n ko n√™n v√¨ install ƒë∆∞·ª£c sharp@0.27.2 theo c√°ch ƒë√≥ r·ªìi s·∫Ω l·∫°i l·ªói ·ªü ch·ªó kh√°c th√¥i\n‚õî Tr√™n server m√† run Docker, c√≥ th·ªÉ b·ªã l·ªói n√†y khi check log container:\nhttps://stackoverflow.com/questions/55763428/react-native-error-enospc-system-limit-for-number-of-file-watchers-reached\nSolution:\n# insert the new value into the system config echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf \u0026amp;\u0026amp; sudo sysctl -p # check that the new value was applied cat /proc/sys/fs/inotify/max_user_watches üÜó Check c√°c LTS version c·ªßa node, t·∫°i th·ªùi ƒëi·ªÉm n√†y th√¨ c√°c version sau l√† LTS latest t∆∞∆°ng ·ª©ng v·ªõi Node 12/14/16:\n$ nvm ls-remote v12.22.12 v14.20.1 v16.18.0 Ch√∫ng ta s·∫Ω th·ª≠ l·∫ßn l∆∞·ª£t c√°c nodejs version 12,14,16 ƒë·ªÉ test.\nN√™n d√πng c√°c LTS version c·ªßa nodejs ƒë·ªÉ s·ª≠ d·ª•ng.\n‚õî 1 s·ªë command install m√† ch·ªâ ƒë·ªãnh arch v√† platform:\nnpm cache clean --force npm install --arch=x64 --platform=linux --legacy-peer-deps nh∆∞ng l√∫c run npm start th√¨ l·∫°i l·ªói:\nError in \u0026quot;/opt/hoangmnsd-the404blog-theme/node_modules/gatsby-transformer-sharp/gatsby-node.js\u0026quot;: 'linux-x64' binaries cannot be used on the 'linux-arm64v8' platform. Please remove the 'node_modules/sharp' directory and run 'npm install' on the 'linux-arm64v8' platform. Error: 'linux-x64' binaries cannot be used on the 'linux-arm64v8' platform. Please remove the 'node_modules/sharp' directory and run 'npm ins tall' on the 'linux-arm64v8' platform. Run l·∫°i:\nnpm cache clean --force npm install --arch=arm64v8 --platform=linux --legacy-peer-deps -\u0026gt; V·∫´n l·ªói nh∆∞ng l√† l·ªói kh√°c üò≠\n-\u0026gt; T√∫m l·∫°i, cu·ªëi c√πng qu√° n·∫£n n√™n m√¨nh chuy·ªÉn sang c·ªë g·∫Øng Dockerize, t·∫°o Dockerfile v√† build image\n3. M·ªôt s·ªë l·ªói v√† command c·∫ßn l∆∞u √Ω khi Dockerize GatsbyJS app C√≥ 2 lo·∫°i image l√† Debian slim image v√† Alpine image th∆∞·ªùng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p (https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/)\nTh∆∞·ªùng th·∫•y m·ªçi ng∆∞·ªùi hay ch·ªçn Alpine image v√¨ n√≥ nh·∫π, nh∆∞ng theo b√†i vi·∫øt tr√™n th√¨ ho recommend n√™n ch·ªçn Debian slim image v·ªõi LTS nodejs version th√¨ t·ªët h∆°n.\nM√¨nh s·∫Ω th·ª≠ d√πng Alpine image tr∆∞·ªõc sau ƒë√≥ l√† Debian slim image.\nD∆∞·ªõi ƒë√¢y l√† 1 s·ªë l·ªói khi m√¨nh c·ªë g·∫Øng build image t·ª´ Dockerfile.\n3.1. L·ªói g·∫∑p khi build t·ª´ Alpine image ‚õî L·ªói ko th·ªÉ install dc sharp@0.27.2: L·ªói n√†y g·∫∑p khi d√πng FROM node:12.18.0-alpine (m√¨nh ch·ªçn v√¨ 12.18.0 l√† version m√† Netlify ƒëang ch·∫°y OK)\n\u0026gt; sharp@0.27.2 install /app/node_modules/gatsby-plugin-manifest/node_modules/sharp \u0026gt; (node install/libvips \u0026amp;\u0026amp; node install/dll-copy \u0026amp;\u0026amp; prebuild-install) || (node-gyp rebuild \u0026amp;\u0026amp; node install/dll-copy) info sharp Downloading https://github.com/lovell/sharp-libvips/releases/download/v8.10.5/libvips-8.10.5-linuxmusl-arm64v8.tar.br ERR! sharp Prebuilt libvips 8.10.5 binaries are not yet available for linuxmusl-arm64v8 info sharp Attempting to build from source via node-gyp but this may fail due to the above error info sharp Please see https://sharp.pixelplumbing.com/install for required dependencies make: Entering directory '/app/node_modules/gatsby-plugin-manifest/node_modules/sharp/build' CC(target) Release/obj.target/nothing/../../../node-addon-api/nothing.o AR(target) Release/obj.target/../../../node-addon-api/nothing.a COPY Release/nothing.a TOUCH Release/obj.target/libvips-cpp.stamp CXX(target) Release/obj.target/sharp/src/common.o In file included from ../src/common.cc:24: /usr/include/vips/vips8:35:10: fatal error: glib-object.h: No such file or directory 35 | #include \u0026lt;glib-object.h\u0026gt; | ^~~~~~~~~~~~~~~ compilation terminated. make: Leaving directory '/app/node_modules/gatsby-plugin-manifest/node_modules/sharp/build' make: *** [sharp.target.mk:139: Release/obj.target/sharp/src/common.o] Error 1 gyp ERR! build error gyp ERR! stack Error: `make` failed with exit code: 2 gyp ERR! stack at ChildProcess.onExit (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:194:23) gyp ERR! stack at ChildProcess.emit (events.js:315:20) gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:275:12) gyp ERR! System Linux 5.4.0-1083-oracle gyp ERR! command \u0026quot;/usr/local/bin/node\u0026quot; \u0026quot;/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\u0026quot; \u0026quot;rebuild\u0026quot; gyp ERR! cwd /app/node_modules/gatsby-plugin-manifest/node_modules/sharp gyp ERR! node -v v12.18.0 gyp ERR! node-gyp -v v5.1.0 -\u0026gt; Ch·ªãu ko fix ƒë∆∞·ª£c, n√™n ƒë·ªïi version nodejs kh√°c:\n FROM node:12.22.12-alpine OK, FROM node:14.18.2-alpine3.15 OK, FROM node:16.18.0-alpine OK.  ‚õî L·ªói python2 (no such package): L·ªói n√†y g·∫∑p khi d√πng FROM node:14.20.1-alpine\nERROR: unable to select packages: python2 (no such package): required by: world[python2] The command '/bin/sh -c apk add --no-cache python2 make gcc g++ autoconf automake libtool \u0026amp;\u0026amp; apk add vips-dev fftw-dev --update-cache \u0026amp;\u0026amp; rm -fR /var/cache/apk/*' returned a non-zero code: 1 Nguy√™n nh√¢n: do b·∫°n ƒëang d√πng python2\nSolution: trong Dockerfile ph·∫£i d√πng python3\n‚õî L·ªói pngquant-bin: L·ªói n√†y g·∫∑p khi d√πng FROM node:14.20.1-alpine\n\u0026gt; pngquant-bin@6.0.1 postinstall /app/node_modules/pngquant-bin \u0026gt; node lib/install.js spawn Unknown system error -8 pngquant pre-build test failed compiling from source Error: pngquant failed to build, make sure that libpng-dev is installed at /app/node_modules/bin-build/node_modules/execa/index.js:231:11 at runMicrotasks (\u0026lt;anonymous\u0026gt;) at processTicksAndRejections (internal/process/task_queues.js:95:5) at async Promise.all (index 0) Nguy√™n nh√¢n: https://github.com/imagemin/pngquant-bin/issues/36#issuecomment-381587705\nSolution: trong Dockerfile add th√™m bash zlib-dev libpng-dev\n‚õî L·ªói Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: g·∫∑p ph·∫£i ·ªü c√°c image nh∆∞:\n FROM node:14.18.2-alpine3.15, FROM node:12.22.12-alpine  internal/modules/cjs/loader.js:1102 throw new ERR_REQUIRE_ESM(filename, parentPath, packageJsonPath); ^ Error [ERR_REQUIRE_ESM]: Must use import to load ES Module: /usr/local/lib/node_modules/gatsby/node_modules/remark-mdx/index.js require() of ES modules is not supported. require() of /usr/local/lib/node_modules/gatsby/node_modules/remark-mdx/index.js from /usr/local/lib/node_modules/gatsby/node_modules/gatsby-recipes/dist/graphq l-server/server.js is an ES module file as it is a .js file whose nearest parent package.json contains \u0026quot;type\u0026quot;: \u0026quot;module\u0026quot; which defines all .js files in that package scope as ES modules. Instead rename index.js to end in .cjs, change the requiring code to use import(), or remove \u0026quot;type\u0026quot;: \u0026quot;module\u0026quot; from /usr/local/lib/node_modules/gatsby/node_modules/remark-mdx/package.json. -\u0026gt; https://github.com/gatsbyjs/gatsby/issues/33713\nSau khi check file package-lock.json:\nNguy√™n nh√¢n: Do gatsby-cli@2.19.3 ƒëang c·∫ßn gatsby-recipes@0.9.3.\nM√† gatsby-recipes@0.9.3 th√¨ l·∫°i c·∫ßn remark-mdx@2.0.0-next.4 v√† remark-mdxjs@2.0.0-next.4.\nSolution: n√™n tƒÉng version gatsby-cli l√™n v3:\nS·ª≠a Dockerfile th√†nh RUN npm install -g gatsby@3\nTuy nhi√™n ƒë·ª´ng nh·∫ßm l·∫´n m√¨nh ƒëang d√πng gatsby ver 3, trong container v·∫´n s·∫Ω th·∫•y gatsby ƒëang ·ªü ver 2:\nbash-5.1$ gatsby -v Gatsby CLI version: 3.14.2 Gatsby version: 2.32.13 3.2. Dockerfile cho Alpine image ƒê√¢y l√† Dockerfile ho√†n ch·ªânh m√† m√¨nh ƒë√£ d√πng ƒë·ªÉ build ƒë∆∞·ª£c image t·ª´ base image l√† NodeJS Alpine:\n#FROM node:12.22.12-alpine #FROM node:16.18.0-alpine FROM node:14.20.1-alpine RUN apk --no-cache update \\  \u0026amp;\u0026amp; apk --no-cache add python3 make gcc g++ autoconf automake libtool \\  \u0026amp;\u0026amp; apk add vips-dev fftw-dev zlib-dev libpng-dev bash --update-cache \\  \u0026amp;\u0026amp; rm -fR /var/cache/apk/* WORKDIR /app COPY ./package*.json /app/ RUN npm cache clean --force RUN npm install -g gatsby@3 RUN npm ci --only=production --no-optional # below command work with Node v14.20.1 but not Node v16.18.0 (remember to remove package-lock.json first) # i use npm install when I want to regenerate package-lock.json # RUN npm install --no-optional # fix error https://stackoverflow.com/q/67639482/9922066 RUN mkdir -p /app/.cache \u0026amp;\u0026amp; chmod -R 777 /app/.cache RUN mkdir -p /app/public \u0026amp;\u0026amp; chmod -R 777 /app/public COPY . /app RUN chown -R node:node /app USER node EXPOSE 8000 CMD [\u0026#34;gatsby\u0026#34;, \u0026#34;develop\u0026#34;, \u0026#34;-H\u0026#34;, \u0026#34;0.0.0.0\u0026#34; ] 3.3. L·ªói g·∫∑p khi build t·ª´ Debian slim image M√¨nh s·∫Ω s·ª≠ d·ª•ng image n√†y: FROM node:16.18.0-bullseye-slim\n‚õî L·ªói Error: pngquant failed to build, make sure that libpng-dev is installed\nnpm ERR! code 1 npm ERR! path /app/node_modules/pngquant-bin npm ERR! command failed npm ERR! command sh -c -- node lib/install.js npm ERR! compiling from source npm ERR! Command failed: /app/node_modules/pngquant-bin/vendor/pngquant --version npm ERR! /app/node_modules/pngquant-bin/vendor/pngquant: 1:ELF: not found npm ERR! /app/node_modules/pngquant-bin/vendor/pngquant: 1: G: not found npm ERR! npm ERR! npm ERR! pngquant pre-build test failed npm ERR! Error: pngquant failed to build, make sure that libpng-dev is installed npm ERR! at /app/node_modules/bin-build/node_modules/execa/index.js:231:11 npm ERR! at processTicksAndRejections (node:internal/process/task_queues:96:5) npm ERR! at async Promise.all (index 0) Nguy√™n nh√¢n: Dockerfile thi·∫øu libpng-dev\nSolution: trong Dockerfile add th√™m libpng-dev\n‚õî L·ªói Command failed: /app/node_modules/mozjpeg/vendor/cjpeg -version\nnpm ERR! code 1 npm ERR! path /app/node_modules/mozjpeg npm ERR! command failed npm ERR! command sh -c -- node lib/install.js npm ERR! compiling from source npm ERR! Command failed: /app/node_modules/mozjpeg/vendor/cjpeg -version npm ERR! /app/node_modules/mozjpeg/vendor/cjpeg: 1: Syntax error: word unexpected (expecting \u0026quot;)\u0026quot;) npm ERR! npm ERR! npm ERR! mozjpeg pre-build test failed npm ERR! Error: Command failed: /bin/sh -c ./configure --enable-static --disable-shared --disable-dependency-tracking --with-jpeg8 --prefix=\u0026quot;/app/node_modules/mozjpeg/vendor\u0026quot; --bindir=\u0026quot;/app/node_modules/mozjpeg/vendor\u0026quot; --libdir=\u0026quot;/app/node_modules/mozjpeg/vendor\u0026quot; npm ERR! ./configure: line 13631: PKG_PROG_PKG_CONFIG: command not found npm ERR! ./configure: line 13810: syntax error near unexpected token `libpng,' npm ERR! ./configure: line 13810: `PKG_CHECK_MODULES(libpng, libpng, HAVE_LIBPNG=1,' npm ERR! Nguy√™n nh√¢n: https://stackoverflow.com/a/64927666/9922066\nSolution: trong Dockerfile add th√™m automake autoconf libtool dpkg pkg-config libpng-dev g++\n‚õî L·ªói Can't find Python executable \u0026quot;python\u0026quot;, you can set the PYTHON env variable.. G·∫∑p khi d√πng c√°c image:\n FROM node:16.18.0-bullseye-slim, FROM node:16.18.0-alpine  Ch√∫ √Ω: N·∫øu m√¨nh gi·ªØ nguy√™n package-lock.json ƒëang tr·ªè ƒë·∫øn \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot;, package.json c≈©ng ƒëang tr·ªè ƒë·∫øn \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot;, run npm ci th√¨ s·∫Ω ko c√≥ l·ªói n√†y.\nNh∆∞ng\u0026hellip;\nQu√° tr√¨nh l√†m m√¨nh th·∫•y:\n package.json c√≥ \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot; l√† ko √Ω nghƒ©a l·∫Øm, v√¨ c√°c package gatsby-transformer-sharp v√† gatsby-plugin-sharp v·∫´n ƒëang tr·ªè ƒë·∫øn sharp@0.27.2. M√¨nh th·ª≠ x√≥a package-lock.json, S·ª≠a package.json t·ª´ \u0026quot;sharp\u0026quot;: \u0026quot;^0.28.3\u0026quot; th√†nh \u0026quot;sharp\u0026quot;: \u0026quot;^0.27.2\u0026quot;. V√† run RUN npm install ƒë·ªÉ generate l·∫°i package-lock.json.  S·∫Ω g·∫∑p l·ªói n√†y:\nnpm ERR! code 2 npm ERR! path /app/node_modules/leveldown npm ERR! command failed npm ERR! command sh -c -- prebuild --install npm ERR! prebuild info begin Prebuild version 4.5.0 npm ERR! prebuild ERR! configure error npm ERR! prebuild ERR! stack Error: Can't find Python executable \u0026quot;python\u0026quot;, you can set the PYTHON env variable. npm ERR! prebuild ERR! stack at PythonFinder.failNoPython (/app/node_modules/node-gyp/lib/configure.js:484:19) npm ERR! prebuild ERR! stack at PythonFinder.\u0026lt;anonymous\u0026gt; (/app/node_modules/node-gyp/lib/configure.js:406:16) npm ERR! prebuild ERR! stack at F (/app/node_modules/which/which.js:68:16) npm ERR! prebuild ERR! stack at E (/app/node_modules/which/which.js:80:29) npm ERR! prebuild ERR! stack at /app/node_modules/which/which.js:89:16 npm ERR! prebuild ERR! stack at /app/node_modules/isexe/index.js:42:5 npm ERR! prebuild ERR! stack at /app/node_modules/isexe/mode.js:8:5 npm ERR! prebuild ERR! stack at FSReqCallback.oncomplete (node:fs:202:21) K·ªÉ c·∫£ khi Dockerfile c√≥ ENV PYTHON=/usr/bin/python3 th√¨ l·∫°i s·∫Ω g·∫∑p l·ªói ti·∫øp theo SyntaxError: invalid syntax:\nnpm ERR! code 2 npm ERR! path /app/node_modules/leveldown npm ERR! command failed npm ERR! command sh -c -- prebuild --install npm ERR! prebuild info begin Prebuild version 4.5.0 npm ERR! prebuild ERR! configure error npm ERR! prebuild ERR! stack Error: Command failed: /usr/bin/python3 -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 npm ERR! prebuild ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! stack ^ npm ERR! prebuild ERR! stack SyntaxError: invalid syntax npm ERR! prebuild ERR! stack npm ERR! prebuild ERR! stack at ChildProcess.exithandler (node:child_process:402:12) npm ERR! prebuild ERR! stack at ChildProcess.emit (node:events:513:28) npm ERR! prebuild ERR! stack at maybeClose (node:internal/child_process:1100:16) npm ERR! prebuild ERR! stack at Process.ChildProcess._handle.onexit (node:internal/child_process:304:5) npm ERR! prebuild ERR! not ok npm ERR! prebuild ERR! build Error: Command failed: /usr/bin/python3 -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! build File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 npm ERR! prebuild ERR! build import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; npm ERR! prebuild ERR! build ^ npm ERR! prebuild ERR! build SyntaxError: invalid syntax npm ERR! prebuild ERR! build npm ERR! prebuild ERR! build at ChildProcess.exithandler (node:child_process:402:12) ƒê·∫øn l·ªói n√†y th√¨ h∆°i n·∫£n, c√≥ l·∫Ω l√† do python3 ko ph√π h·ª£p, ph·∫£i d√πng python2 chƒÉng?\nTh·ª≠ s·ª≠a l·∫°i Dockerfile RUN apt-get install python2 v√† ENV PYTHON=/usr/bin/python2 th√¨ l·∫°i g·∫∑p l·ªói d·ªã h∆°n:\nnpm ERR! code 2 npm ERR! path /app/node_modules/leveldown npm ERR! command failed npm ERR! command sh -c -- prebuild --install npm ERR! make: Entering directory '/app/node_modules/leveldown/build' npm ERR! CXX(target) Release/obj.target/leveldb/deps/leveldb/leveldb-1.18.0/db/builder.o npm ERR! make: Leaving directory '/app/node_modules/leveldown/build' npm ERR! prebuild info begin Prebuild version 4.5.0 npm ERR! prebuild http GET https://nodejs.org/download/release/v16.18.0/node-v16.18.0-headers.tar.gz npm ERR! prebuild http 200 https://nodejs.org/download/release/v16.18.0/node-v16.18.0-headers.tar.gz npm ERR! prebuild http GET https://nodejs.org/download/release/v16.18.0/SHASUMS256.txt npm ERR! prebuild http 200 https://nodejs.org/download/release/v16.18.0/SHASUMS256.txt npm ERR! (node:27) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only. npm ERR! (Use `node --trace-deprecation ...` to show where the warning was created) npm ERR! prebuild info spawn /usr/bin/python2 npm ERR! prebuild info spawn args [ npm ERR! prebuild info spawn args '/app/node_modules/node-gyp/gyp/gyp_main.py', npm ERR! prebuild info spawn args 'binding.gyp', npm ERR! prebuild info spawn args '-f', npm ERR! prebuild info spawn args 'make', npm ERR! prebuild info spawn args '-I', npm ERR! prebuild info spawn args '/app/node_modules/leveldown/build/config.gypi', npm ERR! prebuild info spawn args '-I', npm ERR! prebuild info spawn args '/app/node_modules/node-gyp/addon.gypi', npm ERR! prebuild info spawn args '-I', npm ERR! prebuild info spawn args '/app/node_modules/leveldown/16.18.0/include/node/common.gypi', npm ERR! prebuild info spawn args '-Dlibrary=shared_library', npm ERR! prebuild info spawn args '-Dvisibility=default', npm ERR! prebuild info spawn args '-Dnode_root_dir=/app/node_modules/leveldown/16.18.0', npm ERR! prebuild info spawn args '-Dnode_gyp_dir=/app/node_modules/node-gyp', npm ERR! prebuild info spawn args '-Dnode_lib_file=/app/node_modules/leveldown/16.18.0/\u0026lt;(target_arch)/node.lib', npm ERR! prebuild info spawn args '-Dmodule_root_dir=/app/node_modules/leveldown', npm ERR! prebuild info spawn args '-Dnode_engine=v8', npm ERR! prebuild info spawn args '--depth=.', npm ERR! prebuild info spawn args '--no-parallel', npm ERR! prebuild info spawn args '--generator-output', npm ERR! prebuild info spawn args 'build', npm ERR! prebuild info spawn args '-Goutput_dir=.' npm ERR! prebuild info spawn args ] npm ERR! prebuild info spawn make npm ERR! prebuild info spawn args [ 'BUILDTYPE=Release', '-C', 'build' ] npm ERR! In file included from ../deps/leveldb/leveldb-1.18.0/port/port_posix.h:47, npm ERR! from ../deps/leveldb/leveldb-1.18.0/port/port.h:16, npm ERR! from ../deps/leveldb/leveldb-1.18.0/db/filename.h:14, npm ERR! from ../deps/leveldb/leveldb-1.18.0/db/builder.cc:7: npm ERR! ../deps/leveldb/leveldb-1.18.0/port/atomic_pointer.h:211:2: error: #error Please implement AtomicPointer for this platform. npm ERR! 211 | #error Please implement AtomicPointer for this platform. npm ERR! | ^~~~~ npm ERR! make: *** [deps/leveldb/leveldb.target.mk:162: Release/obj.target/leveldb/deps/leveldb/leveldb-1.18.0/db/builder.o] Error 1 npm ERR! prebuild ERR! build error L·ªói n√†y l·∫°i ch·ªãu ko th·ªÉ hi·ªÉu, coi nh∆∞ ko fix ƒë∆∞·ª£c.\nSolution: M√¨nh chuy·ªÉn sang h∆∞·ªõng kh√°c, s·ª≠ d·ª•ng NodeJS 14:\n FROM node:14.20.1-alpine v√† FROM node:14.20.1-bullseye-slim\nth√¨ l·∫°i OK ko c√≥ l·ªói g√¨, trong container khi start l√™n m√¨nh s·∫Ω l·∫•y dc package-lock.json ra OK. ü§£  Nh∆∞ v·∫≠y d∆∞·ªùng nh∆∞ version NodeJS ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn c√¢u l·ªánh npm install.\n-\u0026gt; K·∫øt lu·∫≠n l·∫°i l√† m√¨nh n√™n s·ª≠ d·ª•ng NodeJS v14.20.1.\n3.4. Dockerfile cho Debian slim image ƒê√¢y l√† Dockerfile ho√†n ch·ªânh m√† m√¨nh ƒë√£ d√πng ƒë·ªÉ build ƒë∆∞·ª£c image t·ª´ base image l√† NodeJS Deiam slim:\n# FROM node:12.22.12-bullseye-slim # FROM node:16.18.0-bullseye-slim FROM node:14.20.1-bullseye-slim RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\  automake \\  autoconf \\  build-essential \\  software-properties-common \\  libtool \\  dpkg \\  pkg-config \\  libpng-dev \\  g++ \\  bash \\  python3 \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY ./package*.json /app/ RUN npm cache clean --force RUN npm install -g gatsby@3 RUN npm ci --only=production --no-optional # below command work with Node v14.20.1 but not Node v16.18.0 (remember to remove package-lock.json first) # i use npm install when I want to regenerate package-lock.json # RUN npm install --no-optional # fix error https://stackoverflow.com/q/67639482/9922066 RUN mkdir -p /app/.cache \u0026amp;\u0026amp; chmod -R 777 /app/.cache RUN mkdir -p /app/public \u0026amp;\u0026amp; chmod -R 777 /app/public COPY . /app RUN chown -R node:node /app USER node EXPOSE 8000 CMD [\u0026#34;gatsby\u0026#34;, \u0026#34;develop\u0026#34;, \u0026#34;-H\u0026#34;, \u0026#34;0.0.0.0\u0026#34; ] 4. Docker Compose file Sau khi ƒë√£ c√≥ Dockerfile th√¨ c√≥ th·ªÉ build dc image:\ncd /hoangmnsd-the404blog-theme/ docker build -t gatsby-app -f Dockerfile.dev . Image ƒë∆∞·ª£c build kh√° n·∫∑ng ~3.6GB:\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE gatsby-app latest ddd14693d9f3 34 minutes ago 3.62GB node 16.18.0-bullseye-slim 1e696a126824 3 days ago 185MB node 16.18.0-alpine e20db79e2fb9 3 days ago 114MB T·∫°o file docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: web: image: gatsby-app:latest container_name: gatsby-app ports: - \u0026#34;8000:8000\u0026#34; volumes: # map all dir but exclude `node_modules, .cache, public` dir - /app/node_modules - /app/.cache/ - /app/public/ - .:/app environment: - NODE_ENV=development Run docker-compose up -d v√† check logs c·ªßa container docker logs gatsby-app ko c√≥ l·ªói g√¨ ƒë·∫∑c bi·ªát l√† OK\nTruy c·∫≠p v√†o http://VM_PUBLIC_IP:8000 th√¨ b·∫°n s·∫Ω th·∫•y m√¥i tr∆∞·ªùng develop ƒë√£ ok\nB√¢y gi·ªù d√πng WinSCP sync folder t·ª´ local l√™n VM:\n Ch·ªçn ch·∫ø ƒë·ªô \u0026ldquo;Keeping remote directory up to date \u0026hellip;\u0026rdquo; Ch·ªçn Synchronize options: Delete files, exclude c√°c folder sau: .git, node_modules, .cache, public  Gi·ªù m·ªü VS code, edit c√°c file trong /content/, /src/, bao g·ªìm c·∫£ c√°c file nh∆∞ /gatsby-config.js xem sao nh√©.\nM·ªçi th·ª© s·∫Ω ƒë∆∞·ª£c sync t·ª± ƒë·ªông v√†o c√°c volume c·ªßa gatsby container, sau ƒë√≥ ph·∫£n √°nh ngay l√™n website c·ªßa b·∫°n ·ªü ƒë∆∞·ªùng d·∫´n http://VM_PUBLIC_IP:8000.\nC·∫ßn ch√∫ √Ω ƒë√¢y l√† qu√° tr√¨nh Develop th√¥i nh√©. Trong m√¥i tr∆∞·ªùng Production, ·ªü file Dockerfile - sau khi build ra ƒë∆∞·ª£c public folder, b·∫°n n√™n ƒë∆∞a artifacts trong public v√†o Nginx image v√† ch·ªâ release Container Nginx ƒë√≥ th√¨ s·∫Ω nh·∫π h∆°n.\n5. Use multi-stage to cache node_modules Khi build c√°c ·ª©ng d·ª•ng NodeJS lu√¥n ph·∫£i ƒë·ªÉ √Ω ƒë·∫øn node_modules, ƒë√¢y l√† folder s·∫Ω t·ªën nhi·ªÅu th·ªùi gian ƒë·ªÉ install v√† n√≥ c≈©ng kh√° n·∫∑ng, l√™n t·ªõi h∆°n 1 Gb.\nS·ª≠ d·ª•ng multi-stage trong Dockerfile gi√∫p ch√∫ng ta khi build l·∫°i c√°c image m·ªõi s·∫Ω ƒë·ª° t·ªën th·ªùi gian h∆°n.\nV·ªõi Dockerfile nh∆∞ ·ªü ph·∫ßn tr∆∞·ªõc th√¨ m·ªói khi ph·∫£i build l·∫°i image ta s·∫Ω t·ªën 120s ƒë·∫øn 200s cho ph·∫ßn run npm ci.\nM·∫∑c d√π npm ci ƒë∆∞·ª£c recommend s·ª≠ d·ª•ng h∆°n so v·ªõi npm install nh∆∞ng nh∆∞·ª£c ƒëi·ªÉm c·ªßa npm ci l·∫°i l√† n√≥ lu√¥n x√≥a node_modules tr∆∞·ªõc khi install packages.\nV√¨ th·∫ø trong Dockerfile m·ªõi s·∫Øp ƒë∆∞·ª£c vi·∫øt s·∫Ω ko d√πng npm ci n·ªØa m√† quay l·∫°i v·ªõi npm install.\nTuy nhi√™n ta bi·∫øt npm install c√≥ 1 khuy·∫øt ƒëi·ªÉm l√† n√≥ s·∫Ω t·ª± ƒë·ªông update package.json v√† package-lock.json ƒë√¢y l√† ƒëi·ªÅu m√¨nh ko mong mu·ªën.\nƒê·ªÉ kh·∫Øc ƒëi·ªÉm khuy·∫øt ƒëi·ªÉm n√†y th√¨ B·∫°n c√≥ th·ªÉ specific version c·ªßa c√°c package trong package.json b·∫±ng c√°ch ki·ªÉu nh∆∞ n√†y:\n\u0026#34;react\u0026#34;: \u0026#34;^16.0.0\u0026#34; // carat: allow 16.1.0 \u0026#34;react\u0026#34;: \u0026#34;~16.0.0\u0026#34; // tilde: allow 16.0.1 \u0026#34;react\u0026#34;: \u0026#34;16.0.0\u0026#34; // exact: only 16.0.0 Trong project n√†y c·ªßa m√¨nh th√¨ ko c·∫ßn n√™n m√¨nh s·∫Ω ƒë·ªÉ nguy√™n package.json nh∆∞ n√≥ hi·ªán t·∫°i.\nM√¨nh s·∫Ω t√°ch Dockerfile th√†nh 2 file ri√™ng:\n File ƒë·∫ßu ti√™n l√† Dockerfile.dev-bullseye-node-cache ƒë·ªÉ build node-cache ch·ª©a node_modules.\nDockerfile.dev-bullseye-node-cache:  FROM node:14.20.1-bullseye-slim as build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ automake \\ autoconf \\ build-essential \\ software-properties-common \\ libtool \\ dpkg \\ pkg-config \\ libpng-dev \\ g++ \\ bash \\ python3 \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY ./package*.json /app/ RUN npm install -g gatsby@3.14.2 --no-optional --no-audit --progress=false RUN npm install --no-optional --no-audit --progress=false FROM alpine as release COPY --from=build /app/node_modules ./node_modules # because gatsby@3.14.2 is global package, it's node_modules store in `/usr/local/lib/node_modules`, i need to cache it to reuse later COPY --from=build /usr/local/lib/node_modules /usr/local/lib/global_node_modules  File th·ª© hai l√† Dockerfile.dev-bullseye-use-cache ƒë·ªÉ build image s·ª≠ d·ª•ng:  FROM node-cache:latest as cache FROM node:14.20.1-bullseye-slim as build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ automake \\ autoconf \\ build-essential \\ software-properties-common \\ libtool \\ dpkg \\ pkg-config \\ libpng-dev \\ g++ \\ bash \\ python3 \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY ./package*.json /app/ COPY --from=cache /node_modules /app/node_modules # because gatsby@3.14.2 is global package, i need to bring it from node-cache to global package path: /usr/local/lib/node_modules COPY --from=cache /usr/local/lib/global_node_modules /usr/local/lib/node_modules # gatsby@3.14.2 work as a binary at /usr/local/bin/gatsby - a symlink from /usr/local/lib/node_modules/gatsby/cli.js RUN ln -s /usr/local/lib/node_modules/gatsby/cli.js /usr/local/bin/gatsby RUN npm install --no-optional --no-audit --progress=false --prefer-offline # fix error https://stackoverflow.com/q/67639482/9922066 RUN mkdir -p /app/.cache \u0026amp;\u0026amp; chmod -R 777 /app/.cache RUN mkdir -p /app/public \u0026amp;\u0026amp; chmod -R 777 /app/public COPY . /app RUN chown -R node:node /app USER node EXPOSE 8000 CMD [\u0026quot;gatsby\u0026quot;, \u0026quot;develop\u0026quot;, \u0026quot;-H\u0026quot;, \u0026quot;0.0.0.0\u0026quot; ] ƒê·ªÉ s·ª≠ d·ª•ng, ƒë·∫ßu ti√™n c·∫ßn build images:\ncd /hoangmnsd-the404blog-theme/ DOCKER_BUILDKIT=1 docker build -t node-cache -f Dockerfile.dev-bullseye-node-cache . # now you have image `node-cache:latest` for re-using in below build step # it may take 382.1s in total to FINISHED, but you only need to run it once docker build -t gatsby-app -f Dockerfile.dev-bullseye-use-cache . # at first time, it may take 120s in `npm install` step # now you have image `gatsby-app:latest` If you change something in package.json, just rebuild the gatsby-app image:\ndocker build -t gatsby-app -f Dockerfile.dev-bullseye-use-cache . # from second time, it may take 25s in `npm install` step Run Docker Compose:\ncd /hoangmnsd-the404blog-theme/ docker-compose up -d # Your app is running on http://localhost:8000  Now, if you change something in ./src/* or ./content/* or ./gatsby-*, docker-compose automaticly mount them to container, no need action on re-build/ re-run container.\n‚õî L·ªói: Tr√™n 1 s·ªë OS (d√πng ki·∫øn tr√∫c arm64), c√≥ v·∫ª Docker BuildKit ko nh·∫≠n ra local image: https://github.com/docker/cli/issues/3286.\nD·∫´n ƒë·∫øn l·ªói n√†y:\n$ DOCKER_BUILDKIT=1 docker build -t gatsby-app -f Dockerfile.dev-bullseye-use-cache . [+] Building 0.9s (5/5) FINISHED =\u0026gt; [internal] load build definition from Dockerfile.dev-bullseye-use-cache 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 61B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 34B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/node:14.20.1-bullseye-slim 0.4s =\u0026gt; ERROR [internal] load metadata for docker.io/library/node-cache:latest 0.8s =\u0026gt; [auth] library/node-cache:pull token for registry-1.docker.io 0.0s ------ \u0026gt; [internal] load metadata for docker.io/library/node-cache:latest: ------ failed to solve with frontend dockerfile.v0: failed to create LLB definition: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed N√≥ c·ª© chƒÉm chƒÉm t√¨m image docker.io/library/node-cache:latest tuy nhi√™n ƒë√≥ l√† 1 image local c·ªßa m√¨nh.\nC√≥ 1 c√°ch workaround: https://github.com/docker/cli/issues/3286#issuecomment-1168394369.\nHi·ªán m√¨nh ch∆∞a th·∫•y c√°ch fix ph√π h·ª£p v√¨ m√¨nh ko mu·ªën d√πng --platform=linux/arm64 trong c√°c command build c≈©ng nh∆∞ trong Dockerfile.\n‚õî L·ªói: exec user process caused: exec format error -\u0026gt; C√πng l√† Ubuntu18.04 OS, nh∆∞ng n·∫øu CPU kh√°c nhau th√¨ c√πng 1 command v·∫´n c√≥ th·ªÉ x·∫£y ra t√¨nh tr·∫°ng: \u0026ldquo;M√°y n√†y l·ªói nh∆∞ng m√°y kia OK\u0026rdquo;.\nL√† do CPU c·ªßa b·∫°n ƒëang d√πng arm64 architecture. Trong khi ph·∫ßn l·ªõn docker image dc thi·∫øt k·∫ø cho amd64 architecture.\nhttps://forums.docker.com/t/daemon-error-responses-exec-format-error-and-container-is-restarting-wait-until-the-container-is-running/110385/2\n6. Ph√¢n bi·ªát CMD v√† ENTRYPOINT trong Dockerfile 1 b√†i vi·∫øt d·ªÖ hi·ªÉu: https://phoenixnap.com/kb/docker-cmd-vs-entrypoint\nT√≥m t·∫Øt:\n CMD th∆∞·ªùng ƒë·ªÉ cu·ªëi c√πng trong Dockerfile, d√πng ƒë·ªÉ run c√°c c√¢u l·ªánh default khi container ƒë∆∞·ª£c run, c√≥ th·ªÉ b·ªã ghi ƒë√® b·ªüi docker run. ENTRYPOINT c≈©ng th∆∞·ªùng ƒë·ªÉ cu·ªëi c√πng trong Dockerfile, d√πng ƒë·ªÉ run c√°c c√¢u l·ªánh b·∫Øt bu·ªôc ph·∫£i ch·∫°y khi container ƒë∆∞·ª£c run.  Ex:\nFROM ubuntu MAINTAINER sofija RUN apt-get update ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;Hello\u0026quot;] CMD [\u0026quot;World1234\u0026quot;] Test:\nsudo docker build . # run `docker run` sudo docker run [container_name] # output: Hello World1234 # run `docker run` with command sudo docker run [container_name] Hoangmnsd # output: Hello Hoangmnsd # -\u0026gt; b·ªüi v√¨ Hoangmnsd ƒë√£ ghi ƒë√® l√™n CMD, nh∆∞ng n√≥ ko th·ªÉ ghi ƒë√® l√™n ENTRYPOINT, n√™n v·∫´n c√≥ ch·ªØ Hello 7. Run on Gitlab CI [PENDING] Ph·∫ßn ti·∫øp theo m√¨nh s·∫Ω c·ªë g·∫Øng ƒë∆∞a ·ª©ng d·ª•ng n√†y l√™n Gitlab CI. √ù t∆∞·ªüng s·∫Ω l√†:\n D√πng VM l√†m Gitlab Runner Vi·∫øt th√™m v√†o Dockerfile ph·∫ßn build ra public folder v√† s·ª≠ d·ª•ng n√≥. Push code l√™n Gitlab, file .gitlab-ci s·∫Ω trigger Runner build ra image v√† push image ƒë√≥ l√™n 1 Registry n√†o ƒë√≥ (C√≥ th·ªÉ ACR). ·ªû d∆∞·ªõi local c√≥ th·ªÉ pull image v·ª´a build v·ªÅ v√† s·ª≠ d·ª•ng.\nƒê√¢y l√† 1 lu·ªìng c∆° b·∫£n c·ªßa CI.  \u0026hellip;\n8. Best practices 8.1. D√πng npm ci thay cho npm install N√™n xem x√©t vi·ªác trong Dockerfile th√™m package-lock.json v√†o r·ªìi m·ªõi install node_modules. ƒêi·ªÅu ƒë√≥ gi√∫p c√°c version ƒë∆∞·ª£c lock l·∫°i. Theo practices th√¨ n√™n s·ª≠ d·ª•ng RUN npm ci --only=production thay cho npm install. ( Adding --only=prod or --production would not install devDependencies and just install dependencies.)\n8.2. D√πng Debian slim image thay v√¨ Alpine image ∆Øu ti√™n c√°c image Debian slim image h∆°n Alpine image v√¨:\n It uses the bullseye image variant which is the current stable Debian 11 version with a far enough end-of-life date. And finally it uses the slim image variant to specify a smaller software footprint of the operating system which results in less than 200MB of image size, including the Node.js runtime and tooling.\n  Node.js Alpine is an unofficial Docker container image build that is maintained by the Node.js Docker team. The Node.js image bundles the Alpine operating system which is powered by the minimal busybox software tooling and the musl C library implementation. These two Node.js Alpine image characteristics contribute to the Docker image being unofficially supported by the Node.js team. Furthermore, many security vulnerabilities scanners can‚Äôt easily detect software artifacts or runtimes on Node.js Alpine images, which is counterproductive to efforts to secure your container images\n V·ªõi Alpine image th√¨ s·∫Ω install c√°c packge b·∫±ng apk command ki·ªÉu n√†y:\nFROM node:14.20.1-alpine RUN apk --no-cache update V·ªõi Debian slim image th√¨ s·∫Ω install c√°c package b·∫±ng apt-get command ki·ªÉu n√†y:\nFROM node:16.17.0-bullseye-slim RUN apt-get update -\u0026gt; C√≥ th·ªÉ th·∫•y apt-get command s·∫Ω g·∫ßn g≈©i th√¢n thi·ªán v·ªõi Ubuntu/Linux h∆°n apk command.\n8.3. Ch·ªâ ƒë·ªãnh user node thay v√¨ root Kh√¥ng run container b·∫±ng root user, trong Dockerfile h√£y d√πng USER node ƒë·ªÉ run c√°c app nodejs.\n8.4. Nh·ªõ run apt-get update H√£y run RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y trong Dockerfile ƒë·ªÉ fix Docker image vulnerabilities.\n8.5. N√™n d√πng file .dockerignore D√πng .dockerignore file ƒë·ªÉ ko ƒë∆∞a c√°c file ko c·∫ßn thi·∫øt v√†o khi build image.\n8.6. D√πng c√°c tools linter v√† dive https://github.com/hadolint/hadolint -\u0026gt; tool ƒë·ªÉ check convention cho Dockerfile.\nhttps://github.com/wagoodman/dive -\u0026gt; tool ƒë·ªÉ check size c·ªßa c√°c layer, t·ª´ ƒë√≥ gi√∫p b·∫°n debug ƒë∆∞·ª£c command n√†o t·ªën nhi·ªÅu dung l∆∞·ª£ng c·ªßa image.\n8.7. COPY v√† chown/chmod tr√™n c√πng 1 command N·∫øu d√πng dive ƒë·ªÉ ph√¢n t√≠ch image t·∫°o b·ªüi Dockerfile nh∆∞ n√†y:\n... COPY . /app RUN chown -R node:node /app ... C√≥ th·ªÉ b·∫°n s·∫Ω th·∫•y vi·ªác command RUN chown -R node:node /app s·∫Ω ti√™u th·ª• 1 l∆∞·ª£ng dung l∆∞·ª£ng nh·∫•t ƒë·ªãnh. ƒê√≥ l√† do vi·ªác change owner c·∫ßn n√≥ ph·∫£i copy file ra 1 read-only layer v√† paste l·∫°i v√†o 1 layer m·ªõi, v√¥ t√¨nh l√†m t·ªën th·ªùi gian v√† duplicate dung l∆∞·ª£ng image l√™n\nN√™n s·ª≠a l·∫°i th√†nh:\n... COPY --chown=node:node . /app ... Khi ƒë√≥ s·∫Ω gi·∫£m ƒë∆∞·ª£c th·ªùi gian v√† dung l∆∞·ª£ng image kh√° nhi·ªÅu. Tr∆∞·ªùng h·ª£p c·ªßa m√¨nh l√† t·ª´ 2.93 GB xu·ªëng 1.85 GB. üòç\n8.8. D√πng multi-stage build ƒë·ªÉ prevent secret leak (?) https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/#\nD√πng MULTI-STAGE BUILDS ƒë·ªÉ ngƒÉn ch·∫∑n vi·ªác leak c√°c sensitive secret v√†o trong image.\nN·∫øu b·∫°n ƒëang build Docker cho c√¥ng vi·ªác, th√¨ kh·∫£ nƒÉng cao b·∫°n s·∫Ω d√πng 1 private NPM registry, trong tr∆∞·ªùng h·ª£p ƒë√≥ b·∫°n s·∫Ω c·∫ßn truy·ªÅn NPM_TOKEN v√†o ƒë·ªÉ run npm install\nV√≠ d·ª•:\nFROM node:16.17.0-bullseye-slim RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dumb-init ENV NODE_ENV production ENV NPM_TOKEN 1234 WORKDIR /usr/src/app COPY --chown=node:node . . #RUN npm ci --only=production RUN echo \u0026quot;//registry.npmjs.org/:_authToken=$NPM_TOKEN\u0026quot; \u0026gt; .npmrc \u0026amp;\u0026amp; \\ npm ci --only=production USER node CMD [\u0026quot;dumb-init\u0026quot;, \u0026quot;node\u0026quot;, \u0026quot;server.js\u0026quot;] -\u0026gt; Vi·∫øt nh∆∞ tr√™n l√† NOT GOOD V√¨ ƒë∆∞a TOKEN v√†o .npmrc file n√™n b·∫°n s·∫Ω c·∫ßn x√≥a file ƒë√≥ ƒëi:\nRUN echo \u0026quot;//registry.npmjs.org/:_authToken=$NPM_TOKEN\u0026quot; \u0026gt; .npmrc \u0026amp;\u0026amp; \\ npm ci --only=production; \\ rm -rf .npmrc Khi build b·∫°n s·∫Ω truy·ªÅn NPM_TOKEN v√†o qu√° bi·∫øn m√¥i tr∆∞·ªùng:\ndocker build . -t nodejs-tutorial --build-arg NPM_TOKEN=1234 -\u0026gt; Nh∆∞ng nh∆∞ th·∫ø v·∫´n NOT GOOD v√¨ khi run docker history IMAGE_NAME s·∫Ω th·∫•y ƒë∆∞·ª£c history v√† gi√° tr·ªã c·ªßa NPM_TOKEN:\nIMAGE CREATED CREATED BY SIZE COMMENT b4c2c78acaba About a minute ago CMD [\u0026quot;dumb-init\u0026quot; \u0026quot;node\u0026quot; \u0026quot;server.js\u0026quot;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago USER node 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago RUN |1 NPM_TOKEN=1234 /bin/sh -c echo \u0026quot;//reg‚Ä¶ 5.71MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago ARG NPM_TOKEN 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago COPY . . # buildkit 15.3kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago WORKDIR /usr/src/app 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago ENV NODE_ENV=production 0B buildkit.dockerfile.v0 ·ªû ƒë√¢y ng∆∞·ªùi ta h∆∞·ªõng d·∫´n chia l√†m 2 stage:\n# --------------\u0026gt; The build image FROM node:latest AS build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dumb-init ARG NPM_TOKEN WORKDIR /usr/src/app COPY package*.json /usr/src/app/ RUN echo \u0026quot;//registry.npmjs.org/:_authToken=$NPM_TOKEN\u0026quot; \u0026gt; .npmrc \u0026amp;\u0026amp; \\ npm ci --only=production \u0026amp;\u0026amp; \\ rm -f .npmrc # --------------\u0026gt; The production image FROM node:16.17.0-bullseye-slim ENV NODE_ENV production COPY --from=build /usr/bin/dumb-init /usr/bin/dumb-init USER node WORKDIR /usr/src/app COPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules COPY --chown=node:node . /usr/src/app CMD [\u0026quot;dumb-init\u0026quot;, \u0026quot;node\u0026quot;, \u0026quot;server.js\u0026quot;] Stage build ƒë∆∞·ª£c s·ª≠ d·ª•ng NPM_TOKEN v√† sau ƒë√≥ s·∫Ω l√† input c·ªßa stage cu·ªëi c√πng.\nKhi ƒë√≥ docker history IMAGE_NAME s·∫Ω KH√îNG th·∫•y ƒë∆∞·ª£c history v√† gi√° tr·ªã c·ªßa NPM_TOKEN\nTuy nhi√™n d√πng c√°ch n√†y b·∫°n s·∫Ω c·∫ßn ch·∫Øc ch·∫Øn file .npmrc ko ·ªü trong file .dockerignore, v√¨ ch√∫ng ta c·∫ßn n√≥ khi RUN npm ci m√†. V√† l·∫°i khi run docker build, secret v·∫´n ·ªü dang plain-text tr√™n command.\nƒêi·ªÅu n√†y g√¢y ra 1 ch√∫t kh√≥ ch·ªãu v√¨ ko ƒë·∫£m b·∫£o security l·∫Øm. H√£y t√¨m hi·ªÉu c√°ch ti·∫øp theo.\n8.9. D√πng mount=type=secret ƒë·ªÉ prevent secret leak V·ªõi c√°ch n√†y b·∫°n c√≥ th·ªÉ ƒë∆∞a file .npmrc v√†o .dockerignore ngay t·ª´ ƒë·∫ßu:\nfile .dockerignore:\n.dockerignore node_modules npm-debug.log Dockerfile .git .gitignore .npmrc Dockerfile:\n# --------------\u0026gt; The build image FROM node:latest AS build RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dumb-init WORKDIR /usr/src/app COPY package*.json /usr/src/app/ RUN --mount=type=secret,mode=0644,id=npmrc,target=/usr/src/app/.npmrc npm ci --only=production # --------------\u0026gt; The production image FROM node:16.17.0-bullseye-slim ENV NODE_ENV production COPY --from=build /usr/bin/dumb-init /usr/bin/dumb-init USER node WORKDIR /usr/src/app COPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules COPY --chown=node:node . /usr/src/app CMD [\u0026quot;dumb-init\u0026quot;, \u0026quot;node\u0026quot;, \u0026quot;server.js\u0026quot;] -\u0026gt; B·∫°n s·∫Ω th·∫•y c√¢u l·ªánh RUN --mount=type=secret,mode=0644,id=npmrc,target=/usr/src/app/.npmrc npm ci --only=production.\nN√≥ ƒë√£ mount c√°i id id=npmrc v√†o trong container ·ªü path /usr/src/app/.npmrc\nC√¢u l·ªánh doker build s·∫Ω nh∆∞ n√†y:\ndocker build . -t nodejs-tutorial --secret id=npmrc,src=.npmrc -\u0026gt; Kh√¥ng c√≤n secret d∆∞·ªõi d·∫°ng plain-text n·ªØa. B·∫°n ch·ªâ ƒë·ªãnh 1 secret v·ªõi id=npmrc, source file ·ªü path .npmrc\nC√°ch n√†y c√≥ th·ªÉ n√≥i l√† chuy√™n nghi·ªáp h∆°n c√°ch tr√™n kh√° nhi·ªÅu ƒë·∫•y.\nT∆∞∆°ng t·ª± nh∆∞ file .npmrc, c√°c b·∫°n c√≥ th·ªÉ √°p d·ª•ng cho tr∆∞·ªùng h·ª£p git clone t·ª´ 1 private git repo v·ªÅ.\nKhi ƒë√≥ ch·∫Øc h·∫≥n c√°c b·∫°n c≈©ng c·∫ßn ƒë∆∞a PRIVATE_KEY v√†o builder ƒë·ªÉ s·ª≠ d·ª•ng, ko mu·ªën cho PRIVATE_KEY v√†o image th√¨ h√£y d√πng c√°ch mount=type=secret n√†y nh√©.\n8.10. D√πng mount=type=ssh ƒë·ªÉ prevent SSH key leak http://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/\n ƒê·ªÉ test th√¨ c·∫ßn t·ª± generate SSH keypair tr√™n m√°y n√†o c≈©ng ƒë∆∞·ª£c, m√¨nh t·∫°o tr√™n Laptop:  ssh-keygen # B√†i n√†y n√≥i r·∫±ng c√¢u l·ªánh tr√™n generate ra 1 b·ªô key y·∫øu v√† d·ªÖ b·ªã bruteforce:  # Source: https://dev.to/levivm/how-to-use-ssh-and-ssh-agent-forwarding-more-secure-ssh-2c32 # N·∫øu c√≥ th·ªÉ h√£y d√πng command sau:  # ssh-keygen -o -a 100 -t ed25519 b·∫°n s·∫Ω ƒë∆∞·ª£c 1 b·ªô key:\ngitlab_com_rsa gitlab_com_rsa.pub  T·∫°o 1 private project tr√™n Gitlab.\nC√°i n√†y t√πy √Ω project n√†o c≈©ng dc, m√¨nh t·∫°o project git@gitlab.com:inmessionante/spring-maven-postgres-docker-k8s.git\n  Add public SSH key gitlab_com_rsa.pub v√†o Gitlab b·∫±ng c√°ch:\nhttps://gitlab.com/-/profile -\u0026gt; SSH Keys Tab -\u0026gt; Add c√°i n·ªôi dung c·ªßa gitlab_com_rsa.pub v√†o\n  Tr√™n VM Run Docker, t·∫°o folder ƒë·ªÉ test:\n  mkdir /opt/devops/docker-ssh-agent-lab Copy file SSH private key gitlab_com_rsa l√™n /home/ubuntu/.ssh (ch·ªó n√†y t√πy b·∫°n, ƒë·ªÉ ƒë√¢u cho secure c≈©ng ƒë∆∞·ª£c)\nFile /opt/devops/docker-ssh-agent-lab/Dockerfile:\n# syntax=docker/dockerfile:1.0.0-experimental FROM alpine RUN apk add --update git openssh # This is necessary to prevent the \u0026quot;git clone\u0026quot; operation from failing # with an \u0026quot;unknown host key\u0026quot; error. RUN mkdir -m 700 /root/.ssh; \\ touch -m 600 /root/.ssh/known_hosts; \\ ssh-keyscan gitlab.com \u0026gt; /root/.ssh/known_hosts # This command has access to the \u0026quot;github\u0026quot; key RUN --mount=type=ssh,id=gitlab git clone git@gitlab.com:inmessionante/spring-maven-postgres-docker-k8s.git Run Docker Build  cd /opt/devops/docker-ssh-agent-lab DOCKER_BUILDKIT=1 docker build --ssh gitlab=/home/ubuntu/.ssh/gitlab_com_rsa -t buildtest . Confirm ƒë√£ git clone ƒë∆∞·ª£c project v√†o trong image  $ docker run buildtest ls -lsa total 80 4 drwxr-xr-x 1 root root 4096 Nov 6 07:11 . 4 drwxr-xr-x 1 root root 4096 Nov 6 07:11 .. 0 -rwxr-xr-x 1 root root 0 Nov 6 07:11 .dockerenv 0 -rw-r--r-- 1 root root 0 Nov 6 07:08 600 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 bin 0 drwxr-xr-x 5 root root 340 Nov 6 07:11 dev 4 drwxr-xr-x 1 root root 4096 Nov 6 07:11 etc 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 home 8 drwxr-xr-x 1 root root 4096 Aug 9 08:49 lib 4 drwxr-xr-x 5 root root 4096 Aug 9 08:49 media 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 mnt 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 opt 0 dr-xr-xr-x 287 root root 0 Nov 6 07:11 proc 4 drwx------ 1 root root 4096 Nov 6 07:08 root 4 drwxr-xr-x 1 root root 4096 Nov 6 07:08 run 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 sbin 4 drwxr-xr-x 7 root root 4096 Nov 6 07:08 spring-maven-postgres-docker-k8s 4 drwxr-xr-x 2 root root 4096 Aug 9 08:49 srv 0 dr-xr-xr-x 13 root root 0 Nov 6 07:11 sys 4 drwxrwxrwt 2 root root 4096 Aug 9 08:49 tmp 8 drwxr-xr-x 1 root root 4096 Nov 6 07:08 usr 8 drwxr-xr-x 1 root root 4096 Nov 6 07:08 var OK?\nD√πng ssh-agent v√† $SSH_AUTH_SOCK  Sau b∆∞·ªõc 6 c√≥ v·∫ª ƒë√£ OK? ·ªû b∆∞·ªõc s·ªë 5 ch√∫ng ta ph·∫£i ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c·ªßa private key, m√¨nh ko th√≠ch ƒëi·ªÅu n√†y l·∫Øm.\nN√™n m√¨nh s·∫Ω d√πng command n√†y:\ncd /opt/devops/docker-ssh-agent-lab DOCKER_BUILDKIT=1 docker build --ssh gitlab=$SSH_AUTH_SOCK -t buildtest . Tuy nhi√™n b·∫°n s·∫Ω b·ªã l·ªói sau n·∫øu ko chu·∫©n b·ªã tr∆∞·ªõc:\n$ DOCKER_BUILDKIT=1 docker build --ssh gitlab=$SSH_AUTH_SOCK -t buildtest . could not parse ssh: [gitlab=]: invalid empty ssh agent socket, make sure SSH_AUTH_SOCK is set B·∫°n c·∫ßn ƒë∆∞a private key v√†o ssh-agent tr∆∞·ªõc cho n√≥ qu·∫£n l√Ω:\n# start the ssh-agent in the background $ eval \u0026#34;$(ssh-agent -s)\u0026#34; Agent pid 9501 # Add your SSH private key to the ssh-agent $ ssh-add ~/.ssh/gitlab_com_rsa Identity added: /home/ubuntu/.ssh/gitlab_com_rsa # List identities that ssh-agent is managing $ ssh-add -L ssh-rsa AAAA...B3NzaC1yc2EAqClFkhB9mUfuVGPMe+upN4AV7LS6toRuwHdTJoUfbcjTc9zttxtoKogMXJu/rrf6Pv7I7ksLOL26PyI3vPacnw/VcrVplNw8367TMtap4K9MoWqup2UGyozm6ycd/inMiyeIy...9s= T·∫°i th·ªùi ƒëi·ªÉm n√†y bi·∫øn m√¥i tr∆∞·ªùng $SSH_AUTH_SOCK ƒë√£ ƒë∆∞·ª£c set, b·∫°n c√≥ th·ªÉ check b·∫±ng command:\necho $SSH_AUTH_SOCK Gi·ªù ch·∫°y l·∫°i command docker build:\n$ DOCKER_BUILDKIT=1 docker build --ssh gitlab=$SSH_AUTH_SOCK -t buildtest . [+] Building 8.5s (14/14) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 38B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; resolve image config for docker.io/docker/dockerfile:1.0.0-experimental 0.9s =\u0026gt; [auth] docker/dockerfile:pull token for registry-1.docker.io 0.0s =\u0026gt; CACHED docker-image://docker.io/docker/dockerfile:1.0.0-experimental@sha256:d2d402b6fa1dae752f 0.0s =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 38B 0.0s =\u0026gt; [internal] load .dockerignore 0.1s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 0.6s =\u0026gt; [auth] library/alpine:pull token for registry-1.docker.io 0.0s =\u0026gt; [1/4] FROM docker.io/library/alpine@sha256:bc41182d7ef5ffc53a40b044e725193bc10142a1243f395ee85 0.0s =\u0026gt; CACHED [2/4] RUN apk add --update git openssh 0.0s =\u0026gt; CACHED [3/4] RUN mkdir -m 700 /root/.ssh; touch -m 600 /root/.ssh/known_hosts; ssh-keyscan 0.0s =\u0026gt; CACHED [4/4] RUN --mount=type=ssh,id=gitlab git clone git@gitlab.com:inmessionante/spring-mave 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:7d93dafe042c123483ffeafee2b63be48bcf3d30b425b47847b2988399cebc65 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/buildtest Sau step 7, Tr√™n m√¥i tr∆∞·ªùng Develop b√¨nh th∆∞·ªùng th√¨ ƒë·∫øn ƒë√¢y m·ªçi th·ª© c√≥ v·∫ª ƒë√£ ·ªïn üòÇ.  Nh∆∞ng tr√™n CI th√¨ kh√°c, VM n√†y s·∫Ω l√† Builder c·ªßa ch√∫ng ta, vi·ªác Builder ch·ª©a private key nghe c√≥ v·∫ª s·∫Ω kh√¥ng secure cho l·∫Øm.\nTrong tr∆∞·ªùng h·ª£p Builder n√†y b·ªã hack, ho·∫∑c nhi·ªÅu ng∆∞·ªùi c√≥ quy·ªÅn truy c·∫≠p, ch√∫ng ta ho√†n to√†n c√≥ th·ªÉ b·ªã leak c√°i private key ·∫•y ra ngo√†i.\nV·∫≠y n·∫øu ch√∫ng ta mu·ªën Builder c≈©ng kh√¥ng th·ªÉ xem ƒë∆∞·ª£c key th√¨ sao. Ta gi·∫£ s·ª≠ m√¨nh ƒëang d√πng Gitlab CI nh√©.\nModify your .gitlab-ci.yml with a before_script action. In the following example, a Debian based image is assumed. Edit to your needs:\nbefore_script: ## ## Install ssh-agent if not already installed, it is required by Docker. ## (change apt-get to yum if you use an RPM-based image) ## - \u0026#39;command -v ssh-agent \u0026gt;/dev/null || ( apt-get update -y \u0026amp;\u0026amp; apt-get install openssh-client -y )\u0026#39; ## ## Run ssh-agent (inside the build environment) ## - eval $(ssh-agent -s) ## ## Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store ## We\u0026#39;re using tr to fix line endings which makes ed25519 keys work ## without extra base64 encoding. ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556 ## - echo \u0026#34;$SSH_PRIVATE_KEY\u0026#34; | tr -d \u0026#39;\\r\u0026#39; | ssh-add - ## ## Create the SSH directory and give it the right permissions ## - mkdir -p ~/.ssh - chmod 700 ~/.ssh ## ## Optionally, if you will be using any Git commands, set the user name and ## and email. ## # - git config --global user.email \u0026#34;user@example.com\u0026#34; # - git config --global user.name \u0026#34;User name\u0026#34; Nh∆∞ v·∫≠y l√† ch·ªâ c·∫ßn truy·ªÅn bi·∫øn m√¥i tr∆∞·ªùng tr√™n Gitlab CI l√† ok, Builder s·∫Ω kh√¥ng l∆∞u c√°c private key n·ªØa.\nƒê·∫øn ƒë√¢y ch√∫ng ta th·∫•y ƒë∆∞·ª£c c√°c t√≠nh nƒÉng h·ªØu √≠ch c·ªßa SSH Agent üòç n√™n m√¨nh quy·∫øt ƒë·ªãnh xa r·ªùi ch·ªß ƒë·ªÅ ch√≠nh c·ªßa b√†i n√†y ƒë·ªÉ take notes v·ªÅ SSH Agent th√™m 1 ch√∫t:\n8.11. SSH Agent Forwarding https://dev.to/levivm/how-to-use-ssh-and-ssh-agent-forwarding-more-secure-ssh-2c32\nGi·∫£ s·ª≠ b·∫°n l√†m vi·ªác tr√™n Laptop (local).\nB·∫°n c√≥ 1 VM Ubuntu tr√™n Cloud. VM n√†y c√≥ th·ªÉ share cho nhi·ªÅu teammate c√πng nhau s·ª≠ d·ª•ng. VM n√†y c·∫ßn pull source t·ª´ 1 private repository tr√™n Gitlab.com.\nB·∫°n th∆∞·ªùng pull source v·ªÅ b·∫±ng 1 SSH private key (m√† b·∫°n ƒë√£ add public key c·ªßa n√≥ v√†o Gitlab r·ªìi).\nC√°i private key ƒë√≥ b·∫°n ƒëang l∆∞u tr√™n VM.\nGi·ªù b·∫°n ko mu·ªën VM ƒë√≥ l∆∞u private key n·ªØa (V√¨ ng∆∞·ªùi kh√°c/admin) c√≥ th·ªÉ v√†o v√† l·∫•y key ƒë√≥.\n-\u0026gt; ƒê√¢y l√† l√∫c c·∫ßn s·ª≠ d·ª•ng SSH Agent Forwarding.\nNg·∫Øn g·ªçn: B·∫°n forward SSH Agent session t·ª´ Laptop l√™n VM. Khi·∫øn n√≥ c√≥ th·ªÉ pull source code v·ªÅ m√† ko c·∫ßn l∆∞u c√°i private key.\nC·∫ßn m√¥ t·∫£ r√µ, tr√™n Laptop c·ªßa b·∫°n c√≥ 2 private key:\n VM_PRIVATE_KEY: d√πng ƒë·ªÉ SSH t·ª´ Laptop l√™n VM. GITLAB_PRIVATE_KEY: d√πng ƒë·ªÉ pull source t·ª´ Gitlab.com v·ªÅ. Tr∆∞·ªõc ƒë√¢y b·∫°n l∆∞u n√≥ tr√™n VM, gi·ªù ko mu·ªën n·ªØa. ubuntu@112.222.54.31: user v√† IP c·ªßa VM.  b√†i n√†y m√¨nh d√πng GitBash tr√™n Windows:\n# start the ssh-agent in the background $ eval \u0026#34;$(ssh-agent -s)\u0026#34; Agent pid 9501 # Add your SSH private key to the ssh-agent $ ssh-add ./GITLAB_PRIVATE_KEY Identity added: ... # T·∫°i th·ªùi ƒëi·ªÉm n√†y: ssh-agent ƒë√£ s·∫µn s√†ng cho b·∫°n forward  # SSH from Laptop to VM and forward agent.  # -A option enables forwarding of the authentication agent connection.  $ ssh -A -i VM_PRIVATE_KEY ubuntu@112.222.54.31 # T·∫°i th·ªùi ƒëi·ªÉm n√†y b·∫°n ƒë√£ login v√†o VM,  # b·∫°n s·∫Ω th·∫•y $SSH_AUTH_SOCK c√≥ gi√° tr·ªã  $ echo $SSH_AUTH_SOCK # output: /tmp/ssh-baaawdgssv/agent.12322 # Test connection $ ssh -T git@gitlab.com # H√£y th·ª≠ pull source t·ª´ 1 private repository c·ªßa b·∫°n tr√™n Gitlab.com v·ªÅ xem sao.  # ch·∫Øc ch·∫Øn ph·∫£i pull ƒë∆∞·ª£c m·ªõi OK nh√©.  Ph·∫£i n√≥i c√°ch n√†y kh√° hay v√† secure, tuy nhi√™n vi·ªác forward ssh-agent l√™n VM c≈©ng ch·∫£ kh√°c n√†o ƒë∆∞a key c·ªßa b·∫°n l√™n VM, h√£y ch·∫Øc ch·∫Øn x√≥a session tr∆∞·ªõc khi b·∫°n logout kh·ªèi con VM d√πng chung n√†y. C√≥ 1 v√†i c√°ch ƒë·ªÉ t·ª± ƒë·ªông chuy·ªán clear ssh-agent l√† b√†i n√†y: https://rabexc.org/posts/pitfalls-of-ssh-agents\nNgo√†i ra, tr∆∞·ªùng h·ª£p Gitlab self-hosted server ƒë·ª©ng sau VPN th√¨ m√¨nh ch∆∞a th·ª≠.\nCREDIT https://walterteng.com/gatsby-docker\nhttps://dev.to/stoutlabs/my-docker-setup-for-gatsbyjs-and-nextjs-5gao\nk·∫øt h·ª£p nginx ƒë·ªÉ serve public dir https://valenciandigital.com/insights/why-containerize-your-gatsby-application\nbest practices: https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/ best practices: https://github.com/nodejs/docker-node/blob/main/docs/BestPractices.md\nbest practices: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nbest practices: https://adambrodziak.pl/dockerfile-good-practices-for-node-and-npm\nhttps://itnext.io/how-to-speed-up-node-js-modules-installation-in-ci-cd-pipeline-as-of-2020-4865d77c0eb7\nhttps://forum.gitlab.com/t/how-to-cache-node-modules-globally-for-all-pipelines-for-a-project/49169\nso s√°nh CMD vs ENTRYPOINT trong Dockerfile: https://phoenixnap.com/kb/docker-cmd-vs-entrypoint\nhttps://remelehane.dev/posts/diy-node-cache-for-docker-ci/\nhttps://blog.saeloun.com/2022/07/12/docker-cache.html\nhttps://stackoverflow.com/questions/54574821/docker-build-not-using-cache-when-copying-gemfile-while-using-cache-from#comment98285860_54574821\nhttps://youtu.be/r_UBWjMUd-0\nhttp://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/\nhttps://docs.gitlab.com/ee/ci/ssh_keys/#ssh-keys-when-using-the-docker-executor\n","href":"/posts/encrypt-dockerize-reactjs-gatsby-app-good-practices/","title":"Dockerize ReactJS/GatsbyJS app and Run it in Dev environment + good practices"},{"content":"","href":"/tags/gatsbyjs/","title":"GatsbyJS"},{"content":"","href":"/tags/reactjs/","title":"ReactJS"},{"content":"","href":"/tags/js/","title":"Js"},{"content":"List c√°c project vi·∫øt b·∫±ng JS th√∫ v·ªã ƒë·ªÉ tham kh·∫£o\nM√¨nh t·ªïng h·ª£p l·∫°i t·ª´ The Odin Project (1 d·ª± √°n/kh√≥a h·ªçc ƒë∆∞·ª£c kh√° nhi·ªÅu ng∆∞·ªùi h·ªçc JS recommend)\n1. Kh√≥a Foundation Project: Recipes https://www.theodinproject.com/lessons/foundations-recipes\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Jason-B-Jiang/Recipes-Website\nhttps://github.com/Criosyom/odin-recipes\nhttps://github.com/Appletri/odin-recipes\nhttps://github.com/dipps18/odin-recipes\nhttps://github.com/TYLPHE/odin-recipes\nProject: Landing Page https://www.theodinproject.com/lessons/foundations-landing-page\n5 solution ti√™u bi·ªÉu nhi·ªÅu like: https://github.com/Criosyom/Hakurei_Shrine\nhttps://github.com/Lun-Dev/Landing-Page\nhttps://github.com/othercatt/odin-landing-page\nhttps://github.com/Appletri/landing-page\nhttps://github.com/piotrnajda3000/landing-page\nProject: Rock Paper Scissors https://www.theodinproject.com/lessons/foundations-rock-paper-scissors\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/lookingcoolonavespa/rock-paper-scissors.git\nhttps://github.com/michalosman/rock-paper-scissors\nhttps://github.com/mooniiDev/rock-paper-scissors-game\nhttps://github.com/Barnards/rochambeau\nhttps://github.com/Tmcerlean/rock-paper-scissors\nProject: Etch-a-Sketch https://www.theodinproject.com/lessons/foundations-etch-a-sketch\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/etch-a-sketch\nhttps://github.com/bscottnz/esketch\nhttps://github.com/rlmoser99/etch-a-sketch\nhttps://github.com/Artis-Dev/etch-a-sketch\nhttps://github.com/andrewjh271/etch-a-sketch\nProject: Calculator https://www.theodinproject.com/lessons/foundations-calculator\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/calculator\nhttps://github.com/Captain-Usopp/Calculator\nhttps://github.com/rlmoser99/calculator\nhttps://github.com/Mike-Monta/calculator-pro.git\nhttps://github.com/Artis-Dev/calculator\n2. Kh√≥a Full-Stack-Javascript Project: Library https://www.theodinproject.com/lessons/node-path-javascript-library\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/library\nhttps://github.com/igorashs/library\nhttps://github.com/Zakarya-Mks/my-library\nhttps://github.com/ginnerzapata/library-app\nhttps://github.com/mooniiDev/library\nhttps://github.com/CatQueenCodes/Project-Library\nProject: Tic Tac Toe https://www.theodinproject.com/lessons/node-path-javascript-tic-tac-toe\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Pety99/tic-tac-toe\nhttps://github.com/vdojnov/Superhero-Tic-Tac-Toe\nhttps://github.com/Theonlyhamstertoh/tictactoe\nhttps://github.com/michalosman/tic-tac-toe\nhttps://github.com/SultanBadri/tic-tac-toe\nProject: Restaurant Page https://www.theodinproject.com/lessons/node-path-javascript-restaurant-page\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/pret3nti0u5/Restaurant-Page\nhttps://github.com/michalosman/restaurant-page\nhttps://github.com/igorashs/restaurant-page\nhttps://github.com/SultanBadri/restaurant-page\nhttps://github.com/ginnerzapata/restaurant\nProject: Todo List https://www.theodinproject.com/lessons/node-path-javascript-todo-list\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/todo-list\nhttps://github.com/bscottnz/todo\nhttps://github.com/YipAnthony/betterToDoList\nhttps://github.com/khunhour/todo_list\nhttps://github.com/Zakarya-Mks/ToDoers\nProject: Weather App https://www.theodinproject.com/lessons/node-path-javascript-weather-app\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/bscottnz/weather-app\nhttps://github.com/edehlol/weather\nhttps://github.com/ding-09/weather-app\nhttps://github.com/michalosman/weather-app\nhttps://github.com/FernanEd/weatherapp\nProject: Recursion https://www.theodinproject.com/lessons/javascript-recursion\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Lofty-Brambles/Recursion\nhttps://github.com/JoshDevHub/JS-Recursion-Exercises\nhttps://github.com/nevz9/TOP-recursion\nhttps://github.com/Roxanoel/recursion\nhttps://github.com/luaroxy/odin-Recursion\nProject: Linked Lists https://www.theodinproject.com/lessons/javascript-linked-lists\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/lnicepei/linked-list\nhttps://github.com/dwgrossberg/linked-lists\nhttps://github.com/luaroxy/odin-linkedlists\nhttps://github.com/HardRoof/linked-list\nhttps://github.com/nickslick03/linked-list\nProject: Binary Search Trees https://www.theodinproject.com/lessons/javascript-binary-search-trees\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/lnicepei/binary-search-tree\nhttps://github.com/Mynameisfrostwalker/binary-search-tree\nhttps://github.com/JoshDevHub/JavaScript-Binary-Search-Tree\nhttps://github.com/HardRoof/binary-search-tree\nhttps://github.com/bananabread08/binary-search-tree\nProject: Knights Travails https://www.theodinproject.com/lessons/javascript-knights-travails\n5 solution ti√™u bi·ªÉu nhi·ªÅu like: https://github.com/Kiwasthal/Knight-travails\nhttps://github.com/JoshDevHub/Knight-Travails-JS/\nhttps://github.com/Appletri/knight-travails\nhttps://github.com/TYLPHE/knights-travails\nhttps://github.com/chri-ss/knights-travails\nProject: Testing Practice https://www.theodinproject.com/lessons/node-path-javascript-testing-practice\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Cambuchi/odin-tdd-practice\nhttps://github.com/mrsuber/odin_project_11_Test_driven_developement\nhttps://github.com/sebapkfd/testing-practice\nhttps://github.com/mooniiDev/testing-practice\nhttps://github.com/ZevaGuillo/Testing-Practice\nProject: Battleship https://www.theodinproject.com/lessons/node-path-javascript-battleship\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/fortypercenttitanium/battleship\nhttps://github.com/michalosman/battleship\nhttps://github.com/igorashs/vue-battleship https://github.com/EriBloo/battleship\nhttps://github.com/JCarlosLucio/battleship\nProject: CV Application https://www.theodinproject.com/lessons/node-path-javascript-cv-application\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/cv-application\nhttps://github.com/abaker93/react-cv-project\nhttps://github.com/mister-lepak/ResumeBuilder\nhttps://github.com/shanesc/top-cv-maker\nhttps://github.com/ze-gomes/cv-app\nProject: Memory Card https://www.theodinproject.com/lessons/node-path-javascript-memory-card\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/igorashs/react-memory-card\nhttps://github.com/michalosman/memory-card\nhttps://github.com/ding-09/memory-game\nhttps://github.com/benngarcia/HXH-Memory-Game\nhttps://github.com/mister-lepak/memoryGame\nProject: Shopping Cart https://www.theodinproject.com/lessons/node-path-javascript-shopping-cart\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/amielfilarca/shopping-cart\nhttps://github.com/michalosman/shopping-cart\nhttps://github.com/pklepa/shopping-cart\nhttps://github.com/JCarlosLucio/shopping-cart\nhttps://github.com/SebastianDarie/Shopping-Cart\nProject: Where\u0026rsquo;s Waldo (A Photo Tagging App) https://www.theodinproject.com/lessons/node-path-javascript-where-s-waldo-a-photo-tagging-app\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/JCarlosLucio/where-is-waldo\nhttps://github.com/KFig21/photo_tagging_app\nhttps://github.com/timkellytk/project-wheres-waldo\nhttps://github.com/zainthedev/waldo\nhttps://github.com/Samuelisch/wheres-waldo\nProject: JavaScript Final Project https://www.theodinproject.com/lessons/node-path-javascript-javascript-final-project\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/julio22b/movie-app-frontend\nhttps://github.com/michalosman/chat-app-firebase\nhttps://github.com/joshwrn/instagram-clone\nhttps://github.com/Ideopunk/twitter-clone\nhttps://github.com/KFig21/petstagram\n","href":"/bk/encrypt-list-js-projects-for-newbie/","title":"List JS projects for newbie"},{"content":"List c√°c project vi·∫øt b·∫±ng JS th√∫ v·ªã ƒë·ªÉ tham kh·∫£o\nM√¨nh t·ªïng h·ª£p l·∫°i t·ª´ The Odin Project (1 d·ª± √°n/kh√≥a h·ªçc ƒë∆∞·ª£c kh√° nhi·ªÅu ng∆∞·ªùi h·ªçc JS recommend)\n1. Kh√≥a Foundation Project: Recipes https://www.theodinproject.com/lessons/foundations-recipes\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Jason-B-Jiang/Recipes-Website\nhttps://github.com/Criosyom/odin-recipes\nhttps://github.com/Appletri/odin-recipes\nhttps://github.com/dipps18/odin-recipes\nhttps://github.com/TYLPHE/odin-recipes\nProject: Landing Page https://www.theodinproject.com/lessons/foundations-landing-page\n5 solution ti√™u bi·ªÉu nhi·ªÅu like: https://github.com/Criosyom/Hakurei_Shrine\nhttps://github.com/Lun-Dev/Landing-Page\nhttps://github.com/othercatt/odin-landing-page\nhttps://github.com/Appletri/landing-page\nhttps://github.com/piotrnajda3000/landing-page\nProject: Rock Paper Scissors https://www.theodinproject.com/lessons/foundations-rock-paper-scissors\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/lookingcoolonavespa/rock-paper-scissors.git\nhttps://github.com/michalosman/rock-paper-scissors\nhttps://github.com/mooniiDev/rock-paper-scissors-game\nhttps://github.com/Barnards/rochambeau\nhttps://github.com/Tmcerlean/rock-paper-scissors\nProject: Etch-a-Sketch https://www.theodinproject.com/lessons/foundations-etch-a-sketch\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/etch-a-sketch\nhttps://github.com/bscottnz/esketch\nhttps://github.com/rlmoser99/etch-a-sketch\nhttps://github.com/Artis-Dev/etch-a-sketch\nhttps://github.com/andrewjh271/etch-a-sketch\nProject: Calculator https://www.theodinproject.com/lessons/foundations-calculator\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/calculator\nhttps://github.com/Captain-Usopp/Calculator\nhttps://github.com/rlmoser99/calculator\nhttps://github.com/Mike-Monta/calculator-pro.git\nhttps://github.com/Artis-Dev/calculator\n2. Kh√≥a Full-Stack-Javascript Project: Library https://www.theodinproject.com/lessons/node-path-javascript-library\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/library\nhttps://github.com/igorashs/library\nhttps://github.com/Zakarya-Mks/my-library\nhttps://github.com/ginnerzapata/library-app\nhttps://github.com/mooniiDev/library\nhttps://github.com/CatQueenCodes/Project-Library\nProject: Tic Tac Toe https://www.theodinproject.com/lessons/node-path-javascript-tic-tac-toe\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Pety99/tic-tac-toe\nhttps://github.com/vdojnov/Superhero-Tic-Tac-Toe\nhttps://github.com/Theonlyhamstertoh/tictactoe\nhttps://github.com/michalosman/tic-tac-toe\nhttps://github.com/SultanBadri/tic-tac-toe\nProject: Restaurant Page https://www.theodinproject.com/lessons/node-path-javascript-restaurant-page\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/pret3nti0u5/Restaurant-Page\nhttps://github.com/michalosman/restaurant-page\nhttps://github.com/igorashs/restaurant-page\nhttps://github.com/SultanBadri/restaurant-page\nhttps://github.com/ginnerzapata/restaurant\nProject: Todo List https://www.theodinproject.com/lessons/node-path-javascript-todo-list\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/todo-list\nhttps://github.com/bscottnz/todo\nhttps://github.com/YipAnthony/betterToDoList\nhttps://github.com/khunhour/todo_list\nhttps://github.com/Zakarya-Mks/ToDoers\nProject: Weather App https://www.theodinproject.com/lessons/node-path-javascript-weather-app\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/bscottnz/weather-app\nhttps://github.com/edehlol/weather\nhttps://github.com/ding-09/weather-app\nhttps://github.com/michalosman/weather-app\nhttps://github.com/FernanEd/weatherapp\nProject: Recursion https://www.theodinproject.com/lessons/javascript-recursion\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Lofty-Brambles/Recursion\nhttps://github.com/JoshDevHub/JS-Recursion-Exercises\nhttps://github.com/nevz9/TOP-recursion\nhttps://github.com/Roxanoel/recursion\nhttps://github.com/luaroxy/odin-Recursion\nProject: Linked Lists https://www.theodinproject.com/lessons/javascript-linked-lists\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/lnicepei/linked-list\nhttps://github.com/dwgrossberg/linked-lists\nhttps://github.com/luaroxy/odin-linkedlists\nhttps://github.com/HardRoof/linked-list\nhttps://github.com/nickslick03/linked-list\nProject: Binary Search Trees https://www.theodinproject.com/lessons/javascript-binary-search-trees\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/lnicepei/binary-search-tree\nhttps://github.com/Mynameisfrostwalker/binary-search-tree\nhttps://github.com/JoshDevHub/JavaScript-Binary-Search-Tree\nhttps://github.com/HardRoof/binary-search-tree\nhttps://github.com/bananabread08/binary-search-tree\nProject: Knights Travails https://www.theodinproject.com/lessons/javascript-knights-travails\n5 solution ti√™u bi·ªÉu nhi·ªÅu like: https://github.com/Kiwasthal/Knight-travails\nhttps://github.com/JoshDevHub/Knight-Travails-JS/\nhttps://github.com/Appletri/knight-travails\nhttps://github.com/TYLPHE/knights-travails\nhttps://github.com/chri-ss/knights-travails\nProject: Testing Practice https://www.theodinproject.com/lessons/node-path-javascript-testing-practice\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/Cambuchi/odin-tdd-practice\nhttps://github.com/mrsuber/odin_project_11_Test_driven_developement\nhttps://github.com/sebapkfd/testing-practice\nhttps://github.com/mooniiDev/testing-practice\nhttps://github.com/ZevaGuillo/Testing-Practice\nProject: Battleship https://www.theodinproject.com/lessons/node-path-javascript-battleship\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/fortypercenttitanium/battleship\nhttps://github.com/michalosman/battleship\nhttps://github.com/igorashs/vue-battleship https://github.com/EriBloo/battleship\nhttps://github.com/JCarlosLucio/battleship\nProject: CV Application https://www.theodinproject.com/lessons/node-path-javascript-cv-application\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/michalosman/cv-application\nhttps://github.com/abaker93/react-cv-project\nhttps://github.com/mister-lepak/ResumeBuilder\nhttps://github.com/shanesc/top-cv-maker\nhttps://github.com/ze-gomes/cv-app\nProject: Memory Card https://www.theodinproject.com/lessons/node-path-javascript-memory-card\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/igorashs/react-memory-card\nhttps://github.com/michalosman/memory-card\nhttps://github.com/ding-09/memory-game\nhttps://github.com/benngarcia/HXH-Memory-Game\nhttps://github.com/mister-lepak/memoryGame\nProject: Shopping Cart https://www.theodinproject.com/lessons/node-path-javascript-shopping-cart\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/amielfilarca/shopping-cart\nhttps://github.com/michalosman/shopping-cart\nhttps://github.com/pklepa/shopping-cart\nhttps://github.com/JCarlosLucio/shopping-cart\nhttps://github.com/SebastianDarie/Shopping-Cart\nProject: Where\u0026rsquo;s Waldo (A Photo Tagging App) https://www.theodinproject.com/lessons/node-path-javascript-where-s-waldo-a-photo-tagging-app\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/JCarlosLucio/where-is-waldo\nhttps://github.com/KFig21/photo_tagging_app\nhttps://github.com/timkellytk/project-wheres-waldo\nhttps://github.com/zainthedev/waldo\nhttps://github.com/Samuelisch/wheres-waldo\nProject: JavaScript Final Project https://www.theodinproject.com/lessons/node-path-javascript-javascript-final-project\n5 solution ti√™u bi·ªÉu nhi·ªÅu like:\nhttps://github.com/julio22b/movie-app-frontend\nhttps://github.com/michalosman/chat-app-firebase\nhttps://github.com/joshwrn/instagram-clone\nhttps://github.com/Ideopunk/twitter-clone\nhttps://github.com/KFig21/petstagram\n","href":"/posts/encrypt-list-js-projects-for-newbie/","title":"List JS projects for newbie"},{"content":"","href":"/tags/locust/","title":"Locust"},{"content":"1. Gi·∫£i th√≠ch c√°c kh√°i ni·ªám 1 c√°ch ng·∫Øn g·ªçn Performance test ko ph·∫£i l√† 1 ki·ªÉu test c·ª• th·ªÉ n√†o, n√≥ l√† 1 kh√°i ni·ªám chung cho t·∫•t c·∫£ c√°c ki·ªÉu test nh·∫±m validate performance v√† x√°c ƒë·ªãnh performance issues.\nLoad test l√† 1 ki·ªÉu test ƒë·ªÉ validate xem application c·ªßa b·∫°n c√≥ th·ªèa m√£n c√°c goals ƒë√£ ƒëc define t·ª´ tr∆∞·ªõc hay ko? Khi x√¢y d·ª±ng app th√¨ b·∫°n ph·∫£i define r·∫±ng app n√†y s·∫Ω cho ph√©p bao nhi√™u CCU? N·∫øu b·∫°n define 50 CCU th√¨ Load test ch·ªâ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c 50 CCU l√† OK.\nStress test l√† 1 ki·ªÉu test ƒë·ªÉ x√°c ƒë·ªãnh behavior c·ªßa application l√† nh∆∞ n√†o n·∫øu n√≥ ph·∫£i ch·ªãu 1 l∆∞·ª£ng traffic l·ªõn h∆°n nhi·ªÅu so v·ªõi expectation. V√≠ d·ª• n·∫øu app c·ªßa b·∫°n define CCU l√† 50 th√¨ Stress test v·ªõi 500 CCU th√¨ App c·ªßa b·∫°n s·∫Ω nh∆∞ n√†o? (throttle? crash? restart?)\nVolume test l√† 1 ki·ªÉu test ƒë∆∞·ª£c d√πng trong 2 case:\n X√°c ƒë·ªãnh performance c·ªßa app khi ph·∫£i ch·ªãu c√°c kh·ªëi l∆∞·ª£ng data kh√°c nhau ƒëi v√†o Database T√≠nh to√°n kh·∫£ nƒÉng c·ªßa app v·ªõi 1 kh·ªëi l∆∞·ª£ng l·ªõn data (v√≠ d·ª• nh∆∞ import large volume data from file to DB\u0026hellip;)  Endurance test l√† 1 ki·ªÉu test ƒë·ªÉ t√¨m ra c√°c issue m√† ch·ªâ c√≥ th·ªÉ x·∫£y ra khi app ƒë√£ ch·∫°y 1 th·ªùi gian d√†i, n√≥i chung l√† nh·ªØng case m√† kh√≥ c√≥ th·ªÉ ph√°t hi·ªán bug n·∫øu th·ªùi gian test ng·∫Øn.\n2. Locust (Basic) 2.1. Basic step Install:\n$ python --version Python 3.9.2 $ pip3 install locust # wait till all done $ locust -V locust 2.12.1 Quick start:\nt·∫°o 1 file locustfile.py:\nfrom locust import HttpUser, task class User(HttpUser): @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) th·∫ø th√¥i, run command locust:\n$ locust [2022-09-30 23:29:17,172] raspian-rpi/WARNING/locust.main: System open file limit '1024' is below minimum setting '10000'. It's not high enough for load testing, and the OS didn't allow locust to increase it by itself. See https://github.com/locustio/locust/wiki/Installation#increasing-maximum-number-of-open-files-limit for more info. [2022-09-30 23:29:17,173] raspian-rpi/INFO/locust.main: Starting web interface at http://0.0.0.0:8089 (accepting connections from all network interfaces) [2022-09-30 23:29:17,205] raspian-rpi/INFO/locust.main: Starting Locust 2.12.1 Gi·ªù b·∫°n c√≥ th·ªÉ truy c·∫≠p v√†o ƒë·ªãa ch·ªâ 127.0.0.1:8089 ƒë·ªÉ xem giao di·ªán Locust:\nƒêi·ªÅn 1 s·ªë th√¥ng tin v√†o giao di·ªán nh∆∞ n√†y r·ªìi ·∫•n Start:\nm√†n h√¨nh ch·ªù trong l√∫c script ƒëang ch·∫°y:\n·∫§n Stop v√† xem k·∫øt qu·∫£: ·∫§n v√†o tab Chart ƒë·ªÉ xem:\nChart Total RPS ƒë√∫ng nh∆∞ t√™n g·ªçi, ch∆∞a bi·∫øt c√°ch tƒÉng l√™n nh∆∞ n√†o?\nChart Response times:\n c√≥ Median Response Time l√† respone time trung b√¨nh c·ªßa 50% s·ªë request c√≥ 95% percentile: l√† respone time trung b√¨nh c·ªßa 95% s·ªë request Chart Number of users: s·ªë l∆∞·ª£ng user theo th·ªùi gian  ·∫§n v√†o tab Download Data s·∫Ω download dc report d∆∞·ªõi d·∫°ng html:\nKh√° hay ƒë·∫•y ch·ª©\nTuy nhi√™n m√¨nh s·∫Ω t√¨m c√°ch ƒë·ªÉ l√†m h·∫øt d∆∞·ªõi d·∫°ng c√¢u l·ªánh v√†o l∆∞u l·∫°i report 1 c√°ch t·ª± ƒë·ªông sau khi test xong.\n2.2. Distributed mode with docker-compose Ch·∫°y ·ªü mode n√†y s·∫Ω t·∫°o ra c√°c container cho master v√† worker, l√†m tƒÉng performance c·ªßa Locust l√™n\nCh√∫ng ta s·∫Ω chu·∫©n b·ªã folder structure ƒë∆°n gi·∫£n nh∆∞ n√†y:\nt·∫°o folder /opt/devops/locust tr∆∞·ªõc nh√©\n/opt/devops/locust $ tree . . ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ html-report/ ‚îú‚îÄ‚îÄ locustfile.py ‚îú‚îÄ‚îÄ master.conf File docker-compose.yml file:\nversion: \u0026#39;3\u0026#39; services: master: image: locustio/locust ports: - \u0026#34;8089:8089\u0026#34; volumes: - /opt/devops/locust:/mnt/locust command: -f /mnt/locust/locustfile.py --config=/mnt/locust/master.conf worker: image: locustio/locust volumes: - /opt/devops/locust:/mnt/locust command: -f /mnt/locust/locustfile.py --worker --master-host master File locustfile.py:\nfrom locust import HttpUser, task class User(HttpUser): @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) File master.conf:\n# master.conf in current directory locustfile = ./locustfile.py headless = true master = true expect-workers = 3 host = http://192.168.1.128:3000 # this is the site you want to test users = 80 spawn-rate = 2 run-time = 2m html = /mnt/locust/html-report/report.html https://docs.locust.io/en/2.12.1/configuration.html#configuration-file\n  users = 200: t·ªïng s·ªë user\n  spawn-rate = 10: l√† s·ªë user per second\nEx: users = 200, spawn-rate = 10=\u0026gt; m·ªói gi√¢y s·∫Ω tƒÉng l√™n 10 user, d·∫ßn d·∫ßn ƒë·∫øn max l√† 200 user\n  run-time = 2m: th·ªùi gian ch·∫°y file test locust 2 minutes\n  headless = true: s·∫Ω ko c√≥ giao di·ªán UI ƒë·ªÉ v√†o xem tr·ª±c ti·∫øp tr√™n port 8089\n  master = true: s·∫Ω run ·ªü mode distributed mode ( bao g·ªìm master v√† c√°c worker)\n  Run docker-compose:\nDo m√¨nh ƒëang set trong master.conf l√† expect-workers = 3 n√™n m√¨nh s·∫Ω run command scale worker=3\ncd /opt/devops/locust docker-compose up --scale worker=3 Docker s·∫Ω t·∫°o ra 1 container cho master v√† 3 container cho worker ƒë·ªÉ run test\nSau khi ch·∫°y xong, v√†o xem report.html tr√™n chrome th√¥i\n2.3. X√°c ƒë·ªãnh CCU N·∫øu b·∫°n mu·ªën xem web app c·ªßa m√¨nh c√≥ th·ªÉ handle ƒëc bao nhi√™u CCU th√¨ c·ª© tƒÉng d·∫ßn s·ªë users l√™n th√¥i\nH√£y tham kh·∫£o link n√†y: https://github.com/locustio/locust/wiki/FAQ#increase-my-request-raterps N√≥ n√≥i r·∫±ng: N·∫øu Chart Response times tƒÉng l√™n b·∫•t ng·ªù khi b·∫°n tƒÉng s·ªë users l√™n th√¨ c√≥ th·ªÉ app c·ªßa b·∫°n ƒë√£ ƒë·∫°t gi·ªõi h·∫°n r·ªìi.\nV√≠ d·ª• con s·ªë trung b√¨nh Response Times m√† b·∫°n ch·∫•p nh·∫≠n ƒë∆∞·ª£c l√† 2000ms (2s) ch·∫≥ng h·∫°n. Nh∆∞ b√†i n√†y (https://medium.com/swlh/load-testing-with-locust-3e74349f9cbf) h·ªç n√≥i r·∫±ng:\n In 2020, the average Time-To-First-Byte (TTFB) speed was found to be 1.28 seconds (1280ms) on desktop and 2.59 seconds (1590ms) on mobile. However, Google‚Äôs best practice is to achieve a time under 200ms.\n Th√¨ b·∫°n ph·∫£i tƒÉng s·ªë user d·∫ßn d·∫ßn l√™n. Cho ƒë·∫øn khi ch·ªâ s·ªë Response Times v∆∞·ª£t qu√° 2000ms th√¨ c√≥ nghƒ©a ƒë√≥ l√† ng∆∞·ª°ng gi·ªõi h·∫°n c·ªßa website r·ªìi.\nV√≠ d·ª• t√¨nh hu·ªëng n√†y khi s·ªë l∆∞·ª£ng user=88 th√¨ 95% request m·∫•t 1,9s ƒë·ªÉ finish (nh·ªè h∆°n expected):\nC√≤n khi user=98 th√¨ 95% request m·∫•t 2,8s ƒë·ªÉ finish (l·ªõn h∆°n expected):\nƒê√≥ l√† khi m√¨nh x√°c ƒë·ªãnh ƒë∆∞·ª£c website c·ªßa m√¨nh ch·ªâ ch·∫•p nh·∫≠n kho·∫£ng ~80 user v·ªõi Average size l√† 28101 bytes m√† th√¥i, khi ƒë√≥ Response Times s·∫Ω kh√¥ng v∆∞·ª£t qu√° 2000ms (2s)\n2.4. Request per second (RPS) Ch·∫Øc h·∫≥n khi l√†m b·∫°n s·∫Ω mu·ªën tƒÉng s·ªë l∆∞·ª£ng RPS l√™n, ho·∫∑c x√°c ƒë·ªãnh bao nhi√™u RPS l√† max.\nNh∆∞ng Locust ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: How many concurrent users can my application support? (source: https://stackoverflow.com/a/27716019/9922066)\n In many cases it\u0026rsquo;s much more relevant to be able to say \u0026ldquo;our system can handle X number of simultaneous users\u0026rdquo;, than \u0026ldquo;our system kan handle Y requests/second\u0026rdquo;. Even though it\u0026rsquo;s often quite easy to determine Y as well, by just simulating more users until the system that is being tested can no longer handle the load.\n(source: https://github.com/locustio/locust/issues/646#issuecomment-360405844)\n M·∫∑c d√π v·∫≠y theo link n√†y th√¨ v·∫´n c√≥ 1 c√°ch: https://stackoverflow.com/a/70935620/9922066\nƒê√≥ l√† s·ª≠a file locustfile.py:\nfrom locust import HttpUser, task, constant_throughput class User(HttpUser): wait_time = constant_throughput(1) @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) ho·∫∑c:\nfrom locust import HttpUser, task, constant_pacing class User(HttpUser): wait_time = constant_pacing(1) @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) Khi m√¨nh set:\nusers = 2 spawn-rate = 2 th√¨ s·∫Ω c√≥ ch√≠nh x√°c 2 RPS trong chart:\nKhi m√¨nh set:\nusers = 50 spawn-rate = 2 th√¨ s·∫Ω c√≥ kho·∫£ng 48 RPS trong chart:\nKhi m√¨nh set:\nusers = 80 spawn-rate = 10 th√¨ s·∫Ω c√≥ kho·∫£ng 70 RPS trong chart:\nKhi m√¨nh set:\nusers = 200 spawn-rate = 10 th√¨ v·∫´n s·∫Ω c√≥ kho·∫£ng 70 RPS trong chart:\nƒêi·ªÅu n√†y ch·ª©ng t·ªè 70 RPS l√† gi·ªõi h·∫°n c·ªßa ch√∫ng ta r·ªìi (k·∫øt qu·∫£ n√†y kh√° match v·ªõi CCU m√† m√¨nh ƒë√£ x√°c ƒë·ªãnh trong ph·∫ßn 2.3)\n2.5. Troubeshooting Tuy nhi√™n gi·ªù c√≥ 1 v·∫•n ƒë·ªÅ, b·∫°n bi·∫øt RPS r·ªìi CCU r·ªìi, nh∆∞ng l·∫°i ko ch·∫Øc ƒë√¢y l√† gi·ªõi h·∫°n c·ªßa Web server hay c·ªßa Locust server?\n1 s·ªë ng∆∞·ªùi t·ª´ng g·∫∑p ph·∫£i c√°c v·∫•n ƒë·ªÅ t∆∞∆°ng t·ª±, h·ªç run Locust tr√™n nh·ªØng VPS r·∫•t m·∫°nh, nh∆∞ng RPS ko v∆∞·ª£t qu√° 300 (trong khi h·ªç expect l√† 3000 RPS): https://github.com/locustio/locust/issues/710\nƒêi·ªÅu n√†y ph·ª• thu·ªôc v√†o kh√° nhi·ªÅu th·ª©:\n CPU/Memory/network bandwidth on Web server CPU/Memory/network bandwidth on Locust server Application code  Khi ƒëi·ªÅu ƒë√≥ x·∫£y ra, n·∫øu ko th·ªÉ debug tr√™n log c·ªßa Web server th√¨ h√£y d√πng htop ƒë·ªÉ theo d√µi CPU/Memory tr√™n c·∫£ Locust server v√† Web server, Khi run test ch√∫ng n√™n th∆∞·ªùng xuy√™n ch·∫°y ·ªü m·ª©c 70-80% CPU, Memory m√† th√¥i\nGi·ªù m√¨nh s·∫Ω th·ª≠ t·∫°o 1 static site tr√™n AWS S3. ƒêi·ªÅu n√†y s·∫Ω ƒë·∫£m b·∫£o r·∫±ng s·∫Ω ko c√≥ v·∫•n ƒë·ªÅ g√¨ ·ªü ph√≠a Web server (v√¨ AWS S3 n·ªïi ti·∫øng v·ªÅ vi·ªác c√≥ th·ªÉ serve ƒë∆∞·ª£c r·∫•t nhi·ªÅu RPS) https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/\nhttps://www.netdepot.com/blog/8-top-amazon-s3-performance-tips\nKhi ƒë√≥, n·∫øu Locust script test 1 website tr√™n S3 m√† ch·ªâ c√≥ RPS kho·∫£ng 100 th√¨ ch·ª©ng t·ªè Locust server c·ªßa m√¨nh ƒë√£ ƒë·∫°t gi·ªõi h·∫°n c·ªßa n√≥.\nM√¨nh test v·ªõi:\nusers = 100 spawn-rate = 5 run-time = 2m K·∫øt qu·∫£ kh√° ·∫•n t∆∞·ª£ng khi RPS l√™n 333, Response time c·ªßa 95% l√† 250 ms th√¥i. Ngo√†i ra c√≥ 5 request l·ªói. Gi·ªù m√¨nh test v·ªõi:\nusers = 500 spawn-rate = 50 run-time = 2m K·∫øt qu·∫£ l√† RPS ƒë·∫°t kho·∫£ng 398, Response time c·ªßa 95% l√† 1700 ms. C√≥ 22 request l·ªói.\nNh∆∞ v·∫≠y m√¨nh c√≥ th·ªÉ k·∫øt lu·∫≠n r·∫±ng Locust server c·ªßa m√¨nh ch·ªâ c√≥ th·ªÉ generate ra kho·∫£ng 300-400 RPS v·ªõi CCU kho·∫£ng 300-400 m√† th√¥i. T·∫•t nhi√™n b√†i test n√†y c√≥ ph·∫ßn ko c√¥ng b·∫±ng v·ªõi l·∫ßn test tr∆∞·ªõc v√¨ l·∫ßn n√†y S3 static site c·ªßa m√¨nh c√≥ Average size ch·ªâ kho·∫£ng 837 bytes (√≠t h∆°n nhi·ªÅu so v·ªõi l·∫ßn tr∆∞·ªõc l√† 28101 bytes)\n3. Locust (Advanced) 3.1. Communicate across Locust Nodes B√†i to√°n m√¨nh t·ª± ƒë·∫∑t ra:\n M√¨nh c√≥ 1 admin account v√† 4 test account. Admin account s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ l·∫•y token ƒë√∫ng 1 l·∫ßn tr√™n Master node, ph·∫£i chia s·∫ª token ƒë√≥ cho c√°c Worker node c√≤n l·∫°i d√πng. Test account s·∫Ω ph·∫£i login ƒë√∫ng 1 l·∫ßn tr√™n c√°c Worker (Worker n√†o th√¨ ko c·∫ßn quan t√¢m). C√≥ 4 test account th√¨ trong final report ph·∫£i c√≥ 4 login request. N√≥i chung 1 test account ko ƒë∆∞·ª£c login 2 l·∫ßn.  M√¨nh s·∫Ω s·ª≠ d·ª•ng Navidrome ƒë·ªÉ gi·∫£ l·∫≠p 1 website ƒë·ªÉ test.\nWorking dir s·∫Ω ki·ªÉu nh∆∞ n√†y:\n. ‚îú‚îÄ‚îÄ locust ‚îÇ¬†‚îú‚îÄ‚îÄ docker-compose.yml ‚îÇ¬†‚îú‚îÄ‚îÄ html-report ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ report.html ‚îÇ¬†‚îú‚îÄ‚îÄ locustfile.py ‚îÇ¬†‚îî‚îÄ‚îÄ master.conf ‚îî‚îÄ‚îÄ navidrome ‚îú‚îÄ‚îÄ data ‚îú‚îÄ‚îÄ docker-compose.yml ‚îî‚îÄ‚îÄ music ‚îî‚îÄ‚îÄ Nhu Vay Nhe Cover - Tu Na.mp3 Clone repo sau v·ªÅ: https://github.com/hoangmnsd/locust-lab, trong ƒë√≥ c√≥ s·∫µn c·∫•u tr√∫c folder r·ªìi.\n3.1.1. Setup Navidrome B·∫°n s·∫Ω th·∫•y trong repo https://github.com/hoangmnsd/locust-lab nh∆∞ sau:\n t·∫°o s·∫µn 2 folder navidrome/data v√† navidrome/music, trong ƒë√≥ cho 1 file mp3 b√†i h√°t b·∫•t k·ª≥ v√†o folder music. t·∫°o file navidrome/docker-compose.yml, ch√∫ √Ω r·∫±ng m√¨nh t·∫°o 1 network bridge common-nw ƒë·ªÉ sau n√†y c√≥ th·ªÉ communicate gi·ªØa 2 docker-compose.yml file kh√°c nhau:  version: \u0026#39;3\u0026#39; services: navidrome: container_name: navidrome image: deluan/navidrome:latest ports: - \u0026#34;4533:4533\u0026#34; environment: # Optional: put your config options customization here. Examples: ND_SCANSCHEDULE: 1h ND_LOGLEVEL: info ND_BASEURL: \u0026#34;\u0026#34; volumes: - \u0026#34;../navidrome/data:/data\u0026#34; - \u0026#34;../navidrome/music:/music:ro\u0026#34; networks: - common-nw networks: common-nw: driver: bridge  Run command:  cd navidrome/ docker-compose up -d Gi·ªù h√£y truy c·∫≠p v√†o ƒë·ªãa ch·ªâ ip c·ªßa Server m√† b·∫°n c√†i Navidrome: http://\u0026lt;IP-ADDRESS\u0026gt;:4533\nT·∫°o admin account v√† test account:\nGi·∫£ s·ª≠ user/password l·∫ßn l∆∞·ª£t l√†:\n admin/admin test1/test1 test2/test2 test3/test3 test4/test4  3.1.2. Setup Locust Trong repo https://github.com/hoangmnsd/locust-lab:\n t·∫°o s·∫µn folder locust/html-report ƒë·ªÉ sau n√†y ch·ª©a file report t·∫°o s·∫µn file locust/docker-compose.yml, ch√∫ √Ω m√¨nh ƒë√£ li√™n k·∫øt v·ªõi navidrome b·∫±ng c√°ch s·ª≠ d·ª•ng external network, ƒëi·ªÅu n√†y cho ph√©p locust s·∫Ω hi·ªÉu khi b·∫°n define g·ªçi ƒë·∫øn host http://navidrome:4533:  version: '3' services: master: image: locustio/locust ports: - \u0026quot;8089:8089\u0026quot; volumes: - .:/mnt/locust command: -f /mnt/locust/locustfile.py --config=/mnt/locust/master.conf networks: - navidrome_common-nw worker: image: locustio/locust volumes: - .:/mnt/locust command: -f /mnt/locust/locustfile.py --worker --master-host master networks: - navidrome_common-nw networks: navidrome_common-nw: external: true  t·∫°o s·∫µn file locust/master.conf, ch√∫ √Ω ch·ªó host m√¨nh ƒëang tr·ªè ƒë·∫øn hostname c·ªßa container service http://navidrome:4533, ƒëi·ªÅu n√†y h·∫°n ch·∫ø l·ªói Failed to establish a new connection: [Errno 111] Connection refused')) or [Errno 113] No route to host khi b·∫°n d√πng localhost:4533, 127.0.0.1:4533:  # master.conf in current directory locustfile = ./locustfile.py headless = true master = true expect-workers = 2 host = http://navidrome:4533 users = 50 spawn-rate = 5 run-time = 1m html = /mnt/locust/html-report/report.html =\u0026gt; file n√†y ƒëang setting s·ªë worker = 2 v√† l∆∞·ª£ng user gi·∫£ l·∫≠p l√† 50. C·∫ßn ph√¢n bi·ªát r√µ, 50 user n√†y ko ph·∫£i l√† s·ªë account t·∫°o ra ƒë√¢u nh√©. 50 users n√†y gi·ªëng nh∆∞ 50 tab kh√°c nhau ƒë∆∞·ª£c b·∫≠t l√™n, th·ª±c hi·ªán ri√™ng c√°c sequence of task trong locustfile.py\n t·∫°o s·∫µn file locust/locustfile.py, ƒë√¢y l√† file quan tr·ªçng m√† m√¨nh mu·ªën gi·∫£i th√≠ch trong ph·∫ßn sau  3.1.3. Run script and explain the log cd locust docker-compose up --scale worker=2 ƒê·∫£m b·∫£o r·∫±ng script locust/locustfile.py ch·∫°y ko c√≥ l·ªói g√¨, b·∫°n c√≥ th·ªÉ download report.html v·ªÅ v√† m·ªü tr√™n chrome ƒë·ªÉ xem:\nGi·∫£i th√≠ch script locust/locustfile.py:\n  decorate @events.init.add_listener s·∫Ω gi√∫p on_locust_init run ƒë·∫ßu ti√™n tr√™n c√°c node v√† ch·ªâ run 1 l·∫ßn. N·∫øu ƒëang run tr√™n Worker, ta s·∫Ω ƒëƒÉng k√Ω (register) message r·∫±ng n·∫øu nh·∫≠n ƒë∆∞·ª£c message type test_users th√¨ h√£y run h√†m setup_test_users, c√≤n n·∫øu nh·∫≠n ƒë∆∞·ª£c message type unique_token th√¨ run h√†m setup_unique_token. N·∫øu ƒëang run tr√™n Master ta s·∫Ω ƒëƒÉng k√Ω message r·∫±ng n·∫øu nh·∫≠n ƒë∆∞·ª£c message acknowledge_users th√¨ h√£y run h√†m on_acknowledge.\n  Sau ƒë√≥ h√†m c√≥ decorate @events.test_start.add_listener s·∫Ω run ƒë·ªÉ l·∫•y admin token n·∫øu ƒëang l√† Master node. Ti·∫øp, n√≥ chia ƒë·ªÅu c√°c test account cho t·ª´ng Worker (ch·ªó n√†y c·ªông tr·ª´ nh√¢n chia h∆°i r·ªëi b·∫°n n√™n ƒë·ªçc k·ªπ). Cu·ªëi c√πng n√≥ g·ª≠i data (c√°c account v√† token) t·ªõi t·ª´ng Worker b·∫±ng h√†m environment.runner.send_message. ƒê√¢y ch√≠nh l√† chi·ªÅu giao ti·∫øp t·ª´ Master ƒë·∫øn Workers.\n  Ti·∫øp theo, h√†m setup_test_users v√† setup_unique_token s·∫Ω ƒë∆∞·ª£c run. Ch·ªó h√†m setup_test_users b·∫°n s·∫Ω th·∫•y khi nh·∫≠n ƒë∆∞·ª£c list account c·ªßa m√¨nh xong, Worker s·∫Ω run h√†m environment.runner.send_message ƒë·ªÉ g·ª≠i t·ªõi Master l·ªùi c·∫£m ∆°n c·ªßa m√¨nh. Khi Master nh·∫≠n ƒë∆∞·ª£c n√≥ s·∫Ω in ra l·ªùi c·∫£m ∆°n c·ªßa Worker. ƒê√¢y ch√≠nh l√† chi·ªÅu giao ti·∫øp t·ª´ Worker ƒë·∫øn Master.\n  Cu·ªôc n√≥i chuy·ªán k·∫øt th√∫c v√† h√†m on_start(self) s·∫Ω m·ªü ƒë·∫ßu cho chu·ªói sequence of tasks. N√≥ l·∫•y token ra t·ª´ list token_shared g√°n v√†o 1 bi·∫øn global token ƒë·ªÉ c√°c task kh√°c c√≥ th·ªÉ s·ª≠ d·ª•ng. Sau ƒë√≥, n√≥ pop() d·∫ßn d·∫ßn t·ª´ng account trong list testuser_filtered ra ƒë·ªÉ login. ƒêi·ªÅu ki·ªán if len(testuser_filtered) \u0026gt; 0: s·∫Ω ƒë·∫£m b·∫£o r·∫±ng script s·∫Ω ko pop from empty list, nh∆∞ v·∫≠y m·∫∑c d√π h√†m on_start(self) run nhi·ªÅu l·∫ßn nh∆∞ng m·ªói test account ch·ªâ login 1 l·∫ßn m√† th√¥i.\n  H√†m getAllSongs, ch√∫ √Ω ch·ªó response.failure(\u0026quot;getAllSongs: Got wrong response\u0026quot;), l√† 1 v√≠ d·ª• n·∫øu b·∫°n mu·ªën ki·ªÉm tra response tr·∫£ v·ªÅ.\n  H√†m getAllAlbums ch·ªó elif response.elapsed.total_seconds() \u0026gt; 0.04:, l√† 1 v√≠ d·ª• n·∫øu b·∫°n mu·ªën assert response time ph·∫£i ·ªü m·ª©c n√†o ƒë√≥.\n  H√†m createDeletePlaylist ch·ªó name=\u0026quot;/api/playlist/[id]\u0026quot;, l√† 1 v√≠ d·ª• ƒë·ªÉ b·∫°n nh√≥m c√°c request l·∫°i trong Final Report, tr√°nh vi·ªác l√†m r·ªëi report b·ªüi qu√° nhi·ªÅu request c√πng lo·∫°i.\n  Trong document c·ªßa Locust c√≥ n√≥i v·ªÅ recommended structure folder nh∆∞ sau: https://docs.locust.io/en/stable/writing-a-locustfile.html#how-to-structure-your-test-code\n  LOG kh√° d√†i nh∆∞ng m√¨nh ch·ªâ l·∫•y ph·∫ßn ƒë·∫ßu th√¥i, xem log n√†y c√πng v·ªõi ƒë·ªçc code s·∫Ω hi·ªÉu ƒë∆∞·ª£c th·ª© t·ª± c√°c h√†m ƒë∆∞·ª£c g·ªçi trong Locust:\n $ docker-compose up --scale worker=2 [+] Running 3/0 ‚†ø Container locust-worker-1 Created 0.0s ‚†ø Container locust-worker-2 Created 0.0s ‚†ø Container locust-master-1 Created 0.0s Attaching to locust-master-1, locust-worker-1, locust-worker-2 locust-master-1 | Initializing locust message types and listeners locust-master-1 | [2022-10-15 08:14:43,657] 6335de046f29/INFO/root: Waiting for workers to be ready, 0 of 2 connected locust-master-1 | Type Name # reqs # fails | Avg Min Max Med | req/s failures/s locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | Aggregated 0 0(0.00%) | 0 0 0 0 | 0.00 0.00 locust-master-1 | locust-master-1 | [2022-10-15 08:14:43,666] 6335de046f29/INFO/locust.runners: Worker 5201f3c32fcb_57d185c2045540ffbeeae3fc9d30e1ed (index 0) reported as ready. 1 workers connected. locust-worker-2 | Initializing locust message types and listeners locust-worker-2 | [2022-10-15 08:14:43,668] 5201f3c32fcb/INFO/locust.main: Starting Locust 2.12.1 locust-master-1 | [2022-10-15 08:14:43,911] 6335de046f29/INFO/locust.runners: Worker 29adedeb6842_18225c26a017418e85458e5356309bba (index 1) reported as ready. 2 workers connected. locust-worker-1 | Initializing locust message types and listeners locust-worker-1 | [2022-10-15 08:14:43,913] 29adedeb6842/INFO/locust.main: Starting Locust 2.12.1 locust-master-1 | [2022-10-15 08:14:44,659] 6335de046f29/INFO/locust.main: Run time limit set to 60 seconds locust-master-1 | [2022-10-15 08:14:44,660] 6335de046f29/INFO/locust.main: Starting Locust 2.12.1 locust-master-1 | [2022-10-15 08:14:44,661] 6335de046f29/INFO/locust.runners: Sending spawn jobs of 50 users at 5.00 spawn rate to 2 ready workers locust-master-1 | MASTER say - Trying to get token on this node locust-master-1 | MASTER say - Number of user to be divided between each worker is 2 locust-master-1 | sending this data [{'account': ('test1', 'test1')}, {'account': ('test2', 'test2')}] to this 5201f3c32fcb_57d185c2045540ffbeeae3fc9d30e1ed locust-master-1 | sending this token [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] to this 5201f3c32fcb_57d185c2045540ffbeeae3fc9d30e1ed locust-master-1 | sending this data [{'account': ('test3', 'test3')}, {'account': ('test4', 'test4')}] to this 29adedeb6842_18225c26a017418e85458e5356309bba locust-worker-2 | Worker say - setup_test_users - msg.data: [{'account': ['test1', 'test1']}, {'account': ['test2', 'test2']}] locust-worker-2 | Worker say - Here is accounts I received: [['test1', 'test1'], ['test2', 'test2']] locust-master-1 | sending this token [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] to this 29adedeb6842_18225c26a017418e85458e5356309bba locust-worker-2 | Worker say - setup_unique_token - msg.data: [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] locust-worker-2 | Worker say - Here is token I received: ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | Worker say - setup_test_users - msg.data: [{'account': ['test3', 'test3']}, {'account': ['test4', 'test4']}] locust-worker-1 | Worker say - Here is accounts I received: [['test3', 'test3'], ['test4', 'test4']] locust-master-1 | MASTER say - Here is message from Woker: Thanks for the 2 users! locust-worker-1 | Worker say - setup_unique_token - msg.data: [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] locust-worker-1 | Worker say - Here is token I received: ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-master-1 | MASTER say - Here is message from Woker: Thanks for the 2 users! locust-worker-2 | on_start running - testuser_filtered = [['test1', 'test1'], ['test2', 'test2']] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [['test3', 'test3'], ['test4', 'test4']] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [['test1', 'test1']] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [['test3', 'test3']] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-master-1 | Type Name # reqs # fails | Avg Min Max Med | req/s failures/s locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | Aggregated 0 0(0.00%) | 0 0 0 0 | 0.00 0.00 locust-master-1 | locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-master-1 | Type Name # reqs # fails | Avg Min Max Med | req/s failures/s locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | GET / 10 0(0.00%) | 33 19 74 30 | 0.00 0.00 locust-master-1 | GET /api/song?_end=15\u0026amp;_order=ASC\u0026amp;_sort=title\u0026amp;_start=0 1 0(0.00%) | 18 18 18 18 | 0.00 0.00 locust-master-1 | POST /auth/login 4 0(0.00%) | 60 56 63 60 | 0.00 0.00 locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | Aggregated 15 0(0.00%) | 40 18 74 37 | 0.00 0.00 locust-master-1 | locust-worker-1 | on_start running - testuser_filtered = [] 3.2. Setup Distributed Locust system on Azure Ch√∫ng ta s·∫Ω d·ª±ng m√¥i tr∆∞·ªùng tr√™n h·ªá th·ªëng Azure ACI:\nhttps://github.com/hoangmnsd/locust-lab/blob/master/locust-az-aci-structure.jpg\nƒêi·ªÉm m·∫°nh c·ªßa h·ªá th·ªëng n√†y l√† b·∫°n c√≥ th·ªÉ scale Worker l√™n s·ªë l∆∞·ª£ng r·∫•t nhi·ªÅu, mi·ªÖn l√† b·∫°n c√≥ ƒë·ªß ti·ªÅn üòÇ\nƒê·∫ßu ti√™n l√† t·∫°o 1 resource group: locust-rg\nS·ª≠a file azure/azuredeploy.parameters.json:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;instances\u0026#34;: { \u0026#34;value\u0026#34;: 1 }, \u0026#34;prefix\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;hahihohe\u0026#34; } } } N·∫øu c·∫ßn s·ªë l∆∞·ª£ng Worker l√† 5 th√¨ s·ª≠a value c·ªßa instances th√†nh 5\nOpen Cloudshell to run these commands:\nexport resourceGroup=locust-rg cd locust-lab/ templateFile=\u0026#34;azure/azuredeploy.json\u0026#34; paramsFile=\u0026#34;azure/azuredeploy.parameters.json\u0026#34; az deployment group create --name \u0026#34;locust\u0026#34; --resource-group $resourceGroup --template-file $templateFile --parameters $paramsFile Go to Storage Account -\u0026gt; File Shares tab -\u0026gt; scripts -\u0026gt; upload files:\n master.conf (l·∫•y t·ª´ locust-lab/locust/master.conf) locustfile.py (l·∫•y t·ª´ locust-lab/locust/locustfile.py) create directory html-report  Edit file t∆∞∆°ng ·ª©ng:\n Click v√†o ... t∆∞∆°ng ·ª©ng m·ªói file n·∫øu c·∫ßn. File master.conf:  s·ª≠a expect-workers t√πy theo s·ªë l∆∞·ª£ng instances trong azure/azuredeploy.parameters.json hi·ªán c√≥ s·ª≠a host tr·ªè ƒë·∫øn navidrome webapp target m√† b·∫°n ƒë√£ build    Restart containers:\naz container restart --name hmmnsd1-master --resource-group $resourceGroup \u0026amp;\u0026amp; \\ az container restart --name hmmnsd1-worker-0 --resource-group $resourceGroup Ho·∫∑c trong Output c·ªßa ARM sau khi deploy xong s·∫Ω c√≥ command ƒë·ªÉ restart containers\nSau khi restart xong ch·ªù Container run test xong th√¨ report s·∫Ω xu·∫•t ra folder html-report trong Storage account. Download file ƒë√≥ v·ªÅ m·ªü trong Chrome l√† xem ƒë∆∞·ª£c.\nC√≥ th·ªÉ c√≥ 1 c√°ch kh√°c l√† deploy tr√™n K8s, xu·∫•t report ra CSV file, ƒë·∫©y v√†o Prometheus b·∫±ng 1 tool (LocustExporter)[https://github.com/ContainerSolutions/locust_exporter], r·ªìi hi·ªÉn th·ªã tr√™n Grafana.\nXong r·ªìi th√¨ c√≥ th·ªÉ x√≥a Resource Group ƒëi ƒë∆∞·ª£c r·ªìi.\nCh√∫ √Ω v·ªÅ gi√° c·ªßa ACI v√† Storage account: https://azure.microsoft.com/en-us/pricing/details/container-instances/\nCREDIT Rakesh\u0026rsquo;s comment on: https://www.softwaretestinghelp.com/what-is-performance-testing-load-testing-stress-testing/\nhttps://docs.locust.io/en/1.5.2/\nhttps://github.com/tavishigupta/locust-blog/tree/master/deployment\nhttps://github.com/locustio/locust/wiki/FAQ#increase-my-request-raterps\nhttps://medium.com/swlh/load-testing-with-locust-3e74349f9cbf\nhttps://docs.locust.io/en/stable/api.html#locust.wait_time.constant_throughput\nhttps://github.com/locustio/locust/issues/646#issuecomment-360405844\nhttps://www.blazemeter.com/blog/locust-python\nhttps://www.blazemeter.com/blog/locust-multiple-users\nhttps://github.com/locustio/locust/wiki/Installation#increasing-maximum-number-of-open-files-limit\nhttps://stackoverflow.com/questions/71470516/how-do-i-set-a-specify-rps-in-locust-like-500-requests-per-seconds\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\nhttps://docs.locust.io/en/stable/writing-a-locustfile.html#how-to-structure-your-test-code\n1 s·ªë example script c·ªßa c·ªông ƒë·ªìng ƒë√≥ng g√≥p: https://github.com/locustio/locust/blob/master/examples\nhttps://docs.locust.io/en/stable/running-distributed.html#communicating-across-nodes\nhttps://github.com/locustio/locust/blob/master/examples/test_data_management.py\nhttps://github.com/locustio/locust/blob/master/examples/custom_messages.py\nhttps://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects\n1 b√†i kh√° hay v·ªÅ Locust tr√™n k8s, s·ª≠ d·ª•ng distributed mode, gi·∫£i th√≠ch c∆° ch·∫ø share msg between workers, v√≠ d·ª• v·ªÅ use-case c·∫ßn d√πng n√≥, c√≥ th·ªÉ k·∫øt h·ª£p locust exporter ƒë·ªÉ export report csv ra prometheus r·ªìi show tr√™n grafana:\nhttps://medium.com/dkatalis/distributed-load-testing-on-k8s-using-locust-6ee4ed6c7ca\n","href":"/bk/encrypt-performance-tests-note-locust/","title":"Performance Tests with Locust"},{"content":"1. Gi·∫£i th√≠ch c√°c kh√°i ni·ªám 1 c√°ch ng·∫Øn g·ªçn Performance test ko ph·∫£i l√† 1 ki·ªÉu test c·ª• th·ªÉ n√†o, n√≥ l√† 1 kh√°i ni·ªám chung cho t·∫•t c·∫£ c√°c ki·ªÉu test nh·∫±m validate performance v√† x√°c ƒë·ªãnh performance issues.\nLoad test l√† 1 ki·ªÉu test ƒë·ªÉ validate xem application c·ªßa b·∫°n c√≥ th·ªèa m√£n c√°c goals ƒë√£ ƒëc define t·ª´ tr∆∞·ªõc hay ko? Khi x√¢y d·ª±ng app th√¨ b·∫°n ph·∫£i define r·∫±ng app n√†y s·∫Ω cho ph√©p bao nhi√™u CCU? N·∫øu b·∫°n define 50 CCU th√¨ Load test ch·ªâ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c 50 CCU l√† OK.\nStress test l√† 1 ki·ªÉu test ƒë·ªÉ x√°c ƒë·ªãnh behavior c·ªßa application l√† nh∆∞ n√†o n·∫øu n√≥ ph·∫£i ch·ªãu 1 l∆∞·ª£ng traffic l·ªõn h∆°n nhi·ªÅu so v·ªõi expectation. V√≠ d·ª• n·∫øu app c·ªßa b·∫°n define CCU l√† 50 th√¨ Stress test v·ªõi 500 CCU th√¨ App c·ªßa b·∫°n s·∫Ω nh∆∞ n√†o? (throttle? crash? restart?)\nVolume test l√† 1 ki·ªÉu test ƒë∆∞·ª£c d√πng trong 2 case:\n X√°c ƒë·ªãnh performance c·ªßa app khi ph·∫£i ch·ªãu c√°c kh·ªëi l∆∞·ª£ng data kh√°c nhau ƒëi v√†o Database T√≠nh to√°n kh·∫£ nƒÉng c·ªßa app v·ªõi 1 kh·ªëi l∆∞·ª£ng l·ªõn data (v√≠ d·ª• nh∆∞ import large volume data from file to DB\u0026hellip;)  Endurance test l√† 1 ki·ªÉu test ƒë·ªÉ t√¨m ra c√°c issue m√† ch·ªâ c√≥ th·ªÉ x·∫£y ra khi app ƒë√£ ch·∫°y 1 th·ªùi gian d√†i, n√≥i chung l√† nh·ªØng case m√† kh√≥ c√≥ th·ªÉ ph√°t hi·ªán bug n·∫øu th·ªùi gian test ng·∫Øn.\n2. Locust (Basic) 2.1. Basic step Install:\n$ python --version Python 3.9.2 $ pip3 install locust # wait till all done $ locust -V locust 2.12.1 Quick start:\nt·∫°o 1 file locustfile.py:\nfrom locust import HttpUser, task class User(HttpUser): @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) th·∫ø th√¥i, run command locust:\n$ locust [2022-09-30 23:29:17,172] raspian-rpi/WARNING/locust.main: System open file limit '1024' is below minimum setting '10000'. It's not high enough for load testing, and the OS didn't allow locust to increase it by itself. See https://github.com/locustio/locust/wiki/Installation#increasing-maximum-number-of-open-files-limit for more info. [2022-09-30 23:29:17,173] raspian-rpi/INFO/locust.main: Starting web interface at http://0.0.0.0:8089 (accepting connections from all network interfaces) [2022-09-30 23:29:17,205] raspian-rpi/INFO/locust.main: Starting Locust 2.12.1 Gi·ªù b·∫°n c√≥ th·ªÉ truy c·∫≠p v√†o ƒë·ªãa ch·ªâ 127.0.0.1:8089 ƒë·ªÉ xem giao di·ªán Locust:\nƒêi·ªÅn 1 s·ªë th√¥ng tin v√†o giao di·ªán nh∆∞ n√†y r·ªìi ·∫•n Start:\nm√†n h√¨nh ch·ªù trong l√∫c script ƒëang ch·∫°y:\n·∫§n Stop v√† xem k·∫øt qu·∫£: ·∫§n v√†o tab Chart ƒë·ªÉ xem:\nChart Total RPS ƒë√∫ng nh∆∞ t√™n g·ªçi, ch∆∞a bi·∫øt c√°ch tƒÉng l√™n nh∆∞ n√†o?\nChart Response times:\n c√≥ Median Response Time l√† respone time trung b√¨nh c·ªßa 50% s·ªë request c√≥ 95% percentile: l√† respone time trung b√¨nh c·ªßa 95% s·ªë request Chart Number of users: s·ªë l∆∞·ª£ng user theo th·ªùi gian  ·∫§n v√†o tab Download Data s·∫Ω download dc report d∆∞·ªõi d·∫°ng html:\nKh√° hay ƒë·∫•y ch·ª©\nTuy nhi√™n m√¨nh s·∫Ω t√¨m c√°ch ƒë·ªÉ l√†m h·∫øt d∆∞·ªõi d·∫°ng c√¢u l·ªánh v√†o l∆∞u l·∫°i report 1 c√°ch t·ª± ƒë·ªông sau khi test xong.\n2.2. Distributed mode with docker-compose Ch·∫°y ·ªü mode n√†y s·∫Ω t·∫°o ra c√°c container cho master v√† worker, l√†m tƒÉng performance c·ªßa Locust l√™n\nCh√∫ng ta s·∫Ω chu·∫©n b·ªã folder structure ƒë∆°n gi·∫£n nh∆∞ n√†y:\nt·∫°o folder /opt/devops/locust tr∆∞·ªõc nh√©\n/opt/devops/locust $ tree . . ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ html-report/ ‚îú‚îÄ‚îÄ locustfile.py ‚îú‚îÄ‚îÄ master.conf File docker-compose.yml file:\nversion: \u0026#39;3\u0026#39; services: master: image: locustio/locust ports: - \u0026#34;8089:8089\u0026#34; volumes: - /opt/devops/locust:/mnt/locust command: -f /mnt/locust/locustfile.py --config=/mnt/locust/master.conf worker: image: locustio/locust volumes: - /opt/devops/locust:/mnt/locust command: -f /mnt/locust/locustfile.py --worker --master-host master File locustfile.py:\nfrom locust import HttpUser, task class User(HttpUser): @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) File master.conf:\n# master.conf in current directory locustfile = ./locustfile.py headless = true master = true expect-workers = 3 host = http://192.168.1.128:3000 # this is the site you want to test users = 80 spawn-rate = 2 run-time = 2m html = /mnt/locust/html-report/report.html https://docs.locust.io/en/2.12.1/configuration.html#configuration-file\n  users = 200: t·ªïng s·ªë user\n  spawn-rate = 10: l√† s·ªë user per second\nEx: users = 200, spawn-rate = 10=\u0026gt; m·ªói gi√¢y s·∫Ω tƒÉng l√™n 10 user, d·∫ßn d·∫ßn ƒë·∫øn max l√† 200 user\n  run-time = 2m: th·ªùi gian ch·∫°y file test locust 2 minutes\n  headless = true: s·∫Ω ko c√≥ giao di·ªán UI ƒë·ªÉ v√†o xem tr·ª±c ti·∫øp tr√™n port 8089\n  master = true: s·∫Ω run ·ªü mode distributed mode ( bao g·ªìm master v√† c√°c worker)\n  Run docker-compose:\nDo m√¨nh ƒëang set trong master.conf l√† expect-workers = 3 n√™n m√¨nh s·∫Ω run command scale worker=3\ncd /opt/devops/locust docker-compose up --scale worker=3 Docker s·∫Ω t·∫°o ra 1 container cho master v√† 3 container cho worker ƒë·ªÉ run test\nSau khi ch·∫°y xong, v√†o xem report.html tr√™n chrome th√¥i\n2.3. X√°c ƒë·ªãnh CCU N·∫øu b·∫°n mu·ªën xem web app c·ªßa m√¨nh c√≥ th·ªÉ handle ƒëc bao nhi√™u CCU th√¨ c·ª© tƒÉng d·∫ßn s·ªë users l√™n th√¥i\nH√£y tham kh·∫£o link n√†y: https://github.com/locustio/locust/wiki/FAQ#increase-my-request-raterps N√≥ n√≥i r·∫±ng: N·∫øu Chart Response times tƒÉng l√™n b·∫•t ng·ªù khi b·∫°n tƒÉng s·ªë users l√™n th√¨ c√≥ th·ªÉ app c·ªßa b·∫°n ƒë√£ ƒë·∫°t gi·ªõi h·∫°n r·ªìi.\nV√≠ d·ª• con s·ªë trung b√¨nh Response Times m√† b·∫°n ch·∫•p nh·∫≠n ƒë∆∞·ª£c l√† 2000ms (2s) ch·∫≥ng h·∫°n. Nh∆∞ b√†i n√†y (https://medium.com/swlh/load-testing-with-locust-3e74349f9cbf) h·ªç n√≥i r·∫±ng:\n In 2020, the average Time-To-First-Byte (TTFB) speed was found to be 1.28 seconds (1280ms) on desktop and 2.59 seconds (1590ms) on mobile. However, Google‚Äôs best practice is to achieve a time under 200ms.\n Th√¨ b·∫°n ph·∫£i tƒÉng s·ªë user d·∫ßn d·∫ßn l√™n. Cho ƒë·∫øn khi ch·ªâ s·ªë Response Times v∆∞·ª£t qu√° 2000ms th√¨ c√≥ nghƒ©a ƒë√≥ l√† ng∆∞·ª°ng gi·ªõi h·∫°n c·ªßa website r·ªìi.\nV√≠ d·ª• t√¨nh hu·ªëng n√†y khi s·ªë l∆∞·ª£ng user=88 th√¨ 95% request m·∫•t 1,9s ƒë·ªÉ finish (nh·ªè h∆°n expected):\nC√≤n khi user=98 th√¨ 95% request m·∫•t 2,8s ƒë·ªÉ finish (l·ªõn h∆°n expected):\nƒê√≥ l√† khi m√¨nh x√°c ƒë·ªãnh ƒë∆∞·ª£c website c·ªßa m√¨nh ch·ªâ ch·∫•p nh·∫≠n kho·∫£ng ~80 user v·ªõi Average size l√† 28101 bytes m√† th√¥i, khi ƒë√≥ Response Times s·∫Ω kh√¥ng v∆∞·ª£t qu√° 2000ms (2s)\n2.4. Request per second (RPS) Ch·∫Øc h·∫≥n khi l√†m b·∫°n s·∫Ω mu·ªën tƒÉng s·ªë l∆∞·ª£ng RPS l√™n, ho·∫∑c x√°c ƒë·ªãnh bao nhi√™u RPS l√† max.\nNh∆∞ng Locust ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: How many concurrent users can my application support? (source: https://stackoverflow.com/a/27716019/9922066)\n In many cases it\u0026rsquo;s much more relevant to be able to say \u0026ldquo;our system can handle X number of simultaneous users\u0026rdquo;, than \u0026ldquo;our system kan handle Y requests/second\u0026rdquo;. Even though it\u0026rsquo;s often quite easy to determine Y as well, by just simulating more users until the system that is being tested can no longer handle the load.\n(source: https://github.com/locustio/locust/issues/646#issuecomment-360405844)\n M·∫∑c d√π v·∫≠y theo link n√†y th√¨ v·∫´n c√≥ 1 c√°ch: https://stackoverflow.com/a/70935620/9922066\nƒê√≥ l√† s·ª≠a file locustfile.py:\nfrom locust import HttpUser, task, constant_throughput class User(HttpUser): wait_time = constant_throughput(1) @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) ho·∫∑c:\nfrom locust import HttpUser, task, constant_pacing class User(HttpUser): wait_time = constant_pacing(1) @task def mainPage(self): self.client.get(\u0026#34;/\u0026#34;) Khi m√¨nh set:\nusers = 2 spawn-rate = 2 th√¨ s·∫Ω c√≥ ch√≠nh x√°c 2 RPS trong chart:\nKhi m√¨nh set:\nusers = 50 spawn-rate = 2 th√¨ s·∫Ω c√≥ kho·∫£ng 48 RPS trong chart:\nKhi m√¨nh set:\nusers = 80 spawn-rate = 10 th√¨ s·∫Ω c√≥ kho·∫£ng 70 RPS trong chart:\nKhi m√¨nh set:\nusers = 200 spawn-rate = 10 th√¨ v·∫´n s·∫Ω c√≥ kho·∫£ng 70 RPS trong chart:\nƒêi·ªÅu n√†y ch·ª©ng t·ªè 70 RPS l√† gi·ªõi h·∫°n c·ªßa ch√∫ng ta r·ªìi (k·∫øt qu·∫£ n√†y kh√° match v·ªõi CCU m√† m√¨nh ƒë√£ x√°c ƒë·ªãnh trong ph·∫ßn 2.3)\n2.5. Troubeshooting Tuy nhi√™n gi·ªù c√≥ 1 v·∫•n ƒë·ªÅ, b·∫°n bi·∫øt RPS r·ªìi CCU r·ªìi, nh∆∞ng l·∫°i ko ch·∫Øc ƒë√¢y l√† gi·ªõi h·∫°n c·ªßa Web server hay c·ªßa Locust server?\n1 s·ªë ng∆∞·ªùi t·ª´ng g·∫∑p ph·∫£i c√°c v·∫•n ƒë·ªÅ t∆∞∆°ng t·ª±, h·ªç run Locust tr√™n nh·ªØng VPS r·∫•t m·∫°nh, nh∆∞ng RPS ko v∆∞·ª£t qu√° 300 (trong khi h·ªç expect l√† 3000 RPS): https://github.com/locustio/locust/issues/710\nƒêi·ªÅu n√†y ph·ª• thu·ªôc v√†o kh√° nhi·ªÅu th·ª©:\n CPU/Memory/network bandwidth on Web server CPU/Memory/network bandwidth on Locust server Application code  Khi ƒëi·ªÅu ƒë√≥ x·∫£y ra, n·∫øu ko th·ªÉ debug tr√™n log c·ªßa Web server th√¨ h√£y d√πng htop ƒë·ªÉ theo d√µi CPU/Memory tr√™n c·∫£ Locust server v√† Web server, Khi run test ch√∫ng n√™n th∆∞·ªùng xuy√™n ch·∫°y ·ªü m·ª©c 70-80% CPU, Memory m√† th√¥i\nGi·ªù m√¨nh s·∫Ω th·ª≠ t·∫°o 1 static site tr√™n AWS S3. ƒêi·ªÅu n√†y s·∫Ω ƒë·∫£m b·∫£o r·∫±ng s·∫Ω ko c√≥ v·∫•n ƒë·ªÅ g√¨ ·ªü ph√≠a Web server (v√¨ AWS S3 n·ªïi ti·∫øng v·ªÅ vi·ªác c√≥ th·ªÉ serve ƒë∆∞·ª£c r·∫•t nhi·ªÅu RPS) https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/\nhttps://www.netdepot.com/blog/8-top-amazon-s3-performance-tips\nKhi ƒë√≥, n·∫øu Locust script test 1 website tr√™n S3 m√† ch·ªâ c√≥ RPS kho·∫£ng 100 th√¨ ch·ª©ng t·ªè Locust server c·ªßa m√¨nh ƒë√£ ƒë·∫°t gi·ªõi h·∫°n c·ªßa n√≥.\nM√¨nh test v·ªõi:\nusers = 100 spawn-rate = 5 run-time = 2m K·∫øt qu·∫£ kh√° ·∫•n t∆∞·ª£ng khi RPS l√™n 333, Response time c·ªßa 95% l√† 250 ms th√¥i. Ngo√†i ra c√≥ 5 request l·ªói. Gi·ªù m√¨nh test v·ªõi:\nusers = 500 spawn-rate = 50 run-time = 2m K·∫øt qu·∫£ l√† RPS ƒë·∫°t kho·∫£ng 398, Response time c·ªßa 95% l√† 1700 ms. C√≥ 22 request l·ªói.\nNh∆∞ v·∫≠y m√¨nh c√≥ th·ªÉ k·∫øt lu·∫≠n r·∫±ng Locust server c·ªßa m√¨nh ch·ªâ c√≥ th·ªÉ generate ra kho·∫£ng 300-400 RPS v·ªõi CCU kho·∫£ng 300-400 m√† th√¥i. T·∫•t nhi√™n b√†i test n√†y c√≥ ph·∫ßn ko c√¥ng b·∫±ng v·ªõi l·∫ßn test tr∆∞·ªõc v√¨ l·∫ßn n√†y S3 static site c·ªßa m√¨nh c√≥ Average size ch·ªâ kho·∫£ng 837 bytes (√≠t h∆°n nhi·ªÅu so v·ªõi l·∫ßn tr∆∞·ªõc l√† 28101 bytes)\n3. Locust (Advanced) 3.1. Communicate across Locust Nodes B√†i to√°n m√¨nh t·ª± ƒë·∫∑t ra:\n M√¨nh c√≥ 1 admin account v√† 4 test account. Admin account s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ l·∫•y token ƒë√∫ng 1 l·∫ßn tr√™n Master node, ph·∫£i chia s·∫ª token ƒë√≥ cho c√°c Worker node c√≤n l·∫°i d√πng. Test account s·∫Ω ph·∫£i login ƒë√∫ng 1 l·∫ßn tr√™n c√°c Worker (Worker n√†o th√¨ ko c·∫ßn quan t√¢m). C√≥ 4 test account th√¨ trong final report ph·∫£i c√≥ 4 login request. N√≥i chung 1 test account ko ƒë∆∞·ª£c login 2 l·∫ßn.  M√¨nh s·∫Ω s·ª≠ d·ª•ng Navidrome ƒë·ªÉ gi·∫£ l·∫≠p 1 website ƒë·ªÉ test.\nWorking dir s·∫Ω ki·ªÉu nh∆∞ n√†y:\n. ‚îú‚îÄ‚îÄ locust ‚îÇ¬†‚îú‚îÄ‚îÄ docker-compose.yml ‚îÇ¬†‚îú‚îÄ‚îÄ html-report ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ report.html ‚îÇ¬†‚îú‚îÄ‚îÄ locustfile.py ‚îÇ¬†‚îî‚îÄ‚îÄ master.conf ‚îî‚îÄ‚îÄ navidrome ‚îú‚îÄ‚îÄ data ‚îú‚îÄ‚îÄ docker-compose.yml ‚îî‚îÄ‚îÄ music ‚îî‚îÄ‚îÄ Nhu Vay Nhe Cover - Tu Na.mp3 Clone repo sau v·ªÅ: https://github.com/hoangmnsd/locust-lab, trong ƒë√≥ c√≥ s·∫µn c·∫•u tr√∫c folder r·ªìi.\n3.1.1. Setup Navidrome B·∫°n s·∫Ω th·∫•y trong repo https://github.com/hoangmnsd/locust-lab nh∆∞ sau:\n t·∫°o s·∫µn 2 folder navidrome/data v√† navidrome/music, trong ƒë√≥ cho 1 file mp3 b√†i h√°t b·∫•t k·ª≥ v√†o folder music. t·∫°o file navidrome/docker-compose.yml, ch√∫ √Ω r·∫±ng m√¨nh t·∫°o 1 network bridge common-nw ƒë·ªÉ sau n√†y c√≥ th·ªÉ communicate gi·ªØa 2 docker-compose.yml file kh√°c nhau:  version: \u0026#39;3\u0026#39; services: navidrome: container_name: navidrome image: deluan/navidrome:latest ports: - \u0026#34;4533:4533\u0026#34; environment: # Optional: put your config options customization here. Examples: ND_SCANSCHEDULE: 1h ND_LOGLEVEL: info ND_BASEURL: \u0026#34;\u0026#34; volumes: - \u0026#34;../navidrome/data:/data\u0026#34; - \u0026#34;../navidrome/music:/music:ro\u0026#34; networks: - common-nw networks: common-nw: driver: bridge  Run command:  cd navidrome/ docker-compose up -d Gi·ªù h√£y truy c·∫≠p v√†o ƒë·ªãa ch·ªâ ip c·ªßa Server m√† b·∫°n c√†i Navidrome: http://\u0026lt;IP-ADDRESS\u0026gt;:4533\nT·∫°o admin account v√† test account:\nGi·∫£ s·ª≠ user/password l·∫ßn l∆∞·ª£t l√†:\n admin/admin test1/test1 test2/test2 test3/test3 test4/test4  3.1.2. Setup Locust Trong repo https://github.com/hoangmnsd/locust-lab:\n t·∫°o s·∫µn folder locust/html-report ƒë·ªÉ sau n√†y ch·ª©a file report t·∫°o s·∫µn file locust/docker-compose.yml, ch√∫ √Ω m√¨nh ƒë√£ li√™n k·∫øt v·ªõi navidrome b·∫±ng c√°ch s·ª≠ d·ª•ng external network, ƒëi·ªÅu n√†y cho ph√©p locust s·∫Ω hi·ªÉu khi b·∫°n define g·ªçi ƒë·∫øn host http://navidrome:4533:  version: '3' services: master: image: locustio/locust ports: - \u0026quot;8089:8089\u0026quot; volumes: - .:/mnt/locust command: -f /mnt/locust/locustfile.py --config=/mnt/locust/master.conf networks: - navidrome_common-nw worker: image: locustio/locust volumes: - .:/mnt/locust command: -f /mnt/locust/locustfile.py --worker --master-host master networks: - navidrome_common-nw networks: navidrome_common-nw: external: true  t·∫°o s·∫µn file locust/master.conf, ch√∫ √Ω ch·ªó host m√¨nh ƒëang tr·ªè ƒë·∫øn hostname c·ªßa container service http://navidrome:4533, ƒëi·ªÅu n√†y h·∫°n ch·∫ø l·ªói Failed to establish a new connection: [Errno 111] Connection refused')) or [Errno 113] No route to host khi b·∫°n d√πng localhost:4533, 127.0.0.1:4533:  # master.conf in current directory locustfile = ./locustfile.py headless = true master = true expect-workers = 2 host = http://navidrome:4533 users = 50 spawn-rate = 5 run-time = 1m html = /mnt/locust/html-report/report.html =\u0026gt; file n√†y ƒëang setting s·ªë worker = 2 v√† l∆∞·ª£ng user gi·∫£ l·∫≠p l√† 50. C·∫ßn ph√¢n bi·ªát r√µ, 50 user n√†y ko ph·∫£i l√† s·ªë account t·∫°o ra ƒë√¢u nh√©. 50 users n√†y gi·ªëng nh∆∞ 50 tab kh√°c nhau ƒë∆∞·ª£c b·∫≠t l√™n, th·ª±c hi·ªán ri√™ng c√°c sequence of task trong locustfile.py\n t·∫°o s·∫µn file locust/locustfile.py, ƒë√¢y l√† file quan tr·ªçng m√† m√¨nh mu·ªën gi·∫£i th√≠ch trong ph·∫ßn sau  3.1.3. Run script and explain the log cd locust docker-compose up --scale worker=2 ƒê·∫£m b·∫£o r·∫±ng script locust/locustfile.py ch·∫°y ko c√≥ l·ªói g√¨, b·∫°n c√≥ th·ªÉ download report.html v·ªÅ v√† m·ªü tr√™n chrome ƒë·ªÉ xem:\nGi·∫£i th√≠ch script locust/locustfile.py:\n  decorate @events.init.add_listener s·∫Ω gi√∫p on_locust_init run ƒë·∫ßu ti√™n tr√™n c√°c node v√† ch·ªâ run 1 l·∫ßn. N·∫øu ƒëang run tr√™n Worker, ta s·∫Ω ƒëƒÉng k√Ω (register) message r·∫±ng n·∫øu nh·∫≠n ƒë∆∞·ª£c message type test_users th√¨ h√£y run h√†m setup_test_users, c√≤n n·∫øu nh·∫≠n ƒë∆∞·ª£c message type unique_token th√¨ run h√†m setup_unique_token. N·∫øu ƒëang run tr√™n Master ta s·∫Ω ƒëƒÉng k√Ω message r·∫±ng n·∫øu nh·∫≠n ƒë∆∞·ª£c message acknowledge_users th√¨ h√£y run h√†m on_acknowledge.\n  Sau ƒë√≥ h√†m c√≥ decorate @events.test_start.add_listener s·∫Ω run ƒë·ªÉ l·∫•y admin token n·∫øu ƒëang l√† Master node. Ti·∫øp, n√≥ chia ƒë·ªÅu c√°c test account cho t·ª´ng Worker (ch·ªó n√†y c·ªông tr·ª´ nh√¢n chia h∆°i r·ªëi b·∫°n n√™n ƒë·ªçc k·ªπ). Cu·ªëi c√πng n√≥ g·ª≠i data (c√°c account v√† token) t·ªõi t·ª´ng Worker b·∫±ng h√†m environment.runner.send_message. ƒê√¢y ch√≠nh l√† chi·ªÅu giao ti·∫øp t·ª´ Master ƒë·∫øn Workers.\n  Ti·∫øp theo, h√†m setup_test_users v√† setup_unique_token s·∫Ω ƒë∆∞·ª£c run. Ch·ªó h√†m setup_test_users b·∫°n s·∫Ω th·∫•y khi nh·∫≠n ƒë∆∞·ª£c list account c·ªßa m√¨nh xong, Worker s·∫Ω run h√†m environment.runner.send_message ƒë·ªÉ g·ª≠i t·ªõi Master l·ªùi c·∫£m ∆°n c·ªßa m√¨nh. Khi Master nh·∫≠n ƒë∆∞·ª£c n√≥ s·∫Ω in ra l·ªùi c·∫£m ∆°n c·ªßa Worker. ƒê√¢y ch√≠nh l√† chi·ªÅu giao ti·∫øp t·ª´ Worker ƒë·∫øn Master.\n  Cu·ªôc n√≥i chuy·ªán k·∫øt th√∫c v√† h√†m on_start(self) s·∫Ω m·ªü ƒë·∫ßu cho chu·ªói sequence of tasks. N√≥ l·∫•y token ra t·ª´ list token_shared g√°n v√†o 1 bi·∫øn global token ƒë·ªÉ c√°c task kh√°c c√≥ th·ªÉ s·ª≠ d·ª•ng. Sau ƒë√≥, n√≥ pop() d·∫ßn d·∫ßn t·ª´ng account trong list testuser_filtered ra ƒë·ªÉ login. ƒêi·ªÅu ki·ªán if len(testuser_filtered) \u0026gt; 0: s·∫Ω ƒë·∫£m b·∫£o r·∫±ng script s·∫Ω ko pop from empty list, nh∆∞ v·∫≠y m·∫∑c d√π h√†m on_start(self) run nhi·ªÅu l·∫ßn nh∆∞ng m·ªói test account ch·ªâ login 1 l·∫ßn m√† th√¥i.\n  H√†m getAllSongs, ch√∫ √Ω ch·ªó response.failure(\u0026quot;getAllSongs: Got wrong response\u0026quot;), l√† 1 v√≠ d·ª• n·∫øu b·∫°n mu·ªën ki·ªÉm tra response tr·∫£ v·ªÅ.\n  H√†m getAllAlbums ch·ªó elif response.elapsed.total_seconds() \u0026gt; 0.04:, l√† 1 v√≠ d·ª• n·∫øu b·∫°n mu·ªën assert response time ph·∫£i ·ªü m·ª©c n√†o ƒë√≥.\n  H√†m createDeletePlaylist ch·ªó name=\u0026quot;/api/playlist/[id]\u0026quot;, l√† 1 v√≠ d·ª• ƒë·ªÉ b·∫°n nh√≥m c√°c request l·∫°i trong Final Report, tr√°nh vi·ªác l√†m r·ªëi report b·ªüi qu√° nhi·ªÅu request c√πng lo·∫°i.\n  Trong document c·ªßa Locust c√≥ n√≥i v·ªÅ recommended structure folder nh∆∞ sau: https://docs.locust.io/en/stable/writing-a-locustfile.html#how-to-structure-your-test-code\n  LOG kh√° d√†i nh∆∞ng m√¨nh ch·ªâ l·∫•y ph·∫ßn ƒë·∫ßu th√¥i, xem log n√†y c√πng v·ªõi ƒë·ªçc code s·∫Ω hi·ªÉu ƒë∆∞·ª£c th·ª© t·ª± c√°c h√†m ƒë∆∞·ª£c g·ªçi trong Locust:\n $ docker-compose up --scale worker=2 [+] Running 3/0 ‚†ø Container locust-worker-1 Created 0.0s ‚†ø Container locust-worker-2 Created 0.0s ‚†ø Container locust-master-1 Created 0.0s Attaching to locust-master-1, locust-worker-1, locust-worker-2 locust-master-1 | Initializing locust message types and listeners locust-master-1 | [2022-10-15 08:14:43,657] 6335de046f29/INFO/root: Waiting for workers to be ready, 0 of 2 connected locust-master-1 | Type Name # reqs # fails | Avg Min Max Med | req/s failures/s locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | Aggregated 0 0(0.00%) | 0 0 0 0 | 0.00 0.00 locust-master-1 | locust-master-1 | [2022-10-15 08:14:43,666] 6335de046f29/INFO/locust.runners: Worker 5201f3c32fcb_57d185c2045540ffbeeae3fc9d30e1ed (index 0) reported as ready. 1 workers connected. locust-worker-2 | Initializing locust message types and listeners locust-worker-2 | [2022-10-15 08:14:43,668] 5201f3c32fcb/INFO/locust.main: Starting Locust 2.12.1 locust-master-1 | [2022-10-15 08:14:43,911] 6335de046f29/INFO/locust.runners: Worker 29adedeb6842_18225c26a017418e85458e5356309bba (index 1) reported as ready. 2 workers connected. locust-worker-1 | Initializing locust message types and listeners locust-worker-1 | [2022-10-15 08:14:43,913] 29adedeb6842/INFO/locust.main: Starting Locust 2.12.1 locust-master-1 | [2022-10-15 08:14:44,659] 6335de046f29/INFO/locust.main: Run time limit set to 60 seconds locust-master-1 | [2022-10-15 08:14:44,660] 6335de046f29/INFO/locust.main: Starting Locust 2.12.1 locust-master-1 | [2022-10-15 08:14:44,661] 6335de046f29/INFO/locust.runners: Sending spawn jobs of 50 users at 5.00 spawn rate to 2 ready workers locust-master-1 | MASTER say - Trying to get token on this node locust-master-1 | MASTER say - Number of user to be divided between each worker is 2 locust-master-1 | sending this data [{'account': ('test1', 'test1')}, {'account': ('test2', 'test2')}] to this 5201f3c32fcb_57d185c2045540ffbeeae3fc9d30e1ed locust-master-1 | sending this token [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] to this 5201f3c32fcb_57d185c2045540ffbeeae3fc9d30e1ed locust-master-1 | sending this data [{'account': ('test3', 'test3')}, {'account': ('test4', 'test4')}] to this 29adedeb6842_18225c26a017418e85458e5356309bba locust-worker-2 | Worker say - setup_test_users - msg.data: [{'account': ['test1', 'test1']}, {'account': ['test2', 'test2']}] locust-worker-2 | Worker say - Here is accounts I received: [['test1', 'test1'], ['test2', 'test2']] locust-master-1 | sending this token [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] to this 29adedeb6842_18225c26a017418e85458e5356309bba locust-worker-2 | Worker say - setup_unique_token - msg.data: [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] locust-worker-2 | Worker say - Here is token I received: ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | Worker say - setup_test_users - msg.data: [{'account': ['test3', 'test3']}, {'account': ['test4', 'test4']}] locust-worker-1 | Worker say - Here is accounts I received: [['test3', 'test3'], ['test4', 'test4']] locust-master-1 | MASTER say - Here is message from Woker: Thanks for the 2 users! locust-worker-1 | Worker say - setup_unique_token - msg.data: [{'admin_token': 'eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'}] locust-worker-1 | Worker say - Here is token I received: ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-master-1 | MASTER say - Here is message from Woker: Thanks for the 2 users! locust-worker-2 | on_start running - testuser_filtered = [['test1', 'test1'], ['test2', 'test2']] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [['test3', 'test3'], ['test4', 'test4']] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [['test1', 'test1']] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [['test3', 'test3']] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-master-1 | Type Name # reqs # fails | Avg Min Max Med | req/s failures/s locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | Aggregated 0 0(0.00%) | 0 0 0 0 | 0.00 0.00 locust-master-1 | locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-2 | on_start running - testuser_filtered = [] locust-worker-2 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-worker-1 | on_start running - testuser_filtered = [] locust-worker-1 | on_start running - token_shared = ['eyJhbGciOiJIUzI1NiIsIngyZDBjYjFhLThjMzctNylKjgAeTigH7fitH0w'] locust-master-1 | Type Name # reqs # fails | Avg Min Max Med | req/s failures/s locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | GET / 10 0(0.00%) | 33 19 74 30 | 0.00 0.00 locust-master-1 | GET /api/song?_end=15\u0026amp;_order=ASC\u0026amp;_sort=title\u0026amp;_start=0 1 0(0.00%) | 18 18 18 18 | 0.00 0.00 locust-master-1 | POST /auth/login 4 0(0.00%) | 60 56 63 60 | 0.00 0.00 locust-master-1 | --------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|----------- locust-master-1 | Aggregated 15 0(0.00%) | 40 18 74 37 | 0.00 0.00 locust-master-1 | locust-worker-1 | on_start running - testuser_filtered = [] 3.2. Setup Distributed Locust system on Azure Ch√∫ng ta s·∫Ω d·ª±ng m√¥i tr∆∞·ªùng tr√™n h·ªá th·ªëng Azure ACI:\nhttps://github.com/hoangmnsd/locust-lab/blob/master/locust-az-aci-structure.jpg\nƒêi·ªÉm m·∫°nh c·ªßa h·ªá th·ªëng n√†y l√† b·∫°n c√≥ th·ªÉ scale Worker l√™n s·ªë l∆∞·ª£ng r·∫•t nhi·ªÅu, mi·ªÖn l√† b·∫°n c√≥ ƒë·ªß ti·ªÅn üòÇ\nƒê·∫ßu ti√™n l√† t·∫°o 1 resource group: locust-rg\nS·ª≠a file azure/azuredeploy.parameters.json:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;instances\u0026#34;: { \u0026#34;value\u0026#34;: 1 }, \u0026#34;prefix\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;hahihohe\u0026#34; } } } N·∫øu c·∫ßn s·ªë l∆∞·ª£ng Worker l√† 5 th√¨ s·ª≠a value c·ªßa instances th√†nh 5\nOpen Cloudshell to run these commands:\nexport resourceGroup=locust-rg cd locust-lab/ templateFile=\u0026#34;azure/azuredeploy.json\u0026#34; paramsFile=\u0026#34;azure/azuredeploy.parameters.json\u0026#34; az deployment group create --name \u0026#34;locust\u0026#34; --resource-group $resourceGroup --template-file $templateFile --parameters $paramsFile Go to Storage Account -\u0026gt; File Shares tab -\u0026gt; scripts -\u0026gt; upload files:\n master.conf (l·∫•y t·ª´ locust-lab/locust/master.conf) locustfile.py (l·∫•y t·ª´ locust-lab/locust/locustfile.py) create directory html-report  Edit file t∆∞∆°ng ·ª©ng:\n Click v√†o ... t∆∞∆°ng ·ª©ng m·ªói file n·∫øu c·∫ßn. File master.conf:  s·ª≠a expect-workers t√πy theo s·ªë l∆∞·ª£ng instances trong azure/azuredeploy.parameters.json hi·ªán c√≥ s·ª≠a host tr·ªè ƒë·∫øn navidrome webapp target m√† b·∫°n ƒë√£ build    Restart containers:\naz container restart --name hmmnsd1-master --resource-group $resourceGroup \u0026amp;\u0026amp; \\ az container restart --name hmmnsd1-worker-0 --resource-group $resourceGroup Ho·∫∑c trong Output c·ªßa ARM sau khi deploy xong s·∫Ω c√≥ command ƒë·ªÉ restart containers\nSau khi restart xong ch·ªù Container run test xong th√¨ report s·∫Ω xu·∫•t ra folder html-report trong Storage account. Download file ƒë√≥ v·ªÅ m·ªü trong Chrome l√† xem ƒë∆∞·ª£c.\nC√≥ th·ªÉ c√≥ 1 c√°ch kh√°c l√† deploy tr√™n K8s, xu·∫•t report ra CSV file, ƒë·∫©y v√†o Prometheus b·∫±ng 1 tool (LocustExporter)[https://github.com/ContainerSolutions/locust_exporter], r·ªìi hi·ªÉn th·ªã tr√™n Grafana.\nXong r·ªìi th√¨ c√≥ th·ªÉ x√≥a Resource Group ƒëi ƒë∆∞·ª£c r·ªìi.\nCh√∫ √Ω v·ªÅ gi√° c·ªßa ACI v√† Storage account: https://azure.microsoft.com/en-us/pricing/details/container-instances/\nCREDIT Rakesh\u0026rsquo;s comment on: https://www.softwaretestinghelp.com/what-is-performance-testing-load-testing-stress-testing/\nhttps://docs.locust.io/en/1.5.2/\nhttps://github.com/tavishigupta/locust-blog/tree/master/deployment\nhttps://github.com/locustio/locust/wiki/FAQ#increase-my-request-raterps\nhttps://medium.com/swlh/load-testing-with-locust-3e74349f9cbf\nhttps://docs.locust.io/en/stable/api.html#locust.wait_time.constant_throughput\nhttps://github.com/locustio/locust/issues/646#issuecomment-360405844\nhttps://www.blazemeter.com/blog/locust-python\nhttps://www.blazemeter.com/blog/locust-multiple-users\nhttps://github.com/locustio/locust/wiki/Installation#increasing-maximum-number-of-open-files-limit\nhttps://stackoverflow.com/questions/71470516/how-do-i-set-a-specify-rps-in-locust-like-500-requests-per-seconds\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\nhttps://docs.locust.io/en/stable/writing-a-locustfile.html#how-to-structure-your-test-code\n1 s·ªë example script c·ªßa c·ªông ƒë·ªìng ƒë√≥ng g√≥p: https://github.com/locustio/locust/blob/master/examples\nhttps://docs.locust.io/en/stable/running-distributed.html#communicating-across-nodes\nhttps://github.com/locustio/locust/blob/master/examples/test_data_management.py\nhttps://github.com/locustio/locust/blob/master/examples/custom_messages.py\nhttps://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects\n1 b√†i kh√° hay v·ªÅ Locust tr√™n k8s, s·ª≠ d·ª•ng distributed mode, gi·∫£i th√≠ch c∆° ch·∫ø share msg between workers, v√≠ d·ª• v·ªÅ use-case c·∫ßn d√πng n√≥, c√≥ th·ªÉ k·∫øt h·ª£p locust exporter ƒë·ªÉ export report csv ra prometheus r·ªìi show tr√™n grafana:\nhttps://medium.com/dkatalis/distributed-load-testing-on-k8s-using-locust-6ee4ed6c7ca\n","href":"/posts/encrypt-performance-tests-note-locust/","title":"Performance Tests with Locust"},{"content":"","href":"/tags/test/","title":"Test"},{"content":"1. Story ƒê√¢y l√† note ƒë·ªÉ x·ª≠ l√Ω khi b·∫°n th·∫•y Server c·ªßa b·∫°n c√≥ v·∫ª nh∆∞ ko th·ªÉ k·∫øt n·ªëi internet ƒë∆∞·ª£c\nC√≥ th·ªÉ b·∫°n s·∫Ω nh√¨n th·∫•y th√¥ng b√°o n√†y ngay khi ssh v√†o server:\nFailed to connect to https://changelogs.ubuntu.com/meta-release-lts. Check you Internet connection or proxy setting Ho·∫∑c khi b·∫°n run 1 v√†i command ƒë∆°n gi·∫£n nh∆∞ sudo apt update, m√†n h√¨nh s·∫Ω d·ª´ng l·∫°i ·ªü 0% v√† cu·ªëi c√πng b√°o l·ªói:\nConnection timeout. Could not connect to nginx.org:443 (xxx) V·∫≠y th√¨ c·∫ßn test m·∫•y comment sau: Try sudo ping 8.8.8.8 inside your distro (Ubuntu).\n=\u0026gt; N·∫øu ko th·∫•y k·∫øt qu·∫£ g√¨, ch·ª©ng t·ªè m√°y b·∫°n ko th·ªÉ k·∫øt n·ªëi internet dc, li√™n quan ƒë·∫øn network r·ªìi.\n=\u0026gt; N·∫øu c√≥ k·∫øt qu·∫£ b√¨nh th∆∞·ªùng, ch·ª©ng t·ªè m√°y b·∫°n v·∫´n c√≥ k·∫øt n·ªëi internet v√† do Ubuntu b·ªã l·ªói n√™n b·∫°n c·∫ßn test ti·∫øp c√°c domain kh√°c: To check dns try sudo ping archive.ubuntu.com\nN·∫øu ch·ªâ archive.ubuntu.com l√† ko ping ƒë∆∞·ª£c, try changing your apt sources to some other mirror. There is plenty of info on how to do this in the web.\nTrong tr∆∞·ªùng h·ª£p c·ªßa m√¨nh, nguy√™n nh√¢n l√† do VM ƒë√≥ ƒëang l√† backend c·ªßa 1 Azure Private Loadbalancer. Ko c√≥ public IP. D·∫´n ƒë·∫øn VM ƒë√≥ ko th·ªÉ access internet ƒë∆∞·ª£c.\nVi·ªác n√†y d·∫´n ƒë·∫øn 1 s·ªë service trong VM ƒë√≥ m√† c·∫ßn connect ƒë·∫øn Internet s·∫Ω b·ªã l·ªói timeout.\nƒê·ªÉ x·ª≠ l√Ω, 1 s·ªë n∆°i c√≥ ki·∫øn tr√∫c ki·ªÉu n√†y:\nVM service n·∫±m sau 1 VM Nginx -\u0026gt; VM Nginx s·∫Ω l√† backend c·ªßa Azure private Loadbalancer.\nƒêi qua 2 l·ªõp nh∆∞ v·∫≠y th√¨ VM service s·∫Ω c√≥ th·ªÉ reach Internet b√¨nh th∆∞·ªùng.\nTuy nhi√™n ƒë√¢y l·∫°i l√† c√°ch t·ªá nh·∫•t trong s·ªë c√°c c√°ch thi·∫øt k·∫ø m√† Microsoft recommend:\nhttps://docs.microsoft.com/en-gb/azure/load-balancer/load-balancer-outbound-connections\n2. Recommended Trong t√†i li·ªáu tr√™n m√¥ t·∫£ 4 c√°ch:\n  C√°ch 1 l√† v·ªõi ƒëi·ªÅu ki·ªán LoadBalancer b·∫°n ƒëang d√πng c√≥ public IP g·∫Øn v√†o (public LoadBalacner). Khi ƒë√≥ ·ªü ph·∫ßn setting b·∫°n s·∫Ω nh√¨n th·∫•y tab Outbound rules ·ªü b√™n tr√°i. N·∫øu ko c√≥ public IP g·∫Øn v√†o LB b·∫°n s·∫Ω ko th·∫•y ƒë√¢u. C√°ch n√†y ƒë∆∞·ª£c ƒë√°nh gi√° l√† OK nh∆∞ng ko scale ƒë∆∞·ª£c. V√† th·ª±c t·∫ø th√¨ v·ªõi 1 s·ªë LB b·∫°n s·∫Ω ko mu·ªën n√≥ b·ªã ƒë∆∞·ª£c public ra, nh∆∞ v·∫≠y ko ƒë·∫£m b·∫£o security l·∫Øm.\n  C√°ch 2 l√† Best recommend: s·ª≠ d·ª•ng NAT Gateway, Azure NAT Gateway c≈©ng kh√° r·∫ª, kh√¥ng nh∆∞ AWS NAT Gateway: https://azure.microsoft.com/en-us/pricing/details/virtual-network/#pricing ($0.045 per hour, $0.045 per GB) 1 th√°ng 730 hours li√™n t·ª•c n·∫øu outbound traffic l√† 1TB th√¨ t·ªën kho·∫£ng 80 $. C√°ch n√†y kh√° OK v√¨ n√≥ gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ c·ªßa c√°ch 1, LB c·ªßa b·∫°n v·∫´n l√† private LB ko expose ra ngo√†i.\n  C√°ch 3 l√† G·∫Øn 1 public IP v√†o VM lu√¥n. C√°ch n√†y nghe ƒë√£ th·∫•y nghi·ªáp d∆∞ r·ªìi.\n  C√°ch 4 ch√≠nh l√† c√°ch m√† m√¨nh m√¥ t·∫£ ·ªü ph·∫ßn 1\n  1 ki·ªÉu thi·∫øt k·∫ø m√† MS recommend:\nhttps://docs.microsoft.com/en-us/azure/architecture/networking/guide/well-architected-network-address-translation-gateway  C√≥ 2 Loadbalancer, 1 l√† external/public LB, 2 l√† internal/private LB. Internal LB ƒë·ªÉ c√°c service g·ªçi nhau n·ªôi b·ªô qua IP c·ªßa internal LB. External LB ƒë·ªÉ serve c√°c inbound traffic (tr∆∞·ªùng h·ª£p m√† b·∫°n mu·ªën expose service c·ªßa b·∫°n ra ngo√†i). C√≤n NAT Gateway l√† ƒë·ªÉ serve c√°c outbound traffic mu·ªën t·ª´ VM ƒëi ra internet (V√≠ d·ª• nh∆∞ 1 VM trong LB mu·ªën call ƒë·∫øn google api ch·∫≥ng h·∫°n).  1 v√†i comment ch√∫ √Ω:\n The answer is contained in this LB outbound connection MS doc page. Outbound connectivity, for vnets with a standard* SKU load balancer does not exist automatically: the VMs requiring outbound access must either be given an external facing IP address, or be put into a backend pool of the load balancer AND the load balancer must have an outbound rule defined for the required outbound traffic on that backend pool (even if the rule merely accepts the default options). For a basic SKU load balancer this isn\u0026rsquo;t necessary.\nhttps://serverfault.com/a/1017065/589181\n CREDIT https://github.com/MicrosoftDocs/WSL/issues/937\nhttps://docs.microsoft.com/en-gb/azure/load-balancer/load-balancer-outbound-connections\n","href":"/bk/encrypt-when-your-vm-cannot-reach-internet-and-stry-abt-load-balancer/","title":"When your VM failed to reach Internet and story about Azure Load Balancer"},{"content":"1. Story ƒê√¢y l√† note ƒë·ªÉ x·ª≠ l√Ω khi b·∫°n th·∫•y Server c·ªßa b·∫°n c√≥ v·∫ª nh∆∞ ko th·ªÉ k·∫øt n·ªëi internet ƒë∆∞·ª£c\nC√≥ th·ªÉ b·∫°n s·∫Ω nh√¨n th·∫•y th√¥ng b√°o n√†y ngay khi ssh v√†o server:\nFailed to connect to https://changelogs.ubuntu.com/meta-release-lts. Check you Internet connection or proxy setting Ho·∫∑c khi b·∫°n run 1 v√†i command ƒë∆°n gi·∫£n nh∆∞ sudo apt update, m√†n h√¨nh s·∫Ω d·ª´ng l·∫°i ·ªü 0% v√† cu·ªëi c√πng b√°o l·ªói:\nConnection timeout. Could not connect to nginx.org:443 (xxx) V·∫≠y th√¨ c·∫ßn test m·∫•y comment sau: Try sudo ping 8.8.8.8 inside your distro (Ubuntu).\n=\u0026gt; N·∫øu ko th·∫•y k·∫øt qu·∫£ g√¨, ch·ª©ng t·ªè m√°y b·∫°n ko th·ªÉ k·∫øt n·ªëi internet dc, li√™n quan ƒë·∫øn network r·ªìi.\n=\u0026gt; N·∫øu c√≥ k·∫øt qu·∫£ b√¨nh th∆∞·ªùng, ch·ª©ng t·ªè m√°y b·∫°n v·∫´n c√≥ k·∫øt n·ªëi internet v√† do Ubuntu b·ªã l·ªói n√™n b·∫°n c·∫ßn test ti·∫øp c√°c domain kh√°c: To check dns try sudo ping archive.ubuntu.com\nN·∫øu ch·ªâ archive.ubuntu.com l√† ko ping ƒë∆∞·ª£c, try changing your apt sources to some other mirror. There is plenty of info on how to do this in the web.\nTrong tr∆∞·ªùng h·ª£p c·ªßa m√¨nh, nguy√™n nh√¢n l√† do VM ƒë√≥ ƒëang l√† backend c·ªßa 1 Azure Private Loadbalancer. Ko c√≥ public IP. D·∫´n ƒë·∫øn VM ƒë√≥ ko th·ªÉ access internet ƒë∆∞·ª£c.\nVi·ªác n√†y d·∫´n ƒë·∫øn 1 s·ªë service trong VM ƒë√≥ m√† c·∫ßn connect ƒë·∫øn Internet s·∫Ω b·ªã l·ªói timeout.\nƒê·ªÉ x·ª≠ l√Ω, 1 s·ªë n∆°i c√≥ ki·∫øn tr√∫c ki·ªÉu n√†y:\nVM service n·∫±m sau 1 VM Nginx -\u0026gt; VM Nginx s·∫Ω l√† backend c·ªßa Azure private Loadbalancer.\nƒêi qua 2 l·ªõp nh∆∞ v·∫≠y th√¨ VM service s·∫Ω c√≥ th·ªÉ reach Internet b√¨nh th∆∞·ªùng.\nTuy nhi√™n ƒë√¢y l·∫°i l√† c√°ch t·ªá nh·∫•t trong s·ªë c√°c c√°ch thi·∫øt k·∫ø m√† Microsoft recommend:\nhttps://docs.microsoft.com/en-gb/azure/load-balancer/load-balancer-outbound-connections\n2. Recommended Trong t√†i li·ªáu tr√™n m√¥ t·∫£ 4 c√°ch:\n  C√°ch 1 l√† v·ªõi ƒëi·ªÅu ki·ªán LoadBalancer b·∫°n ƒëang d√πng c√≥ public IP g·∫Øn v√†o (public LoadBalacner). Khi ƒë√≥ ·ªü ph·∫ßn setting b·∫°n s·∫Ω nh√¨n th·∫•y tab Outbound rules ·ªü b√™n tr√°i. N·∫øu ko c√≥ public IP g·∫Øn v√†o LB b·∫°n s·∫Ω ko th·∫•y ƒë√¢u. C√°ch n√†y ƒë∆∞·ª£c ƒë√°nh gi√° l√† OK nh∆∞ng ko scale ƒë∆∞·ª£c. V√† th·ª±c t·∫ø th√¨ v·ªõi 1 s·ªë LB b·∫°n s·∫Ω ko mu·ªën n√≥ b·ªã ƒë∆∞·ª£c public ra, nh∆∞ v·∫≠y ko ƒë·∫£m b·∫£o security l·∫Øm.\n  C√°ch 2 l√† Best recommend: s·ª≠ d·ª•ng NAT Gateway, Azure NAT Gateway c≈©ng kh√° r·∫ª, kh√¥ng nh∆∞ AWS NAT Gateway: https://azure.microsoft.com/en-us/pricing/details/virtual-network/#pricing ($0.045 per hour, $0.045 per GB) 1 th√°ng 730 hours li√™n t·ª•c n·∫øu outbound traffic l√† 1TB th√¨ t·ªën kho·∫£ng 80 $. C√°ch n√†y kh√° OK v√¨ n√≥ gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ c·ªßa c√°ch 1, LB c·ªßa b·∫°n v·∫´n l√† private LB ko expose ra ngo√†i.\n  C√°ch 3 l√† G·∫Øn 1 public IP v√†o VM lu√¥n. C√°ch n√†y nghe ƒë√£ th·∫•y nghi·ªáp d∆∞ r·ªìi.\n  C√°ch 4 ch√≠nh l√† c√°ch m√† m√¨nh m√¥ t·∫£ ·ªü ph·∫ßn 1\n  1 ki·ªÉu thi·∫øt k·∫ø m√† MS recommend:\nhttps://docs.microsoft.com/en-us/azure/architecture/networking/guide/well-architected-network-address-translation-gateway  C√≥ 2 Loadbalancer, 1 l√† external/public LB, 2 l√† internal/private LB. Internal LB ƒë·ªÉ c√°c service g·ªçi nhau n·ªôi b·ªô qua IP c·ªßa internal LB. External LB ƒë·ªÉ serve c√°c inbound traffic (tr∆∞·ªùng h·ª£p m√† b·∫°n mu·ªën expose service c·ªßa b·∫°n ra ngo√†i). C√≤n NAT Gateway l√† ƒë·ªÉ serve c√°c outbound traffic mu·ªën t·ª´ VM ƒëi ra internet (V√≠ d·ª• nh∆∞ 1 VM trong LB mu·ªën call ƒë·∫øn google api ch·∫≥ng h·∫°n).  1 v√†i comment ch√∫ √Ω:\n The answer is contained in this LB outbound connection MS doc page. Outbound connectivity, for vnets with a standard* SKU load balancer does not exist automatically: the VMs requiring outbound access must either be given an external facing IP address, or be put into a backend pool of the load balancer AND the load balancer must have an outbound rule defined for the required outbound traffic on that backend pool (even if the rule merely accepts the default options). For a basic SKU load balancer this isn\u0026rsquo;t necessary.\nhttps://serverfault.com/a/1017065/589181\n CREDIT https://github.com/MicrosoftDocs/WSL/issues/937\nhttps://docs.microsoft.com/en-gb/azure/load-balancer/load-balancer-outbound-connections\n","href":"/posts/encrypt-when-your-vm-cannot-reach-internet-and-stry-abt-load-balancer/","title":"When your VM failed to reach Internet and story about Azure Load Balancer"},{"content":"","href":"/tags/ansible/","title":"Ansible"},{"content":"This is just my notes when playaround with ansible playbook, not a tutorial\n1. Story 1 Khi vi·∫øt ansible playbook c√≥ khi b·∫°n ph·∫£i x·ª≠ l√Ω nhi·ªÅu items gi·ªëng nhau, ƒë√≥ l√† b·∫°n ph·∫£i d√πng loop, with_items\nƒê·ªÉ tr√°nh vi·ªác t·∫°o qu√° nhi·ªÅu bi·∫øn, set_facts nhi·ªÅu trong playbook, b·∫°n n√™n ƒë∆∞a h·∫øt c√°c item ra file var\nKhi c√≥ th√™m jinja template v√†o th√¨ c≈©ng c·∫ßn bi·∫øt c√°ch ƒë·ªÉ ƒë∆∞a var v√†o\n1.1. Practice V√≠ d·ª• file ./vars/group_vars.yml c·ªßa m√¨nh nh∆∞ n√†y:\n--- services_list: - name: svc_a svc_name: \u0026#34;svcA\u0026#34; svc_dir: \u0026#34;/opt/svcA\u0026#34; svc_user: \u0026#34;a\u0026#34; - name: svc_b svc_name: \u0026#34;svcB\u0026#34; svc_dir: \u0026#34;/opt/B\u0026#34; svc_user: \u0026#34;b\u0026#34; systemd_template_file: ./systemd_template_file.j2 c√°c variables ƒë∆∞·ª£c m√¨nh t·ªï ch·ª©c th√†nh 1 list services_list\nfile ./systemd_template_file.j2:\n[Unit] Description={{ service_name }} After=syslog.target [Service] User={{ service_user }} ExecStart={{ service_dir }}/bin/{{ service_name }} start ExecStop={{ service_dir }}/bin/{{ service_name }} stop [Install] WantedBy=multi-user.target file ./playbook.yml s·∫Ω nh∆∞ n√†y:\n--- - name: Test hosts: myhosts become: true gather_facts: true vars_files: - \u0026#34;./vars/group_vars.yml\u0026#34; tasks: - name: Create serviced service file template: src: \u0026#34;{{ systemd_template_file }}\u0026#34; dest: \u0026#34;/etc/systemd/system/{{ item.svc_name }}.service\u0026#34; owner: root group: root mode: 0644 with_items: \u0026#34;{{ services_list }}\u0026#34; vars: service_name: \u0026#34;{{ item.svc_name }}\u0026#34; service_dir: \u0026#34;{{ item.svc_dir }}\u0026#34; service_user: \u0026#34;{{ item.svc_user }}\u0026#34; Nghƒ©a l√† playbook n√†y ch·ªâ c√≥ 1 task l√† t·∫°o ra l·∫ßn l∆∞·ª£t c√°c file /etc/systemd/system/svc_a.service v√† /etc/systemd/system/svc_b.service.\nV·ªõi n·ªôi dung file ƒë∆∞·ª£c l·∫•y t·ª´ Jinja template ./systemd_template_file.j2, variables ƒë∆∞·ª£c set trong task\nƒê·ªÉ √Ω ƒëo·∫°n n√†y:\nvars: service_name: \u0026#34;{{ item.svc_name }}\u0026#34; service_dir: \u0026#34;{{ item.svc_dir }}\u0026#34; service_user: \u0026#34;{{ item.svc_user }}\u0026#34; ƒê√¢y l√† c√°c variables s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng trong content file ./systemd_template_file.j2\nVi·ªác s·ª≠ d·ª•ng ƒëc c√°c bi·∫øn th·∫ø n√†y gi√∫p cho code Ansbile c·ªßa b·∫°n ng·∫Øn l·∫°i, tuy nhi√™n s·∫Ω kh√≥ hi·ªÉu h∆°n v√¨ nh√¨n ƒë√¢u c≈©ng th·∫•y bi·∫øn ü§£\n2. Story 2 S·∫Ω c√≥ l√∫c b·∫°n mu·ªën ignore 1 l·ªói c·ª• th·ªÉ n√†o ƒë√≥ trong qu√° tr√¨nh run Ansible Playbook\nV√≠ d·ª• b·∫°n c√≥ 1 task m√† thi tho·∫£ng s·∫Ω b·ªã l·ªói (c√≥ th·ªÉ b·ªüi v√¨ b·∫°n run n√≥ nhi·ªÅu l·∫ßn, l·ªói m·∫°ng, etc..) v√† tr·∫£ v·ªÅ message l·ªói l√†: Oh my god! Timeout!.\nB·∫°n mu·ªën ignore c√°i l·ªói Oh my god! Timeout! ƒë√≥.\nC√°ch l√†m:\n# task n√†y tr·∫£ v·ªÅ l·ªói v√† error_code kh√°c 0, b·∫°n register n√≥ v√† ignore n√≥ - name: test shell: echo error; exit 123 register: out ignore_errors: yes # b∆∞·ªõc ti·∫øp b·∫°n define th·∫ø n√†o l√† l·ªói,  # Task tr√™n ch·ªâ l·ªói khi return_code (rc) kh√°c 0 v√† trong stderr ko c√≥ chu·ªói `Oh my god! Timeout!` # ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu task tr√™n tr·∫£ v·ªÅ rc kh√°c 0 v√† trong stderr C√ì chu·ªói `Oh my god! Timeout!` th√¨ s·∫Ω b·ªã ignore - fail: msg=\u0026#34;{{ out.stderr }}\u0026#34; when: \u0026#34;out.rc != 0 and \u0026#39;Oh my god! Timeout!\u0026#39; not in out.stderr\u0026#34; CREDIT https://unix.stackexchange.com/a/355584\n","href":"/bk/encrypt-note-set-var-list-ansible/","title":"Not a tutorial: Set Variable List in Playbook; Ignore errors (Ansible)"},{"content":"This is just my notes when playaround with ansible playbook, not a tutorial\n1. Story 1 Khi vi·∫øt ansible playbook c√≥ khi b·∫°n ph·∫£i x·ª≠ l√Ω nhi·ªÅu items gi·ªëng nhau, ƒë√≥ l√† b·∫°n ph·∫£i d√πng loop, with_items\nƒê·ªÉ tr√°nh vi·ªác t·∫°o qu√° nhi·ªÅu bi·∫øn, set_facts nhi·ªÅu trong playbook, b·∫°n n√™n ƒë∆∞a h·∫øt c√°c item ra file var\nKhi c√≥ th√™m jinja template v√†o th√¨ c≈©ng c·∫ßn bi·∫øt c√°ch ƒë·ªÉ ƒë∆∞a var v√†o\n1.1. Practice V√≠ d·ª• file ./vars/group_vars.yml c·ªßa m√¨nh nh∆∞ n√†y:\n--- services_list: - name: svc_a svc_name: \u0026#34;svcA\u0026#34; svc_dir: \u0026#34;/opt/svcA\u0026#34; svc_user: \u0026#34;a\u0026#34; - name: svc_b svc_name: \u0026#34;svcB\u0026#34; svc_dir: \u0026#34;/opt/B\u0026#34; svc_user: \u0026#34;b\u0026#34; systemd_template_file: ./systemd_template_file.j2 c√°c variables ƒë∆∞·ª£c m√¨nh t·ªï ch·ª©c th√†nh 1 list services_list\nfile ./systemd_template_file.j2:\n[Unit] Description={{ service_name }} After=syslog.target [Service] User={{ service_user }} ExecStart={{ service_dir }}/bin/{{ service_name }} start ExecStop={{ service_dir }}/bin/{{ service_name }} stop [Install] WantedBy=multi-user.target file ./playbook.yml s·∫Ω nh∆∞ n√†y:\n--- - name: Test hosts: myhosts become: true gather_facts: true vars_files: - \u0026#34;./vars/group_vars.yml\u0026#34; tasks: - name: Create serviced service file template: src: \u0026#34;{{ systemd_template_file }}\u0026#34; dest: \u0026#34;/etc/systemd/system/{{ item.svc_name }}.service\u0026#34; owner: root group: root mode: 0644 with_items: \u0026#34;{{ services_list }}\u0026#34; vars: service_name: \u0026#34;{{ item.svc_name }}\u0026#34; service_dir: \u0026#34;{{ item.svc_dir }}\u0026#34; service_user: \u0026#34;{{ item.svc_user }}\u0026#34; Nghƒ©a l√† playbook n√†y ch·ªâ c√≥ 1 task l√† t·∫°o ra l·∫ßn l∆∞·ª£t c√°c file /etc/systemd/system/svc_a.service v√† /etc/systemd/system/svc_b.service.\nV·ªõi n·ªôi dung file ƒë∆∞·ª£c l·∫•y t·ª´ Jinja template ./systemd_template_file.j2, variables ƒë∆∞·ª£c set trong task\nƒê·ªÉ √Ω ƒëo·∫°n n√†y:\nvars: service_name: \u0026#34;{{ item.svc_name }}\u0026#34; service_dir: \u0026#34;{{ item.svc_dir }}\u0026#34; service_user: \u0026#34;{{ item.svc_user }}\u0026#34; ƒê√¢y l√† c√°c variables s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng trong content file ./systemd_template_file.j2\nVi·ªác s·ª≠ d·ª•ng ƒëc c√°c bi·∫øn th·∫ø n√†y gi√∫p cho code Ansbile c·ªßa b·∫°n ng·∫Øn l·∫°i, tuy nhi√™n s·∫Ω kh√≥ hi·ªÉu h∆°n v√¨ nh√¨n ƒë√¢u c≈©ng th·∫•y bi·∫øn ü§£\n2. Story 2 S·∫Ω c√≥ l√∫c b·∫°n mu·ªën ignore 1 l·ªói c·ª• th·ªÉ n√†o ƒë√≥ trong qu√° tr√¨nh run Ansible Playbook\nV√≠ d·ª• b·∫°n c√≥ 1 task m√† thi tho·∫£ng s·∫Ω b·ªã l·ªói (c√≥ th·ªÉ b·ªüi v√¨ b·∫°n run n√≥ nhi·ªÅu l·∫ßn, l·ªói m·∫°ng, etc..) v√† tr·∫£ v·ªÅ message l·ªói l√†: Oh my god! Timeout!.\nB·∫°n mu·ªën ignore c√°i l·ªói Oh my god! Timeout! ƒë√≥.\nC√°ch l√†m:\n# task n√†y tr·∫£ v·ªÅ l·ªói v√† error_code kh√°c 0, b·∫°n register n√≥ v√† ignore n√≥ - name: test shell: echo error; exit 123 register: out ignore_errors: yes # b∆∞·ªõc ti·∫øp b·∫°n define th·∫ø n√†o l√† l·ªói,  # Task tr√™n ch·ªâ l·ªói khi return_code (rc) kh√°c 0 v√† trong stderr ko c√≥ chu·ªói `Oh my god! Timeout!` # ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu task tr√™n tr·∫£ v·ªÅ rc kh√°c 0 v√† trong stderr C√ì chu·ªói `Oh my god! Timeout!` th√¨ s·∫Ω b·ªã ignore - fail: msg=\u0026#34;{{ out.stderr }}\u0026#34; when: \u0026#34;out.rc != 0 and \u0026#39;Oh my god! Timeout!\u0026#39; not in out.stderr\u0026#34; CREDIT https://unix.stackexchange.com/a/355584\n","href":"/posts/encrypt-note-set-var-list-ansible/","title":"Not a tutorial: Set Variable List in Playbook; Ignore errors (Ansible)"},{"content":"1. Story N·∫øu nh∆∞ nh√† b·∫°n c√≥ 1 chi·∫øc ƒëi·ªÅu h√≤a b√¨nh th∆∞·ªùng (ko ph·∫£i lo·∫°i smart), b·∫°n s·∫Ω ph·∫£i ƒëi·ªÅu khi·ªÉn n√≥ th√¥ng qua 1 c√°i ƒëi·ªÅu khi·ªÉn (remote) ri√™ng.\nR·ªìi c√≥ TV b·∫°n l·∫°i c√≥ 1 remote cho n√≥. N·∫øu c√≥ th√™m 1 chi·∫øc qu·∫°t c√¢y, ch·∫Øc b·∫°n c≈©ng s·∫Ω c√≥ remote d√†nh ri√™ng cho n√≥. R·ªìi n·∫øu c√≥ th√™m c√°i qu·∫°t tr·∫ßn, b·∫°n l·∫°i c√≥ th√™m 1 c√°i remote cho n√≥. V·∫≠y l√† b·∫°n ph·∫£i qu·∫£n l√Ω 4 c√°i remote cho 4 m√≥n ƒë·ªì ƒë√≥.\nCh·∫Øc h·∫≥n sau khi setup HASS b·∫°n s·∫Ω mu·ªën m√¨nh c√≥ th·ªÉ ƒëi·ªÅu khi·ªÉn t·∫•t c·∫£ nh·ªØng c√°i remote tr√™n giao di·ªán HASS.\nB·∫£n th√¢n h·ªá th·ªëng HASS kh√¥ng th·ªÉ ho·∫°t ƒë·ªông thay cho chi·∫øc remote c·ªßa b·∫°n, v√¨ th·∫ø b·∫°n c·∫ßn s·ª± h·ªó tr·ª£ c·ªßa m·ªôt c√°i hub -\u0026gt; ƒë√¢y ch√≠nh l√† l√∫c Broadlink RM4 mini xu·∫•t hi·ªán.\nCh√∫ √Ω: RM4 mini ch·ªâ c√≥ th·ªÉ ho·∫°t ƒë·ªông trong 1 ph·∫°m vi nh·∫•t ƒë·ªãnh, gi·ªëng nh∆∞ chi·∫øc remote c·ªßa b·∫°n v·∫≠y. N·∫øu nh√† b·∫°n c√≥ 3 ph√≤ng m·ªói ph√≤ng c√≥ 1 c√°i Tivi, ƒëi·ªÅu h√≤a, qu·∫°t, v..v th√¨ b·∫°n s·∫Ω c·∫ßn mua 3 c√°i RM4 mini n√†y üòÇ th√¨ m·ªõi ƒëi·ªÅu khi·ªÉn h·∫øt ƒë∆∞·ª£c.\nC∆° ch·∫ø r·∫•t ƒë∆°n gi·∫£n: RM4 mini s·∫Ω k·∫øt n·ªëi wifi, n√≥ nh·∫≠n l·ªánh t·ª´ HASS r·ªìi chuy·ªÉn c√°c l·ªánh ƒë√≥ th√†nh s√≥ng h·ªìng ngo·∫°i, truy·ªÅn s√≥ng ƒë√≥ t·ªõi thi·∫øt b·ªã.\nƒê√¢y l√† RM4 mini nh√† m√¨nh:\nC√°c b∆∞·ªõc ƒë·ªÉ k·∫øt n·ªëi RM4 mini t·ªõi Wifi th∆∞·ªùng s·∫Ω ƒë∆∞·ª£c h∆∞·ªõng d·∫´n ngay b·ªõi shop b√°n s·∫£n ph·∫©m:\nC·∫Øm ngu·ªìn usb RM4 mini -\u0026gt; T·∫£i app Broadlink tr√™n ƒëi·ªán tho·∫°i -\u0026gt; b·∫≠t bluetooth -\u0026gt; t√¨m RM4 mini -\u0026gt; pair RM4 mini v·ªõi ƒëi·ªán tho·∫°i -\u0026gt; k·∫øt n·ªëi wifi -\u0026gt; xong\nGi·ªù l√† l√∫c setup tr√™n HASS\n2. Setup integration on HASS V√†o Setting -\u0026gt; Add Integration -\u0026gt; search Broadlink:\nNh·∫≠p host Ip c·ªßa RM4 mini v√†o (trong tr∆∞·ªùng h·ª£p c·ªßa m√¨nh l√† 192.168.1.8):\nH√£y ch√∫ √Ω r·∫±ng phi√™n b·∫£n HASS core 2022.5.2 ko h·ªó tr·ª£ RM4 mini, b·∫°n c·∫ßn upgrade l√™n phi√™n b·∫£n m·ªõi nh∆∞ m√¨nh l√† 2022.8.2\nsau khi nh·∫≠p t√™n cho RM4 mini th√¨ b·∫°n s·∫Ω th·∫•y c√°c entity m·ªõi xu·∫•t hi·ªán:\n3. Learn commands on HASS Ti·∫øp theo b·∫°n c·∫ßn d·∫°y command cho HASS. B·∫£n th√¢n HASS v√† RM4 mini hi·ªán ch∆∞a hi·ªÉu nhau.\n·∫¢nh d∆∞·ªõi l√† ch·∫ø ƒë·ªô Learn Command c·ªßa HASS, m√¨nh ƒëang mu·ªën d·∫°y cho RM4 mini bi·∫øt n√∫t power s·∫Ω g·ª≠i ƒëo·∫°n code g√¨:\nSau khi ·∫•n CALL SERVICE, b·∫°n s·∫Ω th·∫•y RM4 mini ƒë√®n s√°ng, ƒë√¢y l√† l√∫c b·∫°n ·∫•n n√∫t POWER tr√™n ƒëi·ªÅu khi·ªÉn, chƒ©a v√†o chi·∫øc RM4 mini m√† ·∫•n (ko ph·∫£i ·∫•n gi·ªØ nha, m√† l√† ·∫•n 1 c√°i r·ªìi th·∫£ tay), ·∫•n 2 l·∫ßn l√† xong.\nGi·ªù b·∫°n v√†o folder n√†y /opt/hass/config/.storage/broadlink_remote_xxxxx_codes  b·∫°n s·∫Ω th·∫•y list code t∆∞∆°ng ·ª©ng v·ªõi n√∫t Power ƒë√£ ƒë∆∞·ª£c HASS ghi nh·ªõ l·∫°i: L·∫∑p l·∫°i qu√° tr√¨nh tr√™n cho c√°c n√∫t c√≤n l·∫°i: speed, tempup, tempdown, \u0026hellip; Qu√° tr√¨nh n√†y c√≥ th·ªÉ h∆°i l√¢u nh∆∞ng b·∫°n ch·ªâ c·∫ßn l√†m 1 l·∫ßn\nB·∫°n c√≥ th·ªÉ test l·∫°i b·∫±ng c√°ch ch·ªçn ch·∫ø ƒë·ªô Remote: Send Command tr√™n HASS: Sau khi ·∫•n n√∫t CALL SERVICE, ƒë√®n tr√™n RM4 mini s·∫Ω nh√°y 1 c√°i ƒë·ªÉ g·ª≠i s√≥ng h·ªìng ngo·∫°i ƒëi. H√£y check xem ƒëi·ªÅu h√≤a/qu·∫°t/tv c·ªßa b·∫°n c√≥ b·∫≠t l√™n ƒë√∫ng nh∆∞ √Ω b·∫°n ko?\nH√£y ch·∫Øc ch·∫Øn r·∫±ng c√°i RM4 mini ƒëang ·ªü g·∫ßn thi·∫øt b·ªã c·ªßa b·∫°n. Ki·ªÉu nh∆∞ RM4 mini gi·ªù ƒë√£ th√†nh 1 chi·∫øc remote ƒëi·ªÅu khi·ªÉn t·ª´ xa qua HASS\n4. Setup dashboard on HASS Gi·ªù l√† l√∫c b·∫°n ƒë∆∞a c√°c command ƒë√£ h·ªçc ƒë∆∞·ª£c l√™n giao di·ªán c·ªßa HASS th√¥i.\nƒê·∫ßu ti√™n l√† b·∫°n s·∫Ω t·∫°o file sau, t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng n√∫t, s·∫Ω c√≥ ƒëo·∫°n code nh∆∞ n√†y:\n/opt/hass/config/scripts.yaml:\n# Fan Living Room fan_livingroom_power: sequence: - service: remote.send_command data: command: power device: fan_livingroom target: entity_id: remote.rm4_mini_livingroom_remote fan_livingroom_speed: sequence: - service: remote.send_command data: command: speed device: fan_livingroom target: entity_id: remote.rm4_mini_livingroom_remote R·ªìi giao di·ªán c·ªßa HASS s·∫Ω call ƒë·∫øn c√°c script m√† b·∫°n v·ª´a t·∫°o:\ntitle: \u0026#34;Living Room\u0026#34; path: \u0026#34;livingroom\u0026#34; cards: - type: entities title: Panasonic Fan show_header_toggle: false entities: - type: button name: Power show_state: false show_icon: true icon: mdi:power tap_action: action: call-service service: script.fan_livingroom_power data: entity_id: script.fan_livingroom_power - type: button name: Speed show_state: false show_icon: true icon: mdi:fan tap_action: action: call-service service: script.fan_livingroom_speed data: entity_id: script.fan_livingroom_speed ƒëo·∫°n code b√™n tr√™n t·∫°o ra 1 c√°i card to v·ªõi title l√† Panasonic Fan, b√™n trong card ƒë√≥ g·ªìm 2 button Power v√† Speed.\nM·ªói khi c√≥ ng∆∞·ªùi ·∫•n button ƒë√≥, n√≥ s·∫Ω call service script.fan_livingroom_power ho·∫∑c script.fan_livingroom_speed t∆∞∆°ng ·ª©ng.\ngiao di·ªán c·ªßa ƒëo·∫°n code tr√™n nh∆∞ n√†y:\nNh∆∞ng m√¨nh ko mu·ªën ph·∫£i ·∫•n n√∫t Run th√¨ m·ªõi b·∫•t ƒë∆∞·ª£c qu·∫°t, th·∫ø n√™n b·∫°n n√™n s·ª≠a ƒëo·∫°n code, ko s·ª≠ d·ª•ng type: entities n·ªØa, m√† d√πng type: glance:\ntitle: \u0026#34;Living Room\u0026#34; path: \u0026#34;livingroom\u0026#34; cards: - type: glance title: Panasonic Fan entities: - entity: script.fan_livingroom_power icon: \u0026#39;mdi:power\u0026#39; name: Power show_state: false tap_action: action: call-service service: script.fan_livingroom_power service_data: entity_id: script.fan_livingroom_power - entity: script.fan_livingroom_speed icon: \u0026#39;mdi:fan\u0026#39; name: Speed show_state: false tap_action: action: call-service service: script.fan_livingroom_speed service_data: entity_id: script.fan_livingroom_speed giao di·ªán s·∫Ω d·ªÖ nh√¨n h∆°n, ki·ªÉu nh∆∞ n√†y:\n5. L∆∞u √Ω C√≥ 1 v√†i l∆∞u √Ω sau khi m√¨nh s·ª≠ d·ª•ng v√†i ng√†y\n Quay ƒë√∫ng chi·ªÅu c·ªßa RM4 mini:\nB·∫°n c√≥ th·ªÉ s·∫Ω th·∫•y r·∫±ng t·∫°i sao RM4 mini nhi·ªÅu khi ko ƒëi·ªÅu khi·ªÉn ƒë∆∞·ª£c c√°i qu·∫°t c√¢y c·ªßa m√¨nh, ki·ªÉu nh∆∞ \u0026hellip; s√≥ng h·ªìng ngo·∫°i ph√°t ra t·ª´ RM4 mini ko ƒë·∫øn ƒë∆∞·ª£c thi·∫øt b·ªã ·∫•y. Ch·∫£ l·∫Ω n√≥ b·ªã h·ªèng?  Sau 1 h·ªìi t√¨m hi·ªÉu th√¨ m√¨nh m·ªõi bi·∫øt r·∫±ng, ch·ªó ph√°t ra s√≥ng h·ªìng ngo·∫°i:\nko ph·∫£i l√† c√°i ƒë√®n led nh·ªè nh·ªè xinh xinh ·ªü m·∫∑t b√™n‚ùå,\nm√† l√† c·∫£ ph·∫ßn b·ªÅ m·∫∑t b√™n tr√™n (b√¥i v√†ng)‚úÖ:\nTh·∫ø n√™n ƒë·ªÉ RM4 mini ho·∫°t ƒë·ªông t·ªët, th√¨ b·∫°n n√™n h∆∞·ªõng c√°i m·∫∑t k√≠nh/nh·ª±a b√≥ng ·∫•y v·ªÅ ph√≠a thi·∫øt b·ªã nh√©.\nTƒÉng gi·∫£m nhi·ªát ƒë·ªô c·ªßa ƒëi·ªÅu h√≤a:\nƒêi·ªÅu khi·ªÉn ƒëi·ªÅu h√≤a LG c·ªßa m√¨nh c·∫ßn set m·ªói m·ªëc nhi·ªát ƒë·ªô 1 n√∫t ·∫•n.\nB·∫°n s·∫Ω ko th·ªÉ \u0026ldquo;·∫•n n√∫t tƒÉng/gi·∫£m ƒë·ªÉ thay ƒë·ªïi nhi·ªát ƒë·ªô\u0026rdquo;.\nM√† b·∫°n n·∫øu b·∫°n mu·ªën set nhi·ªát ƒë·ªô 26 ƒë·ªô th√¨ c·∫ßn d·∫°y cho RM4 mini bi·∫øt 26 ƒë·ªô l√† nh∆∞ n√†o.\nKhi ƒë√≥ tr√™n giao di·ªán b·∫°n ·∫•n n√∫t 26 ƒë·ªô ƒë·ªÉ set nhi·ªát ƒë·ªô cho ƒëi·ªÅu h√≤a.  M√¨nh ƒë√£ d·∫°y RM4 mini c√°c m·ªëc nhi·ªát ƒë·ªô nh∆∞ n√†y:\nChu·∫©n b·ªã tr∆∞·ªõc: B·∫≠t c√°i remote ƒëi·ªÅu h√≤a l√™n, ch·ªânh s·∫µn v·ªÅ m·ª©c 20 ƒë·ªô. Setting tr√™n HASS nh∆∞ n√†y ƒë·ªÉ d·∫°y n√≥ bi·∫øt 19 ƒë·ªô l√† nh∆∞ n√†o: Ch√∫ √Ω m√¨nh ko ch·ªçn tick v√†o ch·ªó Alternative. Sau khi ·∫•n CALL SERVICE, b·∫°n s·∫Ω tr·ªè remote v√†o RM4 mini r·ªìi ·∫•n 1 ph√°t n√∫t gi·∫£m nhi·ªát ƒë·ªô t·ª´ 20 xu·ªëng 19. Nh∆∞ v·∫≠y l√† xong, RM4 mini s·∫Ω hi·ªÉu th·∫ø n√†o l√† 19 ƒë·ªô.\nL√†m t∆∞∆°ng t·ª± v·ªõi c√°c m·ªëc nhi·ªát ƒë·ªô kh√°c b·∫°n s·∫Ω c√≥ 1 list code nh∆∞ n√†y:\nVi·ªác ƒë∆∞a c√°c script ra dashboard th√¨ t∆∞∆°ng t·ª± nh∆∞ l√†m v·ªõi Fan ·ªü b√™n tr√™n m√¨nh ƒë√£ h∆∞·ªõng d·∫´n\n6. Troubleshoot Trong qu√° tr√¨nh s·ª≠ d·ª•ng c√≥ th·ªÉ s·∫Ω c√≥ l√∫c b·∫°n th·∫•y t·ª± nhi√™n RM4 mini ko ho·∫°t ƒë·ªông n·ªØa, khi v√†o HASS UI check th√¨ ph√°t hi·ªán l·ªói sau:\nNhi·ªÅu kh·∫£ nƒÉng l√Ω do l√† Broadlink ƒë√£ b·ªã thay ƒë·ªïi IP khi reconnect v√†o wifi nh√† b·∫°n.\n(H√£y check l·∫°i xem IP c·ªßa Broadlink m·ªõi l√† g√¨ tr∆∞·ªõc)\n-\u0026gt; H√£y Delete integration ƒëi v√† add l·∫°i nh√©.\nNh·ªõ note l·∫°i c√°i t√™n b√¥i v√†ng n√†y ƒë·ªÉ l√°t n·ªØa add l·∫°i cho tr√πng t√™n:\nƒê√¢y l√† m√†n h√¨nh add l·∫°i:\nXong r·ªìi, kh√° d·ªÖ th√¥i, ch√∫c c√°c b·∫°n th√†nh c√¥ng! üòÅ\nCREDIT https://www.youtube.com/watch?v=jEOyTGaKwaQ\u0026amp;t=634s\u0026amp;ab_channel=EverythingSmartHome\nhttps://www.home-assistant.io/dashboards/glance/\n","href":"/bk/encrypt-connect-home-assistant-to-broadlink-rm4-mini/","title":"Connect Home Assistant to Broadlink Rm4 Mini"},{"content":"1. Story N·∫øu nh∆∞ nh√† b·∫°n c√≥ 1 chi·∫øc ƒëi·ªÅu h√≤a b√¨nh th∆∞·ªùng (ko ph·∫£i lo·∫°i smart), b·∫°n s·∫Ω ph·∫£i ƒëi·ªÅu khi·ªÉn n√≥ th√¥ng qua 1 c√°i ƒëi·ªÅu khi·ªÉn (remote) ri√™ng.\nR·ªìi c√≥ TV b·∫°n l·∫°i c√≥ 1 remote cho n√≥. N·∫øu c√≥ th√™m 1 chi·∫øc qu·∫°t c√¢y, ch·∫Øc b·∫°n c≈©ng s·∫Ω c√≥ remote d√†nh ri√™ng cho n√≥. R·ªìi n·∫øu c√≥ th√™m c√°i qu·∫°t tr·∫ßn, b·∫°n l·∫°i c√≥ th√™m 1 c√°i remote cho n√≥. V·∫≠y l√† b·∫°n ph·∫£i qu·∫£n l√Ω 4 c√°i remote cho 4 m√≥n ƒë·ªì ƒë√≥.\nCh·∫Øc h·∫≥n sau khi setup HASS b·∫°n s·∫Ω mu·ªën m√¨nh c√≥ th·ªÉ ƒëi·ªÅu khi·ªÉn t·∫•t c·∫£ nh·ªØng c√°i remote tr√™n giao di·ªán HASS.\nB·∫£n th√¢n h·ªá th·ªëng HASS kh√¥ng th·ªÉ ho·∫°t ƒë·ªông thay cho chi·∫øc remote c·ªßa b·∫°n, v√¨ th·∫ø b·∫°n c·∫ßn s·ª± h·ªó tr·ª£ c·ªßa m·ªôt c√°i hub -\u0026gt; ƒë√¢y ch√≠nh l√† l√∫c Broadlink RM4 mini xu·∫•t hi·ªán.\nCh√∫ √Ω: RM4 mini ch·ªâ c√≥ th·ªÉ ho·∫°t ƒë·ªông trong 1 ph·∫°m vi nh·∫•t ƒë·ªãnh, gi·ªëng nh∆∞ chi·∫øc remote c·ªßa b·∫°n v·∫≠y. N·∫øu nh√† b·∫°n c√≥ 3 ph√≤ng m·ªói ph√≤ng c√≥ 1 c√°i Tivi, ƒëi·ªÅu h√≤a, qu·∫°t, v..v th√¨ b·∫°n s·∫Ω c·∫ßn mua 3 c√°i RM4 mini n√†y üòÇ th√¨ m·ªõi ƒëi·ªÅu khi·ªÉn h·∫øt ƒë∆∞·ª£c.\nC∆° ch·∫ø r·∫•t ƒë∆°n gi·∫£n: RM4 mini s·∫Ω k·∫øt n·ªëi wifi, n√≥ nh·∫≠n l·ªánh t·ª´ HASS r·ªìi chuy·ªÉn c√°c l·ªánh ƒë√≥ th√†nh s√≥ng h·ªìng ngo·∫°i, truy·ªÅn s√≥ng ƒë√≥ t·ªõi thi·∫øt b·ªã.\nƒê√¢y l√† RM4 mini nh√† m√¨nh:\nC√°c b∆∞·ªõc ƒë·ªÉ k·∫øt n·ªëi RM4 mini t·ªõi Wifi th∆∞·ªùng s·∫Ω ƒë∆∞·ª£c h∆∞·ªõng d·∫´n ngay b·ªõi shop b√°n s·∫£n ph·∫©m:\nC·∫Øm ngu·ªìn usb RM4 mini -\u0026gt; T·∫£i app Broadlink tr√™n ƒëi·ªán tho·∫°i -\u0026gt; b·∫≠t bluetooth -\u0026gt; t√¨m RM4 mini -\u0026gt; pair RM4 mini v·ªõi ƒëi·ªán tho·∫°i -\u0026gt; k·∫øt n·ªëi wifi -\u0026gt; xong\nGi·ªù l√† l√∫c setup tr√™n HASS\n2. Setup integration on HASS V√†o Setting -\u0026gt; Add Integration -\u0026gt; search Broadlink:\nNh·∫≠p host Ip c·ªßa RM4 mini v√†o (trong tr∆∞·ªùng h·ª£p c·ªßa m√¨nh l√† 192.168.1.8):\nH√£y ch√∫ √Ω r·∫±ng phi√™n b·∫£n HASS core 2022.5.2 ko h·ªó tr·ª£ RM4 mini, b·∫°n c·∫ßn upgrade l√™n phi√™n b·∫£n m·ªõi nh∆∞ m√¨nh l√† 2022.8.2\nsau khi nh·∫≠p t√™n cho RM4 mini th√¨ b·∫°n s·∫Ω th·∫•y c√°c entity m·ªõi xu·∫•t hi·ªán:\n3. Learn commands on HASS Ti·∫øp theo b·∫°n c·∫ßn d·∫°y command cho HASS. B·∫£n th√¢n HASS v√† RM4 mini hi·ªán ch∆∞a hi·ªÉu nhau.\n·∫¢nh d∆∞·ªõi l√† ch·∫ø ƒë·ªô Learn Command c·ªßa HASS, m√¨nh ƒëang mu·ªën d·∫°y cho RM4 mini bi·∫øt n√∫t power s·∫Ω g·ª≠i ƒëo·∫°n code g√¨:\nSau khi ·∫•n CALL SERVICE, b·∫°n s·∫Ω th·∫•y RM4 mini ƒë√®n s√°ng, ƒë√¢y l√† l√∫c b·∫°n ·∫•n n√∫t POWER tr√™n ƒëi·ªÅu khi·ªÉn, chƒ©a v√†o chi·∫øc RM4 mini m√† ·∫•n (ko ph·∫£i ·∫•n gi·ªØ nha, m√† l√† ·∫•n 1 c√°i r·ªìi th·∫£ tay), ·∫•n 2 l·∫ßn l√† xong.\nGi·ªù b·∫°n v√†o folder n√†y /opt/hass/config/.storage/broadlink_remote_xxxxx_codes  b·∫°n s·∫Ω th·∫•y list code t∆∞∆°ng ·ª©ng v·ªõi n√∫t Power ƒë√£ ƒë∆∞·ª£c HASS ghi nh·ªõ l·∫°i: L·∫∑p l·∫°i qu√° tr√¨nh tr√™n cho c√°c n√∫t c√≤n l·∫°i: speed, tempup, tempdown, \u0026hellip; Qu√° tr√¨nh n√†y c√≥ th·ªÉ h∆°i l√¢u nh∆∞ng b·∫°n ch·ªâ c·∫ßn l√†m 1 l·∫ßn\nB·∫°n c√≥ th·ªÉ test l·∫°i b·∫±ng c√°ch ch·ªçn ch·∫ø ƒë·ªô Remote: Send Command tr√™n HASS: Sau khi ·∫•n n√∫t CALL SERVICE, ƒë√®n tr√™n RM4 mini s·∫Ω nh√°y 1 c√°i ƒë·ªÉ g·ª≠i s√≥ng h·ªìng ngo·∫°i ƒëi. H√£y check xem ƒëi·ªÅu h√≤a/qu·∫°t/tv c·ªßa b·∫°n c√≥ b·∫≠t l√™n ƒë√∫ng nh∆∞ √Ω b·∫°n ko?\nH√£y ch·∫Øc ch·∫Øn r·∫±ng c√°i RM4 mini ƒëang ·ªü g·∫ßn thi·∫øt b·ªã c·ªßa b·∫°n. Ki·ªÉu nh∆∞ RM4 mini gi·ªù ƒë√£ th√†nh 1 chi·∫øc remote ƒëi·ªÅu khi·ªÉn t·ª´ xa qua HASS\n4. Setup dashboard on HASS Gi·ªù l√† l√∫c b·∫°n ƒë∆∞a c√°c command ƒë√£ h·ªçc ƒë∆∞·ª£c l√™n giao di·ªán c·ªßa HASS th√¥i.\nƒê·∫ßu ti√™n l√† b·∫°n s·∫Ω t·∫°o file sau, t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng n√∫t, s·∫Ω c√≥ ƒëo·∫°n code nh∆∞ n√†y:\n/opt/hass/config/scripts.yaml:\n# Fan Living Room fan_livingroom_power: sequence: - service: remote.send_command data: command: power device: fan_livingroom target: entity_id: remote.rm4_mini_livingroom_remote fan_livingroom_speed: sequence: - service: remote.send_command data: command: speed device: fan_livingroom target: entity_id: remote.rm4_mini_livingroom_remote R·ªìi giao di·ªán c·ªßa HASS s·∫Ω call ƒë·∫øn c√°c script m√† b·∫°n v·ª´a t·∫°o:\ntitle: \u0026#34;Living Room\u0026#34; path: \u0026#34;livingroom\u0026#34; cards: - type: entities title: Panasonic Fan show_header_toggle: false entities: - type: button name: Power show_state: false show_icon: true icon: mdi:power tap_action: action: call-service service: script.fan_livingroom_power data: entity_id: script.fan_livingroom_power - type: button name: Speed show_state: false show_icon: true icon: mdi:fan tap_action: action: call-service service: script.fan_livingroom_speed data: entity_id: script.fan_livingroom_speed ƒëo·∫°n code b√™n tr√™n t·∫°o ra 1 c√°i card to v·ªõi title l√† Panasonic Fan, b√™n trong card ƒë√≥ g·ªìm 2 button Power v√† Speed.\nM·ªói khi c√≥ ng∆∞·ªùi ·∫•n button ƒë√≥, n√≥ s·∫Ω call service script.fan_livingroom_power ho·∫∑c script.fan_livingroom_speed t∆∞∆°ng ·ª©ng.\ngiao di·ªán c·ªßa ƒëo·∫°n code tr√™n nh∆∞ n√†y:\nNh∆∞ng m√¨nh ko mu·ªën ph·∫£i ·∫•n n√∫t Run th√¨ m·ªõi b·∫•t ƒë∆∞·ª£c qu·∫°t, th·∫ø n√™n b·∫°n n√™n s·ª≠a ƒëo·∫°n code, ko s·ª≠ d·ª•ng type: entities n·ªØa, m√† d√πng type: glance:\ntitle: \u0026#34;Living Room\u0026#34; path: \u0026#34;livingroom\u0026#34; cards: - type: glance title: Panasonic Fan entities: - entity: script.fan_livingroom_power icon: \u0026#39;mdi:power\u0026#39; name: Power show_state: false tap_action: action: call-service service: script.fan_livingroom_power service_data: entity_id: script.fan_livingroom_power - entity: script.fan_livingroom_speed icon: \u0026#39;mdi:fan\u0026#39; name: Speed show_state: false tap_action: action: call-service service: script.fan_livingroom_speed service_data: entity_id: script.fan_livingroom_speed giao di·ªán s·∫Ω d·ªÖ nh√¨n h∆°n, ki·ªÉu nh∆∞ n√†y:\n5. L∆∞u √Ω C√≥ 1 v√†i l∆∞u √Ω sau khi m√¨nh s·ª≠ d·ª•ng v√†i ng√†y\n Quay ƒë√∫ng chi·ªÅu c·ªßa RM4 mini:\nB·∫°n c√≥ th·ªÉ s·∫Ω th·∫•y r·∫±ng t·∫°i sao RM4 mini nhi·ªÅu khi ko ƒëi·ªÅu khi·ªÉn ƒë∆∞·ª£c c√°i qu·∫°t c√¢y c·ªßa m√¨nh, ki·ªÉu nh∆∞ \u0026hellip; s√≥ng h·ªìng ngo·∫°i ph√°t ra t·ª´ RM4 mini ko ƒë·∫øn ƒë∆∞·ª£c thi·∫øt b·ªã ·∫•y. Ch·∫£ l·∫Ω n√≥ b·ªã h·ªèng?  Sau 1 h·ªìi t√¨m hi·ªÉu th√¨ m√¨nh m·ªõi bi·∫øt r·∫±ng, ch·ªó ph√°t ra s√≥ng h·ªìng ngo·∫°i:\nko ph·∫£i l√† c√°i ƒë√®n led nh·ªè nh·ªè xinh xinh ·ªü m·∫∑t b√™n‚ùå,\nm√† l√† c·∫£ ph·∫ßn b·ªÅ m·∫∑t b√™n tr√™n (b√¥i v√†ng)‚úÖ:\nTh·∫ø n√™n ƒë·ªÉ RM4 mini ho·∫°t ƒë·ªông t·ªët, th√¨ b·∫°n n√™n h∆∞·ªõng c√°i m·∫∑t k√≠nh/nh·ª±a b√≥ng ·∫•y v·ªÅ ph√≠a thi·∫øt b·ªã nh√©.\nTƒÉng gi·∫£m nhi·ªát ƒë·ªô c·ªßa ƒëi·ªÅu h√≤a:\nƒêi·ªÅu khi·ªÉn ƒëi·ªÅu h√≤a LG c·ªßa m√¨nh c·∫ßn set m·ªói m·ªëc nhi·ªát ƒë·ªô 1 n√∫t ·∫•n.\nB·∫°n s·∫Ω ko th·ªÉ \u0026ldquo;·∫•n n√∫t tƒÉng/gi·∫£m ƒë·ªÉ thay ƒë·ªïi nhi·ªát ƒë·ªô\u0026rdquo;.\nM√† b·∫°n n·∫øu b·∫°n mu·ªën set nhi·ªát ƒë·ªô 26 ƒë·ªô th√¨ c·∫ßn d·∫°y cho RM4 mini bi·∫øt 26 ƒë·ªô l√† nh∆∞ n√†o.\nKhi ƒë√≥ tr√™n giao di·ªán b·∫°n ·∫•n n√∫t 26 ƒë·ªô ƒë·ªÉ set nhi·ªát ƒë·ªô cho ƒëi·ªÅu h√≤a.  M√¨nh ƒë√£ d·∫°y RM4 mini c√°c m·ªëc nhi·ªát ƒë·ªô nh∆∞ n√†y:\nChu·∫©n b·ªã tr∆∞·ªõc: B·∫≠t c√°i remote ƒëi·ªÅu h√≤a l√™n, ch·ªânh s·∫µn v·ªÅ m·ª©c 20 ƒë·ªô. Setting tr√™n HASS nh∆∞ n√†y ƒë·ªÉ d·∫°y n√≥ bi·∫øt 19 ƒë·ªô l√† nh∆∞ n√†o: Ch√∫ √Ω m√¨nh ko ch·ªçn tick v√†o ch·ªó Alternative. Sau khi ·∫•n CALL SERVICE, b·∫°n s·∫Ω tr·ªè remote v√†o RM4 mini r·ªìi ·∫•n 1 ph√°t n√∫t gi·∫£m nhi·ªát ƒë·ªô t·ª´ 20 xu·ªëng 19. Nh∆∞ v·∫≠y l√† xong, RM4 mini s·∫Ω hi·ªÉu th·∫ø n√†o l√† 19 ƒë·ªô.\nL√†m t∆∞∆°ng t·ª± v·ªõi c√°c m·ªëc nhi·ªát ƒë·ªô kh√°c b·∫°n s·∫Ω c√≥ 1 list code nh∆∞ n√†y:\nVi·ªác ƒë∆∞a c√°c script ra dashboard th√¨ t∆∞∆°ng t·ª± nh∆∞ l√†m v·ªõi Fan ·ªü b√™n tr√™n m√¨nh ƒë√£ h∆∞·ªõng d·∫´n\n6. Troubleshoot Trong qu√° tr√¨nh s·ª≠ d·ª•ng c√≥ th·ªÉ s·∫Ω c√≥ l√∫c b·∫°n th·∫•y t·ª± nhi√™n RM4 mini ko ho·∫°t ƒë·ªông n·ªØa, khi v√†o HASS UI check th√¨ ph√°t hi·ªán l·ªói sau:\nNhi·ªÅu kh·∫£ nƒÉng l√Ω do l√† Broadlink ƒë√£ b·ªã thay ƒë·ªïi IP khi reconnect v√†o wifi nh√† b·∫°n.\n(H√£y check l·∫°i xem IP c·ªßa Broadlink m·ªõi l√† g√¨ tr∆∞·ªõc)\n-\u0026gt; H√£y Delete integration ƒëi v√† add l·∫°i nh√©.\nNh·ªõ note l·∫°i c√°i t√™n b√¥i v√†ng n√†y ƒë·ªÉ l√°t n·ªØa add l·∫°i cho tr√πng t√™n:\nƒê√¢y l√† m√†n h√¨nh add l·∫°i:\nXong r·ªìi, kh√° d·ªÖ th√¥i, ch√∫c c√°c b·∫°n th√†nh c√¥ng! üòÅ\nCREDIT https://www.youtube.com/watch?v=jEOyTGaKwaQ\u0026amp;t=634s\u0026amp;ab_channel=EverythingSmartHome\nhttps://www.home-assistant.io/dashboards/glance/\n","href":"/posts/encrypt-connect-home-assistant-to-broadlink-rm4-mini/","title":"Connect Home Assistant to Broadlink Rm4 Mini"},{"content":"This post is about a demo using Azure Synapse Analytics. Analyze data with serverless SQL, then visualize on Power BI desktop\nOverview about Azure Synapse Analytics Azure Synapse contains the same Data Integration engine and experiences as Azure Data Factory, allowing you to create rich at-scale ETL pipelines without leaving Azure Synapse Analytics.\n Ingest data from 90+ data sources Code-Free ETL with Data flow activities Orchestrate notebooks, Spark jobs, stored procedures, SQL scripts, and more  Synapse SQL:¬†is the ability to do T-SQL based analytics in Synapse workspace. Synapse SQL has two consumption models: dedicated and serverless. For the dedicated model, use¬†dedicated SQL pools. A workspace can have any number of these pools. To use the serverless model, use the¬†serverless SQL pools.\nApache Spark: Big Data engine (used for data preparation, data engineering, ETL, and machine learning)Use Data Explorer as a data platform for building near real-time log analytics and IoT analytics solutionsPipelines are how Azure Synapse provides Data Integration - allowing you to move data between services and orchestrate activities.\nWhy I write this C√≥ th·ªÉ b·∫°n s·∫Ω th·∫•y kh√≥ hi·ªÉu khi t·∫°i sao ko l√†m theo c√°c Tutorial c·ªßa MS document, m√† ph·∫£i t√°ch ra vi·∫øt nh∆∞ n√†y.\nTh√¨ b·ªüi v√¨ m·ª•c ƒë√≠ch c·ªßa m√¨nh mu·ªën c√≥ 1 c√°i lu·ªìng t·ª´ ƒë·∫ßu, 1-upload file data l√™n ADLSGen2, 2-r·ªìi Analize data ƒë√≥ tr√™n Synapse Studio, 3-r·ªìi visualize data ƒë√≥ tr√™n Power BI.\nNh∆∞ng MS document th√¨ ch·ªâ h∆∞·ªõng d·∫´n step 1 v√† 2 tr√™n 1 file data NYCTripSmall.parquet 5MB (nh·∫π). C√≤n step 3 l·∫°i l√†m tr√™n 1 b·ªô data kh√°c kho·∫£ng 50GB (qu√° n·∫∑ng).\nN√™n b√†i n√†y gi√∫p b·∫°n l√†m c·∫£ 3 step ƒë√≥ tr√™n 1 file data NYCTripSmall.parquet th√¥i.\n1. Place sample data in ADLS Gen2 Download this parquet file to your computer. (https://azuresynapsestorage.blob.core.windows.net/sampledata/NYCTaxiSmall/NYCTripSmall.parquet)\nIn Synapse Studio, navigate to the Data Hub.\nSelect Linked.\nUnder the category Azure Data Lake Storage Gen2 you\u0026rsquo;ll see an item with a name like myworkspace ( Primary - YOUR_ADLS_GEN2 ).\nSelect the container named YOUR_CONTAINER (Primary). Select Upload and select the NYCTripSmall.parquet file you downloaded.\nOnce the parquet file is uploaded it is available through two equivalent URIs:\nhttps://YOUR_ADLS_GEN2.dfs.core.windows.net/YOUR_CONTAINER/NYCTripSmall.parquet\nabfss://YOUR_CONTAINER@YOUR_ADLS_GEN2.dfs.core.windows.net/NYCTripSmall.parquet\n2. Analyze data with serverless SQL In Synapse Studio, go to the Develop hub\nCreate a new SQL script.\nPaste the following code into the script.\nSELECT TOP 100 * FROM OPENROWSET( BULK \u0026#39;https://YOUR_ADLS_GEN2.dfs.core.windows.net/YOUR_CONTAINER/NYCTripSmall.parquet\u0026#39;, FORMAT=\u0026#39;PARQUET\u0026#39; ) AS [result] Click Run.\nCreate data exploration database\nCREATE DATABASE DataExplorationDB COLLATE Latin1_General_100_BIN2_UTF8 Switch from master to DataExplorationDB using the following command. You can also use the UI control use database to switch your current database:\nUSE DataExplorationDB Create external data source:\nCREATE EXTERNAL DATA SOURCE YOUR_EXT_DATASOURCE WITH ( LOCATION = \u0026#39;https://YOUR_ADLS_GEN2.dfs.core.windows.net\u0026#39;) query from external data source:\nSELECT TOP 100 * FROM OPENROWSET( BULK \u0026#39;/YOUR_CONTAINER/NYCTripSmall.parquet\u0026#39;, DATA_SOURCE = \u0026#39;YOUR_EXT_DATASOURCE\u0026#39;, FORMAT=\u0026#39;PARQUET\u0026#39; ) AS [result] 3. Visualize on Power BI desktop 3.1. Prepare external table B∆∞·ªõc n√†y r·∫•t quan tr·ªçng, nhi·ªÅu l·∫ßn m√¨nh b·ªã l·ªói ·ªü Power BI v√¨ ƒë√£ ko t·∫°o External Table n√†y v·ªõi CREDENTIAL\nB∆∞·ªõc --8 c√≥ th·ªÉ s·∫Ω th·∫•y kh√≥ khi ko bi·∫øt layout c·ªßa file parquet l√† g√¨, th√¨ c√≥ th·ªÉ d√πng c√°ch n√†y:\nchu·ªôt ph·∫£i v√†o file NYCTripSmall.parquet, ch·ªçn SQL script -\u0026gt; Create external table ƒë·ªÉ xem layout table ph·∫ßn ƒë√≥\nCh√∫ √Ω ta s·∫Ω d√πng l·∫°i YOUR_EXT_DATASOURCE, YOUR_CONTAINER, YOUR_ADLS_GEN2 m√† ƒë√£ t·∫°o ·ªü tr√™n\nB·∫°n n√™n copy t·ª´ng step 1 r·ªìi run ƒë·ªÉ hi·ªÉu v·ªÅ c√°c step l√†m g√¨\n-- 1 USE DataExplorationDB CREATE MASTER KEY ENCRYPTION BY PASSWORD = \u0026#39;qwe@##@5324aSD127123g\u0026#39; GO -- 2 CREATE DATABASE SCOPED CREDENTIAL WorkspaceIdentity01 WITH IDENTITY = \u0026#39;Managed Identity\u0026#39; GO -- 3 CREATE LOGIN TestUser01 WITH PASSWORD = \u0026#39;abcdef123!@#\u0026#39; GO -- 4 CREATE USER Test01 FOR LOGIN TestUser01 GO -- 5 GRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::WorkspaceIdentity01 TO Test01 -- 6 IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = \u0026#39;YOUR_EXT_DATASOURCE\u0026#39;) CREATE EXTERNAL DATA SOURCE [YOUR_EXT_DATASOURCE] WITH ( LOCATION = \u0026#39;https://YOUR_ADLS_GEN2.dfs.core.windows.net/YOUR_CONTAINER\u0026#39;, CREDENTIAL = WorkspaceIdentity01 ) GO -- 7 IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = \u0026#39;SynapseParquetFormat\u0026#39;) CREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] WITH ( FORMAT_TYPE = PARQUET) GO -- 8 CREATE EXTERNAL TABLE NYCTripSmallExTable01 ( [DateID] int, [MedallionID] int, [HackneyLicenseID] int, [PickupTimeID] int, [DropoffTimeID] int, [PickupGeographyID] int, [DropoffGeographyID] int, [PickupLatitude] float, [PickupLongitude] float, [PickupLatLong] nvarchar(4000), [DropoffLatitude] float, [DropoffLongitude] float, [DropoffLatLong] nvarchar(4000), [PassengerCount] int, [TripDurationSeconds] int, [TripDistanceMiles] float, [PaymentType] nvarchar(4000), [FareAmount] numeric(19,4), [SurchargeAmount] numeric(19,4), [TaxAmount] numeric(19,4), [TipAmount] numeric(19,4), [TollsAmount] numeric(19,4), [TotalAmount] numeric(19,4) ) WITH ( LOCATION = \u0026#39;NYCTripSmall.parquet\u0026#39;, DATA_SOURCE = [YOUR_EXT_DATASOURCE], FILE_FORMAT = [SynapseParquetFormat] ) GO -- 9 SELECT TOP 100 * FROM NYCTripSmallExTable01 GO N·∫øu run OK h·∫øt, B·∫°n v√†o Data Hub c√≥ th·ªÉ nh√¨n th·∫•y ƒë∆∞·ª£c external table NYCTripSmallExTable01 ƒë√£ ƒë∆∞·ª£c t·∫°o ra\n3.2. Visualize on Power BI desktop M·ªü Power BI desktop ra, ch·ªçn Get Data:\nƒêi·ªÅn th√¥ng tin server SQL:\nƒêi·ªÅn account/password:\nM√†n h√¨nh data s·∫Ω hi·ªán ra b√™n ph·∫£i:\n·∫§n n√∫t Load data:\nCh·ªçn c√°c th√¥ng s·ªë d√πng ƒë·ªÉ visualize:\nCh√∫ √Ω: N·∫øu b·∫°n ƒëang ·ªü sau 1 proxy c·ªßa c√¥ng ty, b·∫°n s·∫Ω ko th·ªÉ Get Data ƒë∆∞·ª£c v√¨ l·ªói k·∫øt n·ªëi c·ªßa Power BI\nSummary Synapse cung c·∫•p 1 ch·ªó l√†m vi·ªác t·∫≠p trung cho Data Engineer\nPipeline c·ªßa n√≥ gi·ªëng nh∆∞ NiFi v·∫≠y d√πng cho m·ª•c ƒë√≠nh ETL (extract, transform, load data) automatically\nM·∫∑c ƒë·ªãnh s·∫Ω c√≥ s·∫µn SQL pool Serverless, t√≠nh ti·ªÅn theo kh·ªëi l∆∞·ª£ng data x·ª≠ l√Ω (1 TB -5 USD)\nC√≥ Apache Spark ƒë·ªÉ run code .NET, pyspark, scala cho Machine Learning. T√≠nh ti·ªÅn theo node.\nD√π b·∫°n c√≥ t·∫°o ra c√°c external table, external data source th√¨ khi b·∫°n x√≥a ch√∫ng ƒëi c≈©ng ko m·∫•t data. V√¨ th·ª±c ch·∫•t ch√∫ng ko persist data, data n·∫±m ·ªü ADLS Gen2 h·∫øt.\nC√°c tab Develop Hub, Integrate Hub c√≥ th·ªÉ export ra file json,zip ƒë·ªÉ sau n√†y import l·∫°i ƒë∆∞·ª£c (ƒëi·ªÅu n√†y c√≥ th·ªÉ quan tr·ªçng trong PaaS)\nAzure Synapse s·ª≠ d·ª•ng Synapse RBAC system (ƒë·ª´ng nh·∫ßm l·∫´n v·ªõi Azure RBAC system). Azure RBAC th√¨ b·∫°n c√≥ th·ªÉ d√πng ARM code ƒë·ªÉ add role assignment cho 1 identity n√†o ƒë√≥. Nh∆∞ng Synapse RBAC th√¨ hi·ªán t·∫°i ko ƒë∆∞·ª£c h·ªó tr·ª£ ƒë·ªÉ t·ª± ƒë·ªông add role assignment cho 1 identtiy b·∫±ng ARM. V√≠ d·ª• cho d·ªÖ hi·ªÉu:\nB·∫°n mu·ªën d√πng ARM code ƒë·ªÉ t·∫°o 1 VM V s·∫Ω c√≥ identity I, I s·∫Ω ƒë∆∞·ª£c add role assignment Synapse Administrator. R·ªìi t·ª´ ƒë√≥ trong VM V b·∫°n s·∫Ω run c√°c command az cli: az synapse ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi Synapse Studio 1 c√°ch programmatically.\nV·∫•n ƒë·ªÅ s·∫Ω n·∫£y sinh ·ªü ch·ªó vi·ªác assign role assignment Synapse Administrator cho I s·∫Ω ko th·ªÉ l√†m b·∫±ng ARM code. M√† b·∫°n ph·∫£i d√πng tay, v√†o Synapse Studio -\u0026gt; Access Control ƒë·ªÉ l√†m.\nNh∆∞ng hi·ªán ch∆∞a th·∫•y cung c·∫•p c√°c Azure ARM template ƒë·ªÉ import SQL script, Pipeline. M·ªõi ch·ªâ c√≥ az cli l√†m ƒë∆∞·ª£c th√¥i.\nAzure Synapse c√≥ th·ªÉ integrate v·ªõi Github ƒë·ªÉ l√†m Source Control. Sau khi setup, t·∫•t c·∫£ thay ƒë·ªïi c·ªßa Synapse s·∫Ω ƒë∆∞·ª£c l∆∞u l·∫°i tr√™n 1 repo tr√™n Github (script SQL, pipeline,,,).\nKhi b·∫°n x√≥a Synapse ƒëi t·∫°o l·∫°i, r·ªìi integrate n√≥ v·ªõi Github repo c≈©, n√≥ c√≥ th·ªÉ kh√¥i ph·ª•c h·∫øt c√°c SQL script, Pipeline cho b·∫°n. (Tuy nhi√™n tab Data hub s·∫Ω ko ph·ª•c h·ªìi l·∫°i ƒë∆∞·ª£c, ch·ªâ tab Develop hub v√† Integrate hub m√† th√¥i)\nCREDIT https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/tutorial-data-analyst\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/tutorial-connect-power-bi-desktop\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-on-demand\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace#place-sample-data-into-the-primary-storage-account\nhttps://www.techtalkcorner.com/create-azure-synapse-anlaytics-serverless-external-table/\n","href":"/bk/encrypt-azure-synapse-basic-data-flow-analyze-visualize-on-powerbi/","title":"Azure Synapse: Analyze data with serverless SQL and visualize it on PowerBI"},{"content":"This post is about a demo using Azure Synapse Analytics. Analyze data with serverless SQL, then visualize on Power BI desktop\nOverview about Azure Synapse Analytics Azure Synapse contains the same Data Integration engine and experiences as Azure Data Factory, allowing you to create rich at-scale ETL pipelines without leaving Azure Synapse Analytics.\n Ingest data from 90+ data sources Code-Free ETL with Data flow activities Orchestrate notebooks, Spark jobs, stored procedures, SQL scripts, and more  Synapse SQL:¬†is the ability to do T-SQL based analytics in Synapse workspace. Synapse SQL has two consumption models: dedicated and serverless. For the dedicated model, use¬†dedicated SQL pools. A workspace can have any number of these pools. To use the serverless model, use the¬†serverless SQL pools.\nApache Spark: Big Data engine (used for data preparation, data engineering, ETL, and machine learning)Use Data Explorer as a data platform for building near real-time log analytics and IoT analytics solutionsPipelines are how Azure Synapse provides Data Integration - allowing you to move data between services and orchestrate activities.\nWhy I write this C√≥ th·ªÉ b·∫°n s·∫Ω th·∫•y kh√≥ hi·ªÉu khi t·∫°i sao ko l√†m theo c√°c Tutorial c·ªßa MS document, m√† ph·∫£i t√°ch ra vi·∫øt nh∆∞ n√†y.\nTh√¨ b·ªüi v√¨ m·ª•c ƒë√≠ch c·ªßa m√¨nh mu·ªën c√≥ 1 c√°i lu·ªìng t·ª´ ƒë·∫ßu, 1-upload file data l√™n ADLSGen2, 2-r·ªìi Analize data ƒë√≥ tr√™n Synapse Studio, 3-r·ªìi visualize data ƒë√≥ tr√™n Power BI.\nNh∆∞ng MS document th√¨ ch·ªâ h∆∞·ªõng d·∫´n step 1 v√† 2 tr√™n 1 file data NYCTripSmall.parquet 5MB (nh·∫π). C√≤n step 3 l·∫°i l√†m tr√™n 1 b·ªô data kh√°c kho·∫£ng 50GB (qu√° n·∫∑ng).\nN√™n b√†i n√†y gi√∫p b·∫°n l√†m c·∫£ 3 step ƒë√≥ tr√™n 1 file data NYCTripSmall.parquet th√¥i.\n1. Place sample data in ADLS Gen2 Download this parquet file to your computer. (https://azuresynapsestorage.blob.core.windows.net/sampledata/NYCTaxiSmall/NYCTripSmall.parquet)\nIn Synapse Studio, navigate to the Data Hub.\nSelect Linked.\nUnder the category Azure Data Lake Storage Gen2 you\u0026rsquo;ll see an item with a name like myworkspace ( Primary - YOUR_ADLS_GEN2 ).\nSelect the container named YOUR_CONTAINER (Primary). Select Upload and select the NYCTripSmall.parquet file you downloaded.\nOnce the parquet file is uploaded it is available through two equivalent URIs:\nhttps://YOUR_ADLS_GEN2.dfs.core.windows.net/YOUR_CONTAINER/NYCTripSmall.parquet\nabfss://YOUR_CONTAINER@YOUR_ADLS_GEN2.dfs.core.windows.net/NYCTripSmall.parquet\n2. Analyze data with serverless SQL In Synapse Studio, go to the Develop hub\nCreate a new SQL script.\nPaste the following code into the script.\nSELECT TOP 100 * FROM OPENROWSET( BULK \u0026#39;https://YOUR_ADLS_GEN2.dfs.core.windows.net/YOUR_CONTAINER/NYCTripSmall.parquet\u0026#39;, FORMAT=\u0026#39;PARQUET\u0026#39; ) AS [result] Click Run.\nCreate data exploration database\nCREATE DATABASE DataExplorationDB COLLATE Latin1_General_100_BIN2_UTF8 Switch from master to DataExplorationDB using the following command. You can also use the UI control use database to switch your current database:\nUSE DataExplorationDB Create external data source:\nCREATE EXTERNAL DATA SOURCE YOUR_EXT_DATASOURCE WITH ( LOCATION = \u0026#39;https://YOUR_ADLS_GEN2.dfs.core.windows.net\u0026#39;) query from external data source:\nSELECT TOP 100 * FROM OPENROWSET( BULK \u0026#39;/YOUR_CONTAINER/NYCTripSmall.parquet\u0026#39;, DATA_SOURCE = \u0026#39;YOUR_EXT_DATASOURCE\u0026#39;, FORMAT=\u0026#39;PARQUET\u0026#39; ) AS [result] 3. Visualize on Power BI desktop 3.1. Prepare external table B∆∞·ªõc n√†y r·∫•t quan tr·ªçng, nhi·ªÅu l·∫ßn m√¨nh b·ªã l·ªói ·ªü Power BI v√¨ ƒë√£ ko t·∫°o External Table n√†y v·ªõi CREDENTIAL\nB∆∞·ªõc --8 c√≥ th·ªÉ s·∫Ω th·∫•y kh√≥ khi ko bi·∫øt layout c·ªßa file parquet l√† g√¨, th√¨ c√≥ th·ªÉ d√πng c√°ch n√†y:\nchu·ªôt ph·∫£i v√†o file NYCTripSmall.parquet, ch·ªçn SQL script -\u0026gt; Create external table ƒë·ªÉ xem layout table ph·∫ßn ƒë√≥\nCh√∫ √Ω ta s·∫Ω d√πng l·∫°i YOUR_EXT_DATASOURCE, YOUR_CONTAINER, YOUR_ADLS_GEN2 m√† ƒë√£ t·∫°o ·ªü tr√™n\nB·∫°n n√™n copy t·ª´ng step 1 r·ªìi run ƒë·ªÉ hi·ªÉu v·ªÅ c√°c step l√†m g√¨\n-- 1 USE DataExplorationDB CREATE MASTER KEY ENCRYPTION BY PASSWORD = \u0026#39;qwe@##@5324aSD127123g\u0026#39; GO -- 2 CREATE DATABASE SCOPED CREDENTIAL WorkspaceIdentity01 WITH IDENTITY = \u0026#39;Managed Identity\u0026#39; GO -- 3 CREATE LOGIN TestUser01 WITH PASSWORD = \u0026#39;abcdef123!@#\u0026#39; GO -- 4 CREATE USER Test01 FOR LOGIN TestUser01 GO -- 5 GRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::WorkspaceIdentity01 TO Test01 -- 6 IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = \u0026#39;YOUR_EXT_DATASOURCE\u0026#39;) CREATE EXTERNAL DATA SOURCE [YOUR_EXT_DATASOURCE] WITH ( LOCATION = \u0026#39;https://YOUR_ADLS_GEN2.dfs.core.windows.net/YOUR_CONTAINER\u0026#39;, CREDENTIAL = WorkspaceIdentity01 ) GO -- 7 IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = \u0026#39;SynapseParquetFormat\u0026#39;) CREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] WITH ( FORMAT_TYPE = PARQUET) GO -- 8 CREATE EXTERNAL TABLE NYCTripSmallExTable01 ( [DateID] int, [MedallionID] int, [HackneyLicenseID] int, [PickupTimeID] int, [DropoffTimeID] int, [PickupGeographyID] int, [DropoffGeographyID] int, [PickupLatitude] float, [PickupLongitude] float, [PickupLatLong] nvarchar(4000), [DropoffLatitude] float, [DropoffLongitude] float, [DropoffLatLong] nvarchar(4000), [PassengerCount] int, [TripDurationSeconds] int, [TripDistanceMiles] float, [PaymentType] nvarchar(4000), [FareAmount] numeric(19,4), [SurchargeAmount] numeric(19,4), [TaxAmount] numeric(19,4), [TipAmount] numeric(19,4), [TollsAmount] numeric(19,4), [TotalAmount] numeric(19,4) ) WITH ( LOCATION = \u0026#39;NYCTripSmall.parquet\u0026#39;, DATA_SOURCE = [YOUR_EXT_DATASOURCE], FILE_FORMAT = [SynapseParquetFormat] ) GO -- 9 SELECT TOP 100 * FROM NYCTripSmallExTable01 GO N·∫øu run OK h·∫øt, B·∫°n v√†o Data Hub c√≥ th·ªÉ nh√¨n th·∫•y ƒë∆∞·ª£c external table NYCTripSmallExTable01 ƒë√£ ƒë∆∞·ª£c t·∫°o ra\n3.2. Visualize on Power BI desktop M·ªü Power BI desktop ra, ch·ªçn Get Data:\nƒêi·ªÅn th√¥ng tin server SQL:\nƒêi·ªÅn account/password:\nM√†n h√¨nh data s·∫Ω hi·ªán ra b√™n ph·∫£i:\n·∫§n n√∫t Load data:\nCh·ªçn c√°c th√¥ng s·ªë d√πng ƒë·ªÉ visualize:\nCh√∫ √Ω: N·∫øu b·∫°n ƒëang ·ªü sau 1 proxy c·ªßa c√¥ng ty, b·∫°n s·∫Ω ko th·ªÉ Get Data ƒë∆∞·ª£c v√¨ l·ªói k·∫øt n·ªëi c·ªßa Power BI\nSummary Synapse cung c·∫•p 1 ch·ªó l√†m vi·ªác t·∫≠p trung cho Data Engineer\nPipeline c·ªßa n√≥ gi·ªëng nh∆∞ NiFi v·∫≠y d√πng cho m·ª•c ƒë√≠nh ETL (extract, transform, load data) automatically\nM·∫∑c ƒë·ªãnh s·∫Ω c√≥ s·∫µn SQL pool Serverless, t√≠nh ti·ªÅn theo kh·ªëi l∆∞·ª£ng data x·ª≠ l√Ω (1 TB -5 USD)\nC√≥ Apache Spark ƒë·ªÉ run code .NET, pyspark, scala cho Machine Learning. T√≠nh ti·ªÅn theo node.\nD√π b·∫°n c√≥ t·∫°o ra c√°c external table, external data source th√¨ khi b·∫°n x√≥a ch√∫ng ƒëi c≈©ng ko m·∫•t data. V√¨ th·ª±c ch·∫•t ch√∫ng ko persist data, data n·∫±m ·ªü ADLS Gen2 h·∫øt.\nC√°c tab Develop Hub, Integrate Hub c√≥ th·ªÉ export ra file json,zip ƒë·ªÉ sau n√†y import l·∫°i ƒë∆∞·ª£c (ƒëi·ªÅu n√†y c√≥ th·ªÉ quan tr·ªçng trong PaaS)\nAzure Synapse s·ª≠ d·ª•ng Synapse RBAC system (ƒë·ª´ng nh·∫ßm l·∫´n v·ªõi Azure RBAC system). Azure RBAC th√¨ b·∫°n c√≥ th·ªÉ d√πng ARM code ƒë·ªÉ add role assignment cho 1 identity n√†o ƒë√≥. Nh∆∞ng Synapse RBAC th√¨ hi·ªán t·∫°i ko ƒë∆∞·ª£c h·ªó tr·ª£ ƒë·ªÉ t·ª± ƒë·ªông add role assignment cho 1 identtiy b·∫±ng ARM. V√≠ d·ª• cho d·ªÖ hi·ªÉu:\nB·∫°n mu·ªën d√πng ARM code ƒë·ªÉ t·∫°o 1 VM V s·∫Ω c√≥ identity I, I s·∫Ω ƒë∆∞·ª£c add role assignment Synapse Administrator. R·ªìi t·ª´ ƒë√≥ trong VM V b·∫°n s·∫Ω run c√°c command az cli: az synapse ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi Synapse Studio 1 c√°ch programmatically.\nV·∫•n ƒë·ªÅ s·∫Ω n·∫£y sinh ·ªü ch·ªó vi·ªác assign role assignment Synapse Administrator cho I s·∫Ω ko th·ªÉ l√†m b·∫±ng ARM code. M√† b·∫°n ph·∫£i d√πng tay, v√†o Synapse Studio -\u0026gt; Access Control ƒë·ªÉ l√†m.\nNh∆∞ng hi·ªán ch∆∞a th·∫•y cung c·∫•p c√°c Azure ARM template ƒë·ªÉ import SQL script, Pipeline. M·ªõi ch·ªâ c√≥ az cli l√†m ƒë∆∞·ª£c th√¥i.\nAzure Synapse c√≥ th·ªÉ integrate v·ªõi Github ƒë·ªÉ l√†m Source Control. Sau khi setup, t·∫•t c·∫£ thay ƒë·ªïi c·ªßa Synapse s·∫Ω ƒë∆∞·ª£c l∆∞u l·∫°i tr√™n 1 repo tr√™n Github (script SQL, pipeline,,,).\nKhi b·∫°n x√≥a Synapse ƒëi t·∫°o l·∫°i, r·ªìi integrate n√≥ v·ªõi Github repo c≈©, n√≥ c√≥ th·ªÉ kh√¥i ph·ª•c h·∫øt c√°c SQL script, Pipeline cho b·∫°n. (Tuy nhi√™n tab Data hub s·∫Ω ko ph·ª•c h·ªìi l·∫°i ƒë∆∞·ª£c, ch·ªâ tab Develop hub v√† Integrate hub m√† th√¥i)\nCREDIT https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/tutorial-data-analyst\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/tutorial-connect-power-bi-desktop\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-on-demand\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace#place-sample-data-into-the-primary-storage-account\nhttps://www.techtalkcorner.com/create-azure-synapse-anlaytics-serverless-external-table/\n","href":"/posts/encrypt-azure-synapse-basic-data-flow-analyze-visualize-on-powerbi/","title":"Azure Synapse: Analyze data with serverless SQL and visualize it on PowerBI"},{"content":"","href":"/tags/synapse/","title":"Synapse"},{"content":"ƒê√¢y l√† b√†i t√≥m t·∫Øt c√°c step c·ªßa clip n√†y: https://www.youtube.com/watch?v=A0fMt8IRKoI\nTrang ch·ªß ƒë·ªÉ theo d√µi c√°c custom card: https://ui-lovelace-minimalist.github.io/UI/usage/custom_cards/custom_card_homeassistant_updates/\nM·ª•c ƒë√≠ch cu·ªëi c√πng l√† import ƒë∆∞·ª£c dashboard Minimalist theme v√†o HASS\nSteps Add repository: Add some required repos - browser-mod: Add some required repos - auto-entities: Add some required repos - button-cards: Add some required repos - card-mod: Add some required repos - mini-graph: Add some required repos - mini-media: Add some required repos - light-entities: Add some required repos - simple-weather: Add some required repos - my-cards: -\u0026gt; restart HASS\nAdd new integration Minimalist:\nSetting integration Minimalist:\nIntegration hi·ªán nh∆∞ n√†y l√† OK: Setting Profile ƒë·ªÉ ch·ªçn theme default:\n-\u0026gt; restart HASS\nL√∫c ƒë·∫ßu Dashboard default s·∫Ω nh∆∞ th·∫ø n√†y:\nEdit file /opt/hass/config/configuration.yaml:\nfrontend: themes: !include_dir_merge_named themes -\u0026gt; restart HASS\nGo to HASS Profile, change Theme default to minimalist-desktop\nTroubleshoot N·∫øu b·∫°n g·∫∑p l·ªói n√†y tr√™n mobile phone, m√¨nh ƒë√£ comment solution b√™n d∆∞·ªõi thread ƒë√≥ lu√¥n: https://github.com/UI-Lovelace-Minimalist/UI/issues/693\nPh·∫ßn c√≤n l·∫°i th√¨ c√°c b·∫°n xem clip ƒë·ªÉ ch·ªânh s·ª≠a dashboard sao cho ph√π h·ª£p v·ªõi nhu c·∫ßu c·ªßa m√¨nh.\nN·∫øu c√≥ 1 ng√†y c√°c b·∫°n b·ªã l·ªói n√†y tr√™n c√°i b√≥ng ƒë√®n ko th·ªÉ k√©o tƒÉng gi·∫£m ƒë·ªô s√°ng ƒë∆∞·ª£c nh∆∞ n√†y:\nCustom Element doesn't exist:my-slider Th√¨ n√™n l√†m c√°c b∆∞·ªõc sau\nDelete /opt/hass/config/custom_components/ui_lovelace_minimalist/cards/my-cards\nDelete /opt/hass/config/www/community/my-cards\nL√™n HACS, redownload l·∫°i Frontend integration n√†y My bundle cards:\nV√†o file /opt/hass/config/configuration.yaml, s·ª≠a nh∆∞ sau:\n# lovelace: # mode: storage # You need to specifiy resource url if setting lovelace.mode = yaml lovelace: mode: yaml resources: - url: /hacsfiles/button-card/button-card.js type: module - url: /hacsfiles/light-entity-card/light-entity-card.js type: module - url: /hacsfiles/lovelace-auto-entities/auto-entities.js type: module - url: /hacsfiles/lovelace-card-mod/card-mod.js type: module - url: /hacsfiles/mini-graph-card/mini-graph-card-bundle.js type: module - url: /hacsfiles/mini-media-player/mini-media-player-bundle.js type: module - url: /hacsfiles/my-cards/my-cards.js type: module - url: /hacsfiles/my-cards/null type: module - url: /hacsfiles/simple-weather-card/simple-weather-card-bundle.js type: module Ch√∫ √Ω m√¨nh ƒë√£ add th√™m:\n - url: /hacsfiles/my-cards/my-cards.js type: module - url: /hacsfiles/my-cards/null type: module -\u0026gt; restart HASS, s·∫Ω fix ƒë∆∞·ª£c l·ªói\nCREDIT https://www.youtube.com/watch?v=A0fMt8IRKoI\nhttps://ui-lovelace-minimalist.github.io/UI/usage/custom_cards/custom_card_homeassistant_updates/\n","href":"/bk/home-assistant-import-new-dashboard-minimalist/","title":"Import new dashboard to Home Assistant: Minimalist"},{"content":"ƒê√¢y l√† b√†i t√≥m t·∫Øt c√°c step c·ªßa clip n√†y: https://www.youtube.com/watch?v=A0fMt8IRKoI\nTrang ch·ªß ƒë·ªÉ theo d√µi c√°c custom card: https://ui-lovelace-minimalist.github.io/UI/usage/custom_cards/custom_card_homeassistant_updates/\nM·ª•c ƒë√≠ch cu·ªëi c√πng l√† import ƒë∆∞·ª£c dashboard Minimalist theme v√†o HASS\nSteps Add repository: Add some required repos - browser-mod: Add some required repos - auto-entities: Add some required repos - button-cards: Add some required repos - card-mod: Add some required repos - mini-graph: Add some required repos - mini-media: Add some required repos - light-entities: Add some required repos - simple-weather: Add some required repos - my-cards: -\u0026gt; restart HASS\nAdd new integration Minimalist:\nSetting integration Minimalist:\nIntegration hi·ªán nh∆∞ n√†y l√† OK: Setting Profile ƒë·ªÉ ch·ªçn theme default:\n-\u0026gt; restart HASS\nL√∫c ƒë·∫ßu Dashboard default s·∫Ω nh∆∞ th·∫ø n√†y:\nEdit file /opt/hass/config/configuration.yaml:\nfrontend: themes: !include_dir_merge_named themes -\u0026gt; restart HASS\nGo to HASS Profile, change Theme default to minimalist-desktop\nTroubleshoot N·∫øu b·∫°n g·∫∑p l·ªói n√†y tr√™n mobile phone, m√¨nh ƒë√£ comment solution b√™n d∆∞·ªõi thread ƒë√≥ lu√¥n: https://github.com/UI-Lovelace-Minimalist/UI/issues/693\nPh·∫ßn c√≤n l·∫°i th√¨ c√°c b·∫°n xem clip ƒë·ªÉ ch·ªânh s·ª≠a dashboard sao cho ph√π h·ª£p v·ªõi nhu c·∫ßu c·ªßa m√¨nh.\nN·∫øu c√≥ 1 ng√†y c√°c b·∫°n b·ªã l·ªói n√†y tr√™n c√°i b√≥ng ƒë√®n ko th·ªÉ k√©o tƒÉng gi·∫£m ƒë·ªô s√°ng ƒë∆∞·ª£c nh∆∞ n√†y:\nCustom Element doesn't exist:my-slider Th√¨ n√™n l√†m c√°c b∆∞·ªõc sau\nDelete /opt/hass/config/custom_components/ui_lovelace_minimalist/cards/my-cards\nDelete /opt/hass/config/www/community/my-cards\nL√™n HACS, redownload l·∫°i Frontend integration n√†y My bundle cards:\nV√†o file /opt/hass/config/configuration.yaml, s·ª≠a nh∆∞ sau:\n# lovelace: # mode: storage # You need to specifiy resource url if setting lovelace.mode = yaml lovelace: mode: yaml resources: - url: /hacsfiles/button-card/button-card.js type: module - url: /hacsfiles/light-entity-card/light-entity-card.js type: module - url: /hacsfiles/lovelace-auto-entities/auto-entities.js type: module - url: /hacsfiles/lovelace-card-mod/card-mod.js type: module - url: /hacsfiles/mini-graph-card/mini-graph-card-bundle.js type: module - url: /hacsfiles/mini-media-player/mini-media-player-bundle.js type: module - url: /hacsfiles/my-cards/my-cards.js type: module - url: /hacsfiles/my-cards/null type: module - url: /hacsfiles/simple-weather-card/simple-weather-card-bundle.js type: module Ch√∫ √Ω m√¨nh ƒë√£ add th√™m:\n - url: /hacsfiles/my-cards/my-cards.js type: module - url: /hacsfiles/my-cards/null type: module -\u0026gt; restart HASS, s·∫Ω fix ƒë∆∞·ª£c l·ªói\nCREDIT https://www.youtube.com/watch?v=A0fMt8IRKoI\nhttps://ui-lovelace-minimalist.github.io/UI/usage/custom_cards/custom_card_homeassistant_updates/\n","href":"/posts/home-assistant-import-new-dashboard-minimalist/","title":"Import new dashboard to Home Assistant: Minimalist"},{"content":"","href":"/tags/letsencrypt/","title":"Letsencrypt"},{"content":"","href":"/tags/nginx/","title":"Nginx"},{"content":"ƒê√¢y l√† ph·∫ßn ti·∫øp theo b√†i n√†y: Setup Home Assistant on Raspberry Pi (Part 2) - DMZ and iptables rule)\n1. Access from external network ƒê·ªçc k·ªπ b√†i sau ƒë·ªÉ bi·∫øt v·ªÅ c√°c parameter c√≥ th·ªÉ s·ª≠ d·ª•ng: https://github.com/linuxserver/docker-swag\nTr∆∞·ªõc ƒë√≥ h√£y ch·∫Øc ch·∫Øn m√¨nh ƒë√£ m·ªü port 80,443 tr√™n iptables DOCKER-USER Chain nh√©\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 67922 4074K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 456 40283 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 28386 20M ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 223 11872 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 572K 404M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 S·ª≠a file docker-compose.yml:\n... swag: image: lscr.io/linuxserver/swag:1.30.0 container_name: swag cap_add: - NET_ADMIN environment: - PUID=1000 - PGID=1000 - TZ=Asia/Ho_Chi_Minh - URL=MYDOMAIN.duckdns.org - VALIDATION=duckdns - SUBDOMAINS=wildcard, #optional - CERTPROVIDER= #optional - DNSPLUGIN=cloudflare #optional - PROPAGATION= #optional - DUCKDNSTOKEN=MY-DUCKDNS-TOKENnn #optional - EMAIL=MY_EMAIL@gmail.com #optional - ONLY_SUBDOMAINS=false #optional - EXTRA_DOMAINS= #optional - STAGING=true #optional volumes: - /opt/swag/config:/config ports: - 443:443 - 80:80 #optional restart: unless-stopped Ch√∫ √Ω:\n Ch·ªó STAGING=true, m√¨nh s·∫Ω test v·ªõi staging certificate tr∆∞·ªõc, OK r·ªìi th√¨ m·ªõi chuy·ªÉn sang STAGING=false (production certificate) /opt/swag/config ƒë√£ ƒë∆∞·ª£c m√¨nh t·∫°o t·ª´ tr∆∞·ªõc tr√™n Server RPi. MY_EMAIL@gmail.com h√£y d√πng email chu·∫©n ƒë·ªÉ sau n√†y nh·∫≠n th√¥ng b√°o. Khi l√†m th·∫ø n√†y ch·ªâ domain MYDOMAIN.duckdns.org l√† c√≥ https secure (https v√† c√≥ valid certificate) th√¥i, c√°c subdomain t·∫°o nh∆∞ test.MYDOMAIN.duckdns.org s·∫Ω ko c√≥ https secure (https nh∆∞ng ko c√≥ valid certificate)  S·ª≠a file HASS configuration /opt/hass/config/configuration.yaml (b∆∞·ªõc n√†y th·ª±c ra m√¨nh ko bi·∫øt n√≥ ·∫£nh h∆∞·ªüng nh∆∞ n√†o):\nhttp: use_x_forwarded_for: true trusted_proxies: - 192.168.1.0/24  # Local Lan - 172.18.0.0/24  # Docker network Run docker-compose up -d\nCheck log c·ªßa swag container ko c√≥ l·ªói l√† ok:\nKhi b·∫°n truy c·∫≠p HTTP http://MYDOMAIN.duckdns.org/ n√≥ s·∫Ω t·ª± ƒë·ªông redirect ƒë·∫øn HTTPS https://MYDOMAIN.duckdns.org/:\nGi·ªù t√¨m ƒë·∫øn file /opt/swag/config/nginx/proxy-confs/homeassistant.subdomain.conf.sample. ƒê√¢y l√† 1 file sample ƒë∆∞·ª£c c·ªông ƒë·ªìng ƒë√≥ng g√≥p v√† Swag ƒë√£ include n√≥ v√†o s·∫µn trong image.\nRename file n√†y th√†nh:/opt/swag/config/nginx/proxy-confs/homeassistant.subdomain.conf\nS·ª≠a n·ªôi dung file 1 ch√∫t:\n set $upstream_app homeassistant; -\u0026gt; s·ª≠a th√†nh: set $upstream_app 192.168.X.XXX; (192.168.X.XXX l√† local IP c·ªßa HASS) Ch·ªó server_name th√¨ t√πy √Ω m√† thay ƒë·ªïi:  server_name MYDOMAIN.duckdns.org; # when you want to access your HASS as `https://MYDOMAIN.duckdns.org` # server_name hass.*; # when you want to access your HASS as `https://hass.MYDOMAIN.duckdns.org` File c·ªßa m√¨nh:\n## Version 2021/10/11 # make sure that your dns has a cname set for homeassistant and that your homeassistant container is not using a base url # As of homeassistant 2021.7.0, it is now required to define the network range your proxy resides in, this is done in Homeassitants configuration.yaml # https://www.home-assistant.io/integrations/http/#trusted_proxies # Example below uses the default dockernetwork ranges, you may need to update this if you dont use defaults. # # http: # use_x_forwarded_for: true # trusted_proxies: # - 172.16.0.0/12 server { listen 443 ssl; listen [::]:443 ssl; server_name hehe.duckdns.org; # when you want to access your HA as `https://hehe.duckdns.org` # server_name hass.*; # when you want to access your HA as `https://hass.hehe.duckdns.org` include /config/nginx/ssl.conf; client_max_body_size 0; # enable for ldap auth, fill in ldap details in ldap.conf #include /config/nginx/ldap.conf; # enable for Authelia #include /config/nginx/authelia-server.conf; location / { # enable the next two lines for http auth #auth_basic \u0026quot;Restricted\u0026quot;; #auth_basic_user_file /config/nginx/.htpasswd; # enable the next two lines for ldap auth #auth_request /auth; #error_page 401 =200 /ldaplogin; # enable for Authelia #include /config/nginx/authelia-location.conf; include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app 192.168.1.128; set $upstream_port 8123; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; } location ~ ^/(api|local|media)/ { include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app 192.168.1.128; set $upstream_port 8123; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; } } Sau ƒë√≥ restart container swag, check log ko c√≥ l·ªói g√¨ l√† OK (M·ªói l·∫ßn thay ƒë·ªïi config trong folder swag ƒë·ªÅu n√™n restart container ƒë·ªÉ apply)\nGi·ªù test:\nT·ª´ m·∫°ng 4G -\u0026gt; http://MYDOMAIN.duckdns.org -\u0026gt; t·ª± redirect sang https://MYDOMAIN.duckdns.org v√† access ƒë∆∞·ª£c HASS\nGi·ªù s·ª≠a file docker-compose.yml: chuy·ªÉn sang STAGING=false ƒë·ªÉ s·ª≠ d·ª•ng production certificate.\nRun docker-compose up -d\nCheck log swag nh∆∞ n√†y l√† OK, ƒë√£ request certificate th√†nh c√¥ng:\nTest truy c·∫≠p t·ª´ m·∫°ng 4G -\u0026gt; http://MYDOMAIN.duckdns.org -\u0026gt; t·ª± redirect sang https://MYDOMAIN.duckdns.org v√† access ƒë∆∞·ª£c HASS m√† khi b·∫°n ·∫•n v√†o c√°i ·ªï kh√≥a tr√™n Browser s·∫Ω th·∫•y m√†u xanh HTTPS\nV·∫≠y l√† th√†nh c√¥ng r·ªìi.\n1.1. N·∫øu mu·ªën disalbe t√≠nh nƒÉng redirect http-\u0026gt;https Gi·ªù v√≠ d·ª• m√¨nh ko mu·ªën n√≥ t·ª´ redirect http sang https th√¨ c·∫ßn s·ª≠a file:\n/opt/swag/config/nginx/site-confs/default, comment ph·∫ßn n√†y l·∫°i:\n# redirect all traffic to https #server { # listen 80 default_server; # listen [::]:80 default_server; # server_name _; # return 301 https://$host$request_uri; #} restart swag container, check l·∫°i b·∫±ng c√°ch t·ª´ m·∫°ng local c≈©ng dc:\ntr∆∞·ªõc khi s·ª≠a n·∫øu b·∫°n run wget http://MYDOMAIN.duckdns.org s·∫Ω nh·∫≠n ƒë∆∞·ª£c th√¥ng b√°o ki·ªÉu 301 Moved Permanently\u0026hellip;\nR·ªìi l·ªói ki·ªÉu port 443 b·ªã refused.\nwget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 20:18:09-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 15.xx.xx.32 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.32|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://MYDOMAIN.duckdns.org/ [following] --2022-09-21 20:18:09-- https://MYDOMAIN.duckdns.org/ Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.32|:443... failed: Connection refused. Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 15.xx.xx.32 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.32|:443... failed: Connection refused Ch√∫ √Ω ch·ªó :80... connected. ch·ª©ng t·ªè traffic ƒë·∫øn port 80 c·ªßa proxy swag r·ªìi xong n√≥ ƒë∆∞·ª£c redirect sang https lu√¥n (Ch√∫ √Ω ch·ªó n√†y swag container c·ªßa m√¨nh ƒëang run tr√™n c·∫£ 2 port 80 v√† 443 nh√©)\nSau khi s·ª≠a:\n$ wget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 14:26:50-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 15.xx.xx.3x Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.3x|:80... failed: Connection refused. v·∫´n l·ªói l√† do port 80 ƒëang b·ªã refused, c√≥ th·ªÉ do ko c√≥ service n√†o ch·∫°y tr√™n port ƒë√≥.\nTuy nhi√™n m√¨nh khuy√™n b·∫°n ko n√™n s·ª≠a, vi·ªác redirect http-\u0026gt;https l√† t·ªët cho ng∆∞·ªùi d√πng.\n2. Access from LAN network Gi·ªù C√≥ 1 v·∫•n ƒë·ªÅ nh·ªè l√† t·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org -\u0026gt; th√¨ s·∫Ω ko v√†o dc HASS üò´ (n√≥ ra c√°i Router dashboard nh√† m·∫°ng)\nC·ª© t∆∞·ªüng do trong ROuter dashboard ƒëang d√πng port 80 n√™n n√≥ t·ª± redirect v·ªÅ, nh∆∞ng sau khi ƒë·ªïi port 80 c·ªßa Router Dashboard th√†nh 8080. Th√¨ gi·ªù t·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org v·∫´n ko v√†o ƒë∆∞·ª£c HASS\nTr√™n Router add 1 rule v√†o Port Forwarding port 80 c·ªßa host -\u0026gt; port 80 c·ªßa RPi Server.\nM·∫∑c d√π t·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org:8123 th√¨ OK. -\u0026gt; ch·ª©ng t·ªè Router c√≥ activate NAT Loopback tr√™n port 8123\nTi·∫øp th·ª≠ expose grafana ra port 80:3000 th√¨ c≈©ng OK. T·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org (ƒë√£ disabled redirect https) -\u0026gt; s·∫Ω v√†o Grafana\nCh·ª©ng t·ªè NAT Loopback c√≥ active v·ªõi port 80. Ch·ªâ ko l√†m dc v·ªõi port 443.\nNh·ªù c√≥ l·ªói 301 Moved Permanently b√™n tr√™n m√† m√¨nh hi·ªÉu r·∫±ng mu·ªën t·ª´ m·∫°ng local k·∫øt n·ªëi ƒë·∫øn HASS ƒëang ·ªü port 8123 th√¨ c·∫ßn port forwarding\n2.1. D√πng Nginx S·ª≠a file /opt/swag/config/nginx/site-confs/default, comment h·∫øt ph·∫ßn redirect l·∫°i (th·ª±c ra ƒë√£ l√†m ·ªü ph·∫ßn 1.1):\n# redirect all traffic to https #server { # listen 80 default_server; # listen [::]:80 default_server; # server_name _; # return 301 https://$host$request_uri; #} S·ª≠a ƒëo·∫°n ƒë·∫ßu file /opt/swag/config/nginx/proxy-confs/homeassistant.subdomain.conf, add listen 80; v√†o:\n~~~ server { listen 80; listen 443 ssl; listen [::]:443 ssl; server_name MYDOMAIN.duckdns.org; ~~~ Nh∆∞ v·∫≠y m√¨nh ra l·ªánh cho Nginx r·∫±ng n√≥ ph·∫£i listen c·∫£ port 80 n·ªØa.\nKhi c√≥ request ƒë·∫øn port 80 n√≥ s·∫Ω ko redirect sang https n·ªØa m√† forward ƒë·∫øn HASS (port 8123) lu√¥n.\n-\u0026gt; restart swag container.\nTest:\nT·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org (tab ·∫©n danh c·ªßa Chrome) -\u0026gt; access ƒë∆∞·ª£c HASS, ch·ª©ng t·ªè traffic ƒë·∫øn port 80 t·ª´ trong LAN ƒë√£ dc forward ƒë·∫øn port 8123\nVi·ªác n√†y ph·∫£i ch·∫°y tr√™n tab ·∫©n danh, ch·ª© n·∫øu ch·∫°y tab th√¥ng th∆∞·ªùng th√¨ http s·∫Ω b·ªã redirect v·ªÅ https v√† ko access ƒë∆∞·ª£c.\nTest b·∫±ng c√°ch call t·ª´ LAN run command:\nwget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 16:31:59-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 1x.x.x.xx3 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|1x.x.x.xx3|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 9331 (9.1K) [text/html] Saving to: ‚Äòindex.html.4‚Äô index.html.4 100%[=================================================\u0026gt;] 9.11K --.-KB/s in 0.007s 2022-09-21 16:31:59 (1.27 MB/s) - ‚Äòindex.html.4‚Äô saved [9331/9331] T·∫°m ch·∫•p nh·∫≠n. Tuy nhi√™n c√°ch n√†y c√≥ v·∫•n ƒë·ªÅ:\n T·ª´ m·∫°ng LAN v√†o http://MYDOMAIN.duckdns.org s·∫Ω ph·∫£i d√πng tab ·∫©n danh, v√¨ n·∫øu ko tr√¨nh duy·ªát s·∫Ω t·ª± ƒë·ªông redirect sang https v√† ko v√†o dc T·ª´ m·∫°ng external v√†o c·∫£ 2 link ƒë·ªÅu ƒë∆∞·ª£c http://MYDOMAIN.duckdns.org v√† https://MYDOMAIN.duckdns.org, ƒëi·ªÅu n√†y ko t·ªët v·ªÅ security  2.2. D√πng iptables C√°ch n√†y ƒë∆∞·ª£c m√¨nh th·ª≠ ƒë·∫ßu ti√™n, nh∆∞ng x·∫øp m·ª•c 2.2 v√¨ m√¨nh ko ∆∞u ti√™n l·∫Øm.\nDo ban ƒë·∫ßu t∆∞·ªüng Nginx ko l√†m ƒë∆∞·ª£c vi·ªác port forwarding HASS n√™n m√¨nh ƒë√£ th·ª≠ c√°ch 2 n√†y, c·ªë g·∫Øng port forward b·∫±ng iptables:\nhttps://serverfault.com/questions/140622/how-can-i-port-forward-with-iptables\nNh·∫±m forward traffic t·ª´ port 80 c·ªßa host v·ªÅ port 8123 c·ªßa HASS:\n# forward traffic t·ª´ LAN (192.168.1.0/24) port 80 s·∫Ω ƒë·∫øn ip 192.168.1.128:8123 iptables -t nat -A PREROUTING -p tcp -i wlan0 -s 192.168.1.0/24 --dport 80 -j DNAT --to-destination 192.168.1.128:8123 # show iptables -L -n -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL DNAT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:80 to:192.168.1.128:8123 Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 MASQUERADE all -- 172.18.0.0/16 0.0.0.0/0 MASQUERADE tcp -- 172.18.0.2 172.18.0.2 tcp dpt:9000 MASQUERADE tcp -- 172.18.0.4 172.18.0.4 tcp dpt:8080 MASQUERADE udp -- 172.18.0.5 172.18.0.5 udp dpt:51820 MASQUERADE tcp -- 172.18.0.6 172.18.0.6 tcp dpt:9001 MASQUERADE tcp -- 172.18.0.6 172.18.0.6 tcp dpt:1883 MASQUERADE tcp -- 172.18.0.7 172.18.0.7 tcp dpt:8200 MASQUERADE tcp -- 172.18.0.9 172.18.0.9 tcp dpt:9090 MASQUERADE tcp -- 172.18.0.10 172.18.0.10 tcp dpt:3000 MASQUERADE tcp -- 172.18.0.8 172.18.0.8 tcp dpt:443 Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9000 to:172.18.0.2:9000 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.18.0.4:8080 DNAT udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:51820 to:172.18.0.5:51820 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9001 to:172.18.0.6:9001 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:1883 to:172.18.0.6:1883 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8200 to:172.18.0.7:8200 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9090 to:172.18.0.9:9090 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3000 to:172.18.0.10:3000 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 to:172.18.0.8:443 T·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org (tab ·∫©n danh c·ªßa Chrome) -\u0026gt; access ƒë∆∞·ª£c HASS, ch·ª©ng t·ªè traffic ƒë·∫øn port 80 t·ª´ trong LAN ƒë√£ dc forward ƒë·∫øn port 8123\nVi·ªác n√†y ph·∫£i ch·∫°y tr√™n tab ·∫©n danh, ch·ª© n·∫øu ch·∫°y tab th√¥ng th∆∞·ªùng th√¨ http s·∫Ω b·ªã redirect v·ªÅ https v√† ko access ƒë∆∞·ª£c.\ntest b·∫±ng c√°ch call t·ª´ LAN run command:\nwget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 16:31:59-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 1x.x.x.xx3 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|1x.x.x.xx3|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 9331 (9.1K) [text/html] Saving to: ‚Äòindex.html.4‚Äô index.html.4 100%[=================================================\u0026gt;] 9.11K --.-KB/s in 0.007s 2022-09-21 16:31:59 (1.27 MB/s) - ‚Äòindex.html.4‚Äô saved [9331/9331] Command ti·∫øp theo m√¨nh th·∫•y ko c·∫ßn l·∫Øm, v√¨ ngay sau khi t·∫°o DNAT rule b√™n tr√™n, m·ªçi th·ª© c√≥ v·∫ª ƒë√£ OK. (nh∆∞ng tr√™n Stackoverflow h·ªç v·∫´n c√≥):\niptables -A FORWARD -p tcp -d 192.168.1.128 --dport 8123 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT m√† m√¨nh x√≥a rule n√†y ƒëi th√¨ c≈©ng ko sao.\nNh∆∞ v·∫≠y C√°ch n√†y (d√πng Iptables) l√† \u0026ldquo;ph·∫ßn n√†o\u0026rdquo; ch·∫•p nh·∫≠n ƒë∆∞·ª£c, n√≥ gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ c·ªßa C√°ch Nginx l√†: iptable rule ch·∫∑n external network connect ƒë·∫øn port 80 (link http://MYDOMAIN.duckdns.org) ho·∫∑c n√≥ s·∫Ω redirect sang https lu√¥n.\nNh∆∞ng c√°ch n√†y v·∫´n c√≤n v·∫•n ƒë·ªÅ:\n T·ª´ m·∫°ng LAN v√†o http://MYDOMAIN.duckdns.org s·∫Ω ph·∫£i d√πng tab ·∫©n danh, v√¨ n·∫øu ko tr√¨nh duy·ªát s·∫Ω t·ª± ƒë·ªông redirect sang https v√† ko v√†o dc.  M√¨nh nghƒ©, n·∫øu c√≥ c√°ch n√†o ƒë√≥ ƒë·ªÉ setup tr√™n Nginx ƒë·ªÉ ko redirect http-\u0026gt;https khi d√πng m·∫°ng LAN th√¨ t·ªët. C√°i n√†y m√¨nh ƒëang t√¨m hi·ªÉu..\n2.3. K·∫øt lu·∫≠n T√≥m l·∫°i d√π c√°ch 2 (d√πng Iptables) gi·∫£i quy·∫øt dc 1 v·∫•n ƒë·ªÅ c·ªßa c√°ch 1 (d√πng Nginx), m√¨nh v·∫´n ko recommend vi·ªác s·ª≠a iptables ƒë·ªÉ forward traffic l·∫Øm. B·ªüi v√¨ ƒë√£ t·ª´ng c√≥ l·∫ßn m√¨nh reset l·∫°i server RPi, h·∫ßu nh∆∞ t·∫•t c·∫£ c√°c rule tr√™n iptables ƒë·ªÅu b·ªã x√≥a h·∫øt. M√† RPi c·ªßa m√¨nh l·∫°i ƒëang ·ªü trong DMZ, g√¢y risk v·ªÅ security.\nT√¨m ƒë∆∞·ª£c b√†i n√†y v·ªÅ vi·ªác save v√† reload iptables rule on reboots: https://askubuntu.com/questions/119393/how-to-save-rules-of-the-iptables\nsudo netfilter-persistent save sudo netfilter-persistent reload Tuy nhi√™n ch∆∞a th·ª≠ restart xem c√≥ m·∫•t rule ko, ho·∫∑c c√≥ c·∫ßn run command ƒë·ªÉ reload rule hay ko?\nV·∫≠y n√™n m√¨nh v·∫´n d√πng c√°ch 1 (Nginx), gi·ªØ 2 link ƒë·ªÉ d√πng:\n1: http://MYDOMAIN.duckdns.org redirect https://MYDOMAIN.duckdns.org -\u0026gt; ƒë·ªÉ integrate v·ªõi Alexa\n2: http://MYDOMAIN.duckdns.org:8123 ƒë·ªÉ gia ƒë√¨nh s·ª≠ d·ª•ng (link n√†y th√¨ do c√≥ NAT loopback tr√™n port 8123 n√™n t·ª´ m·∫°ng 4G hay LAN ƒë·ªÅu access ƒë∆∞·ª£c h·∫øt)\nNgo√†i ra: M√¨nh t·ª± h·ªèi l√† n·∫øu kh√¥ng th·ªÉ Nat Loopback ƒë∆∞·ª£c port 443 ƒë∆∞·ª£c th√¨ vi·ªác m√¨nh ƒë∆∞a RPi ra DMZ c√≥ √Ω nghƒ©a g√¨? C√≥ l·∫Ω th·ª≠ disable DMZ ƒëi xem sao?\n-\u0026gt; Th√¨ ngay sau khi disable DMZ, t·ª´ External network s·∫Ω kh√¥ng th·ªÉ connect ƒë·∫øn https://MYDOMAIN.duckdns.org v√† http://MYDOMAIN.duckdns.org. C√≤n connect ƒë·∫øn http://MYDOMAIN.duckdns.org:8123 v·∫´n ok.\nNh∆∞ v·∫≠y l√† v·ª´a ph·∫£i ƒë∆∞a ra DMZ, v·ª´a ph·∫£i k·∫øt h·ª£p Port Forwarding tr√™n Router.\nM√† ƒë√£ ƒë∆∞a ra DMZ l√† ph·∫£i k·∫øt h·ª£p th√™m c√°c rule tr√™n iptables ƒë·ªÉ filter traffic ƒëi v√†o nh√© (ƒë√£ l√†m ·ªü b√†i tr∆∞·ªõc Part2 ).\n3. N·∫øu mu·ªën expose th√™m 1 service n√†o ƒë√≥ ƒë·ªÉ test Gi·∫£ d·ª• mu·ªën expose th√™m 1 web Grafana ra ngo√†i, ki·ªÉu https://grafana.MYDOMAIN.duckdns.org (tuy nhi√™n s·∫Ω ko c√≥ Browser secured v√¨ cert ch·ªâ valid v·ªõi domain https://MYDOMAIN.duckdns.org th√¥i )\nGi·∫£ s·ª≠ grafana ƒëang run trong Docker container port 3000, container name l√† grafana\nTrong swag config folder s·ª≠a file /opt/swag/config/nginx/proxy-confs/grafana.subdomain.conf.sample, rename th√†nh /opt/swag/config/nginx/proxy-confs/grafana.subdomain.conf v√† s·ª≠a n·ªôi dung file ƒë√≥:\nth·ª±c ra h·∫ßu nh∆∞ ko c·∫ßn s·ª≠a, ch·ªâ c·∫ßn check l·∫°i xem ƒë√∫ng v·ªõi case c·ªßa m√¨nh ch∆∞a th√¥i -\u0026gt; restart swag container.\nGi·ªù s·∫Ω access t·ª´ m·∫°ng 4G v√†o https://grafana.MYDOMAIN.duckdns.org l√† s·∫Ω ƒë∆∞·ª£c. T·∫•t nhi√™n access t·ª´ m·∫°ng LAN s·∫Ω ko ƒëc ƒë√¢u nh√©. M·∫°ng LAN th√¨ v·∫´n d√πng IP th√¥i üò´\nƒê√¢y l√† s·ª± ti·ªán l·ª£i c·ªßa NGINX, b·∫°n ch·ªâ c·∫ßn s·ª≠a trong nginx m√† ko c·∫ßn s·ª≠a g√¨ b√™n ngo√†i Router, iptables c·∫£\n4. About fail2ban Trong swag container ƒë√£ t√≠ch h·ª£p s·∫µn t√≠nh nƒÉng fai2ban:\nhttps://www.fail2ban.org/wiki/index.php/Commands\nThis container includes fail2ban set up with 5 jails by default:\n nginx-http-auth nginx-badbots nginx-botsearch nginx-deny nginx-unauthorized  To enable or disable other jails, modify the file /config/fail2ban/jail.local\nTo modify filters and actions, instead of editing the .conf files, create .local files with the same name and edit those because .conf files get overwritten when the actions and filters are updated. .local files will append whatever\u0026rsquo;s in the .conf files (ie. nginx-http-auth.conf \u0026ndash;\u0026gt; nginx-http-auth.local)\nYou can check which jails are active via:\ndocker exec -it swag fail2ban-client status You can check the status of a specific jail via:\ndocker exec -it swag fail2ban-client status \u0026lt;jail name\u0026gt; You can unban an IP via:\ndocker exec -it swag fail2ban-client set \u0026lt;jail name\u0026gt; unbanip \u0026lt;IP\u0026gt; A list of commands can be found here: https://www.fail2ban.org/wiki/index.php/Commands\nCREDIT 1 v√≠ d·ª• v·ªÅ vi·ªác d√πng Nginx l√†m reverse proxy cho 2 app trong Docker:\nhttps://www.bogotobogo.com/DevOps/Docker/Docker-Compose-Nginx-Reverse-Proxy-Multiple-Containers.php\n1 b√†i vi·∫øt v·ªÅ d√πng Swag v·ªõi Hass ƒë·ªÉ t·∫°o https auto-renewal cert. Swag c√≥ s·∫µn Nginx:\nhttps://community.home-assistant.io/t/nginx-reverse-proxy-set-up-guide-docker/54802\nhttps://github.com/linuxserver/docker-swag\nhttps://pentacent.medium.com/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71 https://hub.docker.com/r/linuxserver/swag\n1 b√†i v·ªÅ s·ª≠ d·ª•ng Traefik l√†m reverse proxy thay cho Nginx:\nhttps://community.home-assistant.io/t/help-multiple-containers-nextcloud-ha-letsencrypt/46725/5\nhttps://thesmarthomejourney.com/2021/11/08/traefik-1-reverse-proxy-setup/\nhttps://thesmarthomejourney.com/2022/01/26/traefik-force-cert-renewal/#Traefik_and_lets_encrypt\nhttps://community.home-assistant.io/t/help-with-ha-in-a-docker-container-and-lets-encrypt/51595\n1 ch√∫ √Ω khi setting hass with certificate: https://community.home-assistant.io/t/help-with-ha-in-a-docker-container-and-lets-encrypt/51595/8\nhttps://community.home-assistant.io/t/configure-ssl-with-docker/196878\n2 b√†i v·ªÅ d√πng nginx v√† lesencript docker m√† ko ph·∫£i l√† swag:\nhttps://pentacent.medium.com/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71\nGithub: https://github.com/wmnnd/nginx-certbot\ntuy nhi√™n c√≥ v·∫ª ƒë√£ l√¢u ko update g√¨ (t·ª´ 2020)\nb√†i n√†y th·∫•y vi·∫øt d·ªÖ hi·ªÉu v√† kh√° clear:\nhttps://evgeniy-khyst.com/letsencrypt-docker-compose/\nhttps://github.com/evgeniy-khist/letsencrypt-docker-compose\nhttps://community.home-assistant.io/t/remote-access-with-docker/314345\nhttps://serverfault.com/questions/140622/how-can-i-port-forward-with-iptables\nhttps://askubuntu.com/questions/119393/how-to-save-rules-of-the-iptables\n","href":"/bk/encrypt-setup-home-assistant-on-raspberry-pi-p3-https/","title":"Setup Home Assistant on Raspberry Pi (Part 3) - Https"},{"content":"ƒê√¢y l√† ph·∫ßn ti·∫øp theo b√†i n√†y: Setup Home Assistant on Raspberry Pi (Part 2) - DMZ and iptables rule)\n1. Access from external network ƒê·ªçc k·ªπ b√†i sau ƒë·ªÉ bi·∫øt v·ªÅ c√°c parameter c√≥ th·ªÉ s·ª≠ d·ª•ng: https://github.com/linuxserver/docker-swag\nTr∆∞·ªõc ƒë√≥ h√£y ch·∫Øc ch·∫Øn m√¨nh ƒë√£ m·ªü port 80,443 tr√™n iptables DOCKER-USER Chain nh√©\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 67922 4074K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 456 40283 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 28386 20M ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 223 11872 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 572K 404M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 S·ª≠a file docker-compose.yml:\n... swag: image: lscr.io/linuxserver/swag:1.30.0 container_name: swag cap_add: - NET_ADMIN environment: - PUID=1000 - PGID=1000 - TZ=Asia/Ho_Chi_Minh - URL=MYDOMAIN.duckdns.org - VALIDATION=duckdns - SUBDOMAINS=wildcard, #optional - CERTPROVIDER= #optional - DNSPLUGIN=cloudflare #optional - PROPAGATION= #optional - DUCKDNSTOKEN=MY-DUCKDNS-TOKENnn #optional - EMAIL=MY_EMAIL@gmail.com #optional - ONLY_SUBDOMAINS=false #optional - EXTRA_DOMAINS= #optional - STAGING=true #optional volumes: - /opt/swag/config:/config ports: - 443:443 - 80:80 #optional restart: unless-stopped Ch√∫ √Ω:\n Ch·ªó STAGING=true, m√¨nh s·∫Ω test v·ªõi staging certificate tr∆∞·ªõc, OK r·ªìi th√¨ m·ªõi chuy·ªÉn sang STAGING=false (production certificate) /opt/swag/config ƒë√£ ƒë∆∞·ª£c m√¨nh t·∫°o t·ª´ tr∆∞·ªõc tr√™n Server RPi. MY_EMAIL@gmail.com h√£y d√πng email chu·∫©n ƒë·ªÉ sau n√†y nh·∫≠n th√¥ng b√°o. Khi l√†m th·∫ø n√†y ch·ªâ domain MYDOMAIN.duckdns.org l√† c√≥ https secure (https v√† c√≥ valid certificate) th√¥i, c√°c subdomain t·∫°o nh∆∞ test.MYDOMAIN.duckdns.org s·∫Ω ko c√≥ https secure (https nh∆∞ng ko c√≥ valid certificate)  S·ª≠a file HASS configuration /opt/hass/config/configuration.yaml (b∆∞·ªõc n√†y th·ª±c ra m√¨nh ko bi·∫øt n√≥ ·∫£nh h∆∞·ªüng nh∆∞ n√†o):\nhttp: use_x_forwarded_for: true trusted_proxies: - 192.168.1.0/24  # Local Lan - 172.18.0.0/24  # Docker network Run docker-compose up -d\nCheck log c·ªßa swag container ko c√≥ l·ªói l√† ok:\nKhi b·∫°n truy c·∫≠p HTTP http://MYDOMAIN.duckdns.org/ n√≥ s·∫Ω t·ª± ƒë·ªông redirect ƒë·∫øn HTTPS https://MYDOMAIN.duckdns.org/:\nGi·ªù t√¨m ƒë·∫øn file /opt/swag/config/nginx/proxy-confs/homeassistant.subdomain.conf.sample. ƒê√¢y l√† 1 file sample ƒë∆∞·ª£c c·ªông ƒë·ªìng ƒë√≥ng g√≥p v√† Swag ƒë√£ include n√≥ v√†o s·∫µn trong image.\nRename file n√†y th√†nh:/opt/swag/config/nginx/proxy-confs/homeassistant.subdomain.conf\nS·ª≠a n·ªôi dung file 1 ch√∫t:\n set $upstream_app homeassistant; -\u0026gt; s·ª≠a th√†nh: set $upstream_app 192.168.X.XXX; (192.168.X.XXX l√† local IP c·ªßa HASS) Ch·ªó server_name th√¨ t√πy √Ω m√† thay ƒë·ªïi:  server_name MYDOMAIN.duckdns.org; # when you want to access your HASS as `https://MYDOMAIN.duckdns.org` # server_name hass.*; # when you want to access your HASS as `https://hass.MYDOMAIN.duckdns.org` File c·ªßa m√¨nh:\n## Version 2021/10/11 # make sure that your dns has a cname set for homeassistant and that your homeassistant container is not using a base url # As of homeassistant 2021.7.0, it is now required to define the network range your proxy resides in, this is done in Homeassitants configuration.yaml # https://www.home-assistant.io/integrations/http/#trusted_proxies # Example below uses the default dockernetwork ranges, you may need to update this if you dont use defaults. # # http: # use_x_forwarded_for: true # trusted_proxies: # - 172.16.0.0/12 server { listen 443 ssl; listen [::]:443 ssl; server_name hehe.duckdns.org; # when you want to access your HA as `https://hehe.duckdns.org` # server_name hass.*; # when you want to access your HA as `https://hass.hehe.duckdns.org` include /config/nginx/ssl.conf; client_max_body_size 0; # enable for ldap auth, fill in ldap details in ldap.conf #include /config/nginx/ldap.conf; # enable for Authelia #include /config/nginx/authelia-server.conf; location / { # enable the next two lines for http auth #auth_basic \u0026quot;Restricted\u0026quot;; #auth_basic_user_file /config/nginx/.htpasswd; # enable the next two lines for ldap auth #auth_request /auth; #error_page 401 =200 /ldaplogin; # enable for Authelia #include /config/nginx/authelia-location.conf; include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app 192.168.1.128; set $upstream_port 8123; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; } location ~ ^/(api|local|media)/ { include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app 192.168.1.128; set $upstream_port 8123; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; } } Sau ƒë√≥ restart container swag, check log ko c√≥ l·ªói g√¨ l√† OK (M·ªói l·∫ßn thay ƒë·ªïi config trong folder swag ƒë·ªÅu n√™n restart container ƒë·ªÉ apply)\nGi·ªù test:\nT·ª´ m·∫°ng 4G -\u0026gt; http://MYDOMAIN.duckdns.org -\u0026gt; t·ª± redirect sang https://MYDOMAIN.duckdns.org v√† access ƒë∆∞·ª£c HASS\nGi·ªù s·ª≠a file docker-compose.yml: chuy·ªÉn sang STAGING=false ƒë·ªÉ s·ª≠ d·ª•ng production certificate.\nRun docker-compose up -d\nCheck log swag nh∆∞ n√†y l√† OK, ƒë√£ request certificate th√†nh c√¥ng:\nTest truy c·∫≠p t·ª´ m·∫°ng 4G -\u0026gt; http://MYDOMAIN.duckdns.org -\u0026gt; t·ª± redirect sang https://MYDOMAIN.duckdns.org v√† access ƒë∆∞·ª£c HASS m√† khi b·∫°n ·∫•n v√†o c√°i ·ªï kh√≥a tr√™n Browser s·∫Ω th·∫•y m√†u xanh HTTPS\nV·∫≠y l√† th√†nh c√¥ng r·ªìi.\n1.1. N·∫øu mu·ªën disalbe t√≠nh nƒÉng redirect http-\u0026gt;https Gi·ªù v√≠ d·ª• m√¨nh ko mu·ªën n√≥ t·ª´ redirect http sang https th√¨ c·∫ßn s·ª≠a file:\n/opt/swag/config/nginx/site-confs/default, comment ph·∫ßn n√†y l·∫°i:\n# redirect all traffic to https #server { # listen 80 default_server; # listen [::]:80 default_server; # server_name _; # return 301 https://$host$request_uri; #} restart swag container, check l·∫°i b·∫±ng c√°ch t·ª´ m·∫°ng local c≈©ng dc:\ntr∆∞·ªõc khi s·ª≠a n·∫øu b·∫°n run wget http://MYDOMAIN.duckdns.org s·∫Ω nh·∫≠n ƒë∆∞·ª£c th√¥ng b√°o ki·ªÉu 301 Moved Permanently\u0026hellip;\nR·ªìi l·ªói ki·ªÉu port 443 b·ªã refused.\nwget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 20:18:09-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 15.xx.xx.32 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.32|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://MYDOMAIN.duckdns.org/ [following] --2022-09-21 20:18:09-- https://MYDOMAIN.duckdns.org/ Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.32|:443... failed: Connection refused. Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 15.xx.xx.32 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.32|:443... failed: Connection refused Ch√∫ √Ω ch·ªó :80... connected. ch·ª©ng t·ªè traffic ƒë·∫øn port 80 c·ªßa proxy swag r·ªìi xong n√≥ ƒë∆∞·ª£c redirect sang https lu√¥n (Ch√∫ √Ω ch·ªó n√†y swag container c·ªßa m√¨nh ƒëang run tr√™n c·∫£ 2 port 80 v√† 443 nh√©)\nSau khi s·ª≠a:\n$ wget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 14:26:50-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 15.xx.xx.3x Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|15.xx.xx.3x|:80... failed: Connection refused. v·∫´n l·ªói l√† do port 80 ƒëang b·ªã refused, c√≥ th·ªÉ do ko c√≥ service n√†o ch·∫°y tr√™n port ƒë√≥.\nTuy nhi√™n m√¨nh khuy√™n b·∫°n ko n√™n s·ª≠a, vi·ªác redirect http-\u0026gt;https l√† t·ªët cho ng∆∞·ªùi d√πng.\n2. Access from LAN network Gi·ªù C√≥ 1 v·∫•n ƒë·ªÅ nh·ªè l√† t·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org -\u0026gt; th√¨ s·∫Ω ko v√†o dc HASS üò´ (n√≥ ra c√°i Router dashboard nh√† m·∫°ng)\nC·ª© t∆∞·ªüng do trong ROuter dashboard ƒëang d√πng port 80 n√™n n√≥ t·ª± redirect v·ªÅ, nh∆∞ng sau khi ƒë·ªïi port 80 c·ªßa Router Dashboard th√†nh 8080. Th√¨ gi·ªù t·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org v·∫´n ko v√†o ƒë∆∞·ª£c HASS\nTr√™n Router add 1 rule v√†o Port Forwarding port 80 c·ªßa host -\u0026gt; port 80 c·ªßa RPi Server.\nM·∫∑c d√π t·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org:8123 th√¨ OK. -\u0026gt; ch·ª©ng t·ªè Router c√≥ activate NAT Loopback tr√™n port 8123\nTi·∫øp th·ª≠ expose grafana ra port 80:3000 th√¨ c≈©ng OK. T·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org (ƒë√£ disabled redirect https) -\u0026gt; s·∫Ω v√†o Grafana\nCh·ª©ng t·ªè NAT Loopback c√≥ active v·ªõi port 80. Ch·ªâ ko l√†m dc v·ªõi port 443.\nNh·ªù c√≥ l·ªói 301 Moved Permanently b√™n tr√™n m√† m√¨nh hi·ªÉu r·∫±ng mu·ªën t·ª´ m·∫°ng local k·∫øt n·ªëi ƒë·∫øn HASS ƒëang ·ªü port 8123 th√¨ c·∫ßn port forwarding\n2.1. D√πng Nginx S·ª≠a file /opt/swag/config/nginx/site-confs/default, comment h·∫øt ph·∫ßn redirect l·∫°i (th·ª±c ra ƒë√£ l√†m ·ªü ph·∫ßn 1.1):\n# redirect all traffic to https #server { # listen 80 default_server; # listen [::]:80 default_server; # server_name _; # return 301 https://$host$request_uri; #} S·ª≠a ƒëo·∫°n ƒë·∫ßu file /opt/swag/config/nginx/proxy-confs/homeassistant.subdomain.conf, add listen 80; v√†o:\n~~~ server { listen 80; listen 443 ssl; listen [::]:443 ssl; server_name MYDOMAIN.duckdns.org; ~~~ Nh∆∞ v·∫≠y m√¨nh ra l·ªánh cho Nginx r·∫±ng n√≥ ph·∫£i listen c·∫£ port 80 n·ªØa.\nKhi c√≥ request ƒë·∫øn port 80 n√≥ s·∫Ω ko redirect sang https n·ªØa m√† forward ƒë·∫øn HASS (port 8123) lu√¥n.\n-\u0026gt; restart swag container.\nTest:\nT·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org (tab ·∫©n danh c·ªßa Chrome) -\u0026gt; access ƒë∆∞·ª£c HASS, ch·ª©ng t·ªè traffic ƒë·∫øn port 80 t·ª´ trong LAN ƒë√£ dc forward ƒë·∫øn port 8123\nVi·ªác n√†y ph·∫£i ch·∫°y tr√™n tab ·∫©n danh, ch·ª© n·∫øu ch·∫°y tab th√¥ng th∆∞·ªùng th√¨ http s·∫Ω b·ªã redirect v·ªÅ https v√† ko access ƒë∆∞·ª£c.\nTest b·∫±ng c√°ch call t·ª´ LAN run command:\nwget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 16:31:59-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 1x.x.x.xx3 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|1x.x.x.xx3|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 9331 (9.1K) [text/html] Saving to: ‚Äòindex.html.4‚Äô index.html.4 100%[=================================================\u0026gt;] 9.11K --.-KB/s in 0.007s 2022-09-21 16:31:59 (1.27 MB/s) - ‚Äòindex.html.4‚Äô saved [9331/9331] T·∫°m ch·∫•p nh·∫≠n. Tuy nhi√™n c√°ch n√†y c√≥ v·∫•n ƒë·ªÅ:\n T·ª´ m·∫°ng LAN v√†o http://MYDOMAIN.duckdns.org s·∫Ω ph·∫£i d√πng tab ·∫©n danh, v√¨ n·∫øu ko tr√¨nh duy·ªát s·∫Ω t·ª± ƒë·ªông redirect sang https v√† ko v√†o dc T·ª´ m·∫°ng external v√†o c·∫£ 2 link ƒë·ªÅu ƒë∆∞·ª£c http://MYDOMAIN.duckdns.org v√† https://MYDOMAIN.duckdns.org, ƒëi·ªÅu n√†y ko t·ªët v·ªÅ security  2.2. D√πng iptables C√°ch n√†y ƒë∆∞·ª£c m√¨nh th·ª≠ ƒë·∫ßu ti√™n, nh∆∞ng x·∫øp m·ª•c 2.2 v√¨ m√¨nh ko ∆∞u ti√™n l·∫Øm.\nDo ban ƒë·∫ßu t∆∞·ªüng Nginx ko l√†m ƒë∆∞·ª£c vi·ªác port forwarding HASS n√™n m√¨nh ƒë√£ th·ª≠ c√°ch 2 n√†y, c·ªë g·∫Øng port forward b·∫±ng iptables:\nhttps://serverfault.com/questions/140622/how-can-i-port-forward-with-iptables\nNh·∫±m forward traffic t·ª´ port 80 c·ªßa host v·ªÅ port 8123 c·ªßa HASS:\n# forward traffic t·ª´ LAN (192.168.1.0/24) port 80 s·∫Ω ƒë·∫øn ip 192.168.1.128:8123 iptables -t nat -A PREROUTING -p tcp -i wlan0 -s 192.168.1.0/24 --dport 80 -j DNAT --to-destination 192.168.1.128:8123 # show iptables -L -n -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL DNAT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:80 to:192.168.1.128:8123 Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 MASQUERADE all -- 172.18.0.0/16 0.0.0.0/0 MASQUERADE tcp -- 172.18.0.2 172.18.0.2 tcp dpt:9000 MASQUERADE tcp -- 172.18.0.4 172.18.0.4 tcp dpt:8080 MASQUERADE udp -- 172.18.0.5 172.18.0.5 udp dpt:51820 MASQUERADE tcp -- 172.18.0.6 172.18.0.6 tcp dpt:9001 MASQUERADE tcp -- 172.18.0.6 172.18.0.6 tcp dpt:1883 MASQUERADE tcp -- 172.18.0.7 172.18.0.7 tcp dpt:8200 MASQUERADE tcp -- 172.18.0.9 172.18.0.9 tcp dpt:9090 MASQUERADE tcp -- 172.18.0.10 172.18.0.10 tcp dpt:3000 MASQUERADE tcp -- 172.18.0.8 172.18.0.8 tcp dpt:443 Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9000 to:172.18.0.2:9000 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.18.0.4:8080 DNAT udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:51820 to:172.18.0.5:51820 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9001 to:172.18.0.6:9001 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:1883 to:172.18.0.6:1883 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8200 to:172.18.0.7:8200 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9090 to:172.18.0.9:9090 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3000 to:172.18.0.10:3000 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 to:172.18.0.8:443 T·ª´ m·∫°ng LAN -\u0026gt; http://MYDOMAIN.duckdns.org (tab ·∫©n danh c·ªßa Chrome) -\u0026gt; access ƒë∆∞·ª£c HASS, ch·ª©ng t·ªè traffic ƒë·∫øn port 80 t·ª´ trong LAN ƒë√£ dc forward ƒë·∫øn port 8123\nVi·ªác n√†y ph·∫£i ch·∫°y tr√™n tab ·∫©n danh, ch·ª© n·∫øu ch·∫°y tab th√¥ng th∆∞·ªùng th√¨ http s·∫Ω b·ªã redirect v·ªÅ https v√† ko access ƒë∆∞·ª£c.\ntest b·∫±ng c√°ch call t·ª´ LAN run command:\nwget http://MYDOMAIN.duckdns.org Will not apply HSTS. The HSTS database must be a regular and non-world-writable file. ERROR: could not open HSTS store at '/home/xxxx/.wget-hsts'. HSTS will be disabled. --2022-09-21 16:31:59-- http://MYDOMAIN.duckdns.org/ Resolving MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)... 1x.x.x.xx3 Connecting to MYDOMAIN.duckdns.org (MYDOMAIN.duckdns.org)|1x.x.x.xx3|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 9331 (9.1K) [text/html] Saving to: ‚Äòindex.html.4‚Äô index.html.4 100%[=================================================\u0026gt;] 9.11K --.-KB/s in 0.007s 2022-09-21 16:31:59 (1.27 MB/s) - ‚Äòindex.html.4‚Äô saved [9331/9331] Command ti·∫øp theo m√¨nh th·∫•y ko c·∫ßn l·∫Øm, v√¨ ngay sau khi t·∫°o DNAT rule b√™n tr√™n, m·ªçi th·ª© c√≥ v·∫ª ƒë√£ OK. (nh∆∞ng tr√™n Stackoverflow h·ªç v·∫´n c√≥):\niptables -A FORWARD -p tcp -d 192.168.1.128 --dport 8123 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT m√† m√¨nh x√≥a rule n√†y ƒëi th√¨ c≈©ng ko sao.\nNh∆∞ v·∫≠y C√°ch n√†y (d√πng Iptables) l√† \u0026ldquo;ph·∫ßn n√†o\u0026rdquo; ch·∫•p nh·∫≠n ƒë∆∞·ª£c, n√≥ gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ c·ªßa C√°ch Nginx l√†: iptable rule ch·∫∑n external network connect ƒë·∫øn port 80 (link http://MYDOMAIN.duckdns.org) ho·∫∑c n√≥ s·∫Ω redirect sang https lu√¥n.\nNh∆∞ng c√°ch n√†y v·∫´n c√≤n v·∫•n ƒë·ªÅ:\n T·ª´ m·∫°ng LAN v√†o http://MYDOMAIN.duckdns.org s·∫Ω ph·∫£i d√πng tab ·∫©n danh, v√¨ n·∫øu ko tr√¨nh duy·ªát s·∫Ω t·ª± ƒë·ªông redirect sang https v√† ko v√†o dc.  M√¨nh nghƒ©, n·∫øu c√≥ c√°ch n√†o ƒë√≥ ƒë·ªÉ setup tr√™n Nginx ƒë·ªÉ ko redirect http-\u0026gt;https khi d√πng m·∫°ng LAN th√¨ t·ªët. C√°i n√†y m√¨nh ƒëang t√¨m hi·ªÉu..\n2.3. K·∫øt lu·∫≠n T√≥m l·∫°i d√π c√°ch 2 (d√πng Iptables) gi·∫£i quy·∫øt dc 1 v·∫•n ƒë·ªÅ c·ªßa c√°ch 1 (d√πng Nginx), m√¨nh v·∫´n ko recommend vi·ªác s·ª≠a iptables ƒë·ªÉ forward traffic l·∫Øm. B·ªüi v√¨ ƒë√£ t·ª´ng c√≥ l·∫ßn m√¨nh reset l·∫°i server RPi, h·∫ßu nh∆∞ t·∫•t c·∫£ c√°c rule tr√™n iptables ƒë·ªÅu b·ªã x√≥a h·∫øt. M√† RPi c·ªßa m√¨nh l·∫°i ƒëang ·ªü trong DMZ, g√¢y risk v·ªÅ security.\nT√¨m ƒë∆∞·ª£c b√†i n√†y v·ªÅ vi·ªác save v√† reload iptables rule on reboots: https://askubuntu.com/questions/119393/how-to-save-rules-of-the-iptables\nsudo netfilter-persistent save sudo netfilter-persistent reload Tuy nhi√™n ch∆∞a th·ª≠ restart xem c√≥ m·∫•t rule ko, ho·∫∑c c√≥ c·∫ßn run command ƒë·ªÉ reload rule hay ko?\nV·∫≠y n√™n m√¨nh v·∫´n d√πng c√°ch 1 (Nginx), gi·ªØ 2 link ƒë·ªÉ d√πng:\n1: http://MYDOMAIN.duckdns.org redirect https://MYDOMAIN.duckdns.org -\u0026gt; ƒë·ªÉ integrate v·ªõi Alexa\n2: http://MYDOMAIN.duckdns.org:8123 ƒë·ªÉ gia ƒë√¨nh s·ª≠ d·ª•ng (link n√†y th√¨ do c√≥ NAT loopback tr√™n port 8123 n√™n t·ª´ m·∫°ng 4G hay LAN ƒë·ªÅu access ƒë∆∞·ª£c h·∫øt)\nNgo√†i ra: M√¨nh t·ª± h·ªèi l√† n·∫øu kh√¥ng th·ªÉ Nat Loopback ƒë∆∞·ª£c port 443 ƒë∆∞·ª£c th√¨ vi·ªác m√¨nh ƒë∆∞a RPi ra DMZ c√≥ √Ω nghƒ©a g√¨? C√≥ l·∫Ω th·ª≠ disable DMZ ƒëi xem sao?\n-\u0026gt; Th√¨ ngay sau khi disable DMZ, t·ª´ External network s·∫Ω kh√¥ng th·ªÉ connect ƒë·∫øn https://MYDOMAIN.duckdns.org v√† http://MYDOMAIN.duckdns.org. C√≤n connect ƒë·∫øn http://MYDOMAIN.duckdns.org:8123 v·∫´n ok.\nNh∆∞ v·∫≠y l√† v·ª´a ph·∫£i ƒë∆∞a ra DMZ, v·ª´a ph·∫£i k·∫øt h·ª£p Port Forwarding tr√™n Router.\nM√† ƒë√£ ƒë∆∞a ra DMZ l√† ph·∫£i k·∫øt h·ª£p th√™m c√°c rule tr√™n iptables ƒë·ªÉ filter traffic ƒëi v√†o nh√© (ƒë√£ l√†m ·ªü b√†i tr∆∞·ªõc Part2 ).\n3. N·∫øu mu·ªën expose th√™m 1 service n√†o ƒë√≥ ƒë·ªÉ test Gi·∫£ d·ª• mu·ªën expose th√™m 1 web Grafana ra ngo√†i, ki·ªÉu https://grafana.MYDOMAIN.duckdns.org (tuy nhi√™n s·∫Ω ko c√≥ Browser secured v√¨ cert ch·ªâ valid v·ªõi domain https://MYDOMAIN.duckdns.org th√¥i )\nGi·∫£ s·ª≠ grafana ƒëang run trong Docker container port 3000, container name l√† grafana\nTrong swag config folder s·ª≠a file /opt/swag/config/nginx/proxy-confs/grafana.subdomain.conf.sample, rename th√†nh /opt/swag/config/nginx/proxy-confs/grafana.subdomain.conf v√† s·ª≠a n·ªôi dung file ƒë√≥:\nth·ª±c ra h·∫ßu nh∆∞ ko c·∫ßn s·ª≠a, ch·ªâ c·∫ßn check l·∫°i xem ƒë√∫ng v·ªõi case c·ªßa m√¨nh ch∆∞a th√¥i -\u0026gt; restart swag container.\nGi·ªù s·∫Ω access t·ª´ m·∫°ng 4G v√†o https://grafana.MYDOMAIN.duckdns.org l√† s·∫Ω ƒë∆∞·ª£c. T·∫•t nhi√™n access t·ª´ m·∫°ng LAN s·∫Ω ko ƒëc ƒë√¢u nh√©. M·∫°ng LAN th√¨ v·∫´n d√πng IP th√¥i üò´\nƒê√¢y l√† s·ª± ti·ªán l·ª£i c·ªßa NGINX, b·∫°n ch·ªâ c·∫ßn s·ª≠a trong nginx m√† ko c·∫ßn s·ª≠a g√¨ b√™n ngo√†i Router, iptables c·∫£\n4. About fail2ban Trong swag container ƒë√£ t√≠ch h·ª£p s·∫µn t√≠nh nƒÉng fai2ban:\nhttps://www.fail2ban.org/wiki/index.php/Commands\nThis container includes fail2ban set up with 5 jails by default:\n nginx-http-auth nginx-badbots nginx-botsearch nginx-deny nginx-unauthorized  To enable or disable other jails, modify the file /config/fail2ban/jail.local\nTo modify filters and actions, instead of editing the .conf files, create .local files with the same name and edit those because .conf files get overwritten when the actions and filters are updated. .local files will append whatever\u0026rsquo;s in the .conf files (ie. nginx-http-auth.conf \u0026ndash;\u0026gt; nginx-http-auth.local)\nYou can check which jails are active via:\ndocker exec -it swag fail2ban-client status You can check the status of a specific jail via:\ndocker exec -it swag fail2ban-client status \u0026lt;jail name\u0026gt; You can unban an IP via:\ndocker exec -it swag fail2ban-client set \u0026lt;jail name\u0026gt; unbanip \u0026lt;IP\u0026gt; A list of commands can be found here: https://www.fail2ban.org/wiki/index.php/Commands\nCREDIT 1 v√≠ d·ª• v·ªÅ vi·ªác d√πng Nginx l√†m reverse proxy cho 2 app trong Docker:\nhttps://www.bogotobogo.com/DevOps/Docker/Docker-Compose-Nginx-Reverse-Proxy-Multiple-Containers.php\n1 b√†i vi·∫øt v·ªÅ d√πng Swag v·ªõi Hass ƒë·ªÉ t·∫°o https auto-renewal cert. Swag c√≥ s·∫µn Nginx:\nhttps://community.home-assistant.io/t/nginx-reverse-proxy-set-up-guide-docker/54802\nhttps://github.com/linuxserver/docker-swag\nhttps://pentacent.medium.com/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71 https://hub.docker.com/r/linuxserver/swag\n1 b√†i v·ªÅ s·ª≠ d·ª•ng Traefik l√†m reverse proxy thay cho Nginx:\nhttps://community.home-assistant.io/t/help-multiple-containers-nextcloud-ha-letsencrypt/46725/5\nhttps://thesmarthomejourney.com/2021/11/08/traefik-1-reverse-proxy-setup/\nhttps://thesmarthomejourney.com/2022/01/26/traefik-force-cert-renewal/#Traefik_and_lets_encrypt\nhttps://community.home-assistant.io/t/help-with-ha-in-a-docker-container-and-lets-encrypt/51595\n1 ch√∫ √Ω khi setting hass with certificate: https://community.home-assistant.io/t/help-with-ha-in-a-docker-container-and-lets-encrypt/51595/8\nhttps://community.home-assistant.io/t/configure-ssl-with-docker/196878\n2 b√†i v·ªÅ d√πng nginx v√† lesencript docker m√† ko ph·∫£i l√† swag:\nhttps://pentacent.medium.com/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71\nGithub: https://github.com/wmnnd/nginx-certbot\ntuy nhi√™n c√≥ v·∫ª ƒë√£ l√¢u ko update g√¨ (t·ª´ 2020)\nb√†i n√†y th·∫•y vi·∫øt d·ªÖ hi·ªÉu v√† kh√° clear:\nhttps://evgeniy-khyst.com/letsencrypt-docker-compose/\nhttps://github.com/evgeniy-khist/letsencrypt-docker-compose\nhttps://community.home-assistant.io/t/remote-access-with-docker/314345\nhttps://serverfault.com/questions/140622/how-can-i-port-forward-with-iptables\nhttps://askubuntu.com/questions/119393/how-to-save-rules-of-the-iptables\n","href":"/posts/encrypt-setup-home-assistant-on-raspberry-pi-p3-https/","title":"Setup Home Assistant on Raspberry Pi (Part 3) - Https"},{"content":"","href":"/tags/iptables/","title":"Iptables"},{"content":"","href":"/tags/raspberrypi/","title":"RaspberryPi"},{"content":"ƒê√¢y l√† ph·∫ßn c·ªë g·∫Øng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ khi m√¨nh l√†m c√°i n√†y: 9. Install DuckDNS (for expose outside access purpose)\nT√≥m t·∫Øt t√¨nh hu·ªëng c·ªßa m√¨nh: C√≥ 1 RPi, ch·∫°y HASS tr√™n docker. D√ông m·∫°ng VNPT, ko th·ªÉ Port Forwarding ƒë∆∞·ª£c port 443 (do router ƒë√£ reserve port 443, 80).\n-\u0026gt; Ph·∫£i d√πng ch·ª©c nƒÉng DMZ - cho RPi v√†o DMZ, nh∆∞ng ch·ª©c nƒÉng n√†y l·∫°i m·ªü all port (bao g·ªìm c·∫£ 443) tr√™n con RPi (risk v·ªÅ security)\n-\u0026gt; Ph·∫£i t√¨m c√°ch ƒë·ªÉ h·∫°n ch·∫ø traffic t·ª´ Internet v√†o DMZ (ch·ªâ cho traffic ƒëi v√†o port 80,443 v√† 1 s·ªë port c·∫ßn thi·∫øt th√¥i)\n-\u0026gt; M√¨nh t√¨m ƒë∆∞·ª£c c√°ch: D√πng iptables command, t·∫°o c√°c rule tr√™n ch√≠nh con RPi (ko cho n√≥ nh·∫≠n traffic t·ª´ b√™n ngo√†i Internet v√†o 1 s·ªë port)\n1. Check xem RPi ƒëang n√≥i chuy·ªán v·ªõi Internet b·∫±ng interface n√†o root@raspian-rpi:/opt/hass# ifconfig br-564d64d5a58a: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fexxxxxxxxxxxx8b prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 0xxxxxxxxxxxx:8b txqueuelen 0 (Ethernet) RX packets 9724621 bytes 14542091134 (13.5 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 9678446 bytes 14944508794 (13.9 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 docker0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fxxxxxxxxxxxxxbb prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 0xxxxxxxxxxxxbb txqueuelen 0 (Ethernet) RX packets 187 bytes 24270 (23.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 207 bytes 27492 (26.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 ether exxxxxxxxxxx6 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 1801769 bytes 441955165 (421.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1801769 bytes 441955165 (421.4 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 wlan0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.128 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fxxxxxxxxxxxxxxx36 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether exxxxxxxxxxxxx97 txqueuelen 1000 (Ethernet) RX packets 5982425 bytes 2708750541 (2.5 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7859191 bytes 1834756180 (1.7 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Trong nhi·ªÅu tr∆∞·ªùng h·ª£p, n·∫øu server c·ªßa b·∫°n c·∫Øm d√¢y LAN ƒë·∫øn Router ƒë·ªÉ v√†o m·∫°ng th√¨ c√≥ nghƒ©a l√† n√≥ d√πng eth0.\nNh∆∞ng v√¨ RPi c·ªßa m√¨nh d√πng Wifi ƒë·ªÉ v√†o m·∫°ng -\u0026gt; n√™n c√°i n√≥ ƒëang d√πng ph·∫£i l√† wlan0 -\u0026gt; ƒê·ªÉ confirm th√¨ B·∫°n s·∫Ω th·∫•y l∆∞·ª£ng package/traffic ƒëi ra v√†o wlan0 kh√° nhi·ªÅu (RX packets 5982425 bytes 2708750541 (2.5 GiB))\nV·∫≠y l√† x√°c ƒë·ªãnh ƒë∆∞·ª£c interface l√† wlan0\n2. Check c√°c rule ƒë√£ t·ªìn t·∫°i trong iptables sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 635K packets, 705M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 4921 packets, 2451K bytes) pkts bytes target prot opt in out source destination 279K 149M DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 279K 149M DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 0 0 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0 152K 85M ACCEPT all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 634 122K DOCKER all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 126K 64M ACCEPT all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 489 114K ACCEPT all -- br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 673K packets, 687M bytes) pkts bytes target prot opt in out source destination Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 20 1128 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.2 tcp dpt:9000 6 312 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.4 tcp dpt:8200 1 40 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:9001 0 0 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:1883 0 0 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.6 tcp dpt:9090 11 564 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.9 tcp dpt:8080 0 0 ACCEPT udp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.10 udp dpt:51820 0 0 ACCEPT tcp -- !docker0 docker0 0.0.0.0/0 172.17.0.2 tcp dpt:4444 39 2180 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.7 tcp dpt:3000 Chain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 0 0 DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 126K 64M DOCKER-ISOLATION-STAGE-2 all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 279K 149M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) pkts bytes target prot opt in out source destination 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 DROP all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 126K 64M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 279K 149M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Ch√∫ √Ω v√†o Chain DOCKER, cho th·∫•y hi·ªán ƒëang m·ªü kh√° nhi·ªÅu port:\ntcp dpt:9000 tcp dpt:8200 tcp dpt:9001 tcp dpt:1883 tcp dpt:9090 tcp dpt:8080 udp dpt:51820 tcp dpt:3000 nh∆∞ng ko c√≥ port 8123 trong list n√†y (Ch·ªó n√†y b·ªüi v√¨ HASS ko expose port 8123 t·ª´ Docker ra ngo√†i n√™n b·∫°n s·∫Ω ko th·∫•y n√≥ trong Chain DOCKER)\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 228ccfed3015 grafana/grafana:8.5.5 \u0026quot;/run.sh\u0026quot; 6 hours ago Up 6 hours 0.0.0.0:3000-\u0026gt;3000/tcp, :::3000-\u0026gt;3000/tcp grafana 105c497b8114 ghcr.io/home-assistant/home-assistant:2022.8.2 \u0026quot;/init\u0026quot; 24 hours ago Up 24 hours homeassistant 45f43bcdae93 prom/prometheus:v2.36.0 \u0026quot;/bin/prometheus --c‚Ä¶\u0026quot; 6 weeks ago Up 24 hours 0.0.0.0:9090-\u0026gt;9090/tcp, :::9090-\u0026gt;9090/tcp prometheus 324fd3e2ce26 portainer/portainer-ce:2.13.1 \u0026quot;/portainer\u0026quot; 2 months ago Up 24 hours 8000/tcp, 9443/tcp, 0.0.0.0:9000-\u0026gt;9000/tcp, :::9000-\u0026gt;9000/tcp portainer 2d80eee17aaf prom/node-exporter:v1.3.1 \u0026quot;/bin/node_exporter\u0026quot; 3 months ago Up 24 hours 9100/tcp monitoring_node_exporter 7c56755c52fe koenkk/zigbee2mqtt:1.25.0 \u0026quot;docker-entrypoint.s‚Ä¶\u0026quot; 3 months ago Up 24 hours 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp zigbee2mqtt b00bc6092048 lscr.io/linuxserver/wireguard:1.0.20210914 \u0026quot;/init\u0026quot; 3 months ago Up 24 hours 0.0.0.0:51820-\u0026gt;51820/udp, :::51820-\u0026gt;51820/udp wireguard ebfecd7ed0b4 lscr.io/linuxserver/duckdns:68a3222a-ls97 \u0026quot;/init\u0026quot; 3 months ago Up 24 hours duckdns ae09a218d43f eclipse-mosquitto:2.0.14 \u0026quot;/docker-entrypoint.‚Ä¶\u0026quot; 3 months ago Up 24 hours 0.0.0.0:1883-\u0026gt;1883/tcp, :::1883-\u0026gt;1883/tcp, 0.0.0.0:9001-\u0026gt;9001/tcp, :::9001-\u0026gt;9001/tcp mosquitto 413fcaac7b9a lscr.io/linuxserver/duplicati:2.0.6 \u0026quot;/init\u0026quot; 3 months ago Up 24 hours 0.0.0.0:8200-\u0026gt;8200/tcp, :::8200-\u0026gt;8200/tcp duplicati Gi·ªù c·∫ßn ch√∫ √Ω v√†o Chain DOCKER-USER, hi·ªán ƒëang ch·ªâ c√≥ 1 rule RETURN, ko c·∫ßn quan t√¢m, nh∆∞ v·∫≠y coi nh∆∞ ko c√≥ rule n√†o. ƒê√¢y ch√≠nh l√† n∆°i m√† c√°c b·∫°n s·∫Ω add rule v√†o, ƒë·ªÉ h·∫°n ch·∫ø traffic ƒëi v√†o c√°c port trong Docker c·ªßa b·∫°n.\n3. D√πng iptables ƒë·ªÉ filter c√°c traffic 3.1. Drop all traffic ƒëi v√†o Chain DOCKER-USER tr∆∞·ªõc, r·ªìi allow t·ª´ng port m·ªôt sau ƒê·∫ßu ti√™n h√£y test ƒë·ªÉ ch·∫Øc ch·∫Øn ƒëang l√†m ƒë√∫ng:\nT·ª´ m·∫°ng 4G -\u0026gt; port 3000, 9000, 8123 -\u0026gt; access ƒë∆∞·ª£c h·∫øt.\nT·∫•t nhi√™n r·ªìi, b·ªüi v√¨ ch√∫ng ta ch∆∞a add rule n√†o m√†.\nGi·ªù add 1 rule ƒë·ªÉ DROP t·∫•t c·∫£ traffic ƒë·∫øn Docker ngo·∫°i tr·ª´ m·∫°ng LAN 192.168.1.0/24:\nCh√∫ √Ω th·ª© t·ª± r·∫•t quan tr·ªçng, c√¢u l·ªánh n√†y ph·∫£i run tr∆∞·ªõc khi ch·∫°y c√°c c√¢u l·ªánh add rule kh√°c\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP Test:\nT·ª´ m·∫°ng local -\u0026gt; port 8123, 3000, 9000, 8200 -\u0026gt; access ƒë∆∞·ª£c. T·ª´ m·∫°ng 4G -\u0026gt; port 8123 -\u0026gt; access ƒë∆∞·ª£c.\nT·ª´ m·∫°ng 4G -\u0026gt; port 3000, 9000, 8200, 8080 -\u0026gt; ko access ƒë∆∞·ª£c.\nPort 3000,9000,8200,8080 ·ªü trong Chain DOCKER n√™n n√≥ ch·ªãu ·∫£nh h∆∞·ªüng b·ªüi rule tr√™n. C√≤n port 8123 th√¨ ko, n√™n b·∫°n v·∫´n access ƒë∆∞·ª£c. HASS c√°c ho·∫°t ƒë·ªông: ko c√≥ l·ªói ƒë·∫∑c bi·ªát.\nTest Duplicati connection to S3: L·ªói lu√¥n ‚ùå\nC√≥ l·∫Ω l√† Duplicati khi test connection to S3 n√≥ c·∫ßn TCP incoming traffic üòÅ\nKo hi·ªÉu v√¨ sao?\nT√¨m th·∫•y 1 post ko ai tr·∫£ l·ªùi:\nhttps://stackoverflow.com/questions/71763830/what-ports-are-required-to-open-for-aws-s3-to-work Th·ª≠ accept port 443:\n# accept traffic TCP incoming interface wlan0 on Docker port 443 from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --dport 443 -j ACCEPT Test l·∫°i Duplicati connection to S3: V·∫´n l·ªói‚ùå\nTh·ª≠ accept port 80:\n# accept traffic TCP incoming interface wlan0 on Docker port 80 from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --dport 80 -j ACCEPT Test l·∫°i Duplicati connection to S3: V·∫´n l·ªói‚ùå\nTh·ª≠ accept port 55000-and 65535:\n# accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 55000:65535 -j ACCEPT Test l·∫°i Duplicati connection to S3: V·∫´n l·ªói‚ùå- nh∆∞ng log l·ªói c√≥ v·∫ª ng·∫Øn h∆°n: A WebException with status Timeout was thrown\n[services.d] done. Amazon.Runtime.AmazonServiceException: A WebException with status Timeout was thrown. ---\u0026gt; System.Net.WebException: The operation has timed out. at System.Net.HttpWebRequest.GetRequestStream () [0x00016] in \u0026lt;9c6e2cb7ddd8473fa420642ddcf7ce48\u0026gt;:0 at Amazon.Runtime.Internal.HttpRequest.GetRequestContent () [0x00000] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.HttpHandler`1[TRequestContent].InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x0006b] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.PipelineHandler.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x0000e] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.Unmarshaller.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x00000] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.PipelineHandler.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x0000e] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.ErrorHandler.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x00000] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 --- End of inner exception stack trace --- sudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 113 110K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 55000:65535 81 4596 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1006K 976M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Nh√¨n th·∫•y 2 rule tcp dpt:443 v√† tcp dpt:80 ko th·∫•y c√≥ traffic g√¨ (c·ªôt pkts, bytes) ch·ª©ng t·ªè v√¥ nghƒ©a khi th√™m v√†o. Ch·ªß y·∫øu l√† rule multiport dports 55000:65535 c·∫ßn thi·∫øt th√¥i.\nQuy·∫øt ƒë·ªãnh debug ƒë·∫øn c√πng ƒë·ªÉ ra v·∫•n ƒë·ªÅ:\nhttps://stackoverflow.com/questions/21771684/iptables-log-and-drop-in-one-rule\nLet\u0026rsquo;s create a chain to log and accept:\niptables -N LOG_ACCEPT And let\u0026rsquo;s populate its rules:\niptables -A LOG_ACCEPT -j LOG --log-prefix \u0026#34;INPUT:ACCEPT:\u0026#34; --log-level 6 iptables -A LOG_ACCEPT -j ACCEPT Now let\u0026rsquo;s create a chain to log and drop:\niptables -N LOG_DROP And let\u0026rsquo;s populate its rules:\niptables -A LOG_DROP -j LOG --log-prefix \u0026#34;INPUT:DROP: \u0026#34; --log-level 6 iptables -A LOG_DROP -j DROP Now you can do all actions in one go by jumping (-j) to you custom chains instead of the default LOG / ACCEPT / REJECT / DROP:\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN, log result to LOG_DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j LOG_DROP # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere, log result to LOG_ACCEPT iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 55000:65535 -j LOG_ACCEPT S·∫Ω ƒë∆∞·ª£c c√°i output nh∆∞ n√†y, ch√∫ √Ω ƒë√∫ng th·ª© t·ª± l√† ok:\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 113 110K LOG_ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 55000:65535 81 4596 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1006K 976M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain LOG_ACCEPT (1 references) pkts bytes target prot opt in out source destination 113 110K LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:ACCEPT:\u0026quot; 113 110K ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 Chain LOG_DROP (1 references) pkts bytes target prot opt in out source destination 81 4596 LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:DROP: \u0026quot; 81 4596 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 Test: Duplicati - Test connection to AWS S3.\nC√πng l√∫c ƒë√≥, v√†o log check: tail -200f /var/log/message, b·∫°n s·∫Ω th·∫•y c√≥ r·∫•t nhi·ªÅu package ACCEPT/DROP ƒë√£ ƒë∆∞·ª£c log l·∫°i:\nƒê∆∞·ª£c Accept:\nSep 16 23:43:48 raspian-rpi kernel: [1598976.871464] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=60 TOS=0x00 PREC=0x20 TTL=51 ID=0 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=65160 RES=0x00 ACK SYN URGP=0 Sep 16 23:43:48 raspian-rpi kernel: [1598977.137601] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=52 TOS=0x00 PREC=0x20 TTL=51 ID=12226 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=508 RES=0x00 ACK URGP=0 Sep 16 23:43:48 raspian-rpi kernel: [1598977.141156] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=1492 TOS=0x00 PREC=0x20 TTL=51 ID=12227 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=508 RES=0x00 ACK URGP=0 Sep 16 23:43:48 raspian-rpi kernel: [1598977.141322] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=1492 TOS=0x00 PREC=0x20 TTL=51 ID=12228 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=508 RES=0x00 ACK PSH URGP=0 B·ªã drop:\nSep 16 23:48:06 raspian-rpi kernel: [1599234.486899] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=52.119.198.223 DST=172.18.0.4 LEN=52 TOS=0x00 PREC=0x20 TTL=238 ID=51533 DF PROTO=TCP SPT=443 DPT=53954 WINDOW=8190 RES=0x00 ACK SYN URGP=0 Sep 16 23:48:20 raspian-rpi kernel: [1599248.485699] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=52.219.36.104 DST=172.18.0.4 LEN=52 TOS=0x00 PREC=0x20 TTL=239 ID=27892 PROTO=TCP SPT=80 DPT=51988 WINDOW=64240 RES=0x00 ACK SYN URGP=0 ƒê·∫øn ƒë√¢y th√¨ c√≥ v·∫ª d·∫ßn hi·ªÉu ra v·∫•n ƒë·ªÅ, open port 55000:65535 l√† ch∆∞a ƒë·ªß cho Duplicati call api c·ªßa AWS S3 SDK.\nL·∫ßn theo LOG th√¨ m√¨nh th·∫•y DPT=33234 c√≥ v·∫ª l√† s·ªë nh·ªè nh·∫•t trong s·ªë c√°c port b·ªã DROP.\nTh·∫ø n√™n m√¨nh s·∫Ω allow port t·ª´ 33000:65535 ƒë·ªÉ test l·∫°i connection.\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 33000:65535 -j ACCEPT nh∆∞ n√†y l√† ok:\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1202K 1060M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Test l·∫°i Duplicati connection to S3: worked üëå -\u0026gt; v·∫≠y l√† ngon r·ªìi\nCheck l·∫°i c√≥ bao nhi√™u pkts/bytes b·ªã DROP trong khi test, th√¨ m√¨nh th·∫•y 0 pkts,bytes -\u0026gt; r·∫•t t·ªët üòÅ\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 93 74599 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1203K 1060M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Check ho·∫°t ƒë·ªông c·ªßa HASS: Log ko c√≥ l·ªói ƒë·∫∑c bi·ªát.\nRestart HASS, check l·∫°i c√≥ 1 package 40 bytes b·ªã drop:\nChain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 339 172K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 1 40 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1211K 1064M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Sau khi theo d√µi 1 th·ªùi gian th√¨ m√¨nh ƒë√£ b·∫Øt dc package b·ªã DROP trong /var/log/message ƒë√≥:\ntail -100f /var/log/message Sep 17 16:27:56 raspian-rpi kernel: [1659225.415084] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=80.94.92.239 DST=172.18.0.9 LEN=40 TOS=0x00 PREC=0x20 TTL=239 ID=54321 PROTO=TCP SPT=37396 DPT=8080 WINDOW=65535 RES=0x00 SYN URGP=0 Sep 17 16:58:12 raspian-rpi kernel: [1661041.646975] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=59.153.245.143 DST=172.18.0.4 LEN=60 TOS=0x00 PREC=0x00 TTL=47 ID=3383 DF PROTO=TCP SPT=13887 DPT=8200 WINDOW=65535 RES=0x00 SYN URGP=0 Sep 17 17:13:35 raspian-rpi kernel: [1661964.791511] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=89.248.165.199 DST=172.18.0.5 LEN=40 TOS=0x00 PREC=0x20 TTL=245 ID=48797 PROTO=TCP SPT=47266 DPT=9001 WINDOW=1024 RES=0x00 SYN URGP=0 N√≥ ƒëi ƒë·∫øn port 8080 c·ªßa zigbee2mqtt container, 9001 c·ªßa mosquitto v√† 8200 c·ªßa duplicati. Ngo√†i ra ko th·∫•y l·ªói g√¨ tr√™n HASS khi ho·∫°t ƒë·ªông -\u0026gt; cho qua v√† theo d√µi ti·∫øp..\n Ti·∫øp theo th·ª≠ add 1 rule ƒë·ªÉ ACCEPT traffic t·ª´ anywhere ƒë·∫øn port 3000,\n(Trong tr∆∞·ªùng h·ª£p b·∫°n c√≥ th√™m 1 app n√†o ƒë√≥ mu·ªën expose ra Internet, gi·∫£ s·ª≠ port 3000):\n# accept traffic TCP incoming interface wlan0 on Docker port 3000 from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --dport 3000 -j ACCEPT Test:\nT·ª´ m·∫°ng 4G -\u0026gt; port 3000, 8123 -\u0026gt; access ƒë∆∞·ª£c. (port 3000 b·ªã ·∫£nh h∆∞·ªüng b·ªüi rule v·ª´a xong, port 8123 th√¨ ko b·ªã ·∫£nh h∆∞·ªüng do n√≥ ko dc expose b·ªüi DOCKER).\nT·ª´ m·∫°ng 4G -\u0026gt; port 9000,8200,8080 -\u0026gt; ko access ƒë∆∞·ª£c.\nV·∫≠y l√† ƒë√∫ng √Ω r·ªìi.\nCh√∫ √Ω th·ª© t·ª± r·∫•t quan tr·ªçng (b·∫£ng d∆∞·ªõi ƒë√¢y ch·ªâ ra rule DROP all traffic ƒëang n·∫±m √°p ch√≥t m·ªõi ƒë∆∞·ª£c nh√©):\nsudo iptables -L -v -n | more .... Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 6472 356K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3000 93 74599 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 2022 494K DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 358K 192M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 N·∫øu mu·ªën remove 1 rule n√†o ƒë√≥ - ch·ªâ c·∫ßn thay ch·ªØ -I b·∫±ng ch·ªØ -D l√† OK, rule ƒë√≥ s·∫Ω b·ªã x√≥a, v√≠ d·ª•:\n# add rule iptables -I DOCKER-USER -p tcp -i wlan0 --dport 3000 -j ACCEPT # remove rule iptables -D DOCKER-USER -p tcp -i wlan0 --dport 3000 -j ACCEPT T·ªïng k·∫øt c√°c rule m√¨nh s·ª≠ d·ª•ng trong ph·∫ßn n√†y:\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 33000:65535 -j ACCEPT Gi·ªù s·∫Ω ti·∫øp t·ª•c x·ª≠ l√Ω ƒë·∫øn port 8123 v√† Chain INPUT\n3.2. Drop all traffic ƒëi v√†o Chain INPUT, r·ªìi allow t·ª´ng port m·ªôt sau tr∆∞·ªõc ti√™n ƒë·ªÉ tr√°nh l√†m HASS b·ªã crash, th√¨ n√™n stop HASS container tr∆∞·ªõc. (ho·∫∑c stop c√°i Automation send error to Telegram).\nSau ƒë√≥:\n# drop all traffic TCP incoming interface wlan0 Chain INPUT, except LAN, log to LOG_DROP iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j LOG_DROP sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 2663K packets, 2572M bytes) pkts bytes target prot opt in out source destination 2 88 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 Trong LOG_DROP s·∫Ω th·∫•y r·∫•t nhi·ªÅu port b·ªã DROP t·ª´ kho·∫£ng 22000-65535\nTrong HASS s·∫Ω th·∫•y 1 s·ªë l·ªói:\ntelegram.error.NetworkError: urllib3 HTTPError HTTPSConnectionPool(host='api.telegram.org', port=443): Max retries exceeded with url: /bot/getMe (Caused by NewConnectionError('\u0026lt;telegram.vendor.ptb_urllib3.urllib3.connection.VerifiedHTTPSConnection object at 0x7f8f6c9f90\u0026gt;: Failed to establish a new connection: [Errno 101] Network unreachable')) requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='route.lgthinq.com', port=46030): Max retries exceeded with url: /v1/service/application/gateway-uri (Caused by ConnectTimeoutError(\u0026lt;urllib3.connection.HTTPSConnection object at 0x7f9a880310\u0026gt;, 'Connection to route.lgthinq.com timed out. (connect timeout=10)')) 2022-09-17 18:07:42.403 ERROR (MainThread) [homeassistant] Error doing job: Future exception was never retrieved requests.exceptions.ConnectionError: HTTPSConnectionPool(host='route.lgthinq.com', port=46030): Max retries exceeded with url: /v1/service/application/gateway-uri (Caused by NewConnectionError('\u0026lt;urllib3.connection.HTTPSConnection object at 0x7f88286320\u0026gt;: Failed to establish a new connection: [Errno -3] Try again')) 2022-09-17 18:02:30.066 ERROR (MainThread) [metno] Access to https://aa015h6buqvih86i1.api.met.no/weatherapi/locationforecast/2.0/complete returned error 'ClientConnectorError' 2022-09-17 18:02:30.553 ERROR (MainThread) [custom_components.hacs] Request exception for 'https://api.github.com/rate_limit' with - Cannot connect to host api.github.com:443 ssl:default [Try again] Traceback (most recent call last): File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 451, in async_can_update response = await self.async_github_api_method(self.githubapi.rate_limit) File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 505, in async_github_api_method raise HacsException(_exception) custom_components.hacs.exceptions.HacsException: Request exception for 'https://api.github.com/rate_limit' with - Cannot connect to host api.github.com:443 ssl:default [Try again] 2022-09-17 18:02:55.077 WARNING (MainThread) [custom_components.smartthinq_sensors] Connection not available. ThinQ platform not ready 2022-09-17 18:02:30.066 ERROR (MainThread) [metno] Access to https://aa015h6buqvih86i1.api.met.no/weatherapi/locationforecast/2.0/complete returned error 'ClientConnectorError' 2022-09-17 18:02:30.553 ERROR (MainThread) [custom_components.hacs] Request exception for 'https://api.github.com/rate_limit' with - Cannot connect to host api.github.com:443 ssl:default [Try again] Traceback (most recent call last): File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 451, in async_can_update response = await self.async_github_api_method(self.githubapi.rate_limit) File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 505, in async_github_api_method raise HacsException(_exception) Qu√° tr√¨nh b·ªã l·ªói n√†y t·∫°o ra nhi·ªÅu file r√°c trong /var/log/journal/* n√™n x√≥a ƒëi b·ªõt, ho·∫∑c setting cho n√≥ ch·ªâ ch·ª©a t·ªëi ƒëa 200M trong ƒë√≥ th√¥i (L√†m theo b√†i n√†y: https://unix.stackexchange.com/questions/130786/can-i-remove-files-in-var-log-journal-and-var-cache-abrt-di-usr) test app Duplicati, ch∆∞a k·ªãp test th√¨ b·ªã server b·ªã treo/ƒë∆° lu√¥n, HASS l·∫°i b·ªã restart, full CPU, ko check dc n·ªØa.\nLog c·ªßa HASS c√≥ l·ªói li√™n quan ƒë·∫øn call api weather ra ngo√†i ko n·ªïi.\nCu·ªëi c√πng ph·∫£i ƒë·ª£i v√†o ƒëc Portainer UI, stop homeassistant container.\nth·∫ø n√™n ph·∫£i add th√™m:\n# accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --match multiport --dport 22000:65535 -j ACCEPT sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 2687K packets, 2589M bytes) pkts bytes target prot opt in out source destination 12 9365 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 22000:65535 344 19784 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 restart HASS -\u0026gt; th√¨ ko c√≤n log l·ªói n·ªØa\nTrong LOG_DROP th√¨ m√¨nh ph√°t hi·ªán nhi·ªÅu traffic ƒë·∫øn c√°c port nh·ªè h∆°n nh∆∞ DPT=5007,4100,81,20000,4228,18617,2525\u0026hellip;etc. N√≥i chung r·∫•t nhi·ªÅu port nh·ªè h∆°n 22000 b·ªã DROP. Nh∆∞ng HASS v·∫´n ho·∫°t ƒë·ªông ·ªïn n√™n cho qua..\nGi·ªù accept traffic on port 8123 ƒë·ªÉ t·ª´ m·∫°ng 4G c√≥ th·ªÉ connect ƒë·∫øn ƒë∆∞·ª£c:\n# accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --dport 8123 -j ACCEPT sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 2699K packets, 2600M bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8123 2203 1380K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 22000:65535 442 25513 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 ƒê·∫øn ƒë√¢y v·ªÅ c∆° b·∫£n m·ªçi th·ª© ƒë√£ OK. Add th√™m accept c√°c port 80,443 l√† ƒë∆∞·ª£c, ƒë·ªÉ sau n√†y l√†m https.\nT·ªïng k·∫øt c√°c rule m√¨nh s·ª≠ d·ª•ng trong ph·∫ßn n√†y:\n# drop all traffic TCP incoming interface wlan0 Chain INPUT, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --match multiport --dport 22000:65535 -j ACCEPT # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --dport 8123 -j ACCEPT C√≤n B√™n d∆∞·ªõi l√† c√°c h∆∞·ªõng m√¨nh ƒë√£ l√†m nh∆∞ng ko recommend l·∫Øm.\n4. M·ªôt s·ªë ch√∫ √Ω 4.1. N·∫øu b·∫°n ch·ªâ add rule iptables v√†o Chain INPUT th√¨ sao Add rule n√†y v√†o chain INPUT:\n# drop all traffic incoming interface wlan0 Chain INPUT, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP Test:\nT·ª´ m·∫°ng 4G -\u0026gt; port 8123 -\u0026gt; ko access ƒë∆∞·ª£c.\nT·ª´ m·∫°ng 4G -\u0026gt; port Docker (3000, 9000, 8200, \u0026hellip;) -\u0026gt; access ƒë∆∞·ª£c h·∫øt.\nT·ªïng k·∫øt: Nh∆∞ v·∫≠y vi·ªác add rule drop traffic ·ªü Chain INPUT ko ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn Chain DOCKER-USER c·∫£. ‚òπ\n4.2. N·∫øu b·∫°n ch·ªâ drop traffic ·ªü t·ª´ng port c·ª• th·ªÉ th√¥i # drop traffic incoming interface wlan0 Chain DOCKER-USER port XXX from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 3000 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 9000 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 9001 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 1883 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 9090 -j DROP iptables -I DOCKER-USER -p udp -i wlan0 ! -s 192.168.1.0/24 --dport 51820 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 4444 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 8200 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 8080 -j DROP # drop traffic incoming interface wlan0 Chain INPUT port 22 t·ª´ anywhere, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 22 -j DROP n√≥ s·∫Ω ƒë∆∞·ª£c nh∆∞ n√†y ( to√†n c√°c rule ƒë∆∞·ª£c add v√†o Chain DOCKER-USER, 1 rule ƒë∆∞·ª£c add v√†o Chain INPUT ƒë·ªÉ ch·∫∑n port 22 ):\nsudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 1492K packets, 1581M bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:22 Chain FORWARD (policy ACCEPT 4921 packets, 2451K bytes) pkts bytes target prot opt in out source destination 838K 745M DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 794K 736M DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 0 0 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0 422K 168M ACCEPT all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 1653 224K DOCKER all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 371K 568M ACCEPT all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 756 176K ACCEPT all -- br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 1559K packets, 1541M bytes) pkts bytes target prot opt in out source destination Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 207 11020 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.2 tcp dpt:9000 34 1720 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:9001 4 208 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:1883 5 240 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.6 tcp dpt:9090 0 0 ACCEPT udp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.10 udp dpt:51820 125 6832 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.7 tcp dpt:3000 86 4704 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.4 tcp dpt:8200 60 3176 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.9 tcp dpt:8080 Chain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 0 0 DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 371K 568M DOCKER-ISOLATION-STAGE-2 all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 794K 736M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) pkts bytes target prot opt in out source destination 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 DROP all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 371K 568M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 45 2600 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:8080 17 908 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:8200 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:4444 0 0 DROP udp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 udp dpt:51820 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:9090 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:1883 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:9001 31 1804 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:9000 52 3120 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:3000 794K 736M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Test:\nT·ª´ m·∫°ng 4G -\u0026gt; port 8123 -\u0026gt; access ƒë∆∞·ª£c. (d·ªÖ hi·ªÉu th√¥i v√¨ n√≥ ko ph·∫£i port trong Docker chain). T·ª´ m·∫°ng 4G -\u0026gt; port Docker -\u0026gt; ko access ƒë∆∞·ª£c. T·ª´ local -\u0026gt; access ƒë∆∞·ª£c h·∫øt.\nTest connection ƒë·∫øn AWS S3 t·ª´ Duplicati: worked Log c·ªßa HASS: ko c√≥ l·ªói g√¨.\nL√™n canyouseeme.org, check h·∫øt c√°c port xem ·ªü ngo√†i c√≥ th·∫•y service n√†o ch·∫°y ko?\nC√≥ v·∫ª t·∫°m ch·∫•p nh·∫≠n th√¥i.\ncheck l·∫°i c√°c port ƒëang open b·∫±ng nmap:\n# UDP scan sudo nmap -sU -p- 192.168.1.128 Starting Nmap 7.80 ( https://nmap.org ) at 2022-09-16 17:01 +07 Nmap scan report for 192.168.1.128 Host is up (0.000041s latency). Not shown: 65529 closed ports PORT STATE SERVICE 68/udp open|filtered dhcpc 546/udp open|filtered dhcpv6-client 5353/udp open zeroconf 34484/udp open|filtered unknown 46861/udp open|filtered unknown 51820/udp open|filtered unknown # TCP scan sudo nmap -sT -p- 192.168.1.128 Starting Nmap 7.80 ( https://nmap.org ) at 2022-09-16 17:05 +07 Nmap scan report for 192.168.1.128 Host is up (0.00072s latency). Not shown: 65527 closed ports PORT STATE SERVICE 22/tcp open ssh 1883/tcp open mqtt 3000/tcp open ppp 8080/tcp open http-proxy 8123/tcp open polipo 8200/tcp open trivnet1 9000/tcp open cslistener 9090/tcp open zeus-admin Nh∆∞ v·∫≠y l√† b·∫°n th·∫•y h·∫ßu h·∫øt c√°c port TCP ƒëang open kia m√¨nh ƒë√£ close l·∫°i b·∫±ng iptables r·ªìi (tr·ª´ 8123 ƒë·ªÉ s·ª≠ d·ª•ng). C√°c port TCP th√¨ c√≥ v√†i port h∆°i l·∫° ko bi·∫øt ƒë·ªÉ l√†m g√¨: 34484, 46861\nTh·ª≠ DROP 2 port l·∫° ho·∫Øc tr√™n xem c√≥ v·∫•n ƒë·ªÅ g√¨ x·∫£y ra, ƒë·∫ßu ti√™n l√† port 34484:\niptables -I INPUT -p udp -i wlan0 ! -s 192.168.1.0/24 --dport 34484 -j DROP Test: ko th·∫•y c√≥ l·ªói n√†o x·∫£y ra. HASS v·∫´n ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng. Ko c√≥ log l·ªói. Duplicati test connection to S3: worked.\nti·∫øp port 46861:\niptables -I INPUT -p udp -i wlan0 ! -s 192.168.1.0/24 --dport 46861 -j DROP Test: ko th·∫•y l·ªói g√¨ x·∫£y ra.\nT·ªïng k·∫øt: C√°ch n√†y v·ªÅ c∆° b·∫£n c≈©ng c√≥ th·ªÉ ch·∫•p nh·∫≠n ƒë∆∞·ª£c. Nh∆∞ng m√¨nh th√≠ch 3.1,3.2 h∆°n.\n5. T√≥m t·∫Øt C√°c command c·∫ßn l√†m:\n# Chain DOCKER-USER # drop any traffic TCP incoming interface wlan0 on Docker from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # This open makes connection from Duplciati to S3 works # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 33000:65535 -j ACCEPT iptables -I DOCKER-USER -p tcp -i wlan0 --dport 80 -j ACCEPT iptables -I DOCKER-USER -p tcp -i wlan0 --dport 443 -j ACCEPT # Chain INPUT # drop all traffic TCP incoming interface wlan0 Chain INPUT from anywhere, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # This open makes HASS working normal # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --match multiport --dport 22000:65535 -j ACCEPT # This open HASS so you can access from Outside Internet # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --dport 8123 -j ACCEPT Confirm:\nChain INPUT (policy ACCEPT 2756K packets, 2653M bytes) pkts bytes target prot opt in out source destination 19 778 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8123 76 44116 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 22000:65535 5 208 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1298K 1123M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Khi mu·ªën Debug: https://stackoverflow.com/a/29544353/9922066\nLet\u0026rsquo;s create a chain to log and accept:\niptables -N LOG_ACCEPT And let\u0026rsquo;s populate its rules:\niptables -A LOG_ACCEPT -j LOG --log-prefix \u0026#34;INPUT:ACCEPT:\u0026#34; --log-level 6 iptables -A LOG_ACCEPT -j ACCEPT Now let\u0026rsquo;s create a chain to log and drop:\niptables -N LOG_DROP And let\u0026rsquo;s populate its rules:\niptables -A LOG_DROP -j LOG --log-prefix \u0026#34;INPUT:DROP: \u0026#34; --log-level 6 iptables -A LOG_DROP -j DROP Confirm:\nChain LOG_ACCEPT (0 references) pkts bytes target prot opt in out source destination 154 129K LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:ACCEPT:\u0026quot; 154 129K ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 Chain LOG_DROP (0 references) pkts bytes target prot opt in out source destination 1088 58637 LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:DROP: \u0026quot; 1088 58637 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 Now you can do all actions in one go by jumping (-j) to you custom chains instead of the default LOG / ACCEPT / REJECT / DROP:\niptables -A \u0026lt;your_chain_here\u0026gt; \u0026lt;your_conditions_here\u0026gt; -j LOG_ACCEPT iptables -A \u0026lt;your_chain_here\u0026gt; \u0026lt;your_conditions_here\u0026gt; -j LOG_DROP CREDIT https://stackoverflow.com/a/56038551/9922066\nhttps://docs.docker.com/network/iptables/\nhttps://stackoverflow.com/a/29544353/9922066\n","href":"/bk/encrypt-setup-home-assistant-on-raspberry-pi-p2-dmz/","title":"Setup Home Assistant on Raspberry Pi (Part 2) - iptables rule and DMZ"},{"content":"ƒê√¢y l√† ph·∫ßn c·ªë g·∫Øng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ khi m√¨nh l√†m c√°i n√†y: 9. Install DuckDNS (for expose outside access purpose)\nT√≥m t·∫Øt t√¨nh hu·ªëng c·ªßa m√¨nh: C√≥ 1 RPi, ch·∫°y HASS tr√™n docker. D√ông m·∫°ng VNPT, ko th·ªÉ Port Forwarding ƒë∆∞·ª£c port 443 (do router ƒë√£ reserve port 443, 80).\n-\u0026gt; Ph·∫£i d√πng ch·ª©c nƒÉng DMZ - cho RPi v√†o DMZ, nh∆∞ng ch·ª©c nƒÉng n√†y l·∫°i m·ªü all port (bao g·ªìm c·∫£ 443) tr√™n con RPi (risk v·ªÅ security)\n-\u0026gt; Ph·∫£i t√¨m c√°ch ƒë·ªÉ h·∫°n ch·∫ø traffic t·ª´ Internet v√†o DMZ (ch·ªâ cho traffic ƒëi v√†o port 80,443 v√† 1 s·ªë port c·∫ßn thi·∫øt th√¥i)\n-\u0026gt; M√¨nh t√¨m ƒë∆∞·ª£c c√°ch: D√πng iptables command, t·∫°o c√°c rule tr√™n ch√≠nh con RPi (ko cho n√≥ nh·∫≠n traffic t·ª´ b√™n ngo√†i Internet v√†o 1 s·ªë port)\n1. Check xem RPi ƒëang n√≥i chuy·ªán v·ªõi Internet b·∫±ng interface n√†o root@raspian-rpi:/opt/hass# ifconfig br-564d64d5a58a: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fexxxxxxxxxxxx8b prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 0xxxxxxxxxxxx:8b txqueuelen 0 (Ethernet) RX packets 9724621 bytes 14542091134 (13.5 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 9678446 bytes 14944508794 (13.9 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 docker0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fxxxxxxxxxxxxxbb prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 0xxxxxxxxxxxxbb txqueuelen 0 (Ethernet) RX packets 187 bytes 24270 (23.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 207 bytes 27492 (26.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 ether exxxxxxxxxxx6 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 1801769 bytes 441955165 (421.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1801769 bytes 441955165 (421.4 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 wlan0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.128 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fxxxxxxxxxxxxxxx36 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether exxxxxxxxxxxxx97 txqueuelen 1000 (Ethernet) RX packets 5982425 bytes 2708750541 (2.5 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7859191 bytes 1834756180 (1.7 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Trong nhi·ªÅu tr∆∞·ªùng h·ª£p, n·∫øu server c·ªßa b·∫°n c·∫Øm d√¢y LAN ƒë·∫øn Router ƒë·ªÉ v√†o m·∫°ng th√¨ c√≥ nghƒ©a l√† n√≥ d√πng eth0.\nNh∆∞ng v√¨ RPi c·ªßa m√¨nh d√πng Wifi ƒë·ªÉ v√†o m·∫°ng -\u0026gt; n√™n c√°i n√≥ ƒëang d√πng ph·∫£i l√† wlan0 -\u0026gt; ƒê·ªÉ confirm th√¨ B·∫°n s·∫Ω th·∫•y l∆∞·ª£ng package/traffic ƒëi ra v√†o wlan0 kh√° nhi·ªÅu (RX packets 5982425 bytes 2708750541 (2.5 GiB))\nV·∫≠y l√† x√°c ƒë·ªãnh ƒë∆∞·ª£c interface l√† wlan0\n2. Check c√°c rule ƒë√£ t·ªìn t·∫°i trong iptables sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 635K packets, 705M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 4921 packets, 2451K bytes) pkts bytes target prot opt in out source destination 279K 149M DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 279K 149M DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 0 0 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0 152K 85M ACCEPT all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 634 122K DOCKER all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 126K 64M ACCEPT all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 489 114K ACCEPT all -- br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 673K packets, 687M bytes) pkts bytes target prot opt in out source destination Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 20 1128 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.2 tcp dpt:9000 6 312 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.4 tcp dpt:8200 1 40 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:9001 0 0 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:1883 0 0 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.6 tcp dpt:9090 11 564 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.9 tcp dpt:8080 0 0 ACCEPT udp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.10 udp dpt:51820 0 0 ACCEPT tcp -- !docker0 docker0 0.0.0.0/0 172.17.0.2 tcp dpt:4444 39 2180 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.7 tcp dpt:3000 Chain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 0 0 DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 126K 64M DOCKER-ISOLATION-STAGE-2 all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 279K 149M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) pkts bytes target prot opt in out source destination 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 DROP all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 126K 64M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 279K 149M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Ch√∫ √Ω v√†o Chain DOCKER, cho th·∫•y hi·ªán ƒëang m·ªü kh√° nhi·ªÅu port:\ntcp dpt:9000 tcp dpt:8200 tcp dpt:9001 tcp dpt:1883 tcp dpt:9090 tcp dpt:8080 udp dpt:51820 tcp dpt:3000 nh∆∞ng ko c√≥ port 8123 trong list n√†y (Ch·ªó n√†y b·ªüi v√¨ HASS ko expose port 8123 t·ª´ Docker ra ngo√†i n√™n b·∫°n s·∫Ω ko th·∫•y n√≥ trong Chain DOCKER)\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 228ccfed3015 grafana/grafana:8.5.5 \u0026quot;/run.sh\u0026quot; 6 hours ago Up 6 hours 0.0.0.0:3000-\u0026gt;3000/tcp, :::3000-\u0026gt;3000/tcp grafana 105c497b8114 ghcr.io/home-assistant/home-assistant:2022.8.2 \u0026quot;/init\u0026quot; 24 hours ago Up 24 hours homeassistant 45f43bcdae93 prom/prometheus:v2.36.0 \u0026quot;/bin/prometheus --c‚Ä¶\u0026quot; 6 weeks ago Up 24 hours 0.0.0.0:9090-\u0026gt;9090/tcp, :::9090-\u0026gt;9090/tcp prometheus 324fd3e2ce26 portainer/portainer-ce:2.13.1 \u0026quot;/portainer\u0026quot; 2 months ago Up 24 hours 8000/tcp, 9443/tcp, 0.0.0.0:9000-\u0026gt;9000/tcp, :::9000-\u0026gt;9000/tcp portainer 2d80eee17aaf prom/node-exporter:v1.3.1 \u0026quot;/bin/node_exporter\u0026quot; 3 months ago Up 24 hours 9100/tcp monitoring_node_exporter 7c56755c52fe koenkk/zigbee2mqtt:1.25.0 \u0026quot;docker-entrypoint.s‚Ä¶\u0026quot; 3 months ago Up 24 hours 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp zigbee2mqtt b00bc6092048 lscr.io/linuxserver/wireguard:1.0.20210914 \u0026quot;/init\u0026quot; 3 months ago Up 24 hours 0.0.0.0:51820-\u0026gt;51820/udp, :::51820-\u0026gt;51820/udp wireguard ebfecd7ed0b4 lscr.io/linuxserver/duckdns:68a3222a-ls97 \u0026quot;/init\u0026quot; 3 months ago Up 24 hours duckdns ae09a218d43f eclipse-mosquitto:2.0.14 \u0026quot;/docker-entrypoint.‚Ä¶\u0026quot; 3 months ago Up 24 hours 0.0.0.0:1883-\u0026gt;1883/tcp, :::1883-\u0026gt;1883/tcp, 0.0.0.0:9001-\u0026gt;9001/tcp, :::9001-\u0026gt;9001/tcp mosquitto 413fcaac7b9a lscr.io/linuxserver/duplicati:2.0.6 \u0026quot;/init\u0026quot; 3 months ago Up 24 hours 0.0.0.0:8200-\u0026gt;8200/tcp, :::8200-\u0026gt;8200/tcp duplicati Gi·ªù c·∫ßn ch√∫ √Ω v√†o Chain DOCKER-USER, hi·ªán ƒëang ch·ªâ c√≥ 1 rule RETURN, ko c·∫ßn quan t√¢m, nh∆∞ v·∫≠y coi nh∆∞ ko c√≥ rule n√†o. ƒê√¢y ch√≠nh l√† n∆°i m√† c√°c b·∫°n s·∫Ω add rule v√†o, ƒë·ªÉ h·∫°n ch·∫ø traffic ƒëi v√†o c√°c port trong Docker c·ªßa b·∫°n.\n3. D√πng iptables ƒë·ªÉ filter c√°c traffic 3.1. Drop all traffic ƒëi v√†o Chain DOCKER-USER tr∆∞·ªõc, r·ªìi allow t·ª´ng port m·ªôt sau ƒê·∫ßu ti√™n h√£y test ƒë·ªÉ ch·∫Øc ch·∫Øn ƒëang l√†m ƒë√∫ng:\nT·ª´ m·∫°ng 4G -\u0026gt; port 3000, 9000, 8123 -\u0026gt; access ƒë∆∞·ª£c h·∫øt.\nT·∫•t nhi√™n r·ªìi, b·ªüi v√¨ ch√∫ng ta ch∆∞a add rule n√†o m√†.\nGi·ªù add 1 rule ƒë·ªÉ DROP t·∫•t c·∫£ traffic ƒë·∫øn Docker ngo·∫°i tr·ª´ m·∫°ng LAN 192.168.1.0/24:\nCh√∫ √Ω th·ª© t·ª± r·∫•t quan tr·ªçng, c√¢u l·ªánh n√†y ph·∫£i run tr∆∞·ªõc khi ch·∫°y c√°c c√¢u l·ªánh add rule kh√°c\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP Test:\nT·ª´ m·∫°ng local -\u0026gt; port 8123, 3000, 9000, 8200 -\u0026gt; access ƒë∆∞·ª£c. T·ª´ m·∫°ng 4G -\u0026gt; port 8123 -\u0026gt; access ƒë∆∞·ª£c.\nT·ª´ m·∫°ng 4G -\u0026gt; port 3000, 9000, 8200, 8080 -\u0026gt; ko access ƒë∆∞·ª£c.\nPort 3000,9000,8200,8080 ·ªü trong Chain DOCKER n√™n n√≥ ch·ªãu ·∫£nh h∆∞·ªüng b·ªüi rule tr√™n. C√≤n port 8123 th√¨ ko, n√™n b·∫°n v·∫´n access ƒë∆∞·ª£c. HASS c√°c ho·∫°t ƒë·ªông: ko c√≥ l·ªói ƒë·∫∑c bi·ªát.\nTest Duplicati connection to S3: L·ªói lu√¥n ‚ùå\nC√≥ l·∫Ω l√† Duplicati khi test connection to S3 n√≥ c·∫ßn TCP incoming traffic üòÅ\nKo hi·ªÉu v√¨ sao?\nT√¨m th·∫•y 1 post ko ai tr·∫£ l·ªùi:\nhttps://stackoverflow.com/questions/71763830/what-ports-are-required-to-open-for-aws-s3-to-work Th·ª≠ accept port 443:\n# accept traffic TCP incoming interface wlan0 on Docker port 443 from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --dport 443 -j ACCEPT Test l·∫°i Duplicati connection to S3: V·∫´n l·ªói‚ùå\nTh·ª≠ accept port 80:\n# accept traffic TCP incoming interface wlan0 on Docker port 80 from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --dport 80 -j ACCEPT Test l·∫°i Duplicati connection to S3: V·∫´n l·ªói‚ùå\nTh·ª≠ accept port 55000-and 65535:\n# accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 55000:65535 -j ACCEPT Test l·∫°i Duplicati connection to S3: V·∫´n l·ªói‚ùå- nh∆∞ng log l·ªói c√≥ v·∫ª ng·∫Øn h∆°n: A WebException with status Timeout was thrown\n[services.d] done. Amazon.Runtime.AmazonServiceException: A WebException with status Timeout was thrown. ---\u0026gt; System.Net.WebException: The operation has timed out. at System.Net.HttpWebRequest.GetRequestStream () [0x00016] in \u0026lt;9c6e2cb7ddd8473fa420642ddcf7ce48\u0026gt;:0 at Amazon.Runtime.Internal.HttpRequest.GetRequestContent () [0x00000] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.HttpHandler`1[TRequestContent].InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x0006b] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.PipelineHandler.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x0000e] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.Unmarshaller.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x00000] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.PipelineHandler.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x0000e] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 at Amazon.Runtime.Internal.ErrorHandler.InvokeSync (Amazon.Runtime.IExecutionContext executionContext) [0x00000] in \u0026lt;e28e89f25c1649a69062a2c53f89d718\u0026gt;:0 --- End of inner exception stack trace --- sudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 113 110K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 55000:65535 81 4596 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1006K 976M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Nh√¨n th·∫•y 2 rule tcp dpt:443 v√† tcp dpt:80 ko th·∫•y c√≥ traffic g√¨ (c·ªôt pkts, bytes) ch·ª©ng t·ªè v√¥ nghƒ©a khi th√™m v√†o. Ch·ªß y·∫øu l√† rule multiport dports 55000:65535 c·∫ßn thi·∫øt th√¥i.\nQuy·∫øt ƒë·ªãnh debug ƒë·∫øn c√πng ƒë·ªÉ ra v·∫•n ƒë·ªÅ:\nhttps://stackoverflow.com/questions/21771684/iptables-log-and-drop-in-one-rule\nLet\u0026rsquo;s create a chain to log and accept:\niptables -N LOG_ACCEPT And let\u0026rsquo;s populate its rules:\niptables -A LOG_ACCEPT -j LOG --log-prefix \u0026#34;INPUT:ACCEPT:\u0026#34; --log-level 6 iptables -A LOG_ACCEPT -j ACCEPT Now let\u0026rsquo;s create a chain to log and drop:\niptables -N LOG_DROP And let\u0026rsquo;s populate its rules:\niptables -A LOG_DROP -j LOG --log-prefix \u0026#34;INPUT:DROP: \u0026#34; --log-level 6 iptables -A LOG_DROP -j DROP Now you can do all actions in one go by jumping (-j) to you custom chains instead of the default LOG / ACCEPT / REJECT / DROP:\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN, log result to LOG_DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j LOG_DROP # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere, log result to LOG_ACCEPT iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 55000:65535 -j LOG_ACCEPT S·∫Ω ƒë∆∞·ª£c c√°i output nh∆∞ n√†y, ch√∫ √Ω ƒë√∫ng th·ª© t·ª± l√† ok:\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 113 110K LOG_ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 55000:65535 81 4596 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1006K 976M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain LOG_ACCEPT (1 references) pkts bytes target prot opt in out source destination 113 110K LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:ACCEPT:\u0026quot; 113 110K ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 Chain LOG_DROP (1 references) pkts bytes target prot opt in out source destination 81 4596 LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:DROP: \u0026quot; 81 4596 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 Test: Duplicati - Test connection to AWS S3.\nC√πng l√∫c ƒë√≥, v√†o log check: tail -200f /var/log/message, b·∫°n s·∫Ω th·∫•y c√≥ r·∫•t nhi·ªÅu package ACCEPT/DROP ƒë√£ ƒë∆∞·ª£c log l·∫°i:\nƒê∆∞·ª£c Accept:\nSep 16 23:43:48 raspian-rpi kernel: [1598976.871464] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=60 TOS=0x00 PREC=0x20 TTL=51 ID=0 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=65160 RES=0x00 ACK SYN URGP=0 Sep 16 23:43:48 raspian-rpi kernel: [1598977.137601] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=52 TOS=0x00 PREC=0x20 TTL=51 ID=12226 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=508 RES=0x00 ACK URGP=0 Sep 16 23:43:48 raspian-rpi kernel: [1598977.141156] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=1492 TOS=0x00 PREC=0x20 TTL=51 ID=12227 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=508 RES=0x00 ACK URGP=0 Sep 16 23:43:48 raspian-rpi kernel: [1598977.141322] INPUT:ACCEPT:IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=139.59.135.67 DST=172.18.0.4 LEN=1492 TOS=0x00 PREC=0x20 TTL=51 ID=12228 DF PROTO=TCP SPT=443 DPT=58882 WINDOW=508 RES=0x00 ACK PSH URGP=0 B·ªã drop:\nSep 16 23:48:06 raspian-rpi kernel: [1599234.486899] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=52.119.198.223 DST=172.18.0.4 LEN=52 TOS=0x00 PREC=0x20 TTL=238 ID=51533 DF PROTO=TCP SPT=443 DPT=53954 WINDOW=8190 RES=0x00 ACK SYN URGP=0 Sep 16 23:48:20 raspian-rpi kernel: [1599248.485699] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=52.219.36.104 DST=172.18.0.4 LEN=52 TOS=0x00 PREC=0x20 TTL=239 ID=27892 PROTO=TCP SPT=80 DPT=51988 WINDOW=64240 RES=0x00 ACK SYN URGP=0 ƒê·∫øn ƒë√¢y th√¨ c√≥ v·∫ª d·∫ßn hi·ªÉu ra v·∫•n ƒë·ªÅ, open port 55000:65535 l√† ch∆∞a ƒë·ªß cho Duplicati call api c·ªßa AWS S3 SDK.\nL·∫ßn theo LOG th√¨ m√¨nh th·∫•y DPT=33234 c√≥ v·∫ª l√† s·ªë nh·ªè nh·∫•t trong s·ªë c√°c port b·ªã DROP.\nTh·∫ø n√™n m√¨nh s·∫Ω allow port t·ª´ 33000:65535 ƒë·ªÉ test l·∫°i connection.\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 33000:65535 -j ACCEPT nh∆∞ n√†y l√† ok:\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1202K 1060M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Test l·∫°i Duplicati connection to S3: worked üëå -\u0026gt; v·∫≠y l√† ngon r·ªìi\nCheck l·∫°i c√≥ bao nhi√™u pkts/bytes b·ªã DROP trong khi test, th√¨ m√¨nh th·∫•y 0 pkts,bytes -\u0026gt; r·∫•t t·ªët üòÅ\nsudo iptables -L -v -n | more Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 93 74599 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1203K 1060M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Check ho·∫°t ƒë·ªông c·ªßa HASS: Log ko c√≥ l·ªói ƒë·∫∑c bi·ªát.\nRestart HASS, check l·∫°i c√≥ 1 package 40 bytes b·ªã drop:\nChain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 339 172K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 1 40 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1211K 1064M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Sau khi theo d√µi 1 th·ªùi gian th√¨ m√¨nh ƒë√£ b·∫Øt dc package b·ªã DROP trong /var/log/message ƒë√≥:\ntail -100f /var/log/message Sep 17 16:27:56 raspian-rpi kernel: [1659225.415084] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=80.94.92.239 DST=172.18.0.9 LEN=40 TOS=0x00 PREC=0x20 TTL=239 ID=54321 PROTO=TCP SPT=37396 DPT=8080 WINDOW=65535 RES=0x00 SYN URGP=0 Sep 17 16:58:12 raspian-rpi kernel: [1661041.646975] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=59.153.245.143 DST=172.18.0.4 LEN=60 TOS=0x00 PREC=0x00 TTL=47 ID=3383 DF PROTO=TCP SPT=13887 DPT=8200 WINDOW=65535 RES=0x00 SYN URGP=0 Sep 17 17:13:35 raspian-rpi kernel: [1661964.791511] INPUT:DROP: IN=wlan0 OUT=br-564d64d5a58a MAC=e4:5f:01:3d:a3:97:d0:96:fb:99:7f:2f:08:00 SRC=89.248.165.199 DST=172.18.0.5 LEN=40 TOS=0x00 PREC=0x20 TTL=245 ID=48797 PROTO=TCP SPT=47266 DPT=9001 WINDOW=1024 RES=0x00 SYN URGP=0 N√≥ ƒëi ƒë·∫øn port 8080 c·ªßa zigbee2mqtt container, 9001 c·ªßa mosquitto v√† 8200 c·ªßa duplicati. Ngo√†i ra ko th·∫•y l·ªói g√¨ tr√™n HASS khi ho·∫°t ƒë·ªông -\u0026gt; cho qua v√† theo d√µi ti·∫øp..\n Ti·∫øp theo th·ª≠ add 1 rule ƒë·ªÉ ACCEPT traffic t·ª´ anywhere ƒë·∫øn port 3000,\n(Trong tr∆∞·ªùng h·ª£p b·∫°n c√≥ th√™m 1 app n√†o ƒë√≥ mu·ªën expose ra Internet, gi·∫£ s·ª≠ port 3000):\n# accept traffic TCP incoming interface wlan0 on Docker port 3000 from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --dport 3000 -j ACCEPT Test:\nT·ª´ m·∫°ng 4G -\u0026gt; port 3000, 8123 -\u0026gt; access ƒë∆∞·ª£c. (port 3000 b·ªã ·∫£nh h∆∞·ªüng b·ªüi rule v·ª´a xong, port 8123 th√¨ ko b·ªã ·∫£nh h∆∞·ªüng do n√≥ ko dc expose b·ªüi DOCKER).\nT·ª´ m·∫°ng 4G -\u0026gt; port 9000,8200,8080 -\u0026gt; ko access ƒë∆∞·ª£c.\nV·∫≠y l√† ƒë√∫ng √Ω r·ªìi.\nCh√∫ √Ω th·ª© t·ª± r·∫•t quan tr·ªçng (b·∫£ng d∆∞·ªõi ƒë√¢y ch·ªâ ra rule DROP all traffic ƒëang n·∫±m √°p ch√≥t m·ªõi ƒë∆∞·ª£c nh√©):\nsudo iptables -L -v -n | more .... Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 6472 356K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3000 93 74599 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 2022 494K DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 358K 192M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 N·∫øu mu·ªën remove 1 rule n√†o ƒë√≥ - ch·ªâ c·∫ßn thay ch·ªØ -I b·∫±ng ch·ªØ -D l√† OK, rule ƒë√≥ s·∫Ω b·ªã x√≥a, v√≠ d·ª•:\n# add rule iptables -I DOCKER-USER -p tcp -i wlan0 --dport 3000 -j ACCEPT # remove rule iptables -D DOCKER-USER -p tcp -i wlan0 --dport 3000 -j ACCEPT T·ªïng k·∫øt c√°c rule m√¨nh s·ª≠ d·ª•ng trong ph·∫ßn n√†y:\n# drop any traffic TCP incoming interface wlan0 from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 33000:65535 -j ACCEPT Gi·ªù s·∫Ω ti·∫øp t·ª•c x·ª≠ l√Ω ƒë·∫øn port 8123 v√† Chain INPUT\n3.2. Drop all traffic ƒëi v√†o Chain INPUT, r·ªìi allow t·ª´ng port m·ªôt sau tr∆∞·ªõc ti√™n ƒë·ªÉ tr√°nh l√†m HASS b·ªã crash, th√¨ n√™n stop HASS container tr∆∞·ªõc. (ho·∫∑c stop c√°i Automation send error to Telegram).\nSau ƒë√≥:\n# drop all traffic TCP incoming interface wlan0 Chain INPUT, except LAN, log to LOG_DROP iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j LOG_DROP sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 2663K packets, 2572M bytes) pkts bytes target prot opt in out source destination 2 88 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 Trong LOG_DROP s·∫Ω th·∫•y r·∫•t nhi·ªÅu port b·ªã DROP t·ª´ kho·∫£ng 22000-65535\nTrong HASS s·∫Ω th·∫•y 1 s·ªë l·ªói:\ntelegram.error.NetworkError: urllib3 HTTPError HTTPSConnectionPool(host='api.telegram.org', port=443): Max retries exceeded with url: /bot/getMe (Caused by NewConnectionError('\u0026lt;telegram.vendor.ptb_urllib3.urllib3.connection.VerifiedHTTPSConnection object at 0x7f8f6c9f90\u0026gt;: Failed to establish a new connection: [Errno 101] Network unreachable')) requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='route.lgthinq.com', port=46030): Max retries exceeded with url: /v1/service/application/gateway-uri (Caused by ConnectTimeoutError(\u0026lt;urllib3.connection.HTTPSConnection object at 0x7f9a880310\u0026gt;, 'Connection to route.lgthinq.com timed out. (connect timeout=10)')) 2022-09-17 18:07:42.403 ERROR (MainThread) [homeassistant] Error doing job: Future exception was never retrieved requests.exceptions.ConnectionError: HTTPSConnectionPool(host='route.lgthinq.com', port=46030): Max retries exceeded with url: /v1/service/application/gateway-uri (Caused by NewConnectionError('\u0026lt;urllib3.connection.HTTPSConnection object at 0x7f88286320\u0026gt;: Failed to establish a new connection: [Errno -3] Try again')) 2022-09-17 18:02:30.066 ERROR (MainThread) [metno] Access to https://aa015h6buqvih86i1.api.met.no/weatherapi/locationforecast/2.0/complete returned error 'ClientConnectorError' 2022-09-17 18:02:30.553 ERROR (MainThread) [custom_components.hacs] Request exception for 'https://api.github.com/rate_limit' with - Cannot connect to host api.github.com:443 ssl:default [Try again] Traceback (most recent call last): File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 451, in async_can_update response = await self.async_github_api_method(self.githubapi.rate_limit) File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 505, in async_github_api_method raise HacsException(_exception) custom_components.hacs.exceptions.HacsException: Request exception for 'https://api.github.com/rate_limit' with - Cannot connect to host api.github.com:443 ssl:default [Try again] 2022-09-17 18:02:55.077 WARNING (MainThread) [custom_components.smartthinq_sensors] Connection not available. ThinQ platform not ready 2022-09-17 18:02:30.066 ERROR (MainThread) [metno] Access to https://aa015h6buqvih86i1.api.met.no/weatherapi/locationforecast/2.0/complete returned error 'ClientConnectorError' 2022-09-17 18:02:30.553 ERROR (MainThread) [custom_components.hacs] Request exception for 'https://api.github.com/rate_limit' with - Cannot connect to host api.github.com:443 ssl:default [Try again] Traceback (most recent call last): File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 451, in async_can_update response = await self.async_github_api_method(self.githubapi.rate_limit) File \u0026quot;/config/custom_components/hacs/base.py\u0026quot;, line 505, in async_github_api_method raise HacsException(_exception) Qu√° tr√¨nh b·ªã l·ªói n√†y t·∫°o ra nhi·ªÅu file r√°c trong /var/log/journal/* n√™n x√≥a ƒëi b·ªõt, ho·∫∑c setting cho n√≥ ch·ªâ ch·ª©a t·ªëi ƒëa 200M trong ƒë√≥ th√¥i (L√†m theo b√†i n√†y: https://unix.stackexchange.com/questions/130786/can-i-remove-files-in-var-log-journal-and-var-cache-abrt-di-usr) test app Duplicati, ch∆∞a k·ªãp test th√¨ b·ªã server b·ªã treo/ƒë∆° lu√¥n, HASS l·∫°i b·ªã restart, full CPU, ko check dc n·ªØa.\nLog c·ªßa HASS c√≥ l·ªói li√™n quan ƒë·∫øn call api weather ra ngo√†i ko n·ªïi.\nCu·ªëi c√πng ph·∫£i ƒë·ª£i v√†o ƒëc Portainer UI, stop homeassistant container.\nth·∫ø n√™n ph·∫£i add th√™m:\n# accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --match multiport --dport 22000:65535 -j ACCEPT sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 2687K packets, 2589M bytes) pkts bytes target prot opt in out source destination 12 9365 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 22000:65535 344 19784 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 restart HASS -\u0026gt; th√¨ ko c√≤n log l·ªói n·ªØa\nTrong LOG_DROP th√¨ m√¨nh ph√°t hi·ªán nhi·ªÅu traffic ƒë·∫øn c√°c port nh·ªè h∆°n nh∆∞ DPT=5007,4100,81,20000,4228,18617,2525\u0026hellip;etc. N√≥i chung r·∫•t nhi·ªÅu port nh·ªè h∆°n 22000 b·ªã DROP. Nh∆∞ng HASS v·∫´n ho·∫°t ƒë·ªông ·ªïn n√™n cho qua..\nGi·ªù accept traffic on port 8123 ƒë·ªÉ t·ª´ m·∫°ng 4G c√≥ th·ªÉ connect ƒë·∫øn ƒë∆∞·ª£c:\n# accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --dport 8123 -j ACCEPT sudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 2699K packets, 2600M bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8123 2203 1380K ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 22000:65535 442 25513 LOG_DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 ƒê·∫øn ƒë√¢y v·ªÅ c∆° b·∫£n m·ªçi th·ª© ƒë√£ OK. Add th√™m accept c√°c port 80,443 l√† ƒë∆∞·ª£c, ƒë·ªÉ sau n√†y l√†m https.\nT·ªïng k·∫øt c√°c rule m√¨nh s·ª≠ d·ª•ng trong ph·∫ßn n√†y:\n# drop all traffic TCP incoming interface wlan0 Chain INPUT, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --match multiport --dport 22000:65535 -j ACCEPT # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --dport 8123 -j ACCEPT C√≤n B√™n d∆∞·ªõi l√† c√°c h∆∞·ªõng m√¨nh ƒë√£ l√†m nh∆∞ng ko recommend l·∫Øm.\n4. M·ªôt s·ªë ch√∫ √Ω 4.1. N·∫øu b·∫°n ch·ªâ add rule iptables v√†o Chain INPUT th√¨ sao Add rule n√†y v√†o chain INPUT:\n# drop all traffic incoming interface wlan0 Chain INPUT, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP Test:\nT·ª´ m·∫°ng 4G -\u0026gt; port 8123 -\u0026gt; ko access ƒë∆∞·ª£c.\nT·ª´ m·∫°ng 4G -\u0026gt; port Docker (3000, 9000, 8200, \u0026hellip;) -\u0026gt; access ƒë∆∞·ª£c h·∫øt.\nT·ªïng k·∫øt: Nh∆∞ v·∫≠y vi·ªác add rule drop traffic ·ªü Chain INPUT ko ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn Chain DOCKER-USER c·∫£. ‚òπ\n4.2. N·∫øu b·∫°n ch·ªâ drop traffic ·ªü t·ª´ng port c·ª• th·ªÉ th√¥i # drop traffic incoming interface wlan0 Chain DOCKER-USER port XXX from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 3000 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 9000 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 9001 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 1883 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 9090 -j DROP iptables -I DOCKER-USER -p udp -i wlan0 ! -s 192.168.1.0/24 --dport 51820 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 4444 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 8200 -j DROP iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 8080 -j DROP # drop traffic incoming interface wlan0 Chain INPUT port 22 t·ª´ anywhere, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 --dport 22 -j DROP n√≥ s·∫Ω ƒë∆∞·ª£c nh∆∞ n√†y ( to√†n c√°c rule ƒë∆∞·ª£c add v√†o Chain DOCKER-USER, 1 rule ƒë∆∞·ª£c add v√†o Chain INPUT ƒë·ªÉ ch·∫∑n port 22 ):\nsudo iptables -L -v -n | more Chain INPUT (policy ACCEPT 1492K packets, 1581M bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:22 Chain FORWARD (policy ACCEPT 4921 packets, 2451K bytes) pkts bytes target prot opt in out source destination 838K 745M DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 794K 736M DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 0 0 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0 422K 168M ACCEPT all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 1653 224K DOCKER all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 371K 568M ACCEPT all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 756 176K ACCEPT all -- br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 1559K packets, 1541M bytes) pkts bytes target prot opt in out source destination Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 207 11020 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.2 tcp dpt:9000 34 1720 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:9001 4 208 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.5 tcp dpt:1883 5 240 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.6 tcp dpt:9090 0 0 ACCEPT udp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.10 udp dpt:51820 125 6832 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.7 tcp dpt:3000 86 4704 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.4 tcp dpt:8200 60 3176 ACCEPT tcp -- !br-564d64d5a58a br-564d64d5a58a 0.0.0.0/0 172.18.0.9 tcp dpt:8080 Chain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 0 0 DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 371K 568M DOCKER-ISOLATION-STAGE-2 all -- br-564d64d5a58a !br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 794K 736M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) pkts bytes target prot opt in out source destination 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 DROP all -- * br-564d64d5a58a 0.0.0.0/0 0.0.0.0/0 371K 568M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 45 2600 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:8080 17 908 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:8200 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:4444 0 0 DROP udp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 udp dpt:51820 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:9090 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:1883 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:9001 31 1804 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:9000 52 3120 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 tcp dpt:3000 794K 736M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Test:\nT·ª´ m·∫°ng 4G -\u0026gt; port 8123 -\u0026gt; access ƒë∆∞·ª£c. (d·ªÖ hi·ªÉu th√¥i v√¨ n√≥ ko ph·∫£i port trong Docker chain). T·ª´ m·∫°ng 4G -\u0026gt; port Docker -\u0026gt; ko access ƒë∆∞·ª£c. T·ª´ local -\u0026gt; access ƒë∆∞·ª£c h·∫øt.\nTest connection ƒë·∫øn AWS S3 t·ª´ Duplicati: worked Log c·ªßa HASS: ko c√≥ l·ªói g√¨.\nL√™n canyouseeme.org, check h·∫øt c√°c port xem ·ªü ngo√†i c√≥ th·∫•y service n√†o ch·∫°y ko?\nC√≥ v·∫ª t·∫°m ch·∫•p nh·∫≠n th√¥i.\ncheck l·∫°i c√°c port ƒëang open b·∫±ng nmap:\n# UDP scan sudo nmap -sU -p- 192.168.1.128 Starting Nmap 7.80 ( https://nmap.org ) at 2022-09-16 17:01 +07 Nmap scan report for 192.168.1.128 Host is up (0.000041s latency). Not shown: 65529 closed ports PORT STATE SERVICE 68/udp open|filtered dhcpc 546/udp open|filtered dhcpv6-client 5353/udp open zeroconf 34484/udp open|filtered unknown 46861/udp open|filtered unknown 51820/udp open|filtered unknown # TCP scan sudo nmap -sT -p- 192.168.1.128 Starting Nmap 7.80 ( https://nmap.org ) at 2022-09-16 17:05 +07 Nmap scan report for 192.168.1.128 Host is up (0.00072s latency). Not shown: 65527 closed ports PORT STATE SERVICE 22/tcp open ssh 1883/tcp open mqtt 3000/tcp open ppp 8080/tcp open http-proxy 8123/tcp open polipo 8200/tcp open trivnet1 9000/tcp open cslistener 9090/tcp open zeus-admin Nh∆∞ v·∫≠y l√† b·∫°n th·∫•y h·∫ßu h·∫øt c√°c port TCP ƒëang open kia m√¨nh ƒë√£ close l·∫°i b·∫±ng iptables r·ªìi (tr·ª´ 8123 ƒë·ªÉ s·ª≠ d·ª•ng). C√°c port TCP th√¨ c√≥ v√†i port h∆°i l·∫° ko bi·∫øt ƒë·ªÉ l√†m g√¨: 34484, 46861\nTh·ª≠ DROP 2 port l·∫° ho·∫Øc tr√™n xem c√≥ v·∫•n ƒë·ªÅ g√¨ x·∫£y ra, ƒë·∫ßu ti√™n l√† port 34484:\niptables -I INPUT -p udp -i wlan0 ! -s 192.168.1.0/24 --dport 34484 -j DROP Test: ko th·∫•y c√≥ l·ªói n√†o x·∫£y ra. HASS v·∫´n ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng. Ko c√≥ log l·ªói. Duplicati test connection to S3: worked.\nti·∫øp port 46861:\niptables -I INPUT -p udp -i wlan0 ! -s 192.168.1.0/24 --dport 46861 -j DROP Test: ko th·∫•y l·ªói g√¨ x·∫£y ra.\nT·ªïng k·∫øt: C√°ch n√†y v·ªÅ c∆° b·∫£n c≈©ng c√≥ th·ªÉ ch·∫•p nh·∫≠n ƒë∆∞·ª£c. Nh∆∞ng m√¨nh th√≠ch 3.1,3.2 h∆°n.\n5. T√≥m t·∫Øt C√°c command c·∫ßn l√†m:\n# Chain DOCKER-USER # drop any traffic TCP incoming interface wlan0 on Docker from anywhere, except LAN iptables -I DOCKER-USER -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # This open makes connection from Duplciati to S3 works # accept traffic TCP incoming interface wlan0 on Docker port XXX from anywhere iptables -I DOCKER-USER -p tcp -i wlan0 --match multiport --dport 33000:65535 -j ACCEPT iptables -I DOCKER-USER -p tcp -i wlan0 --dport 80 -j ACCEPT iptables -I DOCKER-USER -p tcp -i wlan0 --dport 443 -j ACCEPT # Chain INPUT # drop all traffic TCP incoming interface wlan0 Chain INPUT from anywhere, except LAN iptables -I INPUT -p tcp -i wlan0 ! -s 192.168.1.0/24 -j DROP # This open makes HASS working normal # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --match multiport --dport 22000:65535 -j ACCEPT # This open HASS so you can access from Outside Internet # accept traffic TCP incoming interface wlan0 on Chain INPUT port XXX from anywhere iptables -I INPUT -p tcp -i wlan0 --dport 8123 -j ACCEPT Confirm:\nChain INPUT (policy ACCEPT 2756K packets, 2653M bytes) pkts bytes target prot opt in out source destination 19 778 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8123 76 44116 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 22000:65535 5 208 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- wlan0 * 0.0.0.0/0 0.0.0.0/0 multiport dports 33000:65535 0 0 DROP tcp -- wlan0 * !192.168.1.0/24 0.0.0.0/0 1298K 1123M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Khi mu·ªën Debug: https://stackoverflow.com/a/29544353/9922066\nLet\u0026rsquo;s create a chain to log and accept:\niptables -N LOG_ACCEPT And let\u0026rsquo;s populate its rules:\niptables -A LOG_ACCEPT -j LOG --log-prefix \u0026#34;INPUT:ACCEPT:\u0026#34; --log-level 6 iptables -A LOG_ACCEPT -j ACCEPT Now let\u0026rsquo;s create a chain to log and drop:\niptables -N LOG_DROP And let\u0026rsquo;s populate its rules:\niptables -A LOG_DROP -j LOG --log-prefix \u0026#34;INPUT:DROP: \u0026#34; --log-level 6 iptables -A LOG_DROP -j DROP Confirm:\nChain LOG_ACCEPT (0 references) pkts bytes target prot opt in out source destination 154 129K LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:ACCEPT:\u0026quot; 154 129K ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 Chain LOG_DROP (0 references) pkts bytes target prot opt in out source destination 1088 58637 LOG all -- * * 0.0.0.0/0 0.0.0.0/0 LOG flags 0 level 6 prefix \u0026quot;INPUT:DROP: \u0026quot; 1088 58637 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 Now you can do all actions in one go by jumping (-j) to you custom chains instead of the default LOG / ACCEPT / REJECT / DROP:\niptables -A \u0026lt;your_chain_here\u0026gt; \u0026lt;your_conditions_here\u0026gt; -j LOG_ACCEPT iptables -A \u0026lt;your_chain_here\u0026gt; \u0026lt;your_conditions_here\u0026gt; -j LOG_DROP CREDIT https://stackoverflow.com/a/56038551/9922066\nhttps://docs.docker.com/network/iptables/\nhttps://stackoverflow.com/a/29544353/9922066\n","href":"/posts/encrypt-setup-home-assistant-on-raspberry-pi-p2-dmz/","title":"Setup Home Assistant on Raspberry Pi (Part 2) - iptables rule and DMZ"},{"content":"Tr√™n tiktok c·ªßa @Ph·∫°m Ho√†ng S∆°n m√¨nh th·∫•y c√≥ 1 s·ªë c√¥ng th·ª©c t√≠nh l√£i k√©p kh√° hay c·∫ßn ghi nh·ªõ:\nhttps://vt.tiktok.com/ZSRgb7kYa/\nTuy nhi√™n vi·ªác xem l·∫°i clip th√¨ l√¢u qu√° n√™n ƒë√†nh ph·∫£i ghi l·∫°i ra ƒë√¢y ƒë·ªÉ sau n√†y ch·ªâ c·∫ßn l∆∞·ªõt qua ƒë·ªçc l√† nh·ªõ ƒë∆∞·ª£c\nNote 1 N·∫øu b·∫°n c√≥ 600m g·ª≠i ti·∫øt ki·ªám l√£i su·∫•t 10% 1 nƒÉm, sau 1 nƒÉm c√≥ 660m, l·∫•y s·ªë ƒë√≥ g·ª≠i ti·∫øp th√¨ sau 3 nƒÉm s·∫Ω c√≥ 798m:\n600 x 1.1^3 = 798 N·∫øu b·∫°n mu·ªën 3 nƒÉm n·ªØa c√≥ 600m th√¨ gi·ªù b·∫°n c·∫ßn g·ª≠i bao nhi√™u ti·ªÅn v·ªõi l√£i su·∫•t 10% 1 nƒÉm:\n600 x 1.1^-3 = 450 Note 2 Nguy√™n t·∫Øc: k·ª≥ nh·∫≠n l√£i c√†ng th∆∞·ªùng xuy√™n c√†ng t·ªët\nN·∫øu b·∫°n c√≥ 1 t·ªâ, g·ª≠i v·ªõi l√£i su·∫•t 10.5% 1 nƒÉm. Nh·∫≠n l√£i 1 nƒÉm 1 l·∫ßn, t·ªïng ti·ªÅn c√≥ l√†:\n10^9 x (1+10.5%) = 1105000000 N·∫øu b·∫°n c√≥ 1 t·ªâ, g·ª≠i v·ªõi l√£i su·∫•t 10.5% 1 nƒÉm. Nh·∫≠n l√£i 1 nƒÉm 4 l·∫ßn (theo qu√Ω), t·ªïng ti·ªÅn c√≥ l√†:\n10^9 x (1+10.5%:4)^4 = 1109207201 N·∫øu b·∫°n c√≥ 1 t·ªâ, g·ª≠i v·ªõi l√£i su·∫•t 10.5% 1 nƒÉm. Nh·∫≠n l√£i 1 nƒÉm 12 l·∫ßn (theo th√°ng) t·ªïng ti·ªÅn c√≥ l√†:\n10^9 x (1+10.5%:12)^12 = 1110203450 Note 3 N·∫øu b·∫°n b·ªè v√†o ti·∫øt ki·ªám 36m m·ªói nƒÉm, l√£i su·∫•t 7% 1 nƒÉm, trong v√≤ng 15 nƒÉm th√¨ s·ªë ti·∫øt r√∫t ·ªü cu·ªëi nƒÉm 15 l√†:\n36 x 107 : 7 x (1.07^15 - 1) = 967.969 N·∫øu b·∫°n mu·ªën nƒÉm th·ª© 15 th√¨ c√≥ th·ªÉ c√≥ 967m v·ªõi l√£i su·∫•t 7% 1 nƒÉm th√¨ m·ªói nƒÉm c·∫ßn b·ªè v√†o ti·∫øt ki·ªám bao nhi√™u:\n967 : 107 x 7 : (1.07^15 - 1) = 36 N·∫øu b·∫°n mu·ªën x2 s·ªë ti·ªÅn ƒë·∫ßu t∆∞ v·ªõi l√£i su·∫•t 8.6% 1 nƒÉm th√¨ b·∫°n c·∫ßn bao nhi√™u nƒÉm (n):\n# 72 l√† s·ªë c·ªë ƒë·ªãnh n = 72/8.6 = 8.37 (nƒÉm) ","href":"/secrets/encrypt-note-finance/","title":"1 s·ªë c√¥ng th·ª©c ti·∫øt ki·ªám n√™n nh·ªõ"},{"content":"","href":"/secrets/","title":"Secrets"},{"content":"B√†i n√†y h∆∞·ªõng d·∫´n c√°c integrate M√°y gi·∫∑t LG ThinQ FV1409S4W ƒë·∫øn Home Assistant Container\nC√°c b∆∞·ªõc setup T·∫£i app LG ThinQ v·ªÅ, ƒëƒÉng k√Ω account ƒë·ªôc l·∫≠p LG ThinQ, ch√∫ √Ω l√† ko ƒëƒÉng k√Ω b·∫±ng Social Network account nh∆∞ (Google, Facebook or Amazon).\nSau ƒë√≥ l√†m theo c√°c h∆∞·ªõng d·∫´n tr√™n LG ThinQ App ƒë·ªÉ add/k·∫øt n·ªëi v·ªõi m√°y gi·∫∑t.\nCh√∫ √Ω sau khi b·∫≠t n√∫t Power tr√™n m√°y gi·∫∑t.\nB·∫°n c·∫ßn connect App v·ªõi m√°y gi·∫∑t b·∫±ng c√°ch ·∫•n gi·ªØ n√∫t n√†y ch·ª© ko ph·∫£i bi·ªÉu t∆∞·ª£ng wifi nh√©:\nCh·ªù add xong thi·∫øt b·ªã l√† OK:\nClone this repo to local:\nhttps://github.com/ollo69/ha-smartthinq-sensors\nCopy folder https://github.com/ollo69/ha-smartthinq-sensors/custom_components/smartthinq_sensors cho v√†o /opt/hass/config/custom_components/\n-\u0026gt; restart HASS\nV√†o HASS -\u0026gt; Setting -\u0026gt; Devices\u0026amp;Services -\u0026gt; Add Integration, search SmartThinQ LGE Sensors xu·∫•t hi·ªán:\nLogin account m√† b·∫°n v·ª´a m·ªõi t·∫°o tr√™n App mobile:\nAdd ƒë∆∞·ª£c nh∆∞ m√†n h√¨nh n√†y l√† ok:\nV√†o HASS -\u0026gt; Setting -\u0026gt; Devices\u0026amp;Services -\u0026gt; Entities, th·∫•y ƒë∆∞·ª£c list c√°c entity nh∆∞ n√†y l√† ok:\nN·∫øu b·ªã l·ªói nh∆∞ n√†y ch·ª©ng t·ªè b·∫°n n√™n update HASS l√™n version 2022.5.0:\nCreate opt/hass/config/packages/lg_logia_washer.yaml:\nsensor: - platform: template sensors: washer_door_lock: friendly_name: \u0026#34;Washer Door Lock\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;sensor.lg_washer\u0026#39;,\u0026#39;door_lock\u0026#39;) }}\u0026#34; washer_time_display: friendly_name: \u0026#34;Washer Time Display\u0026#34; value_template: \u0026gt;{% if is_state(\u0026#39;sensor.lg_washer_run_state\u0026#39;, \u0026#39;-\u0026#39;) %} {% elif is_state(\u0026#39;sensor.lg_washer_run_state\u0026#39;, \u0026#39;Standby\u0026#39;) %} -:-- {% else %} {{ state_attr(\u0026#39;sensor.lg_washer\u0026#39;, \u0026#39;remain_time\u0026#39;) }} {% endif %} blank: friendly_name: \u0026#34;Blank Sensor\u0026#34; value_template: \u0026#34;\u0026#34; Edit file opt/hass/config/configuration.yaml:\n... homeassistant: packages: !include_dir_named packages Copy all files trong https://github.com/phrz/lg-washer-dryer-card/tree/main/config/www v√†o /opt/hass/config/www\n\u0026ndash;\u0026gt; restart HASS\nL√™n giao di·ªán HASS, edit Dashboard -\u0026gt; add Card manual, paste ƒëo·∫°n code sau:\ntype: vertical-stack cards: - type: picture-elements elements: - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/sensing.png state_image: C·∫£m bi·∫øn t·∫£i tr·ªçng: /local/lg-icons/sensing-on.png style: top: 33% left: 33% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/wash.png state_image: Gi·∫∑t: /local/lg-icons/wash-on.png style: top: 33% left: 51% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/rinse.png state_image: Gi≈©: /local/lg-icons/rinse-on.png style: top: 33% left: 69% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/spin.png state_image: V·∫Øt: /local/lg-icons/spin-on.png style: top: 33% left: 87% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer image: /local/lg-icons/wifi.png state_image: \u0026#39;on\u0026#39;: /local/lg-icons/wifi-on.png style: top: 73% left: 32% width: 10% image-rendering: crisp - type: image entity: sensor.washer_door_lock image: /local/lg-icons/lock.png state_image: \u0026#39;on\u0026#39;: /local/lg-icons/lock-on.png style: top: 73% left: 45% width: 10% image-rendering: crisp - type: state-label entity: sensor.washer_time_display style: color: \u0026#39;#8df427\u0026#39; font-family: segment7 font-size: 50px left: 95% top: 74% transform: translate(-100%,-50%) image: /local/hass-washer-card-bg.png - type: conditional conditions: - entity: sensor.lg_washer_run_state state_not: \u0026#39;-\u0026#39; card: type: entities entities: - entity: sensor.lg_washer type: attribute attribute: current_course name: Current Course icon: mdi:tune-vertical-variant - entity: sensor.lg_washer type: attribute attribute: water_temp name: Water Temperature icon: mdi:coolant-temperature state_color: false - type: entities entities: - entity: sensor.lg_washer_tub_clean_counter name: Tub clean counter icon: mdi:washing-machine - entity: binary_sensor.lg_washer_error_state name: Error state - entity: sensor.lg_washer_error_message name: Error message Th·∫•y ƒë∆∞·ª£c card hi·ªÉn th·ªã nh∆∞ n√†y l√† OK:\nSet Automations Edit file opt/hass/config/automations.yaml:\n# Notify when Washing completed - id: \u0026#39;15911453452345\u0026#39; alias: LG Washer Completed Notify description: Notify to Telegram when LG Washer Completed trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;sensor.lg_washer\u0026#39;\u0026#39;,\u0026#39;\u0026#39;run_completed\u0026#39;\u0026#39;) == \u0026#39;\u0026#39;on\u0026#39;\u0026#39; }}\u0026#39; condition: [] action: - data_template: message: \u0026#39;\u0026#39; title: \u0026#39;ü•≥ Home Assistant: Washing completed.\u0026#39; service: telegram_bot.send_message # Warning when Washer error - id: \u0026#39;165745563453345\u0026#39; alias: LG Washer Reported Error Warning description: Notify to Telegram when LG Washer Reported Error trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;sensor.lg_washer\u0026#39;\u0026#39;,\u0026#39;\u0026#39;error_state\u0026#39;\u0026#39;) == \u0026#39;\u0026#39;on\u0026#39;\u0026#39; }}\u0026#39; condition: [] action: - data_template: message: \u0026#39;Error message: {{ state_attr(\u0026#39;\u0026#39;sensor.lg_washer\u0026#39;\u0026#39;,\u0026#39;\u0026#39;error_message\u0026#39;\u0026#39;) }}\u0026#39; title: \u0026#39;ü§® HASS: Your LG Washer reported error.\u0026#39; service: telegram_bot.send_message CREDIT https://github.com/ollo69/ha-smartthinq-sensors\nhttps://github.com/phrz/lg-washer-dryer-card\nhttps://community.home-assistant.io/t/in-development-lg-smartthinq-component/40157/481\n","href":"/bk/encrypt-connect-home-assistant-to-lg-thinq-washer/","title":"Connect Home Assistant to LG thinQ Washer"},{"content":"B√†i n√†y h∆∞·ªõng d·∫´n c√°c integrate M√°y gi·∫∑t LG ThinQ FV1409S4W ƒë·∫øn Home Assistant Container\nC√°c b∆∞·ªõc setup T·∫£i app LG ThinQ v·ªÅ, ƒëƒÉng k√Ω account ƒë·ªôc l·∫≠p LG ThinQ, ch√∫ √Ω l√† ko ƒëƒÉng k√Ω b·∫±ng Social Network account nh∆∞ (Google, Facebook or Amazon).\nSau ƒë√≥ l√†m theo c√°c h∆∞·ªõng d·∫´n tr√™n LG ThinQ App ƒë·ªÉ add/k·∫øt n·ªëi v·ªõi m√°y gi·∫∑t.\nCh√∫ √Ω sau khi b·∫≠t n√∫t Power tr√™n m√°y gi·∫∑t.\nB·∫°n c·∫ßn connect App v·ªõi m√°y gi·∫∑t b·∫±ng c√°ch ·∫•n gi·ªØ n√∫t n√†y ch·ª© ko ph·∫£i bi·ªÉu t∆∞·ª£ng wifi nh√©:\nCh·ªù add xong thi·∫øt b·ªã l√† OK:\nClone this repo to local:\nhttps://github.com/ollo69/ha-smartthinq-sensors\nCopy folder https://github.com/ollo69/ha-smartthinq-sensors/custom_components/smartthinq_sensors cho v√†o /opt/hass/config/custom_components/\n-\u0026gt; restart HASS\nV√†o HASS -\u0026gt; Setting -\u0026gt; Devices\u0026amp;Services -\u0026gt; Add Integration, search SmartThinQ LGE Sensors xu·∫•t hi·ªán:\nLogin account m√† b·∫°n v·ª´a m·ªõi t·∫°o tr√™n App mobile:\nAdd ƒë∆∞·ª£c nh∆∞ m√†n h√¨nh n√†y l√† ok:\nV√†o HASS -\u0026gt; Setting -\u0026gt; Devices\u0026amp;Services -\u0026gt; Entities, th·∫•y ƒë∆∞·ª£c list c√°c entity nh∆∞ n√†y l√† ok:\nN·∫øu b·ªã l·ªói nh∆∞ n√†y ch·ª©ng t·ªè b·∫°n n√™n update HASS l√™n version 2022.5.0:\nCreate opt/hass/config/packages/lg_logia_washer.yaml:\nsensor: - platform: template sensors: washer_door_lock: friendly_name: \u0026#34;Washer Door Lock\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;sensor.lg_washer\u0026#39;,\u0026#39;door_lock\u0026#39;) }}\u0026#34; washer_time_display: friendly_name: \u0026#34;Washer Time Display\u0026#34; value_template: \u0026gt;{% if is_state(\u0026#39;sensor.lg_washer_run_state\u0026#39;, \u0026#39;-\u0026#39;) %} {% elif is_state(\u0026#39;sensor.lg_washer_run_state\u0026#39;, \u0026#39;Standby\u0026#39;) %} -:-- {% else %} {{ state_attr(\u0026#39;sensor.lg_washer\u0026#39;, \u0026#39;remain_time\u0026#39;) }} {% endif %} blank: friendly_name: \u0026#34;Blank Sensor\u0026#34; value_template: \u0026#34;\u0026#34; Edit file opt/hass/config/configuration.yaml:\n... homeassistant: packages: !include_dir_named packages Copy all files trong https://github.com/phrz/lg-washer-dryer-card/tree/main/config/www v√†o /opt/hass/config/www\n\u0026ndash;\u0026gt; restart HASS\nL√™n giao di·ªán HASS, edit Dashboard -\u0026gt; add Card manual, paste ƒëo·∫°n code sau:\ntype: vertical-stack cards: - type: picture-elements elements: - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/sensing.png state_image: C·∫£m bi·∫øn t·∫£i tr·ªçng: /local/lg-icons/sensing-on.png style: top: 33% left: 33% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/wash.png state_image: Gi·∫∑t: /local/lg-icons/wash-on.png style: top: 33% left: 51% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/rinse.png state_image: Gi≈©: /local/lg-icons/rinse-on.png style: top: 33% left: 69% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer_run_state image: /local/lg-icons/spin.png state_image: V·∫Øt: /local/lg-icons/spin-on.png style: top: 33% left: 87% width: 20% image-rendering: crisp - type: image entity: sensor.lg_washer image: /local/lg-icons/wifi.png state_image: \u0026#39;on\u0026#39;: /local/lg-icons/wifi-on.png style: top: 73% left: 32% width: 10% image-rendering: crisp - type: image entity: sensor.washer_door_lock image: /local/lg-icons/lock.png state_image: \u0026#39;on\u0026#39;: /local/lg-icons/lock-on.png style: top: 73% left: 45% width: 10% image-rendering: crisp - type: state-label entity: sensor.washer_time_display style: color: \u0026#39;#8df427\u0026#39; font-family: segment7 font-size: 50px left: 95% top: 74% transform: translate(-100%,-50%) image: /local/hass-washer-card-bg.png - type: conditional conditions: - entity: sensor.lg_washer_run_state state_not: \u0026#39;-\u0026#39; card: type: entities entities: - entity: sensor.lg_washer type: attribute attribute: current_course name: Current Course icon: mdi:tune-vertical-variant - entity: sensor.lg_washer type: attribute attribute: water_temp name: Water Temperature icon: mdi:coolant-temperature state_color: false - type: entities entities: - entity: sensor.lg_washer_tub_clean_counter name: Tub clean counter icon: mdi:washing-machine - entity: binary_sensor.lg_washer_error_state name: Error state - entity: sensor.lg_washer_error_message name: Error message Th·∫•y ƒë∆∞·ª£c card hi·ªÉn th·ªã nh∆∞ n√†y l√† OK:\nSet Automations Edit file opt/hass/config/automations.yaml:\n# Notify when Washing completed - id: \u0026#39;15911453452345\u0026#39; alias: LG Washer Completed Notify description: Notify to Telegram when LG Washer Completed trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;sensor.lg_washer\u0026#39;\u0026#39;,\u0026#39;\u0026#39;run_completed\u0026#39;\u0026#39;) == \u0026#39;\u0026#39;on\u0026#39;\u0026#39; }}\u0026#39; condition: [] action: - data_template: message: \u0026#39;\u0026#39; title: \u0026#39;ü•≥ Home Assistant: Washing completed.\u0026#39; service: telegram_bot.send_message # Warning when Washer error - id: \u0026#39;165745563453345\u0026#39; alias: LG Washer Reported Error Warning description: Notify to Telegram when LG Washer Reported Error trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;sensor.lg_washer\u0026#39;\u0026#39;,\u0026#39;\u0026#39;error_state\u0026#39;\u0026#39;) == \u0026#39;\u0026#39;on\u0026#39;\u0026#39; }}\u0026#39; condition: [] action: - data_template: message: \u0026#39;Error message: {{ state_attr(\u0026#39;\u0026#39;sensor.lg_washer\u0026#39;\u0026#39;,\u0026#39;\u0026#39;error_message\u0026#39;\u0026#39;) }}\u0026#39; title: \u0026#39;ü§® HASS: Your LG Washer reported error.\u0026#39; service: telegram_bot.send_message CREDIT https://github.com/ollo69/ha-smartthinq-sensors\nhttps://github.com/phrz/lg-washer-dryer-card\nhttps://community.home-assistant.io/t/in-development-lg-smartthinq-component/40157/481\n","href":"/posts/encrypt-connect-home-assistant-to-lg-thinq-washer/","title":"Connect Home Assistant to LG thinQ Washer"},{"content":"L√†m theo 1 s·ªë h∆∞·ªõng d·∫´n tr√™n m·∫°ng ƒë·ªÉ Connect Mi Air Purifier 3H ƒë·∫øn HASS th∆∞·ªùng b·ªã l·ªói:\nplatform: xiaomi_miio ko c√≤n ƒë∆∞·ª£c HASS support, timeout khi connect. Cu·ªëi c√πng m√¨nh ƒë√£ t√¨m ƒë∆∞·ª£c c√°ch ƒë·ªÉ l√†m ƒë∆∞·ª£c. ƒê√≥ l√† d√πng custom component c·ªßa Github n√†y: https://github.com/syssi/xiaomi_airpurifier/custom_components/\nTuy nhi√™n sau kho·∫£ng 3 th√°ng s·ª≠ d·ª•ng, m√¨nh th·∫•y tr√¨nh ƒë·ªô tƒÉng l√™n,\nm√¨nh ph√°t hi·ªán ra c√≥ 1 l·ªói l√† ko th·ªÉ setup ƒë∆∞·ª£c LED Brightness.\nV√† m√¨nh t√¨m ƒë∆∞·ª£c 1 c√°ch n·ªØa l√† d√πng Miio Integration c√≤n OK h∆°n. V√† ∆∞u ƒëi·ªÉm h∆°n l√† ko c·∫ßn nh·∫≠p token v√†o file configuration.yml\nB√†i n√†y m√¨nh h∆∞·ªõng d·∫´n c·∫£ 2 c√°ch: 1 l√† d√πng custom component, 2 l√† d√πng Miio Integration\n1. C√°ch 1: D√πng custom component 1.1. Setup C√†i ƒë·∫∑t app MiHome latest tr√™n Androids, ƒëƒÉng k√Ω, pair n√≥ v·ªõi Air Purifier 3H c·ªßa b·∫°n (·∫•n ƒë·ªìng th·ªùi n√∫t b·∫•m sau m√°y v·ªõi n√∫t c·∫£m ·ª©ng, ch·ªù ƒë·∫øn khi 3 ti·∫øng b√≠p li√™n ti·∫øp), h√£y ƒë·∫£m b·∫£o pair th√†nh c√¥ng nh√©!\nNote: 1 s·ªë b√†i vi·∫øt b·∫£o b·∫°n l·∫•y token b·∫±ng c√°ch c√†i ƒë·∫∑t apk version c≈© (5.4.49) c·ªßa Mi Home th√¨ m√¨nh th·ª≠ ƒë·ªÅu ko ƒë∆∞·ª£c, c√≥ l·∫Ω c√°ch ƒë·∫•y ko c√≤n ph√π h·ª£p n·ªØa r·ªìi, h√£y c√†i version latest lu√¥n\nTr√™n m√°y Windows, download exe sau v·ªÅ: https://github.com/PiotrMachowski/Xiaomi-cloud-tokens-extractor/releases/latest/download/token_extractor.exe\nRun file exe tr√™n, nh·∫≠p user/pass c·ªßa MiHome app, b·∫°n s·∫Ω th·∫•y token c·ªßa thi·∫øt b·ªã Air Purifier trong m·∫°ng c·ªßa b·∫°n, note l·∫°i token v√† model:\nCopy folder https://github.com/syssi/xiaomi_airpurifier/custom_components/xiaomi_miio_airpurifier v√†o /opt/hass/config/custom_components\n-\u0026gt; restart HASS\nedit file /opt/hass/config/configuration.yaml:\n... fan: - platform: xiaomi_miio_airpurifier name: Bedroom Air Purifier 3H host: 192.168.1.9 token: d5xxxxxxxxxxxxxxxxxxxa model: zhimi.airpurifier.mb3 -\u0026gt; restart HASS\nV√†o HASS -\u0026gt; Configuration -\u0026gt; Devices\u0026amp;Services -\u0026gt; Entities:\nS·∫Ω th·∫•y entity c·ªßa Mi Air Purifier xu·∫•t hi·ªán.\nCheck log c·ªßa HASS ko c√≥ l·ªói g√¨ l√† OK (C√≥ th·ªÉ c√≥ warning th√¨ ko sao)\nCreate opt/hass/config/packages/xiaomi_bedroom_airpurifier.yaml\n###FIND \u0026amp; REPLACE ALL bedroom_air_purifier_3h with your entity NAME### sensor: - platform: template sensors: bedroom_air_purifier_3h_temp: friendly_name: \u0026#34;Temperature\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;temperature\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;¬∞C\u0026#34; device_class: \u0026#34;temperature\u0026#34; bedroom_air_purifier_3h_humidity: friendly_name: \u0026#34;Humidity\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;humidity\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;%\u0026#34; device_class: \u0026#34;humidity\u0026#34; bedroom_air_purifier_3h_air_quality_pm25: friendly_name: \u0026#34;Air quality\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;aqi\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;Œºg/m¬≥\u0026#34; icon_template: \u0026#34;mdi:weather-fog\u0026#34; bedroom_air_purifier_3h_speed: friendly_name: \u0026#34;Fan speed\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;motor_speed\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;rpm\u0026#34; icon_template: \u0026#34;mdi:speedometer\u0026#34; bedroom_air_purifier_3h_filter_remaining: friendly_name: \u0026#34;Filter remaining\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;filter_life_remaining\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;%\u0026#34; icon_template: \u0026#34;mdi:heart-outline\u0026#34; switch: - platform: template switches: bedroom_air_purifier_3h_led: friendly_name: \u0026#34;LED\u0026#34; value_template: \u0026#34;{{ is_state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;led\u0026#39;, true) }}\u0026#34; turn_on: service: xiaomi_miio_airpurifier.fan_set_led_on data: entity_id: fan.bedroom_air_purifier_3h turn_off: service: xiaomi_miio_airpurifier.fan_set_led_off data: entity_id: fan.bedroom_air_purifier_3h icon_template: \u0026#34;mdi:lightbulb-outline\u0026#34; bedroom_air_purifier_3h_child_lock: friendly_name: \u0026#34;Children lock\u0026#34; value_template: \u0026#34;{{ is_state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;child_lock\u0026#39;, true) }}\u0026#34; turn_on: service: xiaomi_miio_airpurifier.fan_set_child_lock_on data: entity_id: fan.bedroom_air_purifier_3h turn_off: service: xiaomi_miio_airpurifier.fan_set_child_lock_off data: entity_id: fan.bedroom_air_purifier_3h icon_template: \u0026#34;mdi:lock-outline\u0026#34; bedroom_air_purifier_3h_buzzer: friendly_name: \u0026#34;Buzzer\u0026#34; value_template: \u0026#34;{{ is_state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;buzzer\u0026#39;, true) }}\u0026#34; turn_on: service: xiaomi_miio_airpurifier.fan_set_buzzer_on data: entity_id: fan.bedroom_air_purifier_3h turn_off: service: xiaomi_miio_airpurifier.fan_set_buzzer_off data: entity_id: fan.bedroom_air_purifier_3h icon_template: \u0026#34;mdi:volume-high\u0026#34; input_select: bedroom_air_purifier_3h_mode: name: Mode initial: Fan options: - Auto - Silent - Favorite - Fan icon: \u0026#34;mdi:animation-outline\u0026#34; input_number: bedroom_air_purifier_3h_fan_level: name: \u0026#34;Fan level\u0026#34; initial: 1 min: 1 max: 3 step: 1 icon: \u0026#34;mdi:weather-windy\u0026#34; automation: - id: \u0026#39;9231979hoangmnsd231298\u0026#39; alias: Bedroom Air Purifier mode change trigger: entity_id: input_select.bedroom_air_purifier_3h_mode platform: state action: service: fan.set_preset_mode data_template: entity_id: fan.bedroom_air_purifier_3h preset_mode: \u0026#39;{{ states.input_select.bedroom_air_purifier_3h_mode.state }}\u0026#39; - id: \u0026#39;3467979hoangmnsd2365698\u0026#39; alias: Bedroom Air Purifier fan level change trigger: entity_id: input_number.bedroom_air_purifier_3h_fan_level platform: state action: service: xiaomi_miio_airpurifier.fan_set_fan_level data_template: entity_id: fan.bedroom_air_purifier_3h level: \u0026#39;{{ states.input_number.bedroom_air_purifier_3h_fan_level.state | int }}\u0026#39; edit file opt/hass/config/configuration.yaml:\n... homeassistant: packages: !include_dir_named packages -\u0026gt; restart HASS\nL√™n giao di·ªán HASS, edit Dashboard -\u0026gt; add Card manual, paste ƒëo·∫°n code sau:\ntype: horizontal-stack cards: - type: entities title: Bedroom Air Purifier 3H show_header_toggle: false entities: - entity: fan.bedroom_air_purifier_3h name: Power # Only display Mode when Power is not on state - type: conditional conditions: - entity: fan.bedroom_air_purifier_3h state: \u0026#34;on\u0026#34; row: entity: input_select.bedroom_air_purifier_3h_mode - entity: switch.bedroom_air_purifier_3h_child_lock - entity: switch.bedroom_air_purifier_3h_led - entity: switch.bedroom_air_purifier_3h_buzzer # Only display Fan level when Power is ON state - type: conditional conditions: - entity: fan.bedroom_air_purifier_3h state: \u0026#34;on\u0026#34; row: entity: input_number.bedroom_air_purifier_3h_fan_level - entity: sensor.bedroom_air_purifier_3h_speed - entity: sensor.bedroom_air_purifier_3h_filter_remaining - type: vertical-stack cards: - entities: - entity: sensor.bedroom_air_purifier_3h_air_quality_pm25 - entity: sensor.bedroom_air_purifier_3h_temp - entity: sensor.bedroom_air_purifier_3h_humidity show_header_toggle: false theme: default title: Bedroom Env type: entities - entities: - sensor.bedroom_air_purifier_3h_air_quality_pm25 hours_to_show: 24 refresh_interval: 60 title: Bedroom Air quality type: history-graph Save v√† quay l·∫°i th·ª≠ ƒëi·ªÅu khi·ªÉn xem sao. Check log ko c√≥ l·ªói ERROR l√† OK:\n1.2. Set c√°c Automations 1.2.1. B·∫≠t t·∫Øt theo gi·ªù Edit file opt/hass/config/automations.yaml:\n# Bedroom Air Purifier On Off on Time - id: \u0026#39;15914324182184740\u0026#39; alias: Bedroom Air Purifier On Off on Time description: Turn on/off Bedroom Air Purifier on schedule trigger: - at: \u0026#39;01:00:00\u0026#39; platform: time - at: \u0026#39;07:00:00\u0026#39; platform: time condition: [] action: - data: {} entity_id: fan.bedroom_air_purifier_3h service_template: \u0026gt;{% if trigger.now.hour == 1 %} fan.turn_on {% elif trigger.now.hour == 7 %} fan.turn_off {% else %} fan.turn_off {% endif %} Automation t·ª± ƒë·ªông trigger v√†o 2 th·ªùi ƒëi·ªÉm l√† 1h s√°ng v√† 6h s√°ng. T·ª± ƒë·ªông b·∫≠t n·∫øu l√† 1h s√°ng. T·ª± ƒë·ªông t·∫Øt n·∫øu l√† 6h s√°ng.\n1.2.2. G·ª≠i th√¥ng b√°o khi AQI qu√° cao Edit file opt/hass/config/automations.yaml:\n# Bedroom Air Purifier AQI over 80 Warning - id: \u0026#39;1591183075585\u0026#39; alias: Bedroom Air Purifier AQI over 80 Warning description: Notify to Telegram when Bedroom Air Purifier AQI over 80 trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;aqi\u0026#39;\u0026#39;)| int(0) \u0026gt; 80 }}\u0026#39; condition: [] action: - data_template: message: \u0026#39;Becareful, right now AQI = {{state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;aqi\u0026#39;\u0026#39;)}}\u0026#39; title: \u0026#39;‚ô®Ô∏è Home Assistant: Bedroom AQI is too high!\u0026#39; service: telegram_bot.send_message C√≥ th·ªÉ b·∫°n s·∫Ω t·ª± h·ªèi khi n√†o Automation tr√™n ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2. C√¢u tr·∫£ l·ªùi l√†:\n The Template Trigger‚Äôs template will be evaluated every time the sensor‚Äôs value changes\n Nghƒ©a l√† khi n√†o AQI thay ƒë·ªïi t·ª´ 80 sang s·ªë kh√°c th√¨ Automation tr√™n m·ªõi ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2\nsource: https://community.home-assistant.io/t/trigger-template-will-it-keep-firing/325009/7\n1.2.3. G·ª≠i th√¥ng b√°o khi l√µi l·ªçc h·∫øt th·ªùi gian s·ª≠ d·ª•ng Edit file opt/hass/config/automations.yaml:\n# Bedroom Air Purifier Filter lifetime Warning - id: \u0026#39;1591183684060\u0026#39; alias: Bedroom Air Purifier Filter lifetime Warning description: Bedroom Air Purifier Low Filter Life Notify 5% trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026lt; 6 and state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026gt; 0 }}\u0026#39; condition: [] action: - data_template: message: \u0026gt;- Your filter remaining is {{state_attr(trigger.entity_id,\u0026#39;filter_life_remaining\u0026#39;)}}%  title: \u0026#39;üòì Home Assistant: Air Purifier 3H filter remain warning\u0026#39; service: telegram_bot.send_message C√≥ th·ªÉ b·∫°n s·∫Ω t·ª± h·ªèi khi n√†o Automation tr√™n ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2. C√¢u tr·∫£ l·ªùi l√†:\n The Template Trigger‚Äôs template will be evaluated every time the sensor‚Äôs value changes\n Nghƒ©a l√† khi n√†o Filter remaning thay ƒë·ªïi t·ª´ 5 sang s·ªë kh√°c th√¨ Automation tr√™n m·ªõi ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2\nsource: https://community.home-assistant.io/t/trigger-template-will-it-keep-firing/325009/7\n2. C√°ch 2: D√πng Xiaomi Miio Integration (Recommended) 2.1. Setup C√†i ƒë·∫∑t app MiHome latest tr√™n Androids, ƒëƒÉng k√Ω, pair n√≥ v·ªõi Air Purifier 3H c·ªßa b·∫°n (·∫•n ƒë·ªìng th·ªùi n√∫t b·∫•m sau m√°y v·ªõi n√∫t c·∫£m ·ª©ng, ch·ªù ƒë·∫øn khi 3 ti·∫øng b√≠p li√™n ti·∫øp), h√£y ƒë·∫£m b·∫£o pair th√†nh c√¥ng nh√©!\nV√†o Setting -\u0026gt; Devices,Intergration -\u0026gt; Add Integration -\u0026gt; Search Miio:\nNh·∫≠p User/password m√† c√°c b·∫°n ƒë√£ ƒëƒÉng k√Ω t√†i kho·∫£n b√™n tr√™n\nN√≥ s·∫Ω t·ª± ƒë·ªông t√¨m c√°c devices c√≥ trong account c·ªßa b·∫°n\nHi·ªán nh∆∞ n√†y l√† OK:\nV·∫´n c·∫ßn t·∫°o file n√†y ƒë·ªÉ t·∫°o entity:\n/opt/hass/config/packages/xiaomi_bedroom_airpurifier.yaml\ninput_select: bedroom_air_purifier_3h_mode: name: Mode initial: Fan options: - Auto - Silent - Favorite - Fan icon: \u0026#34;mdi:animation-outline\u0026#34; configurations.yml:\nhomeassistant: packages: !include_dir_named packages Dashboard c·ªßa b·∫°n s·∫Ω c√≥ code nh∆∞ n√†y:\n# using Miio Integration - type: entities title: Air Purifier 33H show_header_toggle: false entities: - entity: fan.mi_air_purifier_33h name: Power - type: conditional conditions: - entity: fan.mi_air_purifier_33h state: \u0026#39;on\u0026#39; row: entity: input_select.bedroom_air_purifier_3h_mode name: Mode - entity: switch.mi_air_purifier_33h_child_lock name: Child lock - entity: select.mi_air_purifier_33h_led_brightness name: Led - entity: switch.mi_air_purifier_33h_buzzer name: Buzzer - type: conditional conditions: - entity: fan.mi_air_purifier_33h state: \u0026#39;on\u0026#39; row: entity: number.mi_air_purifier_33h_fan_level name: Fan level - entity: sensor.mi_air_purifier_33h_motor_speed name: Motor speed - entity: sensor.mi_air_purifier_33h_filter_life_remaining name: Filter Life remaining file automations.yml c·∫ßn add th√™m c√°i n√†y:\n# Bedroom Air Purifier change mode  # (Because Miio Integration doesn\u0026#39;t provide set_mode entity, so... # this automation help to set mode when `input_select.bedroom_air_purifier_3h_mode` is updated ) - id: \u0026#39;9hoangmnsd231979231298\u0026#39; alias: Bedroom Air Purifier mode change trigger: entity_id: input_select.bedroom_air_purifier_3h_mode platform: state action: service: fan.set_preset_mode data_template: entity_id: fan.mi_air_purifier_33h preset_mode: \u0026#39;{{ states.input_select.bedroom_air_purifier_3h_mode.state }}\u0026#39; C√°c Automations kh√°c th√¨ c≈©ng c·∫ßn s·ª≠a v√¨ c√°ch l·∫•y attributes kh√°c nhau:\nautomations.yml\n# Bedroom Air Purifier On Off on Time - id: \u0026#39;1hoangmnsd5914324182184740\u0026#39; alias: Bedroom Air Purifier On Off on Time description: Turn on/off Bedroom Air Purifier on schedule trigger: - at: \u0026#39;23:30:00\u0026#39; platform: time - at: \u0026#39;07:00:00\u0026#39; platform: time condition: [] action: - data: {} entity_id: fan.mi_air_purifier_33h service_template: \u0026gt;{% if trigger.now.hour == 23 %} fan.turn_on {% elif trigger.now.hour == 7 %} fan.turn_off {% else %} fan.turn_off {% endif %} # Bedroom Air Purifier AQI over 80 Warning - id: \u0026#39;1hoangmnsd591183075585\u0026#39; alias: Bedroom Air Purifier AQI over 80 Warning description: Notify to Telegram when Bedroom Air Purifier AQI over 80 trigger: - platform: template value_template: \u0026#39;{{ states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_pm2_5\u0026#39;\u0026#39;)| int(0) \u0026gt; 80 }}\u0026#39; condition: [] action: - data_template: target: - 1xxxxxxx3 message: \u0026#39;Becareful, right now AQI = {{states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_pm2_5\u0026#39;\u0026#39;)}}\u0026#39; title: \u0026#39;‚ô®Ô∏è HASS: Bedroom AQI is too high!\u0026#39; service: telegram_bot.send_message # Bedroom Air Purifier Filter lifetime Warning - id: \u0026#39;1591183684060\u0026#39; alias: Bedroom Air Purifier Filter lifetime Warning description: Bedroom Air Purifier Low Filter Life Notify 5% trigger: - platform: template value_template: \u0026#39;{{ states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026lt; 6 and states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026gt; 0 }}\u0026#39; condition: [] action: - data_template: target: - 1xxxxxxx3 message: \u0026gt;- Your filter remaining is {{states(\u0026#39;sensor.mi_air_purifier_33h_filter_life_remaining\u0026#39;)}}% title: \u0026#39;üòì HASS: Air Purifier 3H filter remain warning\u0026#39; service: telegram_bot.send_message Xong r·ªìi, C√°ch n√†y s·∫Ω nh·∫π nh√†ng h∆°n, m√¨nh th·∫•y d·ªÖ h∆°n nhi·ªÅu\n3. Trouble shooting Qu√° tr√¨nh theo d√µi c√≥ th·ªÉ xu·∫•t hi·ªán 1 s·ªë log WARNING nh∆∞ n√†y:\n2022-05-02 18:45:21 WARNING (MainThread) [homeassistant.components.input_select] Invalid option: None (possible options: Auto, Silent, Favorite, Fan) 2022-05-03 04:19:35 WARNING (MainThread) [homeassistant.helpers.template] Template warning: 'int' got invalid input 'None' when rendering template '{{ state_attr('fan.bedroom_air_purifier_3h','filter_life_remaining')|int \u0026lt; 6 and state_attr('fan.bedroom_air_purifier_3h','filter_life_remaining')|int \u0026gt; 0 }}' but no default was specified. Currently 'int' will return '0', however this template will fail to render in Home Assistant core 2022.1 2022-05-03 11:47:20 WARNING (MainThread) [homeassistant.helpers.entity] Update of fan.bedroom_air_purifier_3h is taking over 10 seconds 2022-05-03 16:15:41 ERROR (SyncWorker_5) [miio.miioprotocol] Got error when receiving: timed out ...  Th√¨ m√¨nh ƒë√£ th·ª≠ fix trong c√°c ƒëo·∫°n code b√™n tr√™n, Ch·ªâ display Mode select khi m√† Power ƒëang ·ªü state on Th√™m c√°c gi√° tr·ªã default khi so s√°nh gi√° tr·ªã int, b·∫±ng syntax int(30) \u0026lt; 6: c√≥ th·ªÉ hi·ªÉu n·∫øu ko c√≥ gi√° tr·ªã th√¨ code s·∫Ω l·∫•y default l√† 30 ƒë·ªÉ so s√°nh v·ªõi 6 ƒêang ch·ªù xem c√°c log l·ªói/warning tr√™n c√≥ xu·∫•t hi·ªán n·ªØa hay ko\u0026hellip;.  4. Tham kh·∫£o 1 b√†i vi·∫øt cho r·∫±ng kh√¥ng n√™n s·ª≠ d·ª•ng ch·∫ø ƒë·ªô Auto c·ªßa Air Purifier:\nhttps://smartairfilters.com/en/blog/data-explains-never-use-purifiers-auto-mode/\nCREDIT https://konnected.vn/home-assistant/home-assistant-tich-hop-may-loc-khong-khi-xiaomi-2020-06-03\nhttps://thesmarthomejourney.com/2021/06/28/smart-fan-with-home-assistant/\nhttps://github.com/syssi/xiaomi_airpurifier\nhttps://mic22.medium.com/how-to-get-token-of-xiaomi-air-purifier-h3-oct-2020-47d0f02451c3\nhttps://github.com/PiotrMachowski/Xiaomi-cloud-tokens-extractor/releases/latest/download/token_extractor.exe\nhttps://community.home-assistant.io/t/trigger-template-will-it-keep-firing/325009/7\nhttps://community.home-assistant.io/t/conditional-inside-entities-card/208819/2\nhttps://community.home-assistant.io/t/issue-with-template-no-default-was-specified-currently-int-will-return-0/387966\nhttps://smartairfilters.com/en/blog/data-explains-never-use-purifiers-auto-mode/\nhttps://community.home-assistant.io/t/lovelace-xiaomi-mi-air-purifier-3h-card/192100/12\n","href":"/bk/encrypt-connect-home-assistant-to-air-purifier3h/","title":"Connect Home Assistant Container to Xiaomi Air Purifier3H"},{"content":"L√†m theo 1 s·ªë h∆∞·ªõng d·∫´n tr√™n m·∫°ng ƒë·ªÉ Connect Mi Air Purifier 3H ƒë·∫øn HASS th∆∞·ªùng b·ªã l·ªói:\nplatform: xiaomi_miio ko c√≤n ƒë∆∞·ª£c HASS support, timeout khi connect. Cu·ªëi c√πng m√¨nh ƒë√£ t√¨m ƒë∆∞·ª£c c√°ch ƒë·ªÉ l√†m ƒë∆∞·ª£c. ƒê√≥ l√† d√πng custom component c·ªßa Github n√†y: https://github.com/syssi/xiaomi_airpurifier/custom_components/\nTuy nhi√™n sau kho·∫£ng 3 th√°ng s·ª≠ d·ª•ng, m√¨nh th·∫•y tr√¨nh ƒë·ªô tƒÉng l√™n,\nm√¨nh ph√°t hi·ªán ra c√≥ 1 l·ªói l√† ko th·ªÉ setup ƒë∆∞·ª£c LED Brightness.\nV√† m√¨nh t√¨m ƒë∆∞·ª£c 1 c√°ch n·ªØa l√† d√πng Miio Integration c√≤n OK h∆°n. V√† ∆∞u ƒëi·ªÉm h∆°n l√† ko c·∫ßn nh·∫≠p token v√†o file configuration.yml\nB√†i n√†y m√¨nh h∆∞·ªõng d·∫´n c·∫£ 2 c√°ch: 1 l√† d√πng custom component, 2 l√† d√πng Miio Integration\n1. C√°ch 1: D√πng custom component 1.1. Setup C√†i ƒë·∫∑t app MiHome latest tr√™n Androids, ƒëƒÉng k√Ω, pair n√≥ v·ªõi Air Purifier 3H c·ªßa b·∫°n (·∫•n ƒë·ªìng th·ªùi n√∫t b·∫•m sau m√°y v·ªõi n√∫t c·∫£m ·ª©ng, ch·ªù ƒë·∫øn khi 3 ti·∫øng b√≠p li√™n ti·∫øp), h√£y ƒë·∫£m b·∫£o pair th√†nh c√¥ng nh√©!\nNote: 1 s·ªë b√†i vi·∫øt b·∫£o b·∫°n l·∫•y token b·∫±ng c√°ch c√†i ƒë·∫∑t apk version c≈© (5.4.49) c·ªßa Mi Home th√¨ m√¨nh th·ª≠ ƒë·ªÅu ko ƒë∆∞·ª£c, c√≥ l·∫Ω c√°ch ƒë·∫•y ko c√≤n ph√π h·ª£p n·ªØa r·ªìi, h√£y c√†i version latest lu√¥n\nTr√™n m√°y Windows, download exe sau v·ªÅ: https://github.com/PiotrMachowski/Xiaomi-cloud-tokens-extractor/releases/latest/download/token_extractor.exe\nRun file exe tr√™n, nh·∫≠p user/pass c·ªßa MiHome app, b·∫°n s·∫Ω th·∫•y token c·ªßa thi·∫øt b·ªã Air Purifier trong m·∫°ng c·ªßa b·∫°n, note l·∫°i token v√† model:\nCopy folder https://github.com/syssi/xiaomi_airpurifier/custom_components/xiaomi_miio_airpurifier v√†o /opt/hass/config/custom_components\n-\u0026gt; restart HASS\nedit file /opt/hass/config/configuration.yaml:\n... fan: - platform: xiaomi_miio_airpurifier name: Bedroom Air Purifier 3H host: 192.168.1.9 token: d5xxxxxxxxxxxxxxxxxxxa model: zhimi.airpurifier.mb3 -\u0026gt; restart HASS\nV√†o HASS -\u0026gt; Configuration -\u0026gt; Devices\u0026amp;Services -\u0026gt; Entities:\nS·∫Ω th·∫•y entity c·ªßa Mi Air Purifier xu·∫•t hi·ªán.\nCheck log c·ªßa HASS ko c√≥ l·ªói g√¨ l√† OK (C√≥ th·ªÉ c√≥ warning th√¨ ko sao)\nCreate opt/hass/config/packages/xiaomi_bedroom_airpurifier.yaml\n###FIND \u0026amp; REPLACE ALL bedroom_air_purifier_3h with your entity NAME### sensor: - platform: template sensors: bedroom_air_purifier_3h_temp: friendly_name: \u0026#34;Temperature\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;temperature\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;¬∞C\u0026#34; device_class: \u0026#34;temperature\u0026#34; bedroom_air_purifier_3h_humidity: friendly_name: \u0026#34;Humidity\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;humidity\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;%\u0026#34; device_class: \u0026#34;humidity\u0026#34; bedroom_air_purifier_3h_air_quality_pm25: friendly_name: \u0026#34;Air quality\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;aqi\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;Œºg/m¬≥\u0026#34; icon_template: \u0026#34;mdi:weather-fog\u0026#34; bedroom_air_purifier_3h_speed: friendly_name: \u0026#34;Fan speed\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;motor_speed\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;rpm\u0026#34; icon_template: \u0026#34;mdi:speedometer\u0026#34; bedroom_air_purifier_3h_filter_remaining: friendly_name: \u0026#34;Filter remaining\u0026#34; value_template: \u0026#34;{{ state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;filter_life_remaining\u0026#39;) }}\u0026#34; unit_of_measurement: \u0026#34;%\u0026#34; icon_template: \u0026#34;mdi:heart-outline\u0026#34; switch: - platform: template switches: bedroom_air_purifier_3h_led: friendly_name: \u0026#34;LED\u0026#34; value_template: \u0026#34;{{ is_state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;led\u0026#39;, true) }}\u0026#34; turn_on: service: xiaomi_miio_airpurifier.fan_set_led_on data: entity_id: fan.bedroom_air_purifier_3h turn_off: service: xiaomi_miio_airpurifier.fan_set_led_off data: entity_id: fan.bedroom_air_purifier_3h icon_template: \u0026#34;mdi:lightbulb-outline\u0026#34; bedroom_air_purifier_3h_child_lock: friendly_name: \u0026#34;Children lock\u0026#34; value_template: \u0026#34;{{ is_state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;child_lock\u0026#39;, true) }}\u0026#34; turn_on: service: xiaomi_miio_airpurifier.fan_set_child_lock_on data: entity_id: fan.bedroom_air_purifier_3h turn_off: service: xiaomi_miio_airpurifier.fan_set_child_lock_off data: entity_id: fan.bedroom_air_purifier_3h icon_template: \u0026#34;mdi:lock-outline\u0026#34; bedroom_air_purifier_3h_buzzer: friendly_name: \u0026#34;Buzzer\u0026#34; value_template: \u0026#34;{{ is_state_attr(\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;, \u0026#39;buzzer\u0026#39;, true) }}\u0026#34; turn_on: service: xiaomi_miio_airpurifier.fan_set_buzzer_on data: entity_id: fan.bedroom_air_purifier_3h turn_off: service: xiaomi_miio_airpurifier.fan_set_buzzer_off data: entity_id: fan.bedroom_air_purifier_3h icon_template: \u0026#34;mdi:volume-high\u0026#34; input_select: bedroom_air_purifier_3h_mode: name: Mode initial: Fan options: - Auto - Silent - Favorite - Fan icon: \u0026#34;mdi:animation-outline\u0026#34; input_number: bedroom_air_purifier_3h_fan_level: name: \u0026#34;Fan level\u0026#34; initial: 1 min: 1 max: 3 step: 1 icon: \u0026#34;mdi:weather-windy\u0026#34; automation: - id: \u0026#39;9231979hoangmnsd231298\u0026#39; alias: Bedroom Air Purifier mode change trigger: entity_id: input_select.bedroom_air_purifier_3h_mode platform: state action: service: fan.set_preset_mode data_template: entity_id: fan.bedroom_air_purifier_3h preset_mode: \u0026#39;{{ states.input_select.bedroom_air_purifier_3h_mode.state }}\u0026#39; - id: \u0026#39;3467979hoangmnsd2365698\u0026#39; alias: Bedroom Air Purifier fan level change trigger: entity_id: input_number.bedroom_air_purifier_3h_fan_level platform: state action: service: xiaomi_miio_airpurifier.fan_set_fan_level data_template: entity_id: fan.bedroom_air_purifier_3h level: \u0026#39;{{ states.input_number.bedroom_air_purifier_3h_fan_level.state | int }}\u0026#39; edit file opt/hass/config/configuration.yaml:\n... homeassistant: packages: !include_dir_named packages -\u0026gt; restart HASS\nL√™n giao di·ªán HASS, edit Dashboard -\u0026gt; add Card manual, paste ƒëo·∫°n code sau:\ntype: horizontal-stack cards: - type: entities title: Bedroom Air Purifier 3H show_header_toggle: false entities: - entity: fan.bedroom_air_purifier_3h name: Power # Only display Mode when Power is not on state - type: conditional conditions: - entity: fan.bedroom_air_purifier_3h state: \u0026#34;on\u0026#34; row: entity: input_select.bedroom_air_purifier_3h_mode - entity: switch.bedroom_air_purifier_3h_child_lock - entity: switch.bedroom_air_purifier_3h_led - entity: switch.bedroom_air_purifier_3h_buzzer # Only display Fan level when Power is ON state - type: conditional conditions: - entity: fan.bedroom_air_purifier_3h state: \u0026#34;on\u0026#34; row: entity: input_number.bedroom_air_purifier_3h_fan_level - entity: sensor.bedroom_air_purifier_3h_speed - entity: sensor.bedroom_air_purifier_3h_filter_remaining - type: vertical-stack cards: - entities: - entity: sensor.bedroom_air_purifier_3h_air_quality_pm25 - entity: sensor.bedroom_air_purifier_3h_temp - entity: sensor.bedroom_air_purifier_3h_humidity show_header_toggle: false theme: default title: Bedroom Env type: entities - entities: - sensor.bedroom_air_purifier_3h_air_quality_pm25 hours_to_show: 24 refresh_interval: 60 title: Bedroom Air quality type: history-graph Save v√† quay l·∫°i th·ª≠ ƒëi·ªÅu khi·ªÉn xem sao. Check log ko c√≥ l·ªói ERROR l√† OK:\n1.2. Set c√°c Automations 1.2.1. B·∫≠t t·∫Øt theo gi·ªù Edit file opt/hass/config/automations.yaml:\n# Bedroom Air Purifier On Off on Time - id: \u0026#39;15914324182184740\u0026#39; alias: Bedroom Air Purifier On Off on Time description: Turn on/off Bedroom Air Purifier on schedule trigger: - at: \u0026#39;01:00:00\u0026#39; platform: time - at: \u0026#39;07:00:00\u0026#39; platform: time condition: [] action: - data: {} entity_id: fan.bedroom_air_purifier_3h service_template: \u0026gt;{% if trigger.now.hour == 1 %} fan.turn_on {% elif trigger.now.hour == 7 %} fan.turn_off {% else %} fan.turn_off {% endif %} Automation t·ª± ƒë·ªông trigger v√†o 2 th·ªùi ƒëi·ªÉm l√† 1h s√°ng v√† 6h s√°ng. T·ª± ƒë·ªông b·∫≠t n·∫øu l√† 1h s√°ng. T·ª± ƒë·ªông t·∫Øt n·∫øu l√† 6h s√°ng.\n1.2.2. G·ª≠i th√¥ng b√°o khi AQI qu√° cao Edit file opt/hass/config/automations.yaml:\n# Bedroom Air Purifier AQI over 80 Warning - id: \u0026#39;1591183075585\u0026#39; alias: Bedroom Air Purifier AQI over 80 Warning description: Notify to Telegram when Bedroom Air Purifier AQI over 80 trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;aqi\u0026#39;\u0026#39;)| int(0) \u0026gt; 80 }}\u0026#39; condition: [] action: - data_template: message: \u0026#39;Becareful, right now AQI = {{state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;aqi\u0026#39;\u0026#39;)}}\u0026#39; title: \u0026#39;‚ô®Ô∏è Home Assistant: Bedroom AQI is too high!\u0026#39; service: telegram_bot.send_message C√≥ th·ªÉ b·∫°n s·∫Ω t·ª± h·ªèi khi n√†o Automation tr√™n ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2. C√¢u tr·∫£ l·ªùi l√†:\n The Template Trigger‚Äôs template will be evaluated every time the sensor‚Äôs value changes\n Nghƒ©a l√† khi n√†o AQI thay ƒë·ªïi t·ª´ 80 sang s·ªë kh√°c th√¨ Automation tr√™n m·ªõi ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2\nsource: https://community.home-assistant.io/t/trigger-template-will-it-keep-firing/325009/7\n1.2.3. G·ª≠i th√¥ng b√°o khi l√µi l·ªçc h·∫øt th·ªùi gian s·ª≠ d·ª•ng Edit file opt/hass/config/automations.yaml:\n# Bedroom Air Purifier Filter lifetime Warning - id: \u0026#39;1591183684060\u0026#39; alias: Bedroom Air Purifier Filter lifetime Warning description: Bedroom Air Purifier Low Filter Life Notify 5% trigger: - platform: template value_template: \u0026#39;{{ state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026lt; 6 and state_attr(\u0026#39;\u0026#39;fan.bedroom_air_purifier_3h\u0026#39;\u0026#39;,\u0026#39;\u0026#39;filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026gt; 0 }}\u0026#39; condition: [] action: - data_template: message: \u0026gt;- Your filter remaining is {{state_attr(trigger.entity_id,\u0026#39;filter_life_remaining\u0026#39;)}}%  title: \u0026#39;üòì Home Assistant: Air Purifier 3H filter remain warning\u0026#39; service: telegram_bot.send_message C√≥ th·ªÉ b·∫°n s·∫Ω t·ª± h·ªèi khi n√†o Automation tr√™n ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2. C√¢u tr·∫£ l·ªùi l√†:\n The Template Trigger‚Äôs template will be evaluated every time the sensor‚Äôs value changes\n Nghƒ©a l√† khi n√†o Filter remaning thay ƒë·ªïi t·ª´ 5 sang s·ªë kh√°c th√¨ Automation tr√™n m·ªõi ƒë∆∞·ª£c trigger l·∫ßn th·ª© 2\nsource: https://community.home-assistant.io/t/trigger-template-will-it-keep-firing/325009/7\n2. C√°ch 2: D√πng Xiaomi Miio Integration (Recommended) 2.1. Setup C√†i ƒë·∫∑t app MiHome latest tr√™n Androids, ƒëƒÉng k√Ω, pair n√≥ v·ªõi Air Purifier 3H c·ªßa b·∫°n (·∫•n ƒë·ªìng th·ªùi n√∫t b·∫•m sau m√°y v·ªõi n√∫t c·∫£m ·ª©ng, ch·ªù ƒë·∫øn khi 3 ti·∫øng b√≠p li√™n ti·∫øp), h√£y ƒë·∫£m b·∫£o pair th√†nh c√¥ng nh√©!\nV√†o Setting -\u0026gt; Devices,Intergration -\u0026gt; Add Integration -\u0026gt; Search Miio:\nNh·∫≠p User/password m√† c√°c b·∫°n ƒë√£ ƒëƒÉng k√Ω t√†i kho·∫£n b√™n tr√™n\nN√≥ s·∫Ω t·ª± ƒë·ªông t√¨m c√°c devices c√≥ trong account c·ªßa b·∫°n\nHi·ªán nh∆∞ n√†y l√† OK:\nV·∫´n c·∫ßn t·∫°o file n√†y ƒë·ªÉ t·∫°o entity:\n/opt/hass/config/packages/xiaomi_bedroom_airpurifier.yaml\ninput_select: bedroom_air_purifier_3h_mode: name: Mode initial: Fan options: - Auto - Silent - Favorite - Fan icon: \u0026#34;mdi:animation-outline\u0026#34; configurations.yml:\nhomeassistant: packages: !include_dir_named packages Dashboard c·ªßa b·∫°n s·∫Ω c√≥ code nh∆∞ n√†y:\n# using Miio Integration - type: entities title: Air Purifier 33H show_header_toggle: false entities: - entity: fan.mi_air_purifier_33h name: Power - type: conditional conditions: - entity: fan.mi_air_purifier_33h state: \u0026#39;on\u0026#39; row: entity: input_select.bedroom_air_purifier_3h_mode name: Mode - entity: switch.mi_air_purifier_33h_child_lock name: Child lock - entity: select.mi_air_purifier_33h_led_brightness name: Led - entity: switch.mi_air_purifier_33h_buzzer name: Buzzer - type: conditional conditions: - entity: fan.mi_air_purifier_33h state: \u0026#39;on\u0026#39; row: entity: number.mi_air_purifier_33h_fan_level name: Fan level - entity: sensor.mi_air_purifier_33h_motor_speed name: Motor speed - entity: sensor.mi_air_purifier_33h_filter_life_remaining name: Filter Life remaining file automations.yml c·∫ßn add th√™m c√°i n√†y:\n# Bedroom Air Purifier change mode  # (Because Miio Integration doesn\u0026#39;t provide set_mode entity, so... # this automation help to set mode when `input_select.bedroom_air_purifier_3h_mode` is updated ) - id: \u0026#39;9hoangmnsd231979231298\u0026#39; alias: Bedroom Air Purifier mode change trigger: entity_id: input_select.bedroom_air_purifier_3h_mode platform: state action: service: fan.set_preset_mode data_template: entity_id: fan.mi_air_purifier_33h preset_mode: \u0026#39;{{ states.input_select.bedroom_air_purifier_3h_mode.state }}\u0026#39; C√°c Automations kh√°c th√¨ c≈©ng c·∫ßn s·ª≠a v√¨ c√°ch l·∫•y attributes kh√°c nhau:\nautomations.yml\n# Bedroom Air Purifier On Off on Time - id: \u0026#39;1hoangmnsd5914324182184740\u0026#39; alias: Bedroom Air Purifier On Off on Time description: Turn on/off Bedroom Air Purifier on schedule trigger: - at: \u0026#39;23:30:00\u0026#39; platform: time - at: \u0026#39;07:00:00\u0026#39; platform: time condition: [] action: - data: {} entity_id: fan.mi_air_purifier_33h service_template: \u0026gt;{% if trigger.now.hour == 23 %} fan.turn_on {% elif trigger.now.hour == 7 %} fan.turn_off {% else %} fan.turn_off {% endif %} # Bedroom Air Purifier AQI over 80 Warning - id: \u0026#39;1hoangmnsd591183075585\u0026#39; alias: Bedroom Air Purifier AQI over 80 Warning description: Notify to Telegram when Bedroom Air Purifier AQI over 80 trigger: - platform: template value_template: \u0026#39;{{ states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_pm2_5\u0026#39;\u0026#39;)| int(0) \u0026gt; 80 }}\u0026#39; condition: [] action: - data_template: target: - 1xxxxxxx3 message: \u0026#39;Becareful, right now AQI = {{states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_pm2_5\u0026#39;\u0026#39;)}}\u0026#39; title: \u0026#39;‚ô®Ô∏è HASS: Bedroom AQI is too high!\u0026#39; service: telegram_bot.send_message # Bedroom Air Purifier Filter lifetime Warning - id: \u0026#39;1591183684060\u0026#39; alias: Bedroom Air Purifier Filter lifetime Warning description: Bedroom Air Purifier Low Filter Life Notify 5% trigger: - platform: template value_template: \u0026#39;{{ states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026lt; 6 and states(\u0026#39;\u0026#39;sensor.mi_air_purifier_33h_filter_life_remaining\u0026#39;\u0026#39;)| int(30) \u0026gt; 0 }}\u0026#39; condition: [] action: - data_template: target: - 1xxxxxxx3 message: \u0026gt;- Your filter remaining is {{states(\u0026#39;sensor.mi_air_purifier_33h_filter_life_remaining\u0026#39;)}}% title: \u0026#39;üòì HASS: Air Purifier 3H filter remain warning\u0026#39; service: telegram_bot.send_message Xong r·ªìi, C√°ch n√†y s·∫Ω nh·∫π nh√†ng h∆°n, m√¨nh th·∫•y d·ªÖ h∆°n nhi·ªÅu\n3. Trouble shooting Qu√° tr√¨nh theo d√µi c√≥ th·ªÉ xu·∫•t hi·ªán 1 s·ªë log WARNING nh∆∞ n√†y:\n2022-05-02 18:45:21 WARNING (MainThread) [homeassistant.components.input_select] Invalid option: None (possible options: Auto, Silent, Favorite, Fan) 2022-05-03 04:19:35 WARNING (MainThread) [homeassistant.helpers.template] Template warning: 'int' got invalid input 'None' when rendering template '{{ state_attr('fan.bedroom_air_purifier_3h','filter_life_remaining')|int \u0026lt; 6 and state_attr('fan.bedroom_air_purifier_3h','filter_life_remaining')|int \u0026gt; 0 }}' but no default was specified. Currently 'int' will return '0', however this template will fail to render in Home Assistant core 2022.1 2022-05-03 11:47:20 WARNING (MainThread) [homeassistant.helpers.entity] Update of fan.bedroom_air_purifier_3h is taking over 10 seconds 2022-05-03 16:15:41 ERROR (SyncWorker_5) [miio.miioprotocol] Got error when receiving: timed out ...  Th√¨ m√¨nh ƒë√£ th·ª≠ fix trong c√°c ƒëo·∫°n code b√™n tr√™n, Ch·ªâ display Mode select khi m√† Power ƒëang ·ªü state on Th√™m c√°c gi√° tr·ªã default khi so s√°nh gi√° tr·ªã int, b·∫±ng syntax int(30) \u0026lt; 6: c√≥ th·ªÉ hi·ªÉu n·∫øu ko c√≥ gi√° tr·ªã th√¨ code s·∫Ω l·∫•y default l√† 30 ƒë·ªÉ so s√°nh v·ªõi 6 ƒêang ch·ªù xem c√°c log l·ªói/warning tr√™n c√≥ xu·∫•t hi·ªán n·ªØa hay ko\u0026hellip;.  4. Tham kh·∫£o 1 b√†i vi·∫øt cho r·∫±ng kh√¥ng n√™n s·ª≠ d·ª•ng ch·∫ø ƒë·ªô Auto c·ªßa Air Purifier:\nhttps://smartairfilters.com/en/blog/data-explains-never-use-purifiers-auto-mode/\nCREDIT https://konnected.vn/home-assistant/home-assistant-tich-hop-may-loc-khong-khi-xiaomi-2020-06-03\nhttps://thesmarthomejourney.com/2021/06/28/smart-fan-with-home-assistant/\nhttps://github.com/syssi/xiaomi_airpurifier\nhttps://mic22.medium.com/how-to-get-token-of-xiaomi-air-purifier-h3-oct-2020-47d0f02451c3\nhttps://github.com/PiotrMachowski/Xiaomi-cloud-tokens-extractor/releases/latest/download/token_extractor.exe\nhttps://community.home-assistant.io/t/trigger-template-will-it-keep-firing/325009/7\nhttps://community.home-assistant.io/t/conditional-inside-entities-card/208819/2\nhttps://community.home-assistant.io/t/issue-with-template-no-default-was-specified-currently-int-will-return-0/387966\nhttps://smartairfilters.com/en/blog/data-explains-never-use-purifiers-auto-mode/\nhttps://community.home-assistant.io/t/lovelace-xiaomi-mi-air-purifier-3h-card/192100/12\n","href":"/posts/encrypt-connect-home-assistant-to-air-purifier3h/","title":"Connect Home Assistant Container to Xiaomi Air Purifier3H"},{"content":"","href":"/tags/docker-compose/","title":"Docker-compose"},{"content":"B√†i n√†y t·ªïng h·ª£p c√°c step g·∫ßn nh∆∞ t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi ƒë·ªÉ setup H·ªá th·ªëng Home Assistant Container tr√™n Raspberry Pi\n1. Install Docker \u0026amp; Docker Compose sudo apt-get update sudo apt-get upgrade -y sudo apt-get install avahi-daemon sudo apt install git -y sudo apt update sudo apt full-upgrade curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker ${USER} reboot to apply: sudo shutdown -r now docker version docker info docker run hello-world # install pip3 sudo apt-get install libffi-dev libssl-dev sudo apt install python3-dev -y sudo apt-get install -y python3 python3-pip sudo shutdown -r now # C√°ch 1 install docker-compose default sudo shutdown -r now sudo su pip3 install docker-compose docker-compose version sudo systemctl enable docker # C√°ch 2 kh√°c ƒë·ªÉ install docker-compose specific version # C·∫ßn bi·∫øt OS c·ªßa b·∫°n l√† g√¨ ƒë√£: uname -s # gi·∫£ output l√† linux uname -m # gi·∫£ s·ª≠ output l√† aarch64 # V√†o trang n√†y l·∫•y link: https://github.com/docker/compose/releases/ # ch·ªçn c√°i t∆∞∆°ng ·ª©ng linux, arch64 l√† OK # gi·∫£ s·ª≠ l·∫•y ƒë∆∞·ª£c: https://github.com/docker/compose/releases/download/2.5.0/docker-compose-linux-aarch64 #install docker-compose nh∆∞ sau: sudo curl -L https://github.com/docker/compose/releases/download/v2.5.0/docker-compose-linux-aarch64 -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version sudo systemctl enable docker N·∫øu v√¨ 1 l√Ω do n√†o ƒë√≥, docker b·ªã l·ªói, b·∫°n mu·ªën uninstall docker kh·ªèi m√°y th√¨:\n# uninstall docker dpkg -l | grep -i docker sudo apt-get purge -y docker-engine docker docker.io docker-ce docker-ce-cli sudo apt-get autoremove -y --purge docker-engine docker docker.io docker-ce sudo rm -rf /var/lib/docker /etc/docker sudo rm -rf /var/run/docker.sock # uninstall docker-compose pip3 uninstall docker-compose # uninstall python sudo apt-get remove libffi-dev libssl-dev sudo apt remove python3-dev -Y sudo apt-get remove -y python3 python3-pip 2. Install Portainer \u0026amp; HASS create file /opt/hass/docker-compose.yml\nversion: \u0026#39;3.0\u0026#39; services: portainer: container_name: portainer image: portainer/portainer restart: always stdin_open: true tty: true ports: - \u0026#34;9000:9000/tcp\u0026#34; environment: - TZ=Asia/Ho_Chi_Minh volumes: - /var/run/docker.sock:/var/run/docker.sock - /opt/portainer:/data homeassistant: container_name: homeassistant image: \u0026#34;ghcr.io/home-assistant/home-assistant:2022.5.2\u0026#34; volumes: - /opt/hass/config:/config - /etc/localtime:/etc/localtime:ro restart: unless-stopped privileged: true network_mode: host cd /opt/hass docker-compose up -d Check access HASS UI: http://RPi_LOCAL_IP:8123/, create account/password\nCheck access Portainer: http://RPi_LOCAL_IP:9000/, create account/password\nEdit file /opt/hass/config/configuration.yaml\nAdd this on top:\npanel_iframe: portainer: title: \u0026#34;Portainer\u0026#34; url: \u0026#34;http://RPi_LOCAL_IP:9000/#/containers\u0026#34; icon: mdi:docker require_admin: true Go to HASS UI: Configuration -\u0026gt; Settings -\u0026gt; Restart to take effect.\nHASS UI will have new frame Portainer on the sidebar:\n3. Install Mosquito edit /opt/hass/docker-compose.yml\nadd these lines:\n.... mosquitto: image: eclipse-mosquitto:2.0.14 container_name: mosquitto volumes: - /opt/mosquitto:/mosquitto ports: - 1883:1883 - 9001:9001 create file /opt/mosquitto/config/mosquitto.conf/:\npersistence true persistence_location /mosquitto/data/ log_dest file /mosquitto/log/mosquitto.log # Authentication #allow_anonymous false #listener 1883 #password_file /mosquitto/config/password.txt Run:\ndocker-compose up -d check log of mosquitto container if there is any error? You should fix it.\nthis is normal: No logs available\nT·ª´ Portainer UI -\u0026gt; v√†o mosquitto container via Console log, t·∫°o user mqttuser v√† nh·∫≠p password b·∫±ng command:\nmosquitto_passwd -c /mosquitto/config/password.txt mqttuser S·ª≠a l·∫°i file /opt/mosquitto/config/mosquitto.conf, uncomment ph·∫ßn Authentication:\npersistence true persistence_location /mosquitto/data/ log_dest file /mosquitto/log/mosquitto.log # Authentication allow_anonymous false listener 1883 password_file /mosquitto/config/password.txt V√†o Portainer, restart mosquitto container, check log no error l√† OK:\nV√†o HASS, Configuration -\u0026gt; Devices \u0026amp; Services -\u0026gt; Integrations -\u0026gt; Add integration -\u0026gt; Search MQTT ƒëi·ªÅn IP c·ªßa RPi (n∆°i run mosquitto container, ex: 192.168.1.4), port: 1883, user: mqttuser password: ******\nM·ª•c ti√™u cu·ªëi c√πng l√† add ƒë∆∞·ª£c Integration MQTT nh∆∞ th·∫ø n√†y:\n4. Install Zigbee2MQTT Mua 1 chi·∫øc USB 3.0 Zigbee Sonoff Dongle, c·∫Øm v√†o RPi:\nD√πng command sau ƒë·ªÉ xem serial id c·ªßa USB v·ª´a c·∫Øm:\n$ ls -l /dev/serial/by-id/ total 0 lrwxrwxrwx 1 root root 13 May 1 21:21 usb-ITead_Sonoff_Zigbee_3.0_USB_Dongle_Plus_e8fc8a915ad9eb119edf148e6fe9f4d9-if00-port0 -\u0026gt; ../../ttyUSB0 -\u0026gt; l·∫•y ƒë∆∞·ª£c ttyUSB0\nedit /opt/hass/docker-compose.yml add these line:\n... zigbee2mqtt: container_name: zigbee2mqtt image: koenkk/zigbee2mqtt:1.25.0 restart: unless-stopped volumes: - /opt/zigbee2mqtt/data:/app/data - /run/udev:/run/udev:ro ports: # Frontend port - 8080:8080 environment: - TZ=Asia/Ho_Chi_Minh devices: # Make sure this matched your adapter location - /dev/ttyUSB0:/dev/ttyUSB0 Run:\ndocker-compose up -d Edit file /opt/zigbee2mqtt/data/configuration.yaml\n# Home Assistant integration (MQTT discovery) homeassistant: true # allow new devices to join permit_join: true # MQTT settings mqtt: # MQTT base topic for zigbee2mqtt MQTT messages base_topic: zigbee2mqtt # MQTT server URL server: \u0026#39;mqtt://mosquitto:1883\u0026#39; # MQTT server authentication, uncomment if required: user: mqttuser password: PASSWORD_OF_mqttuser # Serial settings serial: # Location USB sniffer port: /dev/ttyUSB0 # Enable the Zigbee2MQTT frontend frontend: port: 8080 experimental: new_api: true check log container zigbee2mqtt ko c√≥ l·ªói l√† OK:\nNow you can access UI on port 8080 (ex: 192.168.1.8:8080)\nAdd frame zigbee2mqtt to HASS UI:\nedit /opt/hass/config/configuration.yaml:\nadd these line:\npanel_iframe: ... zigbee2mqtt: title: \u0026#34;Zigbee2MQTT\u0026#34; url: \u0026#34;http://RPi_LOCAL_IP:8080\u0026#34; icon: mdi:zigbee require_admin: true Go to HASS UI: Configuration -\u0026gt; Settings -\u0026gt; Restart to take effect.\nHASS UI will have new frame on the left sidebar.\n5. Connect Zigbee device to Zigbee2MQTT service 5.1. Setup Gi·∫£ s·ª≠ b·∫°n ƒë√£ mua 1 sensor nh∆∞ th·∫ø n√†y:\ntr√™n giao di·ªán Zigbee2MQTT ·∫•n v√†o n√∫t Permit join (All):\nN√≥ s·∫Ω hi·ªán ƒë·∫øm ng∆∞·ª£c kho·∫£ng 250s,\nTrong th·ªùi gian ƒë√≥ ·∫•n l√¨ n√∫t n√†y c·ªßa sensor kho·∫£ng 5s, ko n√™n ·∫•n l√¢u qu√° - sensor s·∫Ω left network:\nCho ƒë·∫øn khi hi·ªán ra device nh∆∞ n√†y l√† OK, sensor ƒë√£ join network zigbee th√†nh c√¥ng:\n1 s·ªë tab th√¥ng tin v·ªÅ sensor tr√™n Zigbee2MQTT:\n5.2. Test N·∫øu b·∫°n ƒë·∫∑t 2 c·ª•c g·∫ßn nhau, tr·∫°ng th√°i s·∫Ω thay ƒë·ªïi closed nh∆∞ n√†y:\nN·∫øu 2 c·ª•c ƒë·∫∑t xa nhau, tr·∫°ng th√°i s·∫Ω l√† open:\nThay ƒë·ªïi ph·∫£n √°nh r·∫•t nhanh, g·∫ßn nh∆∞ ngay l·∫≠p t·ª©c üòç r·∫•t h·ªØu d·ª•ng ph·∫£i ko?\nGi·ªù quay l·∫°i m√†n h√¨nh HASS -\u0026gt; Setting -\u0026gt; Integration, b·∫°n s·∫Ω th·∫•y 1 device ƒë√£ ƒë∆∞·ª£c xu·∫•t hi·ªán d∆∞·ªõi MQTT integration:\n·∫§n v√†o b·∫°n s·∫Ω th·∫•y Sensor c·ªßa b·∫°n c√πng v·ªõi c√°c th√¥ng s·ªë:\n5.3. Automation \u0026amp; Alert Ti·∫øp theo s·∫Ω l√† 1 s·ªë script Automation or Alert ƒë·ªÉ ·ª©ng d·ª•ng c√°c sensor n√†y nh√©!\nAutomation (hass/config/automations.yaml):\n# Warning when Front Door open more than 1 min =\u0026gt; change to use Alert in configuration.json - id: \u0026#39;15324556454\u0026#39; alias: Warning when Front Door open more than 1 min description: \u0026#39;Warning when Front Door open more than 1 min\u0026#39; trigger: - entity_id: binary_sensor.front_door_sensor_contact for: 00:01:00 # wait for x min after changing state from off to on platform: state to: \u0026#39;on\u0026#39; condition: [] action: - data: message: \u0026#39;\u0026#39; title: \u0026#39;ü§î HASS: Front door left open more than a min.\u0026#39; service: telegram_bot.send_message nh∆∞ng c√°i c√°ch d√πng Automation tr√™n ko hay l·∫Øm, HASS cung c·∫•p s·∫µn t√≠nh nƒÉng Alert d√πng hay h∆°n:\nAlert (hass/config/configuration.yaml) repeat sau 3/6/10 min, v·ªÅ sau c·ª© 10min alert 1 l·∫ßn:\n... notify: - platform: telegram name: hoangmnsd chat_id: 816661853 ... alert: # Front door open front_door_open: name: Front door is open entity_id: binary_sensor.front_door_sensor_contact state: \u0026#34;on\u0026#34; # Optional, \u0026#39;on\u0026#39; is the default value repeat: - 3 - 6 - 10 can_acknowledge: true # Optional, default is true skip_first: true # Optional, false is the default message: \u0026#34;üò≤ HASS: Front door left open more than a min.\u0026#34; done_message: \u0026#34;üòâ HASS: Front door closed.\u0026#34; notifiers: - hoangmnsd # need to specify notifier 5.4. Using Blueprint to notify when Sensor battery low C√°ch 1: D√πng sbyx blueprint\nHASS Community cung c·∫•p 1 blueprint ƒë·ªÉ x·ª≠ l√Ω vi·ªác c·∫£nh b√°o battery cho sensor n√†y: https://community.home-assistant.io/t/low-battery-level-detection-notification-for-all-battery-sensors/258664\nN·∫øu ko ·∫•n v√†o n√∫t Import Blueprint m√† b·ªã l·ªói, th√¨ ph·∫£i import th·ªß c√¥ng nh∆∞ sau\nV√†o RPi server, t·∫°o file /opt/hass/config/blueprints/automation/sbyx/low-battery-level-detection-notification-for-all-battery-sensors.yaml\n(content file l·∫•y t·ª´ link n√†y https://gist.github.com/sbyx/1f6f434f0903b872b84c4302637d0890)\n-\u0026gt; restart HASS\nƒê·ªÉ s·ª≠ d·ª•ng blueprint, edit file hass/config/automations.yaml:\nNh∆∞ n√†y l√† n√≥ s·∫Ω notify everyday at 10:00 PM, khi c√≥ 1 sensor n√†o battery d∆∞·ªõi 10%, g·ª≠i message v·ªÅ notify hoangmnsd\n# Warning when Front Door sensor low battery - id: \u0026#39;1653065570149\u0026#39; alias: Low battery level detection \u0026amp; notification for all battery sensors description: \u0026#39;Low battery level detection \u0026amp; notification for all battery sensors\u0026#39; use_blueprint: path: sbyx/low-battery-level-detection-notification-for-all-battery-sensors.yaml input: threshold: 10 time: \u0026#39;22:00:00\u0026#39; actions: - service: notify.hoangmnsd data: message: \u0026#39;‚òπ HASS: {{sensors}} is low battery, about 10%\u0026#39; C√°c b·∫°n c√≥ th·ªÉ ƒë·ªçc file ƒë·ªÉ hi·ªÉu https://gist.github.com/sbyx/1f6f434f0903b872b84c4302637d0890.\nN√≥ theo d√µi t·∫•t c·∫£ c√°c sensor m√† c√≥ device_class = battery\nC√°ch 2: Clone l·∫°i hoangmsnd blueprint\nUpdate 2022.07.16: Sau 1 th·ªùi gian s·ª≠ d·ª•ng c√°c Sonoff Sensor, m√¨nh nh·∫≠n ra r·∫±ng l√∫c n√†o ch√∫ng c≈©ng show 100% battery.\nM√¨nh th·ª±c s·ª± s·∫Ω ko th·ªÉ bi·∫øt th·ªùi l∆∞·ª£ng pin c·ªßa ch√∫ng ƒëang c√≤n bao nhi√™u n·∫øu c·ª© ch·ªâ nh√¨n v√†o c√°i s·ªë 100 ·∫£o l√≤i ƒë√≥.\nTh·∫ø n√™n m√¨nh s·∫Ω clone l·∫°i c√°i blueprint tr√™n, t·∫°o blueprint c·ªßa ri√™ng m√¨nh.\nCh√∫ng ta s·∫Ω d·ª±a tr√™n voltage ƒë·ªÉ theo d√µi, v√¨ ch·ªâ c√≥ voltage m·ªõi gi·∫£m d·∫ßn theo th·ªùi gian.\nMax voltage c·ªßa pin l√† 3200 mV. Ch√∫ng ta s·∫Ω warning khi pin gi·∫£m xu·ªëng d∆∞·ªõi 800 mV.\nV√†o RPi server, t·∫°o file /opt/hass/config/blueprints/automation/hoangmnsd/low-battery-voltage-detection-notification-for-all-battery-sensors.yaml\ncontent file:\nblueprint: name: Low battery level detection \u0026amp; notification for all battery sensors description: Regularly test all sensors with \u0026#39;battery\u0026#39; device-class for crossing a certain battery level threshold and if so execute an action. domain: automation input: threshold: name: Battery warning level threshold description: Battery sensors below threshold are assumed to be low-battery (as well as binary battery sensors with value \u0026#39;on\u0026#39;). default: 800 selector: number: min: 100 max: 3200 unit_of_measurement: \u0026#39;mV\u0026#39; mode: slider step: 100 time: name: Time to test on description: Test is run at configured time default: \u0026#39;10:00:00\u0026#39; selector: time: {} day: name: Weekday to test on description: \u0026#39;Test is run at configured time either everyday (0) or on a given weekday (1: Monday ... 7: Sunday)\u0026#39; default: 0 selector: number: min: 0.0 max: 7.0 mode: slider step: 1.0 exclude: name: Excluded Sensors description: Battery sensors (e.g. smartphone) to exclude from detection. Only entities are supported, devices must be expanded! default: {entity_id: []} selector: target: entity: device_class: battery actions: name: Actions description: Notifications or similar to be run. {{sensors}} is replaced with the names of sensors being low on battery. selector: action: {} source_url: https://gist.github.com/hoangmnsd variables: day: !input \u0026#39;day\u0026#39; threshold: !input \u0026#39;threshold\u0026#39; exclude: !input \u0026#39;exclude\u0026#39; sensors: \u0026gt;-{% set result = namespace(sensors=[]) %} {% for state in states.binary_sensor | selectattr(\u0026#39;attributes.device_class\u0026#39;, \u0026#39;==\u0026#39;, \u0026#39;battery\u0026#39;) %} {% if 0 \u0026lt;= state.attributes.voltage | int(-1) \u0026lt; threshold | int and not state.entity_id in exclude.entity_id %} {% set result.sensors = result.sensors + [state.name] %} {% endif %} {% endfor %} {{result.sensors|join(\u0026#39;, \u0026#39;)}} trigger: - platform: time at: !input \u0026#39;time\u0026#39; condition: - \u0026#39;{{ sensors != \u0026#39;\u0026#39;\u0026#39;\u0026#39; and (day | int == 0 or day | int == now().isoweekday()) }}\u0026#39; action: - choose: [] default: !input \u0026#39;actions\u0026#39; mode: single file automations.yml add th√™m ƒëo·∫°n sau:\n# # Warning when sensor low battery (using hoangmnsd blueprint) - id: \u0026#39;92hoangmnsd3972358762324\u0026#39; alias: Low battery voltage detection \u0026amp; notification for all battery sensors description: \u0026#39;\u0026#39; use_blueprint: path: hoangmnsd/low-battery-voltage-detection-notification-for-all-battery-sensors.yaml input: threshold: 500 time: \u0026#39;22:00:00\u0026#39; actions: - service: notify.hoangmnsd data: message: \u0026#39;‚òπ HASS: {{sensors}}, lets take a look\u0026#39; Th·∫ø l√† xong!\n6. Install HACS (Home Assistant Comunity Store) Go inside the container with docker exec -it homeassistant bash, Run command:\nwget -q -O - https://install.hacs.xyz | bash - restart HASS\nV√†o Configuration -\u0026gt; Add Integration -\u0026gt; Search HACS -\u0026gt; tick ch·ªçn all -\u0026gt; submit\nC·∫•p quy·ªÅn cho n√≥ access v√†o GitHub c·ªßa b·∫°n (s·∫Ω c√≥ 1 key hi·ªán ra ƒë·ªÉ paste v√†o)\nSau ƒë√≥ s·∫Ω th·∫•y HACS hi·ªán b√™n tr√°i sidebar:\nV√≠ d·ª• add 1 Card t·ª´ HACS:\nCh·ªçn HACS -\u0026gt; Frontend -\u0026gt; ·∫§n Explore \u0026amp; dowload repositories (n√∫t m√†u xanh)\nSearch Air Purifier s·∫Ω th·∫•y Air Purifier Card -\u0026gt; Ch·ªçn c√°i c·ªßa @fineemb (ch√∫ √Ω v√¨ c√°i n√†y dc recommended)\nrestart HASS\nV√†o Overview -\u0026gt; edit Dashboard -\u0026gt; Add Card -\u0026gt; S·∫Ω th·∫•y Air Purifier xu·∫•t hi·ªán:\n7. Integrate Telegram Bot C·∫ßn bi·∫øt c√°ch t·∫°o Bot, l·∫•y TELEGRAM_TOKEN v√† CHAT_ID, c√≥ th·ªÉ tham kh·∫£o b√†i n√†y:\nLambda + API Gateway, Telegram Bot and Serverless Webapp\nSau khi l·∫•y dc TELEGRAM_TOKEN v√† CHAT_ID c·ªßa Telegram Bot c·ªßa b·∫°n\nedit /opt/hass/config/configuration.yaml:\n# Example configuration.yaml entry for the Telegram Bot telegram_bot: - platform: polling api_key: TELEGRAM_TOKEN allowed_chat_ids: - CHAT_ID # example: 123456789 for the chat_id of a user #- CHAT_ID_2 # example: -987654321 for the chat_id of a group #- CHAT_ID_3 notify: - platform: telegram name: hoangmnsd chat_id: CHAT_ID # example: 123456789 for the chat_id of a user restart HASS\n7.1. C√°ch s·ª≠ d·ª•ng service Telegram Bot V√≠ d·ª• mu·ªën g·ª≠i message ƒë·∫øn Telegram khi log c·ªßa HASS c√≥ l·ªói.\nedit file /opt/hass/config/configuration.yaml:\nsystem_log: fire_event: true edit file /opt/hass/config/automations.yaml:\n# HASS Notify When Error Log Occured - id: \u0026#39;15888152543\u0026#39; alias: HASS Notify When Error Log Occured description: HASS Notify When Error Log Occured trigger: - event_data: level: ERROR event_type: system_log_event platform: event # Ignore these errors condition: - \u0026#34;{{ \u0026#39;Error while getting Updates: urllib3 HTTPError\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;HacsDisabledReason.RATE_LIMIT\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Error handling request\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Got error when receiving: timed out\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Error while getting Updates: Conflict: terminated by other getUpdates request\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Update for fan.bedroom_air_purifier_3h fails\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; action: # Send to Telegram or any notify platform - data_template: message: \u0026#39;{{trigger.event.data.name}} {{ \u0026#39;\u0026#39;\\n\u0026#39;\u0026#39; -}} {{trigger.event.data.message}}\u0026#39; parse_mode: html title: \u0026#39;üò± Home Assistant: UNEXPECTED ERROR!\u0026#39; service: telegram_bot.send_message # - data_template: # message: \u0026#39;{{trigger.event.data.name}} {{ \u0026#39;\u0026#39;\\n\u0026#39;\u0026#39; -}} {{trigger.event.data.message}}\u0026#39; # title: \u0026#39;üò± Home Assistant: ERROR!\u0026#39; # service: notify.mobile_app_abc restart HASS\nTh·ª≠ t·∫°o l·ªói v√† tr·∫£i nghi·ªám xem c√≥ nh·∫≠n ƒë∆∞·ª£c tin nh·∫Øn ko nh√©?\nC√≥ 1 c√°ch ƒë·ªÉ debug tr∆∞·ªùng h·ª£p n√†y:\nOpen two browser tabs, each one connected to your Home Assistant instance.\nIn first tab, go to Developer Tools \u0026gt; Events, enter system_log_event in ‚ÄúEvent to subscribe to‚Äù then click ‚ÄúListen to events‚Äù:\nIn second tab, go to Developer Tools \u0026gt; Services, select ‚ÄúSystem Log: Write‚Äù, enter some text in ‚ÄúMessage‚Äù, enable ‚ÄúLevel‚Äù, select ‚Äúerror‚Äù, then click ‚ÄúCall Service‚Äù.\nIn first tab, you can see the structure of payload:\n8. Install Duplicati (for backup purpose) edit /opt/hass/docker-compose.yml add these line:\n... duplicati: image: lscr.io/linuxserver/duplicati:2.0.6 container_name: duplicati environment: - PUID=1000 - PGID=1000 - TZ=TZ=Asia/Ho_Chi_Minh - CLI_ARGS= #optional volumes: - /opt/duplicati/config:/config - /opt/duplicati/backups:/backups - /opt:/source ports: - 8200:8200 restart: unless-stopped Run:\ndocker-compose up -d Add frame duplicati to HASS UI: edit /opt/hass/config/configuration.yaml:\nadd these line:\npanel_iframe: ... duplicati: title: \u0026#34;Duplicati\u0026#34; url: \u0026#34;http://RPi_LOCAL_IP:8200\u0026#34; icon: mdi:backup-restore require_admin: true restart HASS\nTh√¥ng qua Portainer, check log c·ªßa container ko c√≥ l·ªói g√¨ l√† OK:\nB·∫°n c√≥ th·ªÉ access v√†o giao di·ªán Duplicati qua HASS UI nh∆∞ sau:\n8.1. Setup Duplicati backup to AWS S3 First you should creat an user (eg: duplicati_backup) in IAM service, note that select Programmatic access type:\nYour user should be attached this custom policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;iam:GetUser\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::AWS_S3_BUCKET\u0026#34;, \u0026#34;arn:aws:iam::AWS_ACCOUNT_ID:user/duplicati_backup\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectAcl\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::AWS_S3_BUCKET/*\u0026#34; } ] } Ch·ªçn Add Backup, ch·ªçn S3, nh·∫≠p h·∫øt c√°c t√πy ch·ªçn nh∆∞ n√†y:\nCh√Ω √Ω khi ch·ªçn test connection h√£y ch·ªçn No ·ªü ƒë√¢y:\nTest connection c·∫ßn works nh∆∞ n√†y:\nM√†n h√¨nh select source, ko n√™n ch·ªçn containerd, duplicati, portainer v√¨ nh·ªØng c√°i √Ω ko c·∫ßn backup ƒë√¢u, c√≤n c√°i hass/config/.storage th√¨ b·∫°n n√™n ssh v√†o RPi, d√πng command chown pi:pi hass/config/.storage/* -R ƒë·ªÉ c√≥ th·ªÉ backup ƒë∆∞·ª£c:\nM√†n h√¨nh select schedule:\nM√†n h√¨nh select size, ko quan tr·ªçng l·∫Øm:\nSau khi ƒë√£ save xong b·ªô config, c√≥ th·ªÉ ·∫•n Run now ƒë·ªÉ test:\nN·∫øu b·ªã l·ªói n√†y c√≥ nghƒ©a l√† b·∫°n c√≥ 1 folder n√†o ƒë√≥ ko th·ªÉ backup, c√≥ l·∫Ω l√† containerd, duplicati, ho·∫∑c v√¨ l·ªói permission n√™n duplicati ko th·ªÉ backup ƒë∆∞·ª£c ch√∫ng, n√™n test l·∫°i v·ªõi 1 file nh·ªè xem sao:\nBackup log ko c√≥ l·ªói g√¨ l√† ok, ho·∫∑c n·∫øu c√≥ WARNING th√¨ b·∫°n xem nh·ªØng file ƒë√≥ n·∫øu ko c·∫ßn thi·∫øt th√¨ c≈©ng ok: 9. Install DuckDNS (for expose outside access purpose) Setup tr√™n Router, t√¨m ph·∫ßn NAT -\u0026gt; Port Forwarding ƒë·ªÉ t·∫°o 1 rule forward t·ª´ port 8123 ƒë·∫øn port 8123 c·ªßa RPi IP: ƒêƒÉng k√Ω account DuckDNS.org v√† b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c token nh∆∞ n√†y: Add subdomain theo √Ω b·∫°n v√†o (gi·∫£ s·ª≠ l√† YOUR_SUBDOMAIN.duckdns.org), update IP l√† 8.8.8.8:\nEdit file /opt/hass/docker-compose.yaml:\nversion: \u0026#34;3.0\u0026#34; services: ... duckdns: image: lscr.io/linuxserver/duckdns:68a3222a-ls97 container_name: duckdns environment: - PUID=1000 #optional - PGID=1000 #optional - TZ=Asia/Ho_Chi_Minh - SUBDOMAINS=YOUR_SUBDOMAIN # eg: `facebook` not `facebook.duckdns.org` - TOKEN=DUCKDNS_TOKEN # replace DUCKDNS_TOKEN by your token from duckdns.org - LOG_FILE=true #optional volumes: - /opt/duckdns/config:/config restart: unless-stopped Run:\ndocker-compose up -d # check log docker logs duckdns N·∫øu log nh∆∞ n√†y l√† OK:\n[cont-init.d] 10-adduser: exited 0. [cont-init.d] 40-config: executing... Retrieving subdomain and token from the environment variables log will be output to file Your IP was updated at Sat May 7 22:37:48 +07 2022 [cont-init.d] 40-config: exited 0. [cont-init.d] 90-custom-folders: executing... [cont-init.d] 90-custom-folders: exited 0. [cont-init.d] 99-custom-files: executing... [custom-init] no custom files found exiting... [cont-init.d] 99-custom-files: exited 0. [cont-init.d] done. [services.d] starting services [services.d] done. Quay l·∫°i duckdns.org s·∫Ω th·∫•y IP c·ªßa m√¨nh ƒë√£ ƒëc c·∫≠p nh·∫≠t l√™n, ko c√≤n l√† 8.8.8.8 n·ªØa:\nƒê·∫øn l√∫c n√†y v√†o ƒë·ªãa ch·ªâ subdomain m√† b·∫°n ƒë√£ ƒëƒÉng k√Ω l√† c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c HASS t·ª´ internet r·ªìi:\nV√¨ 1 s·ªë h·∫°n ch·∫ø li√™n quan ƒë·∫øn HASS Container, Router DASAN c·ªßa VNPT m√† m√¨nh ko config dc HTTPS b·∫±ng Letsencrypt. Hy v·ªçng th·ªùi gian t·ªõi c√≥ th·ªÉ l√†m ƒë∆∞·ª£c. C√°c h·∫°n ch·∫ø v√≠ d·ª• nh∆∞:\nv√† h·∫°n ch·∫ø khi HASS ch·∫°y tr√™n Container ko c√≥ Add-on Duckdns nh∆∞ b·∫£n HASS OS (b·∫£n ƒë√≥ support Letsencrypt s·∫µn lu√¥n)\n10. Install Wireguard 10.1. Setup Gi·∫£ s·ª≠ b·∫°n ƒë√£ ƒëƒÉng k√Ω 1 subdomain tr√™n duckdns.org t√™n l√† YOUR_SUB_DOMAIN.duckdns.org.\nB·∫°n c≈©ng ƒë√£ update IP c·ªßa YOUR_SUB_DOMAIN.duckdns.org tr·ªè ƒë·∫øn public IP c·ªßa RPi.\nEdit file /opt/hass/docker-compose.yaml:\nversion: \u0026#34;3.0\u0026#34; services: ... wireguard: image: lscr.io/linuxserver/wireguard:1.0.20210914 container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 - PGID=1000 - TZ=Asia/Ho_Chi_Minh - SERVERURL=YOUR_SUB_DOMAIN.duckdns.org #optional - SERVERPORT=51820 #optional - PEERS=myphonelg,myphonevsm #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=10.13.13.0 #optional - ALLOWEDIPS=0.0.0.0/0 #optional - LOG_CONFS=true #optional volumes: - /opt/wireguard/config:/config - /opt/wireguard/modules:/lib/modules ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped Ch√∫ √Ω s·ª≠a YOUR_SUB_DOMAIN. N·∫øu ko c√≥ domain c√≥ th·ªÉ d√πng public IP.\nPh·∫ßn PEERS=: ƒëang ƒë·∫∑t t√™n client cho d·ªÖ nh·ªõ, b·∫°n c√≥ th·ªÉ ƒë·∫∑t t·ª± do v√≠ d·ª• PEERS=mylaptop\nRun:\ndocker-compose up -d # check log docker logs wireguard [s6-init] making user provided files available at /var/run/s6/etc...exited 0. [s6-init] ensuring user provided files have correct perms...exited 0. [fix-attrs.d] applying ownership \u0026amp; permissions fixes... [fix-attrs.d] done. [cont-init.d] executing container initialization scripts... [cont-init.d] 01-envfile: executing... [cont-init.d] 01-envfile: exited 0. [cont-init.d] 01-migrations: executing... [migrations] started [migrations] no migrations found [cont-init.d] 01-migrations: exited 0. [cont-init.d] 02-tamper-check: executing... [cont-init.d] 02-tamper-check: exited 0. [cont-init.d] 10-adduser: executing... ------------------------------------- _ () | | ___ _ __ | | / __| | | / \\ | | \\__ \\ | | | () | |_| |___/ |_| \\__/ Brought to you by linuxserver.io ------------------------------------- To support the app dev(s) visit: WireGuard: https://www.wireguard.com/donations/ To support LSIO projects visit: https://www.linuxserver.io/donate/ ------------------------------------- GID/UID ------------------------------------- User uid: 1000 User gid: 1000 ------------------------------------- [cont-init.d] 10-adduser: exited 0. [cont-init.d] 30-module: executing... Uname info: Linux 880be7489d30 5.15.32-v8+ #1538 SMP PREEMPT Thu Mar 31 19:40:39 BST 2022 aarch64 aarch64 aarch64 GNU/Linux **** It seems the wireguard module is already active. Skipping kernel header install and module compilation. **** [cont-init.d] 30-module: exited 0. [cont-init.d] 40-confs: executing... **** Server mode is selected **** **** External server address is set to \u0026lt;REDACTED-SUBDOMAIN\u0026gt;.duckdns.org **** **** External server port is set to 51820. Make sure that port is properly forwarded to port 51820 inside this container **** **** Internal subnet is set to 10.13.13.0 **** **** AllowedIPs for peers 0.0.0.0/0 **** **** PEERDNS var is either not set or is set to \u0026quot;auto\u0026quot;, setting peer DNS to 10.13.13.1 to use wireguard docker host's DNS. **** **** No wg0.conf found (maybe an initial install), generating 1 server and 1 peer/client confs **** grep: /config/peer*/*.conf: No such file or directory PEER 1 QR code: [REDACTED] [cont-init.d] 40-confs: exited 0. [cont-init.d] 90-custom-folders: executing... [cont-init.d] 90-custom-folders: exited 0. [cont-init.d] 99-custom-scripts: executing... [custom-init] no custom files found exiting... [cont-init.d] 99-custom-scripts: exited 0. [cont-init.d] done. [services.d] starting services [services.d] done. [#] ip link add wg0 type wireguard [#] wg setconf wg0 /dev/fd/63 [#] ip -4 address add 10.13.13.1 dev wg0 [#] ip link set mtu 1420 up dev wg0 .:53 CoreDNS-1.9.1 linux/arm64, go1.17.8, 4b597f8 [#] ip -4 route add 10.13.13.2/32 dev wg0 [#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -A FORWARD -o wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE Tr√™n Router s·ª≠a Port Forwarding, add th√™m 1 rule From port 51820 to port 51820 local IP l√† IP c·ªßa RPi.\nRun c√¢u l·ªánh sau ƒë·ªÉ l·∫•y QR code c·ªßa peer1:\ndocker exec -it wireguard /app/show-peer 1 ho·∫∑c docker exec -it wireguard /app/show-peer myphonelg Tr√™n ƒëi·ªán tho·∫°i, t·∫£i app Wireguard v·ªÅ, scan QR code b√™n tr√™n, ƒë·∫∑t t√™n cho VPN ƒë√≥, enable n√≥ l√™n.\nTh·ª≠ d√πng s√≥ng 4G v√† truy c·∫≠p 192.168.1.x:8123 (local IP address c·ªßa HASS) xem sao. N·∫øu truy c·∫≠p ƒë∆∞·ª£c l√† OK.\nV·∫≠y l√† ch·ªâ c·∫ßn b·∫≠t VPN tr√™n ƒëi·ªán tho·∫°i l√™n, l√† b·∫°n ƒë√£ c√≥ th·ªÉ connect ƒë·∫øn HASS nh∆∞ ·ªü nh√† r·ªìi üòÅ\n10.2. Restrict peers permission Tuy nhi√™n sau khi b·∫≠t VPN l√™n, n·∫øu b·∫°n c√≥ th·ªÉ access t·∫•t c·∫£ c√°c IP local k·ªÉ c·∫£: 192.168.1.1 (ƒë√¢y l√† IP c·ªßa Router, r·∫•t quan tr·ªçng) th√¨ r·∫•t nguy hi·ªÉm. Ch·ªâ c·∫ßn 1 user n√†o k·∫øt n·ªôi VPN c·ªßa b·∫°n, h·ªç c√≥ th·ªÉ truy c·∫≠p router nh√† b·∫°n lu√¥n.\nVi·ªác thay ƒë·ªïi setting v·ªÅ permission tr√™n Docker Compose file l√† ko th·ªÉ, m√† ph·∫£i k·∫øt h·ª£p c√°c c√¢u l·ªánh iptables c·ªßa linux m·ªõi ƒë∆∞·ª£c.\nEdit file /opt/wireguard/config/wg0.conf:\n[Interface] Address = 10.13.13.1 ListenPort = 51820 PrivateKey = REDACTED PostUp = /config/mnsd-scripts/postup.sh PostDown = /config/mnsd-scripts/postdown.sh [Peer] # peer_myphonelg PublicKey = REDACTED PresharedKey = REDACTED AllowedIPs = 10.13.13.4/32 [Peer] # peer_myphonevsm PublicKey = REDACTED PresharedKey = REDACTED AllowedIPs = 10.13.13.5/32 -\u0026gt; Nh∆∞ b·∫°n th·∫•y m√¨nh ƒë√£ s·ª≠a ƒë·ªÉ file wg0.conf call ƒë·∫øn 2 file postup.sh v√† postdown.sh.\nT·∫°o file sau /opt/wireguard/config/mnsd-scripts/postup.sh:\nWIREGUARD_INTERFACE=wg0 WIREGUARD_LAN=10.13.13.0/24 MASQUERADE_INTERFACE=eth0 iptables -t nat -I POSTROUTING -o $MASQUERADE_INTERFACE -j MASQUERADE -s $WIREGUARD_LAN # Add a WIREGUARD_wg0 chain to the FORWARD chain CHAIN_NAME=\u0026#34;WIREGUARD_$WIREGUARD_INTERFACE\u0026#34; iptables -N $CHAIN_NAME iptables -A FORWARD -j $CHAIN_NAME # Accept related or established traffic iptables -A $CHAIN_NAME -o $WIREGUARD_INTERFACE -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT # 1.Accept traffic from any peers to anywhere #iptables -A $CHAIN_NAME -s $WIREGUARD_LAN -i $WIREGUARD_INTERFACE -j ACCEPT # 2.Accept traffic from any peers to HAAS IP only iptables -A $CHAIN_NAME -s $WIREGUARD_LAN -i $WIREGUARD_INTERFACE -d 192.168.1.8 -j ACCEPT # 3.Accept traffic from myphonelg 10.13.13.4 to anywhere (both LAN and Internet) #iptables -A $CHAIN_NAME -s 10.13.13.4 -i $WIREGUARD_INTERFACE -j ACCEPT # 4.Accept traffic from myphonelg 10.13.13.4 to HAAS IP only #iptables -A $CHAIN_NAME -s 10.13.13.4 -i $WIREGUARD_INTERFACE -d 192.168.1.8 -j ACCEPT # 5.Accept traffic from myphonevsm 10.13.13.5 to LAN only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.0/24 -j ACCEPT # 6.Accept traffic from myphonevsm 10.13.13.5 to HAAS IP only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.8 -j ACCEPT # 7.Accept traffic from myphonevsm 10.13.13.5 to HAAS IP and port 8123 only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.8 -p tcp --dport 8123 -j ACCEPT # 8.Limit user myphonevsm to access HTTP/S Web internet only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.1 -p udp --dport 53 -j ACCEPT # below drop traffic from user to any LAN #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -j DROP # accept outgoing connections to HTTP(S) ports to any IP address #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 0.0.0.0/0 -p tcp -m multiport --dports 80,443 -j ACCEPT # Drop everything else coming through the Wireguard interface iptables -A $CHAIN_NAME -i $WIREGUARD_INTERFACE -j DROP # Return to FORWARD chain iptables -A $CHAIN_NAME -j RETURN -\u0026gt; Nh∆∞ b·∫°n th·∫•y m√¨nh t·∫°o s·∫µn r·∫•t nhi·ªÅu rule (t·ª´ 1-\u0026gt;8). B·∫°n d√πng c√°i n√†o th√¨ uncomment c√°i ƒë√≥ (c√≥ th·ªÉ k·∫øt h·ª£p nhi·ªÅu c√°i). Nh∆∞ hi·ªán t·∫°i m√¨nh ch·ªçn c√°i 2, t·∫•t c·∫£ traffic c·ªßa peers s·∫Ω ch·ªâ ƒë∆∞·ª£c accept khi connect ƒë·∫øn IP c·ªßa HASS m√† th√¥i\nT·∫°o file sau /opt/wireguard/config/mnsd-scripts/postdown.sh:\nWIREGUARD_INTERFACE=wg0 WIREGUARD_LAN=10.13.13.0/24 MASQUERADE_INTERFACE=eth0 CHAIN_NAME=\u0026#34;WIREGUARD_$WIREGUARD_INTERFACE\u0026#34; iptables -t nat -D POSTROUTING -o $MASQUERADE_INTERFACE -j MASQUERADE -s $WIREGUARD_LAN # Remove and delete the WIREGUARD_wg0 chain iptables -D FORWARD -j $CHAIN_NAME iptables -F $CHAIN_NAME iptables -X $CHAIN_NAME Change quy·ªÅn cho 2 file trong folder scripts (ch√∫ √Ω b∆∞·ªõc n√†y quan tr·ªçng, n·∫øu ko s·∫Ω b·ªã l·ªói):\nsudo chmod 777 /opt/wireguard/config/mnsd-scripts/* -\u0026gt; restart wireguard container. Check log ko c√≥ l·ªói l√† OK.\nGi·ªù h√£y th·ª≠ test k·∫øt n·ªëi xem sao nh√©. C√°c peer s·∫Ω ko th·ªÉ access v√†o b·∫•t c·ª© ƒë√¢u - ngo·∫°i tr·ª´ 192.168.1.8\n10.3. Allow traffic from client to 192.168.1.0/24 only, except internet N·∫øu b·∫°n mu·ªën Client lu√¥n b·∫≠t VPN nh∆∞ng ch·ªâ c√≥ k·∫øt n·ªëi ƒë·∫øn 192.168.1.8 l√† ƒëi qua VPN, c√≤n c√°c traffic ƒë·∫øn Google, myip.com th√¨ ko d√πng VPN (d√π n√≥ ƒëang b·∫≠t). B·∫°n c·∫ßn s·ª≠a tr√™n Client app:\nAllowedIPs = 192.168.1.0/24 l√† ƒë∆∞·ª£c. Nh∆∞ v·∫≠y b·∫°n v·∫´n c√≥ th·ªÉ b·∫≠t VPN ƒë·ªÉ ƒë√≥ su·ªët ng√†y. Ch·ªâ c√°c traffic ƒë·∫øn 192.168.1.0/24 m·ªõi qua VPN.\n10.4. Allow traffic from client to internet only, except local intranet N·∫øu b·∫°n mu·ªën Client lu√¥n b·∫≠t VPN nh∆∞ng ch·ªâ c√°c traffic ra Internet Google, myip.com l√† ƒëi qua VPN, c√≤n traffic ƒë·∫øn 192.168.1.8 th√¨ ƒëi trong m·∫°ng n·ªôi b·ªô c·ªßa Client. B·∫°n c·∫ßn s·ª≠a tr√™n Client app:\nEdit v√† b·ªè tick ·ªü √¥ Block untunneled traffic (kill switch)\nNh∆∞ v·∫≠y Client c√≥ th·ªÉ b·∫≠t VPN ƒë·ªÉ ƒë√≥ su·ªët ng√†y ƒë·ªÉ ƒë·ªçc Medium ch·∫≥ng h·∫°n. Khi h·ªç c·∫ßn truy c·∫≠p v√†o 1 website intranet 192.168.1.8 c·ªßa h·ªç th√¨ h·ªç v·∫´n v√†o b√¨nh th∆∞·ªùng.\n10.5. Add more peer N·∫øu b·∫°n mu·ªën c√≥ th√™m 1 thi·∫øt b·ªã n·ªØa k·∫øt n·ªëi v√†o Wireguard, ch·ªâ c·∫ßn s·ª≠a l·∫°i file /opt/hass/docker-compose.yaml: t·ª´ PEERS=1 th√†nh PEERS=2, ho·∫∑c th√†nh PEERS=myphonelg,myphonevsm\n-\u0026gt; restart wireguard container\nN·∫øu restart xong m√† ko th·∫•y folder peer_xxx trong wireguard/config/ th√¨ c·∫ßn stop container wireguard r·ªìi run l·∫°i:\ndocker-compose up -d R·ªìi run command n√†y ƒë·ªÉ show QR code c·ªßa peer2:\ndocker exec -it wireguard /app/show-peer 2 ho·∫∑c docker exec -it wireguard /app/show-peer myphonevsm Trong tr∆∞·ªùng h·ª£p thi·∫øt b·ªã c·ªßa b·∫°n ko th·ªÉ qu√©t QR code, v√≠ d·ª• nh∆∞ m√°y t√≠nh, laptop:\ncopy content trong file sau: /opt/devops/wireguard/config/peer_mylaptop/peer_mylaptop.conf\n[Interface] Address = 10.x.x.9 PrivateKey = qxxxxxxxxxxxxxxxxxxxxxxxxxxxxQ= ListenPort = 51821 DNS = 10.x.x.9 [Peer] PublicKey = 7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx= PresharedKey = yzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz= Endpoint = x.x.x.x:51821 AllowedIPs = 0.0.0.0/0 Ch·ªçn WireGuard -\u0026gt; Add empty tunnel -\u0026gt; paste content tr√™n v√†o m√†n h√¨nh, r·ªìi Save -\u0026gt; OK\n11. Install Watchman via HACS ƒê√¢y l√† 1 tool gi√∫p generate ra report, gi√∫p b·∫°n bi·∫øt ƒë∆∞·ª£c c√≥ entity n√†o b·ªã missing trong automation, script hay ko?\nt·ª´ report ƒë√≥ b·∫°n ƒëi t√¨m c√°ch fix\nv√†o HACS -\u0026gt; Explore repositories -\u0026gt; Search Watchman\nv√†o Setting -\u0026gt; Integrations -\u0026gt; Add Watchman\n-\u0026gt; restart HASS\nƒë·ª£i 1 l√∫c cho c√°c entities start h·∫øt\nv√†o Developer tools -\u0026gt; Services -\u0026gt; search Watchman -\u0026gt; ·∫•n Call service\nv√†o hass/config s·∫Ω th·∫•y file watchman_report.txt xu·∫•t hi·ªán:\nƒë·ªçc report r·ªìi ƒëi·ªÅu tra c√°c entities ko c·∫ßn thi·∫øt th√¥i üòÉ\n12. Install Grafana + Prometheus 12.1. Install M·ª•c ƒë√≠ch l√† ƒë·ªÉ theo d√µi RAM, CPU, etc \u0026hellip; c·ªßa RPi\nChu·∫©n b·ªã persistent volume:\nsudo mkdir -p /opt/prometheus-grafana/{grafana,prometheus} cd /opt/ wget https://raw.githubusercontent.com/grafana/grafana/main/conf/defaults.ini -O prometheus-grafana/grafana/grafana.ini tee prometheus-grafana/promethueus/prometheus.yml\u0026lt;\u0026lt;EOF global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [] scheme: http timeout: 10s api_version: v1 scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;,\u0026#39;node-exporter:9100\u0026#39;] EOF tee prometheus-grafana/grafana/datasource.yml\u0026lt;\u0026lt;EOF apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://\u0026lt;RPi_local_IP\u0026gt;:9090 isDefault: true access: proxy editable: true EOF File docker-compose.yaml:\nversion: \u0026#39;3.0\u0026#39; services: prometheus: image: prom/prometheus:v2.36.0 container_name: prometheus volumes: # - /opt/prometheus-grafana/prometheus/prometheus_data:/prometheus - /opt/prometheus-grafana/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml ports: - 9090:9090 user: \u0026#34;1000\u0026#34; grafana: image: grafana/grafana:8.5.5 container_name: grafana volumes: - /opt/prometheus-grafana/grafana/grafana_data:/var/lib/grafana - /opt/prometheus-grafana/grafana/grafana.ini:/etc/grafana/grafana.ini - /opt/prometheus-grafana/grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yaml ports: - 3000:3000 links: - prometheus user: \u0026#34;1000\u0026#34; node-exporter: image: prom/node-exporter:v1.3.1 container_name: monitoring_node_exporter restart: unless-stopped expose: - 9100 docker-compose up -d Login Grafana: http://RPi_IP:3000 (admin/admin)\nƒê·ªïi password\n12.2. Import Dashboard https://easycode.page/monitoring-on-raspberry-pi-with-node-exporter-prometheus-and-grafana/\nAdd Grafana v√†o l√†m 1 iframe cho HASS:\nFile /opt/prometheus-grafana/grafana/grafana.ini, s·ª≠a attribute sau:\nallow_embedding = true File /opt/hass/config/configuration.yaml:\npanel_iframe: grafana: title: \u0026#34;Grafana\u0026#34; url: \u0026#34;http://RPI_IP:3000\u0026#34; icon: mdi:chart-areaspline require_admin: true -\u0026gt; restart Grafana, restart HASS\nTi·∫øp theo c·∫ßn l√†m 1 s·ªë rule ƒë·ªÉ Grafana t·ª± ƒë·ªông g·ª≠i tin nh·∫Øn khi nhi·ªát ƒë·ªô CPU qu√° cao, ho·∫∑c khi full RAM, full disk\u0026hellip;\n12.3. Setup template message Ch·ªß y·∫øu d·ª±a tr√™n b√†i n√†y: https://grafana.com/docs/grafana/latest/alerting/unified-alerting/message-templating/\nGi·∫£ s·ª≠ b·∫°n mu·ªën setup khi CPU l√™n 8% th√¨ s·∫Ω message v√†o Telegram cho b·∫°n\nV√†o Dashboard CPU Basic ch·ªçn Edit:\ntab Alert:\nT·∫°o 1 rule t∆∞∆°ng t·ª± nh∆∞ n√†y:\nCh·ªó n√†y nghƒ©a l√† rule ƒë·∫•y s·∫Ω evaluate 1 ph√∫t 1 l·∫ßn trong v√≤ng 2 ph√∫t sau khi th·∫•y CPU \u0026gt; 8%\nB·∫°n c√≥ th·ªÉ truy·ªÅn c√°c label v√†o Alert t√πy √Ω, ·ªü ƒë√¢y m√¨nh th√™m label component=CPU_BUSY\n-\u0026gt; SAVE\nV√†o tab Alert -\u0026gt; Contact point, ch√∫ng ta s·∫Ω t·∫°o 1 contact point telegram_contactpoint v√† 2 template myalert v√† telegram_template\n2 message template n·ªôi dung nh∆∞ n√†y:\nmyalert:\n{{ define \u0026quot;myalert\u0026quot; }} [{{.Status}}] {{ .Labels.alertname }} Labels: {{ range .Labels.SortedPairs }} {{ .Name }}: {{ .Value }} {{ end }} {{ end }} telegram_template:\n{{ define \u0026quot;telegram_template\u0026quot; }} {{ if gt (len .Alerts.Firing) 0 }} {{ len .Alerts.Firing }} firing: {{ range .Alerts.Firing }} {{ template \u0026quot;myalert\u0026quot; .}} {{ end }} {{ end }} {{ if gt (len .Alerts.Resolved) 0 }} {{ len .Alerts.Resolved }} resolved: {{ range .Alerts.Resolved }} {{ template \u0026quot;myalert\u0026quot; .}} {{ end }} {{ end }} {{ end }} V√†o ph·∫ßn contact point t·∫°o Telegram Contactpoint:\nCh√∫ √Ω field Message ƒëi·ªÅn:\nAlert summary: {{ template \u0026quot;telegram_template\u0026quot; . }} Th·ª≠ Test Contact Point v·ªõi custom label m√¨nh ƒëi·ªÅn l√† hahaha:\nB·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c message tr√™n Telegram nh∆∞ n√†y:\nGi·ªù b·∫°n c√≥ th·ªÉ ƒë√£ m∆∞·ªùng t∆∞·ª£ng ƒë∆∞·ª£c flow c·ªßa qu√° tr√¨nh Alert nh∆∞ th·∫ø n√†o. H√£y th·ª≠ t·ª± s·ª≠a theo √Ω m√¨nh nh√© üòÅ\n13. Setup cAdvisor M√¨nh mu·ªën theo d√µi chi ti·∫øt c√°c container, xem c√°i n√†o d√πng nhi·ªÅu CPU nh·∫•t, c√°i n√†o ng·ªën nhi·ªÅu traffic nh·∫•t\u0026hellip;\nƒê√¢y l√† l√∫c s·ª≠ d·ª•ng cAdvisor\ns·ª≠a file docker-compose.yml:\ncadvisor: container_name: cadvisor # image: gcr.io/cadvisor/cadvisor:v0.47.0 # not ARM supported version image: gcr.io/cadvisor/cadvisor-arm64:v0.47.0 # https://github.com/google/cadvisor/issues/3190 ports: - 8081:8080 # M√¨nh expose ra 8081 ƒë·ªÉ tr√°nh duplicate volumes: - /:/rootfs:ro - /var/run:/var/run:ro - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro devices: - /dev/kmsg ·ªû ƒë√¢y ch√∫ √Ω r·∫±ng m√¨nh ƒëang s·ª≠ d·ª•ng version gcr.io/cadvisor/cadvisor-arm64 v√¨ ƒë√¢y l√† image d√πng cho RasberryPi (chip arm64). N·∫øu b·∫°n d√πng b·∫£n ko support chip arm th√¨ kh·∫£ nƒÉng s·∫Ω b·ªã l·ªói ·ªü ƒë√¢u ƒë√≥.\ns·ª≠a file /opt/prometheus-grafana/prometheus/prometheus.yml: Add th√™m cadvisor:8080 ·ªü d√≤ng cu·ªëi c√πng\nglobal: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [] scheme: http timeout: 10s api_version: v1 scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;,\u0026#39;node-exporter:9100\u0026#39;,\u0026#39;cadvisor:8080\u0026#39;] restart container prometheus:\ndocker container restart prometheus V√†o ƒë√¢y confirm xem Prometheus ƒë√£ connect ƒë∆∞·ª£c ƒë·∫øn cAdvisor ch∆∞a:\nhttp://RPI_IP:9090/targets?search=\nM√¨nh import template n√†y: https://grafana.com/grafana/dashboards/15120-raspberry-pi-docker-monitoring/\nC√≥ th·ªÉ s·∫Ω b·ªã l·ªói c√°i Root FS Used ko hi·ªÉn th·ªã dung l∆∞·ª£ng th·∫ª nh·ªõ ƒëang s·ª≠ d·ª•ng. L√∫c ƒë√≥ m√¨nh c·∫ßn s·ª≠a l·∫°i nh∆∞ sau:\nS·ª≠a c√¢u query nh∆∞ sau:\nN·ªôi dung c√¢u query m√¨nh paste ra ƒë√¢y:\n100 - ((node_filesystem_avail_bytes{device=\u0026quot;/dev/root\u0026quot;, fstype=\u0026quot;ext4\u0026quot;, instance=\u0026quot;node-exporter:9100\u0026quot;, job=\u0026quot;prometheus\u0026quot;, mountpoint=\u0026quot;/etc/hostname\u0026quot;} * 100) / node_filesystem_size_bytes{device=\u0026quot;/dev/root\u0026quot;, fstype=\u0026quot;ext4\u0026quot;, instance=\u0026quot;node-exporter:9100\u0026quot;, job=\u0026quot;prometheus\u0026quot;, mountpoint=\u0026quot;/etc/hostname\u0026quot;}) Sau ƒë√≥ th√¨ s·∫Ω l·∫•y ƒë∆∞·ª£c th√¥ng tin Root FS Used nh∆∞ tr√™n h√¨nh l√† kho·∫£ng 75%\nK√©o xu·ªëng d∆∞·ªõi s·∫Ω th·∫•y ƒë∆∞·ª£c max CPU c·ªßa t·ª´ng container, r·ªìi traffic c·ªßa t·ª´ng container, bla bla\u0026hellip; c·∫ßn theo d√µi c√°i n√†o th√¨ ƒë·∫∑t Alert cho c√°i √Ω th√¥i\n14. Setup Nmap Tracker ƒê·ªÉ x√°c ƒë·ªãnh 1 person c√≥ ƒëang ·ªü nh√† ko th√¨ c√≥ nhi·ªÅu c√°ch. Tuy nhi√™n ph·∫ßn n√†y m√¨nh s·∫Ω d√πng Nmap Tracker ƒë·ªÉ x√°c ƒë·ªãnh.\nC∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa n√≥ l√†, RPi s·∫Ω run command nmap ƒë·ªÉ xem c√≥ bao nhi√™u device ƒëang k·∫øt n·ªëi v√†o wifi c·ªßa nh√† m√¨nh. M·ªói device s·∫Ω l√† 1 entity.\nT·ª´ ƒë√≥ x√°c ƒë·ªãnh 1 person c√≥ ƒëang ·ªü nh√† hay ko b·∫±ng c√°ch xem device c·ªßa h·ªç c√≥ ƒëang connect WIFI hay ko.\nTuy nhi√™n n√™n c·∫©n th·∫≠n v√¨ 1 s·ªë thi·∫øt b·ªã ·ªü ch·∫ø ƒë·ªô idle s·∫Ω t·ª± ng·∫Øt k·∫øt n·ªëi WiFi. V√† c√≥ 1 s·ªë thi·∫øt b·ªã d√π ƒëang k·∫øt n·ªëi ƒë·∫øn WiFi nh∆∞ng l·∫°i ko ƒë∆∞·ª£c nmap ph√°t hi·ªán ra.\nCh√∫ √Ω h√£y c√†i s·∫µn nmap command tr√™n RPi nh√©.\nV√†o Setting -\u0026gt; Integrations -\u0026gt; Add Integration, search Nmap Tracker: ƒëi·ªÅn c√°c th√¥ng tin v·ªÅ subnet ip, ip c·ªßa n∆°i s·∫Ω run command nmap, th∆∞·ªùng th√¨ c·ª© ƒë·ªÉ default:\nsau khi Submit th√¨ restart HASS.\nB·∫°n s·∫Ω th·∫•y m√†n h√¨nh Integrations c√≥ Nmap Tracker xu·∫•t hi·ªán v·ªõi 1 s·ªë entities ƒë√≥ ch√≠nh l√† devices ƒëang k·∫øt n·ªëi:\nT·∫°i m√†n h√¨nh sau, h√£y s·ª≠a c√°c entty ƒë·ªÉ ƒë·ªïi t√™n cho d·ªÖ nh√¨n, enable ch√∫ng l√™n, sao cho STATUS nh∆∞ khung ƒë·ªè l√† ok:\nSau ƒë√≥ quay l·∫°i Setting -\u0026gt; People, ch·ªçn Person m√† b·∫°n mu·ªën track device, select device to track:\nQuay l·∫°i Setting -\u0026gt; Developer Tool, search person b·∫°n s·∫Ω th·∫•y ng∆∞·ªùi ƒë√≥ ƒëang c√≥ state=home, v√¨ device c·ªßa h·ªç ƒëang k·∫øt n·ªëi wifi:\nGi·ªù b·∫°n c√≥ th·ªÉ ƒë∆∞a entity person ƒë√≥ ra ngo√†i dashboard ƒë·ªÉ theo d√µi xem h·ªç c√≥ nh√† hay ko üòã\nCh√∫ √Ω r·∫±ng n·∫øu b·∫°n c√≥ 1 chi·∫øc Iphone ƒëang k·∫øt n·ªëi v√†o m·∫°ng, ƒë·ªÉ x√°c ƒë·ªãnh ƒë√¢u l√† ƒë·ªãa ch·ªâ MAC c·ªßa Iphone trong list k·∫øt qu·∫£ c·ªßa Nmap Tracker, b·∫°n c·∫ßn v√†o Setting c·ªßa Iphone ph·∫ßn n√†y:\nSetting -\u0026gt; Wifi -\u0026gt; click v√†o bi·ªÉu t∆∞·ª£ng ch·ªØ i (info) b√™n c·∫°nh WiFi b·∫°n ƒëang k·∫øt n·ªëi.\n‚úî Ph·∫ßn ƒë·ªãa ch·ªâ Wifi address ch√≠nh l√† MAC address c·ªßa iphone trong m·∫°ng c·ªßa b·∫°n\n‚ùå Nhi·ªÅu ng∆∞·ªùi check ƒë·ªãa ch·ªâ MAC c·ªßa Iphone m√† v√†o General -\u0026gt; About -\u0026gt; WiFi address l√† sai nh√© ‚ùå, s·∫Ω kh√¥ng kh·ªõp v·ªõi k·∫øt qu·∫£ tr·∫£ v·ªÅ c·ªßa Nmap Tracker ƒë√¢u\n15. About setting up Automations 15.1. Note 1 Khi setup Automations, b·∫°n n√™n bi·∫øt c√°ch nh√∫ng scripts v√†o Automation th√¨ s·∫Ω l√†m ƒë∆∞·ª£c nhi·ªÅu th·ª© h∆°n.\nGi·∫£ s·ª≠ m√¨nh ƒëang c√≥ 1 Automation nh∆∞ n√†y:\n/opt/hass/config/automations.yaml:\n# Turn On/Off Small room Wall Light/ Bulb base on motion - id: \u0026#39;9hoangmnsd7234756123978961235\u0026#39; alias: Turn On/Off Small room Wall Light/Bulb base on motion description: Turn On/Off Small room Wall Light/Bulb base on motion trigger: - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;on\u0026#39; - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;off\u0026#39; for: 00:05:00 # delay for x min after changing state # Only active between 9AM to 11PM condition: alias: \u0026#34;Time 09~23\u0026#34; condition: time # At least one of the following is required. after: \u0026#34;09:00:00\u0026#34; before: \u0026#34;23:00:00\u0026#34; weekday: - mon - tue - wed - thu - fri - sat - sun action: - data: {} entity_id: light.small_room_light_bulb service_template: \u0026gt;{% if is_state(\u0026#39;binary_sensor.motion_sensor_04_occupancy\u0026#39;, \u0026#39;on\u0026#39;) %} light.turn_on {% else %} light.turn_off {% endif %} - data: {} entity_id: switch.small_room_wall_light service_template: \u0026gt;{% if is_state(\u0026#39;binary_sensor.motion_sensor_04_occupancy\u0026#39;, \u0026#39;on\u0026#39;) %} switch.turn_on {% else %} switch.turn_off {% endif %} N√≥ turn on 2 c√°i ƒë√®n khi motion detected.\nV√† turn off 2 c√°i ƒë√®n khi motion cleared trong 5 ph√∫t.\nNh∆∞ng gi·ªù m√¨nh mu·ªën: N·∫øu sau 5 ph√∫t motion cleared th√¨ ch·ªâ turn_off 1 ƒë√®n, r·ªìi delay 10s, r·ªìi m·ªõi turn_off ƒë√®n 2.\nN·∫øu s·ª≠a th√™m delay 10s v√†o gi·ªØa 2 action th√¨ s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn l√∫c turn_on c≈©ng s·∫Ω ph·∫£i delay. M√† b·∫£n th√¢n h√†m delay th√¨ ko th·ªÉ c√≥ condition.\nTh·∫ø n√™n m√¨nh c·∫ßn nh√∫ng script v√†o automations nh∆∞ sau:\n/opt/hass/config/automations.yaml:\n# Turn On/Off Small room Wall Light/ Bulb base on motion - id: \u0026#39;9817hoangmnsd8324378098546\u0026#39; alias: Turn On/Off Small room Wall Light/Bulb base on motion description: Turn On/Off Small room Wall Light/Bulb base on motion mode: parallel trigger: - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;on\u0026#39; - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;off\u0026#39; for: 00:05:00 # delay for x min after changing state # Only active between 9AM to 11PM condition: alias: \u0026#34;Time 09~23\u0026#34; condition: time # At least one of the following is required. after: \u0026#34;09:00:00\u0026#34; before: \u0026#34;23:00:00\u0026#34; weekday: - mon - tue - wed - thu - fri - sat - sun action: - data: {} service_template: \u0026#34;script.light_bulb_{{trigger.to_state.state}}\u0026#34; /opt/hass/config/scripts.yaml:\nlight_bulb_on: sequence: - service: script.turn_off data: entity_id: script.light_bulb_off - service: light.turn_on entity_id: light.small_room_light_bulb - service: switch.turn_on entity_id: switch.small_room_wall_light light_bulb_off: sequence: - service: light.turn_off entity_id: light.small_room_light_bulb - delay: \u0026#39;00:00:10\u0026#39; - service: switch.turn_off entity_id: switch.small_room_wall_light Ch√∫ √Ω l√† c·∫ßn ƒë·ªÉ mode: parallel ƒë·ªÉ khi trong kho·∫£ng ch·ªù 10s sau khi motion cleared, n·∫øu motion detected th√¨ n√≥ s·∫Ω cancel c√°i script.light_bulb_off ƒëang ch·∫°y\nƒê√¢y l√† 1 ph∆∞∆°ng ph√°p r·∫•t hay v√¨ vi·ªác ƒë∆∞a ra script s·∫Ω ƒëem l·∫°i cho ch√∫ng ta nhi·ªÅu ƒë·∫•t di·ªÖn h∆°n.\n15.2. Note 2 C√°ch li√™n k·∫øt gi·ªØa input_boolean state v·ªõi script:\nPh·∫ßn n√†y s·∫Ω h∆∞·ªõng d·∫´n t·∫°o 1 button leg_mode tr√™n giao di·ªán, t·ª´ button ƒë√≥ call ƒë·∫øn script ƒë·ªÉ flash ƒë√®n b√†n theo nhu c·∫ßu\nfile /opt/hass/config/configuration.yaml:\ninput_boolean: leg_mode: name: Leg mode icon: mdi:shoe-print initial: false giao di·ªán UI:\n- type: horizontal-stack cards: - type: \u0026#39;custom:button-card\u0026#39; template: card_input_boolean entity: input_boolean.leg_mode Automation /opt/hass/config/automations.yaml (c√≥ th·ªÉ b·∫°n nghƒ© ko c·∫ßn file n√†y, nh∆∞ng th·ª±c ch·∫•t ph·∫£i c√≥ ƒë·ªÉ call ƒë·∫øn script. N·∫øu ko th√¨ script s·∫Ω ko bi·∫øt ƒë∆∞·ª£c trigger khi n√†o):\n# Turn On/Off Small room Bulb base on Leg Mode input_boolean - id: \u0026#39;6hoangmnsd3052389573534892y34\u0026#39; alias: Turn On/Off Leg Mode input_boolean description: Turn On/Off Leg Mode input_boolean mode: parallel trigger: - entity_id: input_boolean.leg_mode platform: state to: \u0026#39;on\u0026#39; - entity_id: input_boolean.leg_mode platform: state to: \u0026#39;off\u0026#39; condition: [] action: - data: {} service_template: \u0026#34;script.light_bulb_leg_mode_{{trigger.to_state.state}}\u0026#34; file /opt/hass/config/scripts.yaml:\n# Smallroom Desk Lightbulb Leg mode light_bulb_leg_mode_child: mode: restart sequence: - service: light.turn_on target: entity_id: light.small_room_light_bulb - alias: \u0026#34;Cycle light\u0026#34; repeat: while: - condition: state entity_id: input_boolean.leg_mode state: \u0026#39;on\u0026#39; # Don\u0026#39;t do it too many times: 3600s / 15s = 240 - condition: template value_template: \u0026#34;{{ repeat.index \u0026lt;= 240 }}\u0026#34; sequence: - service: light.turn_on data: brightness_pct: 100 rgb_color: [124,252,0] # lawngreen entity_id: light.small_room_light_bulb - delay: \u0026#39;00:00:05\u0026#39; - service: light.turn_on data: brightness_pct: 100 rgb_color: [255,0,0] # red entity_id: light.small_room_light_bulb - delay: \u0026#39;00:00:10\u0026#39; light_bulb_leg_mode_on: sequence: - condition: state entity_id: input_boolean.leg_mode state: \u0026#39;on\u0026#39; - service: script.turn_off data: entity_id: script.light_bulb_leg_mode_off - alias: \u0026#34;Call the child script\u0026#34; service: script.light_bulb_leg_mode_child light_bulb_leg_mode_off: sequence: - condition: state entity_id: input_boolean.leg_mode state: \u0026#39;off\u0026#39; - service: script.turn_off data: entity_id: script.light_bulb_leg_mode_on - service: light.turn_on # revert to normal brightness data: brightness_pct: 100 color_temp: 350 entity_id: light.small_room_light_bulb - service: light.turn_off entity_id: light.small_room_light_bulb Nh∆∞ v·∫≠y m·ªói khi m√¨nh turn on/off button Leg mode tr√™n giao di·ªán, n√≥ s·∫Ω trigger Automation t·ª´ ƒë√≥ call ƒë·∫øn script b√™n d∆∞·ªõi.\nVi·ªác n·∫Øm ƒëc c√°ch li√™n k·∫øt gi·ªØa c√°c th√†nh ph·∫ßn automation, UI card input_boolean, script gi√∫p b·∫°n hi·ªán th·ª±c h√≥a ƒë∆∞·ª£c nhi·ªÅu √Ω t∆∞·ªüng h∆°n.\n15.3. Note 3 Dynamic even the title text. Note n√†y n√≥i v·ªÅ c√°ch m√¨nh l√†m cho title c·ªßa glance card tr·ªü n√™n dynamic, n√≥ s·∫Ω thay ƒë·ªïi t√πy theo tr·∫°ng th√°i m√† m√¨nh ·∫•n n√∫t:\nV√≠ d·ª• khi m√¨nh ·∫•n v√†o c√°i 29 ƒë·ªô C, th√¨ ƒëo·∫°n text \u0026ldquo;LG Air Conditioner\u0026rdquo; s·∫Ω chuy·ªÉn th√†nh \u0026ldquo;LG Air Conditioner(29)\u0026rdquo;.\nƒêi·ªÅu n√†y l√† kh√¥ng th·ªÉ ƒë·ªëi v·ªõi c√°c default card. V√¨ th·∫ø b·∫°n c·∫ßn import 1 lo·∫°i custom card m·ªõi\nv√†o HACS -\u0026gt; t√¨m card n√†y:\ndownload n√≥ v·ªÅ:\ntrong file /opt/hass/config/configuration.yaml c·∫ßn th√™m config sau:\n# You need to specifiy resource url if setting lovelace.mode = yaml lovelace: mode: yaml resources: - url: /hacsfiles/config-template-card/config-template-card.js type: module v·∫´n trong file ƒë√≥, define 1 input_select:\ninput_select: living_room_ac_title_base_on_temp: name: living_room_ac_title_base_on_temp options: - LG Air Conditioner - LG Air Conditioner(29) - LG Air Conditioner(28) initial: LG Air Conditioner -\u0026gt; restart HASS\ntr√™n UI dashboard b·∫°n c·∫ßn s·ª≠a glance card, ƒë·ªÉ s·ª≠ d·ª•ng dc c√°i config-template-card m√† m·ªõi download v·ªÅ:\n- type: \u0026#39;custom:config-template-card\u0026#39; variables: AC_LIVINGROOM_TITLE: states[\u0026#39;input_select.living_room_ac_title_base_on_temp\u0026#39;].state entities: - script.ac_livingroom_power - script.ac_livingroom_jetmode - script.ac_livingroom_temp29 - script.ac_livingroom_temp28 card: type: glance title: \u0026#34;${AC_LIVINGROOM_TITLE}\u0026#34; style: |ha-card { background-color: {{ \u0026#39;#AAFFAA\u0026#39; if is_state(\u0026#39;binary_sensor.living_room_ac_sensor_contact\u0026#39;, \u0026#39;on\u0026#39;) else \u0026#39;#FFFFFF\u0026#39; }}; } entities: - entity: script.ac_livingroom_power name: Power show_state: false show_icon: true icon: mdi:power tap_action: action: call-service service: script.ac_livingroom_power data: entity_id: script.ac_livingroom_power - entity: script.ac_livingroom_jetmode name: JetMode show_state: false show_icon: true icon: mdi:snowflake tap_action: action: call-service service: script.ac_livingroom_jetmode data: entity_id: script.ac_livingroom_jetmode - entity: script.ac_livingroom_temp29 name: 29¬∞C show_state: false show_icon: true icon: mdi:temperature-celsius tap_action: action: call-service service: script.ac_livingroom_temp29 data: entity_id: script.ac_livingroom_temp29 trong /opt/hass/config/scripts.yaml b·∫°n c·∫ßn add th√™m action ƒë·ªÉ n√≥ thay ƒë·ªïi option cho input_select:\nac_livingroom_temp29: sequence: - service: remote.send_command data: command: temp29 device: ac_livingroom target: entity_id: remote.rm4_mini_livingroom_remote - service: input_select.select_option  data: entity_id: input_select.living_room_ac_title_base_on_temp option: \u0026#39;LG Air Conditioner(29)\u0026#39; -\u0026gt; restart HASS\nV·ªÅ c∆° b·∫£n, b·∫°n ƒë√£ d√πng input_select ƒë·ªÉ thay ƒë·ªïi gi√° tr·ªã c·ªßa title. ƒêi·ªÅu n√†y nh·ªù c√¥ng r·∫•t l·ªõn c·ªßa custom:config-template-card.\n16. Setup Navidrome ƒê√¢y l√† 1 web app ƒë·ªÉ chia s·∫ª nh·∫°c t·ª´ m√°y m√¨nh ra internet. C√≥ th·ªÉ share cho m·ªçi ng∆∞·ªùi nghe c√πng:\nhttps://github.com/navidrome/navidrome/\nhttps://www.navidrome.org/docs/installation/docker/\nhttps://www.navidrome.org/docs/usage/configuration-options/#configuration-file\nChu·∫©n b·ªã folder:\n /opt/navidrome/data ƒë·ªÉ ch·ª©a config /opt/navidrome/music ƒë·ªÉ ch·ª©a c√°c file mp3  add v√†o docker-compose.yml\nnavidrome: container_name: navidrome image: deluan/navidrome:0.47.5 user: 1000:1000 # should be owner of volumes ports: - \u0026#34;4533:4533\u0026#34; restart: unless-stopped environment: # Optional: put your config options customization here. Examples: ND_SCANSCHEDULE: 1h ND_LOGLEVEL: info  ND_SESSIONTIMEOUT: 24h ND_BASEURL: \u0026#34;/\u0026#34; ND_REVERSEPROXYWHITELIST: \u0026#34;192.168.1.0/24,172.18.0.0/16\u0026#34; # m·∫∑c d√π ƒë∆∞a v√†o nh∆∞ng ch∆∞a hi·ªÉu c√°ch d√πng volumes: - \u0026#34;/opt/navidrome/data:/data\u0026#34; - \u0026#34;/opt/navidrome/music:/music:ro\u0026#34; V·∫≠y l√† xong, b·∫°n c√≥ th·ªÉ access v√†o port 4533 ƒë·ªÉ xem trang web ho·∫°t ƒë·ªông.\nM√¨nh ƒëang d√πng swag ƒë·ªÉ expose trang navidrome n√†y ra ngo√†i internet d∆∞·ªõi domain https://navidrome.MYDOMAIN.duckdns.org. Nh∆∞ v·∫≠y l√† ·ªü C√¥ng ty c≈©ng c√≥ th·ªÉ nghe ƒë∆∞·ª£c list nh·∫°c ·ªü nh√† r·ªìi üòÅüòÅ\nHi·ªán t·∫°i c√≥ 1 v·∫•n ƒë·ªÅ:\n M√¨nh v·∫´n ch∆∞a nh√∫ng dc trang n√†y v√†o l√†m 1 iframe c·ªßa HASS, n√≥ c·ª© b√°o l·ªói t·ª´ ch·ªëi k·∫øt n·ªëi\n-\u0026gt; th·∫ø n√™n t·ª´ m·∫°ng LAN mu·ªën v√†o l√† ph·∫£i v√†o theo local IP: 192.168.x.x:4533\nv·∫•n ƒë·ªÅ n√†y ƒë∆∞·ª£c th·∫£o lu·∫≠n trong topic n√†y: https://github.com/navidrome/navidrome/issues/248#issuecomment-911143096  T√≥m t·∫Øt ƒë·ªÉ x·ª≠ l√Ω v·∫•n ƒë·ªÅ iframe th√¨ c·∫ßn ph·∫£i s·ª≠ d·ª•ng nginx ƒë·ªÉ ghi ƒë√® l√™n c√°i setting X-Frame-Options: deny c·ªßa Navidrome\nƒê·ªÉ l√†m ƒë∆∞·ª£c th√¨ m√¨nh ƒë√£ ph·∫£i setup Nginx theo b√†i Setup Home Assistant on Raspberry Pi (Part 3) - Https\nSau khi c√≥ nginx r·ªìi th√¨:\nTrong folder /opt/swag/config/nginx/proxy-confs/ rename file navidrome.subdomain.conf.sample -\u0026gt; navidrome.subdomain.conf Trong file n√†y s·ª≠a n·ªôi dung nh∆∞ sau:\n~~~ # redirect all traffic to https # server { # listen 80; # listen [::]:80; # server_name navidrome.*; # return 301 https://$host$request_uri; # } server { listen 80; listen 443 ssl; listen [::]:443 ssl; server_name navidrome.*; include /config/nginx/ssl.conf; client_max_body_size 0; location / { include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app navidrome; set $upstream_port 4533; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; proxy_hide_header X-Frame-Options; } } -\u0026gt; nh∆∞ v·∫≠y m√¨nh ƒë√£ set cho nginx ko redirect http sang https, v√† listen tr√™n c·∫£ port 80.\nserver name c·ªßa Navidrome s·∫Ω l√† navidrome.MYDOMAIN.duckdns.org.\nM√¨nh c≈©ng ƒë√£ th√™m proxy_hide_header X-Frame-Options; ƒë·ªÉ ghi ƒë√® l√™n setting X-Frame-Options c·ªßa Navidrome\nB√¢y gi·ªù t·ª´ Internet c√≥ th·ªÉ access v√†o c·∫£ 2 link n√†y ƒë·ªÅu ok:\n https://navidrome.MYDOMAIN.duckdns.org http://navidrome.MYDOMAIN.duckdns.org  Gi·ªù setting trong HASS file /opt/hass/config/configuration.yaml:\n# panel iframe panel_iframe: ... navidrome: title: \u0026#34;Navidrome\u0026#34; url: \u0026#34;http://navidrome.MYDOMAIN.duckdns.org\u0026#34; icon: mdi:music require_admin: false -\u0026gt; restart swag container, HASS\nTest:\n T·ª´ M·∫°ng 4G -\u0026gt; HASS http http://MYDOMAIN.duckdns.org -\u0026gt; t·ª± redirect https, v√†o dc HASS -\u0026gt; ko m·ªü ƒëc iframe Navidrome. T·ª´ M·∫°ng 4G -\u0026gt; HASS http http://MYDOMAIN.duckdns.org:8123 -\u0026gt; v√†o dc HASS, m·ªü ƒëc iframe Navidrome. T·ª´ m·∫°ng LAN -\u0026gt; HASS http http://MYDOMAIN.duckdns.org:8123-\u0026gt; v√†o dc HASS, m·ªü ƒëc iframe Navidrome. T·ª´ m·∫°ng 4G -\u0026gt; http://navidrome.MYDOMAIN.duckdns.org -\u0026gt; v√†o ƒëc Navidrome. T·ª´ m·∫°ng 4G -\u0026gt; https://navidrome.MYDOMAIN.duckdns.org -\u0026gt; v√†o ƒëc Navidrome.  17. Troubleshoot docker overlay https://forums.docker.com/t/some-way-to-clean-up-identify-contents-of-var-lib-docker-overlay/30604/59\nN·∫øu b·∫°n th·∫•y d√πng l∆∞·ª£ng trong /var/lib/docker/overlay2/ nhi·ªÅu 1 c√°ch b·∫•t th∆∞·ªùng, c√≥ 1 v√†i step n√™n ki·ªÉm tra:\nXem t·ªïng c·ªông file log c√≥ nhi·ªÅu ko?\ndu -shc /var/lib/docker/containers/*/*.log Xem s·ªë l∆∞·ª£ng file trong folder diff:\ndu -shc /var/lib/docker/overlay2/*/diff Sort c√°c file theo th·ª© t·ª± dung l∆∞·ª£ng gi·∫£m d·∫ßn:\ndu -s /var/lib/docker/overlay2/*/diff |sort -n -r Xem folder to nh·∫•t ƒë∆∞·ª£c d√πng cho container n√†o:\ndocker inspect $(docker container ls -q) |grep \u0026#39;FOLDER_ID_HERE\u0026#39; -B 100 -A 100 Sau ƒë√≥ b·∫°n c√≥ th·ªÉ l·ª±a ch·ªçn x√≥a container ƒë√≥ r·ªìi run l·∫°i xem sao.\nHo·∫∑c gi·ªõi h·∫°n l·∫°i dung l∆∞·ª£ng logging c·ªßa container ƒë√≥ r·ªìi run l·∫°i.\n1 s·ªë c√°ch kh√°c:\n# X√≥a WARNING! This will remove: # - all stopped containers # - all networks not used by at least one container # - all dangling images # - all dangling build cache docker system prune X√°c ƒë·ªãnh v·ªÅ folder n√†o ƒëang size to nh·∫•t, v√≠ d·ª• sau ƒë√¢y folder to nh·∫•t l√† /volumes 30G:\n/var/lib/docker# sudo du -h --max-depth=1 325M ./buildkit 4.0K ./runtimes 8.0K ./tmp 4.0K ./swarm 100K ./network 92K ./containers 4.3M ./image 1.9G ./overlay2 16K ./plugins 30G ./volumes 4.0K ./trust 33G . X√≥a c√°c volume th·ª´a ƒëi:\ndocker volume prune WARNING! This will remove all local volumes not used by at least one container. Are you sure you want to continue? [y/N] y ... Total reclaimed space: 22.61GB 18. Setup HASS.Agent on Windows machine T√¨nh hu·ªëng l√†: M√¨nh c√≥ 1 Laptop ƒëang s·ª≠ d·ª•ng Smart Plug. M√¨nh mu·ªën n·∫øu pin dc s·∫°c ƒë·∫ßy tr√™n 99% s·∫Ω t·ª± ng·∫Øt Plug, v√† n·∫øu pin d∆∞·ªõi 10% th√¨ t·ª± b·∫≠t plug.\nƒê·ªÉ l√†m ƒëi·ªÅu n√†y th√¨ Laptop c·ªßa b·∫°n c·∫ßn g·ª≠i th√¥ng tin cho HASS.-\u0026gt; B·∫°n c·∫ßn c√†i HASS.Agent tr√™n Windows.\nSau khi download v√† install xong (m√¨nh c√†i version 2022.13.0)\nSetup URL c·ªßa HASS, v√† Long-live token:\nSetup connect ƒë·∫øn MQTT server, ch·ªó n√†y c·∫ßn nh·∫≠p user/password c·ªßa MQTT server nh√©:\nSau ƒë√≥ ch·ªù HASS.Agent restart ƒë·ªÉ setup, r·ªìi v√†o dashboard -\u0026gt; ch·ªçn Local Sensors ƒë·ªÉ add sensor v√†o:\nAdd new:\nCh·ªçn Battery -\u0026gt; ·∫•n Store Sensor:\n·∫§n ti·∫øp Store and Active sensor\nXOng r·ªìi, quay l·∫°i HASS -\u0026gt; Setting -\u0026gt; Devices, s·∫Ω th·∫•y Laptop hi·ªán ra v√† c√°c battery sensor t∆∞∆°ng ·ª©ng\nGi·ªù b·∫°n c√≥ th·ªÉ vi·∫øt Automation cho c√°c sensor ƒë√≥, hi·ªán t·∫°i m√¨nh ƒëang d√πng c√°i n√†y:\n- id: \u0026#39;2hoangmnsd3sadsdsdsfd4132123f\u0026#39; alias: Turn On/Off Laptop PLug base on Battery description: Turn On/Off Laptop PLug base on Battery trigger: # platform: time_pattern # minutes: \u0026#39;/1\u0026#39; # every 1 minute - below: \u0026#39;10\u0026#39; entity_id: sensor.asusk501lb_battery_charge_remaining_percentage platform: numeric_state - above: \u0026#39;99\u0026#39; entity_id: sensor.asusk501lb_battery_charge_remaining_percentage platform: numeric_state condition: [] action: - data: {} entity_id: switch.smart_plug_01 service_template: \u0026gt;{% set p = states(\u0026#39;sensor.asusk501lb_battery_charge_remaining_percentage\u0026#39;) | int(0) %} {% if p \u0026gt;= 99 %} switch.turn_off {% elif p \u0026lt;= 10 %} switch.turn_on {% endif %} 19. Setup NFC Tag Xem 1 s·ªë clip youtube th·∫•y dc 1 s·ªë t√¨nh nƒÉng h·ªØu d·ª•ng c·ªßa NFC Tag trong nh√† m√¨nh:\n  ƒê√¥i khi ng·ªìi ƒÉn c∆°m, m√¨nh ko mu·ªën c·ª© n√≥i \u0026ldquo;Alexa, stop\u0026rdquo;, \u0026ldquo;Alexa, play news\u0026rdquo;, \u0026ldquo;Alexa music\u0026rdquo; m√£i.\n  Ho·∫∑c trong ph√≤ng ng·ªß, m√¨nh ko mu·ªën ph·∫£i g√†o l√™n \u0026ldquo;Alexa, turn off Bedroom Light\u0026rdquo; (Alexa ƒë·∫∑t ngo√†i ph√≤ng kh√°ch, m√¨nh ch∆∞a c√≥ ti·ªÅn mua th√™m Alexa trong ph√≤ng ng·ªß üòÜ).\n  Th·∫ø n√™n, M√¨nh mu·ªën c√≥ 1 NFC Tag d√°n tr√™n m·∫∑t b√†n/ƒë·∫ßu gi∆∞·ªùng, m√¨nh qu∆° ƒëi·ªán tho·∫°i l√† n√≥ s·∫Ω t·ª± ra l·ªánh lu√¥n thay m√¨nh lu√¥n (Androids v·∫´n ph·∫£i unlock ƒëi·ªán tho·∫°i tr∆∞·ªõc, Iphone ko c·∫ßn unlock nh∆∞ng ph·∫£i s√°ng m√†n h√¨nh), ko c·∫ßn g√†o l√™n, ko c·∫ßn m·ªü app tr√™n ƒëi·ªán tho·∫°i r·ªìi t√¨m ƒë·∫øn button ·∫•n n√∫t n·ªØa.\nV√† b·ªüi v√¨ trong nh√† m√¨nh c√≥ nhi·ªÅu lo·∫°i ƒëi·ªán tho·∫°i n√™n c≈©ng c·∫ßn setup ri√™ng cho t·ª´ng lo·∫°i.\nIphone th√¨ c·∫ßn k·∫øt h·ª£p v·ªõi app Shortcut (ch·ªâ h·ªó tr·ª£ NFC Tag t·ª´ iphone X tr·ªü ƒëi)\nAndroids th√¨ ko ph·∫£i ƒëi·ªán tho·∫°i n√†o c≈©ng c√≥ chip NFC, v√≠ d·ª• Vsnart Live 4 ko c√≥ NFC th√¨ ko scan ƒë∆∞·ª£c.\nVi·ªác setup th√¨ ch·ªß y·∫øu nh·ªù video n√†y l√† ƒë·ªß:\nhttps://www.youtube.com/watch?v=VsjTzm2JFJ0\u0026amp;t=374s\u0026amp;ab_channel=JuanMTech\nT√≥m t·∫Øt n·∫øu ko mu·ªën xem l·∫°i Video tr√™n:\nƒê·∫ßu ti√™n l√† l√™n Shoppee mua 1 l·ªë NFC tag v·ªÅ, gi√° kh√° r·∫ª th√¥i, lo·∫°i n√†o c≈©ng ƒë∆∞·ª£c v√¨ HA App t∆∞∆°ng th√≠ch v·ªõi m·ªçi lo·∫°i NFC Tag (m√¨nh c≈©ng ch∆∞a research ƒë·ªÉ bi·∫øt n√™n d√πng lo·∫°i NFC tag n√†o)\nSau khi ƒë√£ c√≥ Tag. C√†i Home Assistant App tr√™n ƒëi·ªán tho·∫°i.\nV√†o ph·∫ßn Setting -\u0026gt; Tags -\u0026gt; Add Tag -\u0026gt; ƒê·∫∑t t√™n cho Tag\nQu∆° ƒëi·ªán tho·∫°i tr√™n C√°i Tag m·ªõi, n√≥ s·∫Ω ghi ƒë√® auto-gen ID m·ªõi cho Tag\nXong ph·∫ßn ƒëi·ªán tho·∫°i, gi·ªù v√†o l√†m tr√™n m√°y t√≠nh, t·∫°o Automation cho Tag ƒë√≥ (Ch√∫ √Ω copy c√°i ID c·ªßa Tag tr∆∞·ªõc)\nV√≠ d·ª• Automation cho vi·ªác scan Tag c·ªßa m√¨nh:\n# Control Bedroom Light base on NFC Tag - id: \u0026#39;handle-tag-scan-bedroom-light1\u0026#39; alias: \u0026#34;Handle Tag Scan: Bedroom Light 1\u0026#34; mode: single # Hide warnings when triggered while in delay. max_exceeded: silent trigger: - platform: tag tag_id: 84df4xxxxxxxxxxxxxxxxxxxxx15f01785 condition: [] action: - service: switch.toggle data: {} target: entity_id: switch.bed_room_light Reload l·∫°i Automation. Test b·∫±ng c√°ch Qu∆° ƒëi·ªán tho·∫°i tr√™n Tag. B∆∞·ªõc n√†y ƒêi·ªán tho·∫°i Androids c·∫ßn unlock tr∆∞·ªõc.\nƒêi·ªán tho·∫°i Iphone s·∫Ω c·∫ßn unlock v√† ch·∫°m v√†o c√°i notification pop-up ra. -\u0026gt; h∆°i phi·ªÅn.\nGi·ªù fix c√°i h∆°i phi·ªÅn c·ªßa Iphone ƒë√≥:\n  Tr√™n Iphone t√¨m app Shortcut ch·ªçn Automation -\u0026gt; NFC -\u0026gt; Scan -\u0026gt; Name the Tag -\u0026gt; Click Next on top right\n  Add action -\u0026gt; search Home Assistant App -\u0026gt; select HA fire event -\u0026gt; change shortcut_event to anything (ko c√≥ space), v√≠ d·ª• change th√†nh BedroomLightTag1 -\u0026gt; Next\n  Uncheck Ask before running -\u0026gt; Uncheck Notify when run\n  S·ª≠a Automation tr√™n Home Assistant, add th√™m trigger platform: event nh∆∞ n√†y, ch√∫ √Ω event_type ch·ªçn ƒë√∫ng c√°i ƒë√£ ƒë·∫∑t t√™n BedroomLightTag1 trong App Shortcut:\n  # Control Bedroom Light base on NFC Tag - id: \u0026#39;handle-tag-scan-bedroom-light1\u0026#39; alias: \u0026#34;Handle Tag Scan: Bedroom Light 1\u0026#34; mode: single # Hide warnings when triggered while in delay. max_exceeded: silent trigger: - platform: tag tag_id: 84xxxxxxxxxxxxxxxxxxxxx85 - platform: event event_type: BedroomLightTag1 condition: [] action: - service: switch.toggle data: {} target: entity_id: switch.bed_room_light   Reload Automation tr√™n Home Assistant\n  Test l·∫°i, ko c·∫ßn unlock Iphone, nh∆∞ng c·∫ßn ch·∫°m m√†n h√¨nh s√°ng, Qu∆° Iphone tr√™n Tag -\u0026gt; Done üòò\n  ƒêi·ªán tho·∫°i n√†o c≈©ng ph·∫£i l√†m (Tuy nhi√™n ko ph·∫£i Iphone n√†o app Shortcut c≈©ng c√≥ option NFC ƒë√¢u)\n  Troubeshooting:\nƒê√¥i khi c√≥ l·ªói l√† scan NFC Tag xong Automation ko ch·∫°y, c√≥ th·ªÉ l√† do m√¨nh v·ª´a s·ª≠a Automation li√™n quan ƒë·∫øn vi·ªác Scan NFC:\nhttps://community.home-assistant.io/t/trouble-using-nfc-tags-ha-says-never-scanned/438377\n-\u0026gt; V√†o App x√≥a cache, data ƒëi, k·∫øt n·ªëi l·∫°i v·ªõi server HA v√† login l·∫°i l√† OK\nReview Qu√° tr√¨nh s·ª≠ d·ª•ng 1,2 tu·∫ßn m√¨nh g·∫∑p 1 s·ªë l·ªói:\n C·ª≠a ƒë√≥ng nh∆∞ng h·ªá th·ªëng v·∫´n b√°o l√† ƒëang m·ªü. Ko c√≤n chuy·ªÉn ƒë·ªông nh∆∞ng sensor v·∫´n ·ªü tr·∫°ng th√°i Detected motion trong v√≤ng 15p.  M√¨nh nghƒ© l√† do qos c·ªßa MQTT server ƒëang set l√† qos=0. M√¨nh ƒë√£ config l·∫°i MQTT server qos=2. Ch·ªù th√™m kho·∫£ng 1.2 tu·∫ßn n·ªØa xem c√≥ ok ko‚Ä¶\nSau kho·∫£ng 1 tu·∫ßn d√πng, th√¨ c√≥ 1 l·ªói nh∆∞ n√†y:\n Motion sensor v·∫´n ·ªü tr·∫°ng th√°i Detected motion trong v√≤ng 2 ti·∫øng, m·∫∑c d√π ko c√≥ chuy·ªÉn ƒë·ªông n√†o, sau 2 ti·∫øng th√¨ n√≥ m·ªõi chuy·ªÉn v·ªÅ tr·∫°ng th√°i Cleared.  N√≥i chung l√† qos=2 th√¨ ch·∫Øc ch·∫Øn l√† message s·∫Ω ƒë·∫øn dc MQTT server nh∆∞ng bao l√¢u m·ªõi ƒë·∫øn th√¨ ko bi·∫øt (·ªü ƒë√¢y l√† 2 ti·∫øng m·ªõi ƒë·∫øn) üòÇ\n 1 l·ªói n·ªØa l√† Motion ko ph√°t hi·ªán ƒë∆∞·ª£c chuy·ªÉn ƒë·ªông. R·ªìi 1 l√∫c sau th√¨ l·∫°i ph√°t hi·ªán chuy·ªÉn ƒë·ªông b√¨nh th∆∞·ªùng. L·ªói n√†y ƒë√¥i khi x·∫£y ra.  CREDIT https://www.jfrog.com/connect/post/install-docker-compose-on-raspberry-pi/\nhttps://www.zigbee2mqtt.io/guide/getting-started/#installation\nhttps://www.youtube.com/watch?v=cZV2OOXLtEI\u0026amp;ab_channel=HomeAutomationGuy\nhttps://www.home-assistant.io/installation/raspberrypi#install-home-assistant-container\nhttps://www.jfrog.com/connect/post/install-docker-compose-on-raspberry-pi/\nhttps://peyanski.com/how-to-install-home-assistant-community-store-hacs/#Initial_install_of_Home_Assistant_Community_Store_HACS_on_Home_Assistant_Container\nhttps://konnected.vn/home-assistant/home-assistant-thong-bao-den-dien-thoai-may-tinh-2020-05-11\nhttps://www.home-assistant.io/integrations/telegram/\nhttps://hub.docker.com/r/linuxserver/duplicati\nhttps://community.home-assistant.io/t/automation-based-on-log/337925/6?u=super318\nhttps://hub.docker.com/r/linuxserver/duckdns\nhttps://konnected.vn/home-assistant/home-assistant-truy-cap-bang-domain-va-cai-dat-chung-chi-https-2020-05-18\nhttps://www.addictedtotech.net/home-vpn-using-wireguard-docker-on-a-raspberry-pi-4/\nhttps://hub.docker.com/r/linuxserver/wireguard\nhttps://viblo.asia/p/docker-dung-vpn-server-wireguard-tren-docker-Qbq5QBOmKD8\nhttps://github.com/linuxserver/docker-wireguard\nhttps://www.youtube.com/watch?v=aGIg6N9HzSg\nhttps://gist.github.com/qdm12/4e0e4f9d1a34db9cf63ebb0997827d0d\nhttps://www.youtube.com/watch?v=XKD5vBZLKgE\nhttps://techviewleo.com/run-prometheus-and-grafana-using-docker-compose/\nhttps://easycode.page/monitoring-on-raspberry-pi-with-node-exporter-prometheus-and-grafana/\nhttps://grafana.com/grafana/dashboards/1860\nhttps://geektechstuff.com/2020/06/25/grafana-prometheus-and-node-exporter-on-raspberry-pi-via-ansible-raspberry-pi-and-linux/\nhttps://www.home-assistant.io/integrations/person/\nhttps://www.home-assistant.io/integrations/nmap_tracker/\nhttps://www.home-assistant.io/integrations/input_boolean/\nhttps://www.home-assistant.io/docs/scripts/#counted-repeat\nhttps://community.home-assistant.io/t/is-it-possible-to-use-dynamic-name-in-e-g-sensor-card/298677\nhttps://community.home-assistant.io/t/100-templatable-lovelace-configurations/105241\nhttps://github.com/iantrich/config-template-card\nhttps://www.home-assistant.io/integrations/input_select/\nhttps://community.home-assistant.io/t/hass-agent-windows-client-to-receive-notifications-use-commands-sensors-quick-actions-and-more/369094\nhttps://community.home-assistant.io/t/charge-my-android-tablet-automatically-when-battery-goes-less-than-10-stop-charging-when-100/61296/6\nhttps://www.youtube.com/watch?v=B4SnJPVbSXc\u0026amp;t=432s\u0026amp;ab_channel=EverythingSmartHome\nhttps://github.com/LAB02-Research/HASS.Agent\nhttps://superuser.com/a/1612473\nhttps://github.com/oijkn/Docker-Raspberry-PI-Monitoring\nhttps://thesmarthomejourney.com/2022/07/25/monitoring-smarthome-prometheus/\n","href":"/bk/encrypt-setup-home-assistant-on-raspberry-pi-and-addons/","title":"Setup Home Assistant on Raspberry Pi (Part 1) - Addons"},{"content":"B√†i n√†y t·ªïng h·ª£p c√°c step g·∫ßn nh∆∞ t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi ƒë·ªÉ setup H·ªá th·ªëng Home Assistant Container tr√™n Raspberry Pi\n1. Install Docker \u0026amp; Docker Compose sudo apt-get update sudo apt-get upgrade -y sudo apt-get install avahi-daemon sudo apt install git -y sudo apt update sudo apt full-upgrade curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker ${USER} reboot to apply: sudo shutdown -r now docker version docker info docker run hello-world # install pip3 sudo apt-get install libffi-dev libssl-dev sudo apt install python3-dev -y sudo apt-get install -y python3 python3-pip sudo shutdown -r now # C√°ch 1 install docker-compose default sudo shutdown -r now sudo su pip3 install docker-compose docker-compose version sudo systemctl enable docker # C√°ch 2 kh√°c ƒë·ªÉ install docker-compose specific version # C·∫ßn bi·∫øt OS c·ªßa b·∫°n l√† g√¨ ƒë√£: uname -s # gi·∫£ output l√† linux uname -m # gi·∫£ s·ª≠ output l√† aarch64 # V√†o trang n√†y l·∫•y link: https://github.com/docker/compose/releases/ # ch·ªçn c√°i t∆∞∆°ng ·ª©ng linux, arch64 l√† OK # gi·∫£ s·ª≠ l·∫•y ƒë∆∞·ª£c: https://github.com/docker/compose/releases/download/2.5.0/docker-compose-linux-aarch64 #install docker-compose nh∆∞ sau: sudo curl -L https://github.com/docker/compose/releases/download/v2.5.0/docker-compose-linux-aarch64 -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version sudo systemctl enable docker N·∫øu v√¨ 1 l√Ω do n√†o ƒë√≥, docker b·ªã l·ªói, b·∫°n mu·ªën uninstall docker kh·ªèi m√°y th√¨:\n# uninstall docker dpkg -l | grep -i docker sudo apt-get purge -y docker-engine docker docker.io docker-ce docker-ce-cli sudo apt-get autoremove -y --purge docker-engine docker docker.io docker-ce sudo rm -rf /var/lib/docker /etc/docker sudo rm -rf /var/run/docker.sock # uninstall docker-compose pip3 uninstall docker-compose # uninstall python sudo apt-get remove libffi-dev libssl-dev sudo apt remove python3-dev -Y sudo apt-get remove -y python3 python3-pip 2. Install Portainer \u0026amp; HASS create file /opt/hass/docker-compose.yml\nversion: \u0026#39;3.0\u0026#39; services: portainer: container_name: portainer image: portainer/portainer restart: always stdin_open: true tty: true ports: - \u0026#34;9000:9000/tcp\u0026#34; environment: - TZ=Asia/Ho_Chi_Minh volumes: - /var/run/docker.sock:/var/run/docker.sock - /opt/portainer:/data homeassistant: container_name: homeassistant image: \u0026#34;ghcr.io/home-assistant/home-assistant:2022.5.2\u0026#34; volumes: - /opt/hass/config:/config - /etc/localtime:/etc/localtime:ro restart: unless-stopped privileged: true network_mode: host cd /opt/hass docker-compose up -d Check access HASS UI: http://RPi_LOCAL_IP:8123/, create account/password\nCheck access Portainer: http://RPi_LOCAL_IP:9000/, create account/password\nEdit file /opt/hass/config/configuration.yaml\nAdd this on top:\npanel_iframe: portainer: title: \u0026#34;Portainer\u0026#34; url: \u0026#34;http://RPi_LOCAL_IP:9000/#/containers\u0026#34; icon: mdi:docker require_admin: true Go to HASS UI: Configuration -\u0026gt; Settings -\u0026gt; Restart to take effect.\nHASS UI will have new frame Portainer on the sidebar:\n3. Install Mosquito edit /opt/hass/docker-compose.yml\nadd these lines:\n.... mosquitto: image: eclipse-mosquitto:2.0.14 container_name: mosquitto volumes: - /opt/mosquitto:/mosquitto ports: - 1883:1883 - 9001:9001 create file /opt/mosquitto/config/mosquitto.conf/:\npersistence true persistence_location /mosquitto/data/ log_dest file /mosquitto/log/mosquitto.log # Authentication #allow_anonymous false #listener 1883 #password_file /mosquitto/config/password.txt Run:\ndocker-compose up -d check log of mosquitto container if there is any error? You should fix it.\nthis is normal: No logs available\nT·ª´ Portainer UI -\u0026gt; v√†o mosquitto container via Console log, t·∫°o user mqttuser v√† nh·∫≠p password b·∫±ng command:\nmosquitto_passwd -c /mosquitto/config/password.txt mqttuser S·ª≠a l·∫°i file /opt/mosquitto/config/mosquitto.conf, uncomment ph·∫ßn Authentication:\npersistence true persistence_location /mosquitto/data/ log_dest file /mosquitto/log/mosquitto.log # Authentication allow_anonymous false listener 1883 password_file /mosquitto/config/password.txt V√†o Portainer, restart mosquitto container, check log no error l√† OK:\nV√†o HASS, Configuration -\u0026gt; Devices \u0026amp; Services -\u0026gt; Integrations -\u0026gt; Add integration -\u0026gt; Search MQTT ƒëi·ªÅn IP c·ªßa RPi (n∆°i run mosquitto container, ex: 192.168.1.4), port: 1883, user: mqttuser password: ******\nM·ª•c ti√™u cu·ªëi c√πng l√† add ƒë∆∞·ª£c Integration MQTT nh∆∞ th·∫ø n√†y:\n4. Install Zigbee2MQTT Mua 1 chi·∫øc USB 3.0 Zigbee Sonoff Dongle, c·∫Øm v√†o RPi:\nD√πng command sau ƒë·ªÉ xem serial id c·ªßa USB v·ª´a c·∫Øm:\n$ ls -l /dev/serial/by-id/ total 0 lrwxrwxrwx 1 root root 13 May 1 21:21 usb-ITead_Sonoff_Zigbee_3.0_USB_Dongle_Plus_e8fc8a915ad9eb119edf148e6fe9f4d9-if00-port0 -\u0026gt; ../../ttyUSB0 -\u0026gt; l·∫•y ƒë∆∞·ª£c ttyUSB0\nedit /opt/hass/docker-compose.yml add these line:\n... zigbee2mqtt: container_name: zigbee2mqtt image: koenkk/zigbee2mqtt:1.25.0 restart: unless-stopped volumes: - /opt/zigbee2mqtt/data:/app/data - /run/udev:/run/udev:ro ports: # Frontend port - 8080:8080 environment: - TZ=Asia/Ho_Chi_Minh devices: # Make sure this matched your adapter location - /dev/ttyUSB0:/dev/ttyUSB0 Run:\ndocker-compose up -d Edit file /opt/zigbee2mqtt/data/configuration.yaml\n# Home Assistant integration (MQTT discovery) homeassistant: true # allow new devices to join permit_join: true # MQTT settings mqtt: # MQTT base topic for zigbee2mqtt MQTT messages base_topic: zigbee2mqtt # MQTT server URL server: \u0026#39;mqtt://mosquitto:1883\u0026#39; # MQTT server authentication, uncomment if required: user: mqttuser password: PASSWORD_OF_mqttuser # Serial settings serial: # Location USB sniffer port: /dev/ttyUSB0 # Enable the Zigbee2MQTT frontend frontend: port: 8080 experimental: new_api: true check log container zigbee2mqtt ko c√≥ l·ªói l√† OK:\nNow you can access UI on port 8080 (ex: 192.168.1.8:8080)\nAdd frame zigbee2mqtt to HASS UI:\nedit /opt/hass/config/configuration.yaml:\nadd these line:\npanel_iframe: ... zigbee2mqtt: title: \u0026#34;Zigbee2MQTT\u0026#34; url: \u0026#34;http://RPi_LOCAL_IP:8080\u0026#34; icon: mdi:zigbee require_admin: true Go to HASS UI: Configuration -\u0026gt; Settings -\u0026gt; Restart to take effect.\nHASS UI will have new frame on the left sidebar.\n5. Connect Zigbee device to Zigbee2MQTT service 5.1. Setup Gi·∫£ s·ª≠ b·∫°n ƒë√£ mua 1 sensor nh∆∞ th·∫ø n√†y:\ntr√™n giao di·ªán Zigbee2MQTT ·∫•n v√†o n√∫t Permit join (All):\nN√≥ s·∫Ω hi·ªán ƒë·∫øm ng∆∞·ª£c kho·∫£ng 250s,\nTrong th·ªùi gian ƒë√≥ ·∫•n l√¨ n√∫t n√†y c·ªßa sensor kho·∫£ng 5s, ko n√™n ·∫•n l√¢u qu√° - sensor s·∫Ω left network:\nCho ƒë·∫øn khi hi·ªán ra device nh∆∞ n√†y l√† OK, sensor ƒë√£ join network zigbee th√†nh c√¥ng:\n1 s·ªë tab th√¥ng tin v·ªÅ sensor tr√™n Zigbee2MQTT:\n5.2. Test N·∫øu b·∫°n ƒë·∫∑t 2 c·ª•c g·∫ßn nhau, tr·∫°ng th√°i s·∫Ω thay ƒë·ªïi closed nh∆∞ n√†y:\nN·∫øu 2 c·ª•c ƒë·∫∑t xa nhau, tr·∫°ng th√°i s·∫Ω l√† open:\nThay ƒë·ªïi ph·∫£n √°nh r·∫•t nhanh, g·∫ßn nh∆∞ ngay l·∫≠p t·ª©c üòç r·∫•t h·ªØu d·ª•ng ph·∫£i ko?\nGi·ªù quay l·∫°i m√†n h√¨nh HASS -\u0026gt; Setting -\u0026gt; Integration, b·∫°n s·∫Ω th·∫•y 1 device ƒë√£ ƒë∆∞·ª£c xu·∫•t hi·ªán d∆∞·ªõi MQTT integration:\n·∫§n v√†o b·∫°n s·∫Ω th·∫•y Sensor c·ªßa b·∫°n c√πng v·ªõi c√°c th√¥ng s·ªë:\n5.3. Automation \u0026amp; Alert Ti·∫øp theo s·∫Ω l√† 1 s·ªë script Automation or Alert ƒë·ªÉ ·ª©ng d·ª•ng c√°c sensor n√†y nh√©!\nAutomation (hass/config/automations.yaml):\n# Warning when Front Door open more than 1 min =\u0026gt; change to use Alert in configuration.json - id: \u0026#39;15324556454\u0026#39; alias: Warning when Front Door open more than 1 min description: \u0026#39;Warning when Front Door open more than 1 min\u0026#39; trigger: - entity_id: binary_sensor.front_door_sensor_contact for: 00:01:00 # wait for x min after changing state from off to on platform: state to: \u0026#39;on\u0026#39; condition: [] action: - data: message: \u0026#39;\u0026#39; title: \u0026#39;ü§î HASS: Front door left open more than a min.\u0026#39; service: telegram_bot.send_message nh∆∞ng c√°i c√°ch d√πng Automation tr√™n ko hay l·∫Øm, HASS cung c·∫•p s·∫µn t√≠nh nƒÉng Alert d√πng hay h∆°n:\nAlert (hass/config/configuration.yaml) repeat sau 3/6/10 min, v·ªÅ sau c·ª© 10min alert 1 l·∫ßn:\n... notify: - platform: telegram name: hoangmnsd chat_id: 816661853 ... alert: # Front door open front_door_open: name: Front door is open entity_id: binary_sensor.front_door_sensor_contact state: \u0026#34;on\u0026#34; # Optional, \u0026#39;on\u0026#39; is the default value repeat: - 3 - 6 - 10 can_acknowledge: true # Optional, default is true skip_first: true # Optional, false is the default message: \u0026#34;üò≤ HASS: Front door left open more than a min.\u0026#34; done_message: \u0026#34;üòâ HASS: Front door closed.\u0026#34; notifiers: - hoangmnsd # need to specify notifier 5.4. Using Blueprint to notify when Sensor battery low C√°ch 1: D√πng sbyx blueprint\nHASS Community cung c·∫•p 1 blueprint ƒë·ªÉ x·ª≠ l√Ω vi·ªác c·∫£nh b√°o battery cho sensor n√†y: https://community.home-assistant.io/t/low-battery-level-detection-notification-for-all-battery-sensors/258664\nN·∫øu ko ·∫•n v√†o n√∫t Import Blueprint m√† b·ªã l·ªói, th√¨ ph·∫£i import th·ªß c√¥ng nh∆∞ sau\nV√†o RPi server, t·∫°o file /opt/hass/config/blueprints/automation/sbyx/low-battery-level-detection-notification-for-all-battery-sensors.yaml\n(content file l·∫•y t·ª´ link n√†y https://gist.github.com/sbyx/1f6f434f0903b872b84c4302637d0890)\n-\u0026gt; restart HASS\nƒê·ªÉ s·ª≠ d·ª•ng blueprint, edit file hass/config/automations.yaml:\nNh∆∞ n√†y l√† n√≥ s·∫Ω notify everyday at 10:00 PM, khi c√≥ 1 sensor n√†o battery d∆∞·ªõi 10%, g·ª≠i message v·ªÅ notify hoangmnsd\n# Warning when Front Door sensor low battery - id: \u0026#39;1653065570149\u0026#39; alias: Low battery level detection \u0026amp; notification for all battery sensors description: \u0026#39;Low battery level detection \u0026amp; notification for all battery sensors\u0026#39; use_blueprint: path: sbyx/low-battery-level-detection-notification-for-all-battery-sensors.yaml input: threshold: 10 time: \u0026#39;22:00:00\u0026#39; actions: - service: notify.hoangmnsd data: message: \u0026#39;‚òπ HASS: {{sensors}} is low battery, about 10%\u0026#39; C√°c b·∫°n c√≥ th·ªÉ ƒë·ªçc file ƒë·ªÉ hi·ªÉu https://gist.github.com/sbyx/1f6f434f0903b872b84c4302637d0890.\nN√≥ theo d√µi t·∫•t c·∫£ c√°c sensor m√† c√≥ device_class = battery\nC√°ch 2: Clone l·∫°i hoangmsnd blueprint\nUpdate 2022.07.16: Sau 1 th·ªùi gian s·ª≠ d·ª•ng c√°c Sonoff Sensor, m√¨nh nh·∫≠n ra r·∫±ng l√∫c n√†o ch√∫ng c≈©ng show 100% battery.\nM√¨nh th·ª±c s·ª± s·∫Ω ko th·ªÉ bi·∫øt th·ªùi l∆∞·ª£ng pin c·ªßa ch√∫ng ƒëang c√≤n bao nhi√™u n·∫øu c·ª© ch·ªâ nh√¨n v√†o c√°i s·ªë 100 ·∫£o l√≤i ƒë√≥.\nTh·∫ø n√™n m√¨nh s·∫Ω clone l·∫°i c√°i blueprint tr√™n, t·∫°o blueprint c·ªßa ri√™ng m√¨nh.\nCh√∫ng ta s·∫Ω d·ª±a tr√™n voltage ƒë·ªÉ theo d√µi, v√¨ ch·ªâ c√≥ voltage m·ªõi gi·∫£m d·∫ßn theo th·ªùi gian.\nMax voltage c·ªßa pin l√† 3200 mV. Ch√∫ng ta s·∫Ω warning khi pin gi·∫£m xu·ªëng d∆∞·ªõi 800 mV.\nV√†o RPi server, t·∫°o file /opt/hass/config/blueprints/automation/hoangmnsd/low-battery-voltage-detection-notification-for-all-battery-sensors.yaml\ncontent file:\nblueprint: name: Low battery level detection \u0026amp; notification for all battery sensors description: Regularly test all sensors with \u0026#39;battery\u0026#39; device-class for crossing a certain battery level threshold and if so execute an action. domain: automation input: threshold: name: Battery warning level threshold description: Battery sensors below threshold are assumed to be low-battery (as well as binary battery sensors with value \u0026#39;on\u0026#39;). default: 800 selector: number: min: 100 max: 3200 unit_of_measurement: \u0026#39;mV\u0026#39; mode: slider step: 100 time: name: Time to test on description: Test is run at configured time default: \u0026#39;10:00:00\u0026#39; selector: time: {} day: name: Weekday to test on description: \u0026#39;Test is run at configured time either everyday (0) or on a given weekday (1: Monday ... 7: Sunday)\u0026#39; default: 0 selector: number: min: 0.0 max: 7.0 mode: slider step: 1.0 exclude: name: Excluded Sensors description: Battery sensors (e.g. smartphone) to exclude from detection. Only entities are supported, devices must be expanded! default: {entity_id: []} selector: target: entity: device_class: battery actions: name: Actions description: Notifications or similar to be run. {{sensors}} is replaced with the names of sensors being low on battery. selector: action: {} source_url: https://gist.github.com/hoangmnsd variables: day: !input \u0026#39;day\u0026#39; threshold: !input \u0026#39;threshold\u0026#39; exclude: !input \u0026#39;exclude\u0026#39; sensors: \u0026gt;-{% set result = namespace(sensors=[]) %} {% for state in states.binary_sensor | selectattr(\u0026#39;attributes.device_class\u0026#39;, \u0026#39;==\u0026#39;, \u0026#39;battery\u0026#39;) %} {% if 0 \u0026lt;= state.attributes.voltage | int(-1) \u0026lt; threshold | int and not state.entity_id in exclude.entity_id %} {% set result.sensors = result.sensors + [state.name] %} {% endif %} {% endfor %} {{result.sensors|join(\u0026#39;, \u0026#39;)}} trigger: - platform: time at: !input \u0026#39;time\u0026#39; condition: - \u0026#39;{{ sensors != \u0026#39;\u0026#39;\u0026#39;\u0026#39; and (day | int == 0 or day | int == now().isoweekday()) }}\u0026#39; action: - choose: [] default: !input \u0026#39;actions\u0026#39; mode: single file automations.yml add th√™m ƒëo·∫°n sau:\n# # Warning when sensor low battery (using hoangmnsd blueprint) - id: \u0026#39;92hoangmnsd3972358762324\u0026#39; alias: Low battery voltage detection \u0026amp; notification for all battery sensors description: \u0026#39;\u0026#39; use_blueprint: path: hoangmnsd/low-battery-voltage-detection-notification-for-all-battery-sensors.yaml input: threshold: 500 time: \u0026#39;22:00:00\u0026#39; actions: - service: notify.hoangmnsd data: message: \u0026#39;‚òπ HASS: {{sensors}}, lets take a look\u0026#39; Th·∫ø l√† xong!\n6. Install HACS (Home Assistant Comunity Store) Go inside the container with docker exec -it homeassistant bash, Run command:\nwget -q -O - https://install.hacs.xyz | bash - restart HASS\nV√†o Configuration -\u0026gt; Add Integration -\u0026gt; Search HACS -\u0026gt; tick ch·ªçn all -\u0026gt; submit\nC·∫•p quy·ªÅn cho n√≥ access v√†o GitHub c·ªßa b·∫°n (s·∫Ω c√≥ 1 key hi·ªán ra ƒë·ªÉ paste v√†o)\nSau ƒë√≥ s·∫Ω th·∫•y HACS hi·ªán b√™n tr√°i sidebar:\nV√≠ d·ª• add 1 Card t·ª´ HACS:\nCh·ªçn HACS -\u0026gt; Frontend -\u0026gt; ·∫§n Explore \u0026amp; dowload repositories (n√∫t m√†u xanh)\nSearch Air Purifier s·∫Ω th·∫•y Air Purifier Card -\u0026gt; Ch·ªçn c√°i c·ªßa @fineemb (ch√∫ √Ω v√¨ c√°i n√†y dc recommended)\nrestart HASS\nV√†o Overview -\u0026gt; edit Dashboard -\u0026gt; Add Card -\u0026gt; S·∫Ω th·∫•y Air Purifier xu·∫•t hi·ªán:\n7. Integrate Telegram Bot C·∫ßn bi·∫øt c√°ch t·∫°o Bot, l·∫•y TELEGRAM_TOKEN v√† CHAT_ID, c√≥ th·ªÉ tham kh·∫£o b√†i n√†y:\nLambda + API Gateway, Telegram Bot and Serverless Webapp\nSau khi l·∫•y dc TELEGRAM_TOKEN v√† CHAT_ID c·ªßa Telegram Bot c·ªßa b·∫°n\nedit /opt/hass/config/configuration.yaml:\n# Example configuration.yaml entry for the Telegram Bot telegram_bot: - platform: polling api_key: TELEGRAM_TOKEN allowed_chat_ids: - CHAT_ID # example: 123456789 for the chat_id of a user #- CHAT_ID_2 # example: -987654321 for the chat_id of a group #- CHAT_ID_3 notify: - platform: telegram name: hoangmnsd chat_id: CHAT_ID # example: 123456789 for the chat_id of a user restart HASS\n7.1. C√°ch s·ª≠ d·ª•ng service Telegram Bot V√≠ d·ª• mu·ªën g·ª≠i message ƒë·∫øn Telegram khi log c·ªßa HASS c√≥ l·ªói.\nedit file /opt/hass/config/configuration.yaml:\nsystem_log: fire_event: true edit file /opt/hass/config/automations.yaml:\n# HASS Notify When Error Log Occured - id: \u0026#39;15888152543\u0026#39; alias: HASS Notify When Error Log Occured description: HASS Notify When Error Log Occured trigger: - event_data: level: ERROR event_type: system_log_event platform: event # Ignore these errors condition: - \u0026#34;{{ \u0026#39;Error while getting Updates: urllib3 HTTPError\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;HacsDisabledReason.RATE_LIMIT\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Error handling request\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Got error when receiving: timed out\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Error while getting Updates: Conflict: terminated by other getUpdates request\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; - \u0026#34;{{ \u0026#39;Update for fan.bedroom_air_purifier_3h fails\u0026#39; not in trigger.event.data.message[0] }}\u0026#34; action: # Send to Telegram or any notify platform - data_template: message: \u0026#39;{{trigger.event.data.name}} {{ \u0026#39;\u0026#39;\\n\u0026#39;\u0026#39; -}} {{trigger.event.data.message}}\u0026#39; parse_mode: html title: \u0026#39;üò± Home Assistant: UNEXPECTED ERROR!\u0026#39; service: telegram_bot.send_message # - data_template: # message: \u0026#39;{{trigger.event.data.name}} {{ \u0026#39;\u0026#39;\\n\u0026#39;\u0026#39; -}} {{trigger.event.data.message}}\u0026#39; # title: \u0026#39;üò± Home Assistant: ERROR!\u0026#39; # service: notify.mobile_app_abc restart HASS\nTh·ª≠ t·∫°o l·ªói v√† tr·∫£i nghi·ªám xem c√≥ nh·∫≠n ƒë∆∞·ª£c tin nh·∫Øn ko nh√©?\nC√≥ 1 c√°ch ƒë·ªÉ debug tr∆∞·ªùng h·ª£p n√†y:\nOpen two browser tabs, each one connected to your Home Assistant instance.\nIn first tab, go to Developer Tools \u0026gt; Events, enter system_log_event in ‚ÄúEvent to subscribe to‚Äù then click ‚ÄúListen to events‚Äù:\nIn second tab, go to Developer Tools \u0026gt; Services, select ‚ÄúSystem Log: Write‚Äù, enter some text in ‚ÄúMessage‚Äù, enable ‚ÄúLevel‚Äù, select ‚Äúerror‚Äù, then click ‚ÄúCall Service‚Äù.\nIn first tab, you can see the structure of payload:\n8. Install Duplicati (for backup purpose) edit /opt/hass/docker-compose.yml add these line:\n... duplicati: image: lscr.io/linuxserver/duplicati:2.0.6 container_name: duplicati environment: - PUID=1000 - PGID=1000 - TZ=TZ=Asia/Ho_Chi_Minh - CLI_ARGS= #optional volumes: - /opt/duplicati/config:/config - /opt/duplicati/backups:/backups - /opt:/source ports: - 8200:8200 restart: unless-stopped Run:\ndocker-compose up -d Add frame duplicati to HASS UI: edit /opt/hass/config/configuration.yaml:\nadd these line:\npanel_iframe: ... duplicati: title: \u0026#34;Duplicati\u0026#34; url: \u0026#34;http://RPi_LOCAL_IP:8200\u0026#34; icon: mdi:backup-restore require_admin: true restart HASS\nTh√¥ng qua Portainer, check log c·ªßa container ko c√≥ l·ªói g√¨ l√† OK:\nB·∫°n c√≥ th·ªÉ access v√†o giao di·ªán Duplicati qua HASS UI nh∆∞ sau:\n8.1. Setup Duplicati backup to AWS S3 First you should creat an user (eg: duplicati_backup) in IAM service, note that select Programmatic access type:\nYour user should be attached this custom policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;iam:GetUser\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::AWS_S3_BUCKET\u0026#34;, \u0026#34;arn:aws:iam::AWS_ACCOUNT_ID:user/duplicati_backup\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectAcl\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::AWS_S3_BUCKET/*\u0026#34; } ] } Ch·ªçn Add Backup, ch·ªçn S3, nh·∫≠p h·∫øt c√°c t√πy ch·ªçn nh∆∞ n√†y:\nCh√Ω √Ω khi ch·ªçn test connection h√£y ch·ªçn No ·ªü ƒë√¢y:\nTest connection c·∫ßn works nh∆∞ n√†y:\nM√†n h√¨nh select source, ko n√™n ch·ªçn containerd, duplicati, portainer v√¨ nh·ªØng c√°i √Ω ko c·∫ßn backup ƒë√¢u, c√≤n c√°i hass/config/.storage th√¨ b·∫°n n√™n ssh v√†o RPi, d√πng command chown pi:pi hass/config/.storage/* -R ƒë·ªÉ c√≥ th·ªÉ backup ƒë∆∞·ª£c:\nM√†n h√¨nh select schedule:\nM√†n h√¨nh select size, ko quan tr·ªçng l·∫Øm:\nSau khi ƒë√£ save xong b·ªô config, c√≥ th·ªÉ ·∫•n Run now ƒë·ªÉ test:\nN·∫øu b·ªã l·ªói n√†y c√≥ nghƒ©a l√† b·∫°n c√≥ 1 folder n√†o ƒë√≥ ko th·ªÉ backup, c√≥ l·∫Ω l√† containerd, duplicati, ho·∫∑c v√¨ l·ªói permission n√™n duplicati ko th·ªÉ backup ƒë∆∞·ª£c ch√∫ng, n√™n test l·∫°i v·ªõi 1 file nh·ªè xem sao:\nBackup log ko c√≥ l·ªói g√¨ l√† ok, ho·∫∑c n·∫øu c√≥ WARNING th√¨ b·∫°n xem nh·ªØng file ƒë√≥ n·∫øu ko c·∫ßn thi·∫øt th√¨ c≈©ng ok: 9. Install DuckDNS (for expose outside access purpose) Setup tr√™n Router, t√¨m ph·∫ßn NAT -\u0026gt; Port Forwarding ƒë·ªÉ t·∫°o 1 rule forward t·ª´ port 8123 ƒë·∫øn port 8123 c·ªßa RPi IP: ƒêƒÉng k√Ω account DuckDNS.org v√† b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c token nh∆∞ n√†y: Add subdomain theo √Ω b·∫°n v√†o (gi·∫£ s·ª≠ l√† YOUR_SUBDOMAIN.duckdns.org), update IP l√† 8.8.8.8:\nEdit file /opt/hass/docker-compose.yaml:\nversion: \u0026#34;3.0\u0026#34; services: ... duckdns: image: lscr.io/linuxserver/duckdns:68a3222a-ls97 container_name: duckdns environment: - PUID=1000 #optional - PGID=1000 #optional - TZ=Asia/Ho_Chi_Minh - SUBDOMAINS=YOUR_SUBDOMAIN # eg: `facebook` not `facebook.duckdns.org` - TOKEN=DUCKDNS_TOKEN # replace DUCKDNS_TOKEN by your token from duckdns.org - LOG_FILE=true #optional volumes: - /opt/duckdns/config:/config restart: unless-stopped Run:\ndocker-compose up -d # check log docker logs duckdns N·∫øu log nh∆∞ n√†y l√† OK:\n[cont-init.d] 10-adduser: exited 0. [cont-init.d] 40-config: executing... Retrieving subdomain and token from the environment variables log will be output to file Your IP was updated at Sat May 7 22:37:48 +07 2022 [cont-init.d] 40-config: exited 0. [cont-init.d] 90-custom-folders: executing... [cont-init.d] 90-custom-folders: exited 0. [cont-init.d] 99-custom-files: executing... [custom-init] no custom files found exiting... [cont-init.d] 99-custom-files: exited 0. [cont-init.d] done. [services.d] starting services [services.d] done. Quay l·∫°i duckdns.org s·∫Ω th·∫•y IP c·ªßa m√¨nh ƒë√£ ƒëc c·∫≠p nh·∫≠t l√™n, ko c√≤n l√† 8.8.8.8 n·ªØa:\nƒê·∫øn l√∫c n√†y v√†o ƒë·ªãa ch·ªâ subdomain m√† b·∫°n ƒë√£ ƒëƒÉng k√Ω l√† c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c HASS t·ª´ internet r·ªìi:\nV√¨ 1 s·ªë h·∫°n ch·∫ø li√™n quan ƒë·∫øn HASS Container, Router DASAN c·ªßa VNPT m√† m√¨nh ko config dc HTTPS b·∫±ng Letsencrypt. Hy v·ªçng th·ªùi gian t·ªõi c√≥ th·ªÉ l√†m ƒë∆∞·ª£c. C√°c h·∫°n ch·∫ø v√≠ d·ª• nh∆∞:\nv√† h·∫°n ch·∫ø khi HASS ch·∫°y tr√™n Container ko c√≥ Add-on Duckdns nh∆∞ b·∫£n HASS OS (b·∫£n ƒë√≥ support Letsencrypt s·∫µn lu√¥n)\n10. Install Wireguard 10.1. Setup Gi·∫£ s·ª≠ b·∫°n ƒë√£ ƒëƒÉng k√Ω 1 subdomain tr√™n duckdns.org t√™n l√† YOUR_SUB_DOMAIN.duckdns.org.\nB·∫°n c≈©ng ƒë√£ update IP c·ªßa YOUR_SUB_DOMAIN.duckdns.org tr·ªè ƒë·∫øn public IP c·ªßa RPi.\nEdit file /opt/hass/docker-compose.yaml:\nversion: \u0026#34;3.0\u0026#34; services: ... wireguard: image: lscr.io/linuxserver/wireguard:1.0.20210914 container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 - PGID=1000 - TZ=Asia/Ho_Chi_Minh - SERVERURL=YOUR_SUB_DOMAIN.duckdns.org #optional - SERVERPORT=51820 #optional - PEERS=myphonelg,myphonevsm #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=10.13.13.0 #optional - ALLOWEDIPS=0.0.0.0/0 #optional - LOG_CONFS=true #optional volumes: - /opt/wireguard/config:/config - /opt/wireguard/modules:/lib/modules ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped Ch√∫ √Ω s·ª≠a YOUR_SUB_DOMAIN. N·∫øu ko c√≥ domain c√≥ th·ªÉ d√πng public IP.\nPh·∫ßn PEERS=: ƒëang ƒë·∫∑t t√™n client cho d·ªÖ nh·ªõ, b·∫°n c√≥ th·ªÉ ƒë·∫∑t t·ª± do v√≠ d·ª• PEERS=mylaptop\nRun:\ndocker-compose up -d # check log docker logs wireguard [s6-init] making user provided files available at /var/run/s6/etc...exited 0. [s6-init] ensuring user provided files have correct perms...exited 0. [fix-attrs.d] applying ownership \u0026amp; permissions fixes... [fix-attrs.d] done. [cont-init.d] executing container initialization scripts... [cont-init.d] 01-envfile: executing... [cont-init.d] 01-envfile: exited 0. [cont-init.d] 01-migrations: executing... [migrations] started [migrations] no migrations found [cont-init.d] 01-migrations: exited 0. [cont-init.d] 02-tamper-check: executing... [cont-init.d] 02-tamper-check: exited 0. [cont-init.d] 10-adduser: executing... ------------------------------------- _ () | | ___ _ __ | | / __| | | / \\ | | \\__ \\ | | | () | |_| |___/ |_| \\__/ Brought to you by linuxserver.io ------------------------------------- To support the app dev(s) visit: WireGuard: https://www.wireguard.com/donations/ To support LSIO projects visit: https://www.linuxserver.io/donate/ ------------------------------------- GID/UID ------------------------------------- User uid: 1000 User gid: 1000 ------------------------------------- [cont-init.d] 10-adduser: exited 0. [cont-init.d] 30-module: executing... Uname info: Linux 880be7489d30 5.15.32-v8+ #1538 SMP PREEMPT Thu Mar 31 19:40:39 BST 2022 aarch64 aarch64 aarch64 GNU/Linux **** It seems the wireguard module is already active. Skipping kernel header install and module compilation. **** [cont-init.d] 30-module: exited 0. [cont-init.d] 40-confs: executing... **** Server mode is selected **** **** External server address is set to \u0026lt;REDACTED-SUBDOMAIN\u0026gt;.duckdns.org **** **** External server port is set to 51820. Make sure that port is properly forwarded to port 51820 inside this container **** **** Internal subnet is set to 10.13.13.0 **** **** AllowedIPs for peers 0.0.0.0/0 **** **** PEERDNS var is either not set or is set to \u0026quot;auto\u0026quot;, setting peer DNS to 10.13.13.1 to use wireguard docker host's DNS. **** **** No wg0.conf found (maybe an initial install), generating 1 server and 1 peer/client confs **** grep: /config/peer*/*.conf: No such file or directory PEER 1 QR code: [REDACTED] [cont-init.d] 40-confs: exited 0. [cont-init.d] 90-custom-folders: executing... [cont-init.d] 90-custom-folders: exited 0. [cont-init.d] 99-custom-scripts: executing... [custom-init] no custom files found exiting... [cont-init.d] 99-custom-scripts: exited 0. [cont-init.d] done. [services.d] starting services [services.d] done. [#] ip link add wg0 type wireguard [#] wg setconf wg0 /dev/fd/63 [#] ip -4 address add 10.13.13.1 dev wg0 [#] ip link set mtu 1420 up dev wg0 .:53 CoreDNS-1.9.1 linux/arm64, go1.17.8, 4b597f8 [#] ip -4 route add 10.13.13.2/32 dev wg0 [#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -A FORWARD -o wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE Tr√™n Router s·ª≠a Port Forwarding, add th√™m 1 rule From port 51820 to port 51820 local IP l√† IP c·ªßa RPi.\nRun c√¢u l·ªánh sau ƒë·ªÉ l·∫•y QR code c·ªßa peer1:\ndocker exec -it wireguard /app/show-peer 1 ho·∫∑c docker exec -it wireguard /app/show-peer myphonelg Tr√™n ƒëi·ªán tho·∫°i, t·∫£i app Wireguard v·ªÅ, scan QR code b√™n tr√™n, ƒë·∫∑t t√™n cho VPN ƒë√≥, enable n√≥ l√™n.\nTh·ª≠ d√πng s√≥ng 4G v√† truy c·∫≠p 192.168.1.x:8123 (local IP address c·ªßa HASS) xem sao. N·∫øu truy c·∫≠p ƒë∆∞·ª£c l√† OK.\nV·∫≠y l√† ch·ªâ c·∫ßn b·∫≠t VPN tr√™n ƒëi·ªán tho·∫°i l√™n, l√† b·∫°n ƒë√£ c√≥ th·ªÉ connect ƒë·∫øn HASS nh∆∞ ·ªü nh√† r·ªìi üòÅ\n10.2. Restrict peers permission Tuy nhi√™n sau khi b·∫≠t VPN l√™n, n·∫øu b·∫°n c√≥ th·ªÉ access t·∫•t c·∫£ c√°c IP local k·ªÉ c·∫£: 192.168.1.1 (ƒë√¢y l√† IP c·ªßa Router, r·∫•t quan tr·ªçng) th√¨ r·∫•t nguy hi·ªÉm. Ch·ªâ c·∫ßn 1 user n√†o k·∫øt n·ªôi VPN c·ªßa b·∫°n, h·ªç c√≥ th·ªÉ truy c·∫≠p router nh√† b·∫°n lu√¥n.\nVi·ªác thay ƒë·ªïi setting v·ªÅ permission tr√™n Docker Compose file l√† ko th·ªÉ, m√† ph·∫£i k·∫øt h·ª£p c√°c c√¢u l·ªánh iptables c·ªßa linux m·ªõi ƒë∆∞·ª£c.\nEdit file /opt/wireguard/config/wg0.conf:\n[Interface] Address = 10.13.13.1 ListenPort = 51820 PrivateKey = REDACTED PostUp = /config/mnsd-scripts/postup.sh PostDown = /config/mnsd-scripts/postdown.sh [Peer] # peer_myphonelg PublicKey = REDACTED PresharedKey = REDACTED AllowedIPs = 10.13.13.4/32 [Peer] # peer_myphonevsm PublicKey = REDACTED PresharedKey = REDACTED AllowedIPs = 10.13.13.5/32 -\u0026gt; Nh∆∞ b·∫°n th·∫•y m√¨nh ƒë√£ s·ª≠a ƒë·ªÉ file wg0.conf call ƒë·∫øn 2 file postup.sh v√† postdown.sh.\nT·∫°o file sau /opt/wireguard/config/mnsd-scripts/postup.sh:\nWIREGUARD_INTERFACE=wg0 WIREGUARD_LAN=10.13.13.0/24 MASQUERADE_INTERFACE=eth0 iptables -t nat -I POSTROUTING -o $MASQUERADE_INTERFACE -j MASQUERADE -s $WIREGUARD_LAN # Add a WIREGUARD_wg0 chain to the FORWARD chain CHAIN_NAME=\u0026#34;WIREGUARD_$WIREGUARD_INTERFACE\u0026#34; iptables -N $CHAIN_NAME iptables -A FORWARD -j $CHAIN_NAME # Accept related or established traffic iptables -A $CHAIN_NAME -o $WIREGUARD_INTERFACE -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT # 1.Accept traffic from any peers to anywhere #iptables -A $CHAIN_NAME -s $WIREGUARD_LAN -i $WIREGUARD_INTERFACE -j ACCEPT # 2.Accept traffic from any peers to HAAS IP only iptables -A $CHAIN_NAME -s $WIREGUARD_LAN -i $WIREGUARD_INTERFACE -d 192.168.1.8 -j ACCEPT # 3.Accept traffic from myphonelg 10.13.13.4 to anywhere (both LAN and Internet) #iptables -A $CHAIN_NAME -s 10.13.13.4 -i $WIREGUARD_INTERFACE -j ACCEPT # 4.Accept traffic from myphonelg 10.13.13.4 to HAAS IP only #iptables -A $CHAIN_NAME -s 10.13.13.4 -i $WIREGUARD_INTERFACE -d 192.168.1.8 -j ACCEPT # 5.Accept traffic from myphonevsm 10.13.13.5 to LAN only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.0/24 -j ACCEPT # 6.Accept traffic from myphonevsm 10.13.13.5 to HAAS IP only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.8 -j ACCEPT # 7.Accept traffic from myphonevsm 10.13.13.5 to HAAS IP and port 8123 only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.8 -p tcp --dport 8123 -j ACCEPT # 8.Limit user myphonevsm to access HTTP/S Web internet only #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 192.168.1.1 -p udp --dport 53 -j ACCEPT # below drop traffic from user to any LAN #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -j DROP # accept outgoing connections to HTTP(S) ports to any IP address #iptables -A $CHAIN_NAME -s 10.13.13.5 -i $WIREGUARD_INTERFACE -d 0.0.0.0/0 -p tcp -m multiport --dports 80,443 -j ACCEPT # Drop everything else coming through the Wireguard interface iptables -A $CHAIN_NAME -i $WIREGUARD_INTERFACE -j DROP # Return to FORWARD chain iptables -A $CHAIN_NAME -j RETURN -\u0026gt; Nh∆∞ b·∫°n th·∫•y m√¨nh t·∫°o s·∫µn r·∫•t nhi·ªÅu rule (t·ª´ 1-\u0026gt;8). B·∫°n d√πng c√°i n√†o th√¨ uncomment c√°i ƒë√≥ (c√≥ th·ªÉ k·∫øt h·ª£p nhi·ªÅu c√°i). Nh∆∞ hi·ªán t·∫°i m√¨nh ch·ªçn c√°i 2, t·∫•t c·∫£ traffic c·ªßa peers s·∫Ω ch·ªâ ƒë∆∞·ª£c accept khi connect ƒë·∫øn IP c·ªßa HASS m√† th√¥i\nT·∫°o file sau /opt/wireguard/config/mnsd-scripts/postdown.sh:\nWIREGUARD_INTERFACE=wg0 WIREGUARD_LAN=10.13.13.0/24 MASQUERADE_INTERFACE=eth0 CHAIN_NAME=\u0026#34;WIREGUARD_$WIREGUARD_INTERFACE\u0026#34; iptables -t nat -D POSTROUTING -o $MASQUERADE_INTERFACE -j MASQUERADE -s $WIREGUARD_LAN # Remove and delete the WIREGUARD_wg0 chain iptables -D FORWARD -j $CHAIN_NAME iptables -F $CHAIN_NAME iptables -X $CHAIN_NAME Change quy·ªÅn cho 2 file trong folder scripts (ch√∫ √Ω b∆∞·ªõc n√†y quan tr·ªçng, n·∫øu ko s·∫Ω b·ªã l·ªói):\nsudo chmod 777 /opt/wireguard/config/mnsd-scripts/* -\u0026gt; restart wireguard container. Check log ko c√≥ l·ªói l√† OK.\nGi·ªù h√£y th·ª≠ test k·∫øt n·ªëi xem sao nh√©. C√°c peer s·∫Ω ko th·ªÉ access v√†o b·∫•t c·ª© ƒë√¢u - ngo·∫°i tr·ª´ 192.168.1.8\n10.3. Allow traffic from client to 192.168.1.0/24 only, except internet N·∫øu b·∫°n mu·ªën Client lu√¥n b·∫≠t VPN nh∆∞ng ch·ªâ c√≥ k·∫øt n·ªëi ƒë·∫øn 192.168.1.8 l√† ƒëi qua VPN, c√≤n c√°c traffic ƒë·∫øn Google, myip.com th√¨ ko d√πng VPN (d√π n√≥ ƒëang b·∫≠t). B·∫°n c·∫ßn s·ª≠a tr√™n Client app:\nAllowedIPs = 192.168.1.0/24 l√† ƒë∆∞·ª£c. Nh∆∞ v·∫≠y b·∫°n v·∫´n c√≥ th·ªÉ b·∫≠t VPN ƒë·ªÉ ƒë√≥ su·ªët ng√†y. Ch·ªâ c√°c traffic ƒë·∫øn 192.168.1.0/24 m·ªõi qua VPN.\n10.4. Allow traffic from client to internet only, except local intranet N·∫øu b·∫°n mu·ªën Client lu√¥n b·∫≠t VPN nh∆∞ng ch·ªâ c√°c traffic ra Internet Google, myip.com l√† ƒëi qua VPN, c√≤n traffic ƒë·∫øn 192.168.1.8 th√¨ ƒëi trong m·∫°ng n·ªôi b·ªô c·ªßa Client. B·∫°n c·∫ßn s·ª≠a tr√™n Client app:\nEdit v√† b·ªè tick ·ªü √¥ Block untunneled traffic (kill switch)\nNh∆∞ v·∫≠y Client c√≥ th·ªÉ b·∫≠t VPN ƒë·ªÉ ƒë√≥ su·ªët ng√†y ƒë·ªÉ ƒë·ªçc Medium ch·∫≥ng h·∫°n. Khi h·ªç c·∫ßn truy c·∫≠p v√†o 1 website intranet 192.168.1.8 c·ªßa h·ªç th√¨ h·ªç v·∫´n v√†o b√¨nh th∆∞·ªùng.\n10.5. Add more peer N·∫øu b·∫°n mu·ªën c√≥ th√™m 1 thi·∫øt b·ªã n·ªØa k·∫øt n·ªëi v√†o Wireguard, ch·ªâ c·∫ßn s·ª≠a l·∫°i file /opt/hass/docker-compose.yaml: t·ª´ PEERS=1 th√†nh PEERS=2, ho·∫∑c th√†nh PEERS=myphonelg,myphonevsm\n-\u0026gt; restart wireguard container\nN·∫øu restart xong m√† ko th·∫•y folder peer_xxx trong wireguard/config/ th√¨ c·∫ßn stop container wireguard r·ªìi run l·∫°i:\ndocker-compose up -d R·ªìi run command n√†y ƒë·ªÉ show QR code c·ªßa peer2:\ndocker exec -it wireguard /app/show-peer 2 ho·∫∑c docker exec -it wireguard /app/show-peer myphonevsm Trong tr∆∞·ªùng h·ª£p thi·∫øt b·ªã c·ªßa b·∫°n ko th·ªÉ qu√©t QR code, v√≠ d·ª• nh∆∞ m√°y t√≠nh, laptop:\ncopy content trong file sau: /opt/devops/wireguard/config/peer_mylaptop/peer_mylaptop.conf\n[Interface] Address = 10.x.x.9 PrivateKey = qxxxxxxxxxxxxxxxxxxxxxxxxxxxxQ= ListenPort = 51821 DNS = 10.x.x.9 [Peer] PublicKey = 7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx= PresharedKey = yzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz= Endpoint = x.x.x.x:51821 AllowedIPs = 0.0.0.0/0 Ch·ªçn WireGuard -\u0026gt; Add empty tunnel -\u0026gt; paste content tr√™n v√†o m√†n h√¨nh, r·ªìi Save -\u0026gt; OK\n11. Install Watchman via HACS ƒê√¢y l√† 1 tool gi√∫p generate ra report, gi√∫p b·∫°n bi·∫øt ƒë∆∞·ª£c c√≥ entity n√†o b·ªã missing trong automation, script hay ko?\nt·ª´ report ƒë√≥ b·∫°n ƒëi t√¨m c√°ch fix\nv√†o HACS -\u0026gt; Explore repositories -\u0026gt; Search Watchman\nv√†o Setting -\u0026gt; Integrations -\u0026gt; Add Watchman\n-\u0026gt; restart HASS\nƒë·ª£i 1 l√∫c cho c√°c entities start h·∫øt\nv√†o Developer tools -\u0026gt; Services -\u0026gt; search Watchman -\u0026gt; ·∫•n Call service\nv√†o hass/config s·∫Ω th·∫•y file watchman_report.txt xu·∫•t hi·ªán:\nƒë·ªçc report r·ªìi ƒëi·ªÅu tra c√°c entities ko c·∫ßn thi·∫øt th√¥i üòÉ\n12. Install Grafana + Prometheus 12.1. Install M·ª•c ƒë√≠ch l√† ƒë·ªÉ theo d√µi RAM, CPU, etc \u0026hellip; c·ªßa RPi\nChu·∫©n b·ªã persistent volume:\nsudo mkdir -p /opt/prometheus-grafana/{grafana,prometheus} cd /opt/ wget https://raw.githubusercontent.com/grafana/grafana/main/conf/defaults.ini -O prometheus-grafana/grafana/grafana.ini tee prometheus-grafana/promethueus/prometheus.yml\u0026lt;\u0026lt;EOF global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [] scheme: http timeout: 10s api_version: v1 scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;,\u0026#39;node-exporter:9100\u0026#39;] EOF tee prometheus-grafana/grafana/datasource.yml\u0026lt;\u0026lt;EOF apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://\u0026lt;RPi_local_IP\u0026gt;:9090 isDefault: true access: proxy editable: true EOF File docker-compose.yaml:\nversion: \u0026#39;3.0\u0026#39; services: prometheus: image: prom/prometheus:v2.36.0 container_name: prometheus volumes: # - /opt/prometheus-grafana/prometheus/prometheus_data:/prometheus - /opt/prometheus-grafana/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml ports: - 9090:9090 user: \u0026#34;1000\u0026#34; grafana: image: grafana/grafana:8.5.5 container_name: grafana volumes: - /opt/prometheus-grafana/grafana/grafana_data:/var/lib/grafana - /opt/prometheus-grafana/grafana/grafana.ini:/etc/grafana/grafana.ini - /opt/prometheus-grafana/grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yaml ports: - 3000:3000 links: - prometheus user: \u0026#34;1000\u0026#34; node-exporter: image: prom/node-exporter:v1.3.1 container_name: monitoring_node_exporter restart: unless-stopped expose: - 9100 docker-compose up -d Login Grafana: http://RPi_IP:3000 (admin/admin)\nƒê·ªïi password\n12.2. Import Dashboard https://easycode.page/monitoring-on-raspberry-pi-with-node-exporter-prometheus-and-grafana/\nAdd Grafana v√†o l√†m 1 iframe cho HASS:\nFile /opt/prometheus-grafana/grafana/grafana.ini, s·ª≠a attribute sau:\nallow_embedding = true File /opt/hass/config/configuration.yaml:\npanel_iframe: grafana: title: \u0026#34;Grafana\u0026#34; url: \u0026#34;http://RPI_IP:3000\u0026#34; icon: mdi:chart-areaspline require_admin: true -\u0026gt; restart Grafana, restart HASS\nTi·∫øp theo c·∫ßn l√†m 1 s·ªë rule ƒë·ªÉ Grafana t·ª± ƒë·ªông g·ª≠i tin nh·∫Øn khi nhi·ªát ƒë·ªô CPU qu√° cao, ho·∫∑c khi full RAM, full disk\u0026hellip;\n12.3. Setup template message Ch·ªß y·∫øu d·ª±a tr√™n b√†i n√†y: https://grafana.com/docs/grafana/latest/alerting/unified-alerting/message-templating/\nGi·∫£ s·ª≠ b·∫°n mu·ªën setup khi CPU l√™n 8% th√¨ s·∫Ω message v√†o Telegram cho b·∫°n\nV√†o Dashboard CPU Basic ch·ªçn Edit:\ntab Alert:\nT·∫°o 1 rule t∆∞∆°ng t·ª± nh∆∞ n√†y:\nCh·ªó n√†y nghƒ©a l√† rule ƒë·∫•y s·∫Ω evaluate 1 ph√∫t 1 l·∫ßn trong v√≤ng 2 ph√∫t sau khi th·∫•y CPU \u0026gt; 8%\nB·∫°n c√≥ th·ªÉ truy·ªÅn c√°c label v√†o Alert t√πy √Ω, ·ªü ƒë√¢y m√¨nh th√™m label component=CPU_BUSY\n-\u0026gt; SAVE\nV√†o tab Alert -\u0026gt; Contact point, ch√∫ng ta s·∫Ω t·∫°o 1 contact point telegram_contactpoint v√† 2 template myalert v√† telegram_template\n2 message template n·ªôi dung nh∆∞ n√†y:\nmyalert:\n{{ define \u0026quot;myalert\u0026quot; }} [{{.Status}}] {{ .Labels.alertname }} Labels: {{ range .Labels.SortedPairs }} {{ .Name }}: {{ .Value }} {{ end }} {{ end }} telegram_template:\n{{ define \u0026quot;telegram_template\u0026quot; }} {{ if gt (len .Alerts.Firing) 0 }} {{ len .Alerts.Firing }} firing: {{ range .Alerts.Firing }} {{ template \u0026quot;myalert\u0026quot; .}} {{ end }} {{ end }} {{ if gt (len .Alerts.Resolved) 0 }} {{ len .Alerts.Resolved }} resolved: {{ range .Alerts.Resolved }} {{ template \u0026quot;myalert\u0026quot; .}} {{ end }} {{ end }} {{ end }} V√†o ph·∫ßn contact point t·∫°o Telegram Contactpoint:\nCh√∫ √Ω field Message ƒëi·ªÅn:\nAlert summary: {{ template \u0026quot;telegram_template\u0026quot; . }} Th·ª≠ Test Contact Point v·ªõi custom label m√¨nh ƒëi·ªÅn l√† hahaha:\nB·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c message tr√™n Telegram nh∆∞ n√†y:\nGi·ªù b·∫°n c√≥ th·ªÉ ƒë√£ m∆∞·ªùng t∆∞·ª£ng ƒë∆∞·ª£c flow c·ªßa qu√° tr√¨nh Alert nh∆∞ th·∫ø n√†o. H√£y th·ª≠ t·ª± s·ª≠a theo √Ω m√¨nh nh√© üòÅ\n13. Setup cAdvisor M√¨nh mu·ªën theo d√µi chi ti·∫øt c√°c container, xem c√°i n√†o d√πng nhi·ªÅu CPU nh·∫•t, c√°i n√†o ng·ªën nhi·ªÅu traffic nh·∫•t\u0026hellip;\nƒê√¢y l√† l√∫c s·ª≠ d·ª•ng cAdvisor\ns·ª≠a file docker-compose.yml:\ncadvisor: container_name: cadvisor # image: gcr.io/cadvisor/cadvisor:v0.47.0 # not ARM supported version image: gcr.io/cadvisor/cadvisor-arm64:v0.47.0 # https://github.com/google/cadvisor/issues/3190 ports: - 8081:8080 # M√¨nh expose ra 8081 ƒë·ªÉ tr√°nh duplicate volumes: - /:/rootfs:ro - /var/run:/var/run:ro - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro devices: - /dev/kmsg ·ªû ƒë√¢y ch√∫ √Ω r·∫±ng m√¨nh ƒëang s·ª≠ d·ª•ng version gcr.io/cadvisor/cadvisor-arm64 v√¨ ƒë√¢y l√† image d√πng cho RasberryPi (chip arm64). N·∫øu b·∫°n d√πng b·∫£n ko support chip arm th√¨ kh·∫£ nƒÉng s·∫Ω b·ªã l·ªói ·ªü ƒë√¢u ƒë√≥.\ns·ª≠a file /opt/prometheus-grafana/prometheus/prometheus.yml: Add th√™m cadvisor:8080 ·ªü d√≤ng cu·ªëi c√πng\nglobal: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [] scheme: http timeout: 10s api_version: v1 scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;,\u0026#39;node-exporter:9100\u0026#39;,\u0026#39;cadvisor:8080\u0026#39;] restart container prometheus:\ndocker container restart prometheus V√†o ƒë√¢y confirm xem Prometheus ƒë√£ connect ƒë∆∞·ª£c ƒë·∫øn cAdvisor ch∆∞a:\nhttp://RPI_IP:9090/targets?search=\nM√¨nh import template n√†y: https://grafana.com/grafana/dashboards/15120-raspberry-pi-docker-monitoring/\nC√≥ th·ªÉ s·∫Ω b·ªã l·ªói c√°i Root FS Used ko hi·ªÉn th·ªã dung l∆∞·ª£ng th·∫ª nh·ªõ ƒëang s·ª≠ d·ª•ng. L√∫c ƒë√≥ m√¨nh c·∫ßn s·ª≠a l·∫°i nh∆∞ sau:\nS·ª≠a c√¢u query nh∆∞ sau:\nN·ªôi dung c√¢u query m√¨nh paste ra ƒë√¢y:\n100 - ((node_filesystem_avail_bytes{device=\u0026quot;/dev/root\u0026quot;, fstype=\u0026quot;ext4\u0026quot;, instance=\u0026quot;node-exporter:9100\u0026quot;, job=\u0026quot;prometheus\u0026quot;, mountpoint=\u0026quot;/etc/hostname\u0026quot;} * 100) / node_filesystem_size_bytes{device=\u0026quot;/dev/root\u0026quot;, fstype=\u0026quot;ext4\u0026quot;, instance=\u0026quot;node-exporter:9100\u0026quot;, job=\u0026quot;prometheus\u0026quot;, mountpoint=\u0026quot;/etc/hostname\u0026quot;}) Sau ƒë√≥ th√¨ s·∫Ω l·∫•y ƒë∆∞·ª£c th√¥ng tin Root FS Used nh∆∞ tr√™n h√¨nh l√† kho·∫£ng 75%\nK√©o xu·ªëng d∆∞·ªõi s·∫Ω th·∫•y ƒë∆∞·ª£c max CPU c·ªßa t·ª´ng container, r·ªìi traffic c·ªßa t·ª´ng container, bla bla\u0026hellip; c·∫ßn theo d√µi c√°i n√†o th√¨ ƒë·∫∑t Alert cho c√°i √Ω th√¥i\n14. Setup Nmap Tracker ƒê·ªÉ x√°c ƒë·ªãnh 1 person c√≥ ƒëang ·ªü nh√† ko th√¨ c√≥ nhi·ªÅu c√°ch. Tuy nhi√™n ph·∫ßn n√†y m√¨nh s·∫Ω d√πng Nmap Tracker ƒë·ªÉ x√°c ƒë·ªãnh.\nC∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa n√≥ l√†, RPi s·∫Ω run command nmap ƒë·ªÉ xem c√≥ bao nhi√™u device ƒëang k·∫øt n·ªëi v√†o wifi c·ªßa nh√† m√¨nh. M·ªói device s·∫Ω l√† 1 entity.\nT·ª´ ƒë√≥ x√°c ƒë·ªãnh 1 person c√≥ ƒëang ·ªü nh√† hay ko b·∫±ng c√°ch xem device c·ªßa h·ªç c√≥ ƒëang connect WIFI hay ko.\nTuy nhi√™n n√™n c·∫©n th·∫≠n v√¨ 1 s·ªë thi·∫øt b·ªã ·ªü ch·∫ø ƒë·ªô idle s·∫Ω t·ª± ng·∫Øt k·∫øt n·ªëi WiFi. V√† c√≥ 1 s·ªë thi·∫øt b·ªã d√π ƒëang k·∫øt n·ªëi ƒë·∫øn WiFi nh∆∞ng l·∫°i ko ƒë∆∞·ª£c nmap ph√°t hi·ªán ra.\nCh√∫ √Ω h√£y c√†i s·∫µn nmap command tr√™n RPi nh√©.\nV√†o Setting -\u0026gt; Integrations -\u0026gt; Add Integration, search Nmap Tracker: ƒëi·ªÅn c√°c th√¥ng tin v·ªÅ subnet ip, ip c·ªßa n∆°i s·∫Ω run command nmap, th∆∞·ªùng th√¨ c·ª© ƒë·ªÉ default:\nsau khi Submit th√¨ restart HASS.\nB·∫°n s·∫Ω th·∫•y m√†n h√¨nh Integrations c√≥ Nmap Tracker xu·∫•t hi·ªán v·ªõi 1 s·ªë entities ƒë√≥ ch√≠nh l√† devices ƒëang k·∫øt n·ªëi:\nT·∫°i m√†n h√¨nh sau, h√£y s·ª≠a c√°c entty ƒë·ªÉ ƒë·ªïi t√™n cho d·ªÖ nh√¨n, enable ch√∫ng l√™n, sao cho STATUS nh∆∞ khung ƒë·ªè l√† ok:\nSau ƒë√≥ quay l·∫°i Setting -\u0026gt; People, ch·ªçn Person m√† b·∫°n mu·ªën track device, select device to track:\nQuay l·∫°i Setting -\u0026gt; Developer Tool, search person b·∫°n s·∫Ω th·∫•y ng∆∞·ªùi ƒë√≥ ƒëang c√≥ state=home, v√¨ device c·ªßa h·ªç ƒëang k·∫øt n·ªëi wifi:\nGi·ªù b·∫°n c√≥ th·ªÉ ƒë∆∞a entity person ƒë√≥ ra ngo√†i dashboard ƒë·ªÉ theo d√µi xem h·ªç c√≥ nh√† hay ko üòã\nCh√∫ √Ω r·∫±ng n·∫øu b·∫°n c√≥ 1 chi·∫øc Iphone ƒëang k·∫øt n·ªëi v√†o m·∫°ng, ƒë·ªÉ x√°c ƒë·ªãnh ƒë√¢u l√† ƒë·ªãa ch·ªâ MAC c·ªßa Iphone trong list k·∫øt qu·∫£ c·ªßa Nmap Tracker, b·∫°n c·∫ßn v√†o Setting c·ªßa Iphone ph·∫ßn n√†y:\nSetting -\u0026gt; Wifi -\u0026gt; click v√†o bi·ªÉu t∆∞·ª£ng ch·ªØ i (info) b√™n c·∫°nh WiFi b·∫°n ƒëang k·∫øt n·ªëi.\n‚úî Ph·∫ßn ƒë·ªãa ch·ªâ Wifi address ch√≠nh l√† MAC address c·ªßa iphone trong m·∫°ng c·ªßa b·∫°n\n‚ùå Nhi·ªÅu ng∆∞·ªùi check ƒë·ªãa ch·ªâ MAC c·ªßa Iphone m√† v√†o General -\u0026gt; About -\u0026gt; WiFi address l√† sai nh√© ‚ùå, s·∫Ω kh√¥ng kh·ªõp v·ªõi k·∫øt qu·∫£ tr·∫£ v·ªÅ c·ªßa Nmap Tracker ƒë√¢u\n15. About setting up Automations 15.1. Note 1 Khi setup Automations, b·∫°n n√™n bi·∫øt c√°ch nh√∫ng scripts v√†o Automation th√¨ s·∫Ω l√†m ƒë∆∞·ª£c nhi·ªÅu th·ª© h∆°n.\nGi·∫£ s·ª≠ m√¨nh ƒëang c√≥ 1 Automation nh∆∞ n√†y:\n/opt/hass/config/automations.yaml:\n# Turn On/Off Small room Wall Light/ Bulb base on motion - id: \u0026#39;9hoangmnsd7234756123978961235\u0026#39; alias: Turn On/Off Small room Wall Light/Bulb base on motion description: Turn On/Off Small room Wall Light/Bulb base on motion trigger: - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;on\u0026#39; - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;off\u0026#39; for: 00:05:00 # delay for x min after changing state # Only active between 9AM to 11PM condition: alias: \u0026#34;Time 09~23\u0026#34; condition: time # At least one of the following is required. after: \u0026#34;09:00:00\u0026#34; before: \u0026#34;23:00:00\u0026#34; weekday: - mon - tue - wed - thu - fri - sat - sun action: - data: {} entity_id: light.small_room_light_bulb service_template: \u0026gt;{% if is_state(\u0026#39;binary_sensor.motion_sensor_04_occupancy\u0026#39;, \u0026#39;on\u0026#39;) %} light.turn_on {% else %} light.turn_off {% endif %} - data: {} entity_id: switch.small_room_wall_light service_template: \u0026gt;{% if is_state(\u0026#39;binary_sensor.motion_sensor_04_occupancy\u0026#39;, \u0026#39;on\u0026#39;) %} switch.turn_on {% else %} switch.turn_off {% endif %} N√≥ turn on 2 c√°i ƒë√®n khi motion detected.\nV√† turn off 2 c√°i ƒë√®n khi motion cleared trong 5 ph√∫t.\nNh∆∞ng gi·ªù m√¨nh mu·ªën: N·∫øu sau 5 ph√∫t motion cleared th√¨ ch·ªâ turn_off 1 ƒë√®n, r·ªìi delay 10s, r·ªìi m·ªõi turn_off ƒë√®n 2.\nN·∫øu s·ª≠a th√™m delay 10s v√†o gi·ªØa 2 action th√¨ s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn l√∫c turn_on c≈©ng s·∫Ω ph·∫£i delay. M√† b·∫£n th√¢n h√†m delay th√¨ ko th·ªÉ c√≥ condition.\nTh·∫ø n√™n m√¨nh c·∫ßn nh√∫ng script v√†o automations nh∆∞ sau:\n/opt/hass/config/automations.yaml:\n# Turn On/Off Small room Wall Light/ Bulb base on motion - id: \u0026#39;9817hoangmnsd8324378098546\u0026#39; alias: Turn On/Off Small room Wall Light/Bulb base on motion description: Turn On/Off Small room Wall Light/Bulb base on motion mode: parallel trigger: - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;on\u0026#39; - entity_id: binary_sensor.motion_sensor_04_occupancy platform: state to: \u0026#39;off\u0026#39; for: 00:05:00 # delay for x min after changing state # Only active between 9AM to 11PM condition: alias: \u0026#34;Time 09~23\u0026#34; condition: time # At least one of the following is required. after: \u0026#34;09:00:00\u0026#34; before: \u0026#34;23:00:00\u0026#34; weekday: - mon - tue - wed - thu - fri - sat - sun action: - data: {} service_template: \u0026#34;script.light_bulb_{{trigger.to_state.state}}\u0026#34; /opt/hass/config/scripts.yaml:\nlight_bulb_on: sequence: - service: script.turn_off data: entity_id: script.light_bulb_off - service: light.turn_on entity_id: light.small_room_light_bulb - service: switch.turn_on entity_id: switch.small_room_wall_light light_bulb_off: sequence: - service: light.turn_off entity_id: light.small_room_light_bulb - delay: \u0026#39;00:00:10\u0026#39; - service: switch.turn_off entity_id: switch.small_room_wall_light Ch√∫ √Ω l√† c·∫ßn ƒë·ªÉ mode: parallel ƒë·ªÉ khi trong kho·∫£ng ch·ªù 10s sau khi motion cleared, n·∫øu motion detected th√¨ n√≥ s·∫Ω cancel c√°i script.light_bulb_off ƒëang ch·∫°y\nƒê√¢y l√† 1 ph∆∞∆°ng ph√°p r·∫•t hay v√¨ vi·ªác ƒë∆∞a ra script s·∫Ω ƒëem l·∫°i cho ch√∫ng ta nhi·ªÅu ƒë·∫•t di·ªÖn h∆°n.\n15.2. Note 2 C√°ch li√™n k·∫øt gi·ªØa input_boolean state v·ªõi script:\nPh·∫ßn n√†y s·∫Ω h∆∞·ªõng d·∫´n t·∫°o 1 button leg_mode tr√™n giao di·ªán, t·ª´ button ƒë√≥ call ƒë·∫øn script ƒë·ªÉ flash ƒë√®n b√†n theo nhu c·∫ßu\nfile /opt/hass/config/configuration.yaml:\ninput_boolean: leg_mode: name: Leg mode icon: mdi:shoe-print initial: false giao di·ªán UI:\n- type: horizontal-stack cards: - type: \u0026#39;custom:button-card\u0026#39; template: card_input_boolean entity: input_boolean.leg_mode Automation /opt/hass/config/automations.yaml (c√≥ th·ªÉ b·∫°n nghƒ© ko c·∫ßn file n√†y, nh∆∞ng th·ª±c ch·∫•t ph·∫£i c√≥ ƒë·ªÉ call ƒë·∫øn script. N·∫øu ko th√¨ script s·∫Ω ko bi·∫øt ƒë∆∞·ª£c trigger khi n√†o):\n# Turn On/Off Small room Bulb base on Leg Mode input_boolean - id: \u0026#39;6hoangmnsd3052389573534892y34\u0026#39; alias: Turn On/Off Leg Mode input_boolean description: Turn On/Off Leg Mode input_boolean mode: parallel trigger: - entity_id: input_boolean.leg_mode platform: state to: \u0026#39;on\u0026#39; - entity_id: input_boolean.leg_mode platform: state to: \u0026#39;off\u0026#39; condition: [] action: - data: {} service_template: \u0026#34;script.light_bulb_leg_mode_{{trigger.to_state.state}}\u0026#34; file /opt/hass/config/scripts.yaml:\n# Smallroom Desk Lightbulb Leg mode light_bulb_leg_mode_child: mode: restart sequence: - service: light.turn_on target: entity_id: light.small_room_light_bulb - alias: \u0026#34;Cycle light\u0026#34; repeat: while: - condition: state entity_id: input_boolean.leg_mode state: \u0026#39;on\u0026#39; # Don\u0026#39;t do it too many times: 3600s / 15s = 240 - condition: template value_template: \u0026#34;{{ repeat.index \u0026lt;= 240 }}\u0026#34; sequence: - service: light.turn_on data: brightness_pct: 100 rgb_color: [124,252,0] # lawngreen entity_id: light.small_room_light_bulb - delay: \u0026#39;00:00:05\u0026#39; - service: light.turn_on data: brightness_pct: 100 rgb_color: [255,0,0] # red entity_id: light.small_room_light_bulb - delay: \u0026#39;00:00:10\u0026#39; light_bulb_leg_mode_on: sequence: - condition: state entity_id: input_boolean.leg_mode state: \u0026#39;on\u0026#39; - service: script.turn_off data: entity_id: script.light_bulb_leg_mode_off - alias: \u0026#34;Call the child script\u0026#34; service: script.light_bulb_leg_mode_child light_bulb_leg_mode_off: sequence: - condition: state entity_id: input_boolean.leg_mode state: \u0026#39;off\u0026#39; - service: script.turn_off data: entity_id: script.light_bulb_leg_mode_on - service: light.turn_on # revert to normal brightness data: brightness_pct: 100 color_temp: 350 entity_id: light.small_room_light_bulb - service: light.turn_off entity_id: light.small_room_light_bulb Nh∆∞ v·∫≠y m·ªói khi m√¨nh turn on/off button Leg mode tr√™n giao di·ªán, n√≥ s·∫Ω trigger Automation t·ª´ ƒë√≥ call ƒë·∫øn script b√™n d∆∞·ªõi.\nVi·ªác n·∫Øm ƒëc c√°ch li√™n k·∫øt gi·ªØa c√°c th√†nh ph·∫ßn automation, UI card input_boolean, script gi√∫p b·∫°n hi·ªán th·ª±c h√≥a ƒë∆∞·ª£c nhi·ªÅu √Ω t∆∞·ªüng h∆°n.\n15.3. Note 3 Dynamic even the title text. Note n√†y n√≥i v·ªÅ c√°ch m√¨nh l√†m cho title c·ªßa glance card tr·ªü n√™n dynamic, n√≥ s·∫Ω thay ƒë·ªïi t√πy theo tr·∫°ng th√°i m√† m√¨nh ·∫•n n√∫t:\nV√≠ d·ª• khi m√¨nh ·∫•n v√†o c√°i 29 ƒë·ªô C, th√¨ ƒëo·∫°n text \u0026ldquo;LG Air Conditioner\u0026rdquo; s·∫Ω chuy·ªÉn th√†nh \u0026ldquo;LG Air Conditioner(29)\u0026rdquo;.\nƒêi·ªÅu n√†y l√† kh√¥ng th·ªÉ ƒë·ªëi v·ªõi c√°c default card. V√¨ th·∫ø b·∫°n c·∫ßn import 1 lo·∫°i custom card m·ªõi\nv√†o HACS -\u0026gt; t√¨m card n√†y:\ndownload n√≥ v·ªÅ:\ntrong file /opt/hass/config/configuration.yaml c·∫ßn th√™m config sau:\n# You need to specifiy resource url if setting lovelace.mode = yaml lovelace: mode: yaml resources: - url: /hacsfiles/config-template-card/config-template-card.js type: module v·∫´n trong file ƒë√≥, define 1 input_select:\ninput_select: living_room_ac_title_base_on_temp: name: living_room_ac_title_base_on_temp options: - LG Air Conditioner - LG Air Conditioner(29) - LG Air Conditioner(28) initial: LG Air Conditioner -\u0026gt; restart HASS\ntr√™n UI dashboard b·∫°n c·∫ßn s·ª≠a glance card, ƒë·ªÉ s·ª≠ d·ª•ng dc c√°i config-template-card m√† m·ªõi download v·ªÅ:\n- type: \u0026#39;custom:config-template-card\u0026#39; variables: AC_LIVINGROOM_TITLE: states[\u0026#39;input_select.living_room_ac_title_base_on_temp\u0026#39;].state entities: - script.ac_livingroom_power - script.ac_livingroom_jetmode - script.ac_livingroom_temp29 - script.ac_livingroom_temp28 card: type: glance title: \u0026#34;${AC_LIVINGROOM_TITLE}\u0026#34; style: |ha-card { background-color: {{ \u0026#39;#AAFFAA\u0026#39; if is_state(\u0026#39;binary_sensor.living_room_ac_sensor_contact\u0026#39;, \u0026#39;on\u0026#39;) else \u0026#39;#FFFFFF\u0026#39; }}; } entities: - entity: script.ac_livingroom_power name: Power show_state: false show_icon: true icon: mdi:power tap_action: action: call-service service: script.ac_livingroom_power data: entity_id: script.ac_livingroom_power - entity: script.ac_livingroom_jetmode name: JetMode show_state: false show_icon: true icon: mdi:snowflake tap_action: action: call-service service: script.ac_livingroom_jetmode data: entity_id: script.ac_livingroom_jetmode - entity: script.ac_livingroom_temp29 name: 29¬∞C show_state: false show_icon: true icon: mdi:temperature-celsius tap_action: action: call-service service: script.ac_livingroom_temp29 data: entity_id: script.ac_livingroom_temp29 trong /opt/hass/config/scripts.yaml b·∫°n c·∫ßn add th√™m action ƒë·ªÉ n√≥ thay ƒë·ªïi option cho input_select:\nac_livingroom_temp29: sequence: - service: remote.send_command data: command: temp29 device: ac_livingroom target: entity_id: remote.rm4_mini_livingroom_remote - service: input_select.select_option  data: entity_id: input_select.living_room_ac_title_base_on_temp option: \u0026#39;LG Air Conditioner(29)\u0026#39; -\u0026gt; restart HASS\nV·ªÅ c∆° b·∫£n, b·∫°n ƒë√£ d√πng input_select ƒë·ªÉ thay ƒë·ªïi gi√° tr·ªã c·ªßa title. ƒêi·ªÅu n√†y nh·ªù c√¥ng r·∫•t l·ªõn c·ªßa custom:config-template-card.\n16. Setup Navidrome ƒê√¢y l√† 1 web app ƒë·ªÉ chia s·∫ª nh·∫°c t·ª´ m√°y m√¨nh ra internet. C√≥ th·ªÉ share cho m·ªçi ng∆∞·ªùi nghe c√πng:\nhttps://github.com/navidrome/navidrome/\nhttps://www.navidrome.org/docs/installation/docker/\nhttps://www.navidrome.org/docs/usage/configuration-options/#configuration-file\nChu·∫©n b·ªã folder:\n /opt/navidrome/data ƒë·ªÉ ch·ª©a config /opt/navidrome/music ƒë·ªÉ ch·ª©a c√°c file mp3  add v√†o docker-compose.yml\nnavidrome: container_name: navidrome image: deluan/navidrome:0.47.5 user: 1000:1000 # should be owner of volumes ports: - \u0026#34;4533:4533\u0026#34; restart: unless-stopped environment: # Optional: put your config options customization here. Examples: ND_SCANSCHEDULE: 1h ND_LOGLEVEL: info  ND_SESSIONTIMEOUT: 24h ND_BASEURL: \u0026#34;/\u0026#34; ND_REVERSEPROXYWHITELIST: \u0026#34;192.168.1.0/24,172.18.0.0/16\u0026#34; # m·∫∑c d√π ƒë∆∞a v√†o nh∆∞ng ch∆∞a hi·ªÉu c√°ch d√πng volumes: - \u0026#34;/opt/navidrome/data:/data\u0026#34; - \u0026#34;/opt/navidrome/music:/music:ro\u0026#34; V·∫≠y l√† xong, b·∫°n c√≥ th·ªÉ access v√†o port 4533 ƒë·ªÉ xem trang web ho·∫°t ƒë·ªông.\nM√¨nh ƒëang d√πng swag ƒë·ªÉ expose trang navidrome n√†y ra ngo√†i internet d∆∞·ªõi domain https://navidrome.MYDOMAIN.duckdns.org. Nh∆∞ v·∫≠y l√† ·ªü C√¥ng ty c≈©ng c√≥ th·ªÉ nghe ƒë∆∞·ª£c list nh·∫°c ·ªü nh√† r·ªìi üòÅüòÅ\nHi·ªán t·∫°i c√≥ 1 v·∫•n ƒë·ªÅ:\n M√¨nh v·∫´n ch∆∞a nh√∫ng dc trang n√†y v√†o l√†m 1 iframe c·ªßa HASS, n√≥ c·ª© b√°o l·ªói t·ª´ ch·ªëi k·∫øt n·ªëi\n-\u0026gt; th·∫ø n√™n t·ª´ m·∫°ng LAN mu·ªën v√†o l√† ph·∫£i v√†o theo local IP: 192.168.x.x:4533\nv·∫•n ƒë·ªÅ n√†y ƒë∆∞·ª£c th·∫£o lu·∫≠n trong topic n√†y: https://github.com/navidrome/navidrome/issues/248#issuecomment-911143096  T√≥m t·∫Øt ƒë·ªÉ x·ª≠ l√Ω v·∫•n ƒë·ªÅ iframe th√¨ c·∫ßn ph·∫£i s·ª≠ d·ª•ng nginx ƒë·ªÉ ghi ƒë√® l√™n c√°i setting X-Frame-Options: deny c·ªßa Navidrome\nƒê·ªÉ l√†m ƒë∆∞·ª£c th√¨ m√¨nh ƒë√£ ph·∫£i setup Nginx theo b√†i Setup Home Assistant on Raspberry Pi (Part 3) - Https\nSau khi c√≥ nginx r·ªìi th√¨:\nTrong folder /opt/swag/config/nginx/proxy-confs/ rename file navidrome.subdomain.conf.sample -\u0026gt; navidrome.subdomain.conf Trong file n√†y s·ª≠a n·ªôi dung nh∆∞ sau:\n~~~ # redirect all traffic to https # server { # listen 80; # listen [::]:80; # server_name navidrome.*; # return 301 https://$host$request_uri; # } server { listen 80; listen 443 ssl; listen [::]:443 ssl; server_name navidrome.*; include /config/nginx/ssl.conf; client_max_body_size 0; location / { include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app navidrome; set $upstream_port 4533; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; proxy_hide_header X-Frame-Options; } } -\u0026gt; nh∆∞ v·∫≠y m√¨nh ƒë√£ set cho nginx ko redirect http sang https, v√† listen tr√™n c·∫£ port 80.\nserver name c·ªßa Navidrome s·∫Ω l√† navidrome.MYDOMAIN.duckdns.org.\nM√¨nh c≈©ng ƒë√£ th√™m proxy_hide_header X-Frame-Options; ƒë·ªÉ ghi ƒë√® l√™n setting X-Frame-Options c·ªßa Navidrome\nB√¢y gi·ªù t·ª´ Internet c√≥ th·ªÉ access v√†o c·∫£ 2 link n√†y ƒë·ªÅu ok:\n https://navidrome.MYDOMAIN.duckdns.org http://navidrome.MYDOMAIN.duckdns.org  Gi·ªù setting trong HASS file /opt/hass/config/configuration.yaml:\n# panel iframe panel_iframe: ... navidrome: title: \u0026#34;Navidrome\u0026#34; url: \u0026#34;http://navidrome.MYDOMAIN.duckdns.org\u0026#34; icon: mdi:music require_admin: false -\u0026gt; restart swag container, HASS\nTest:\n T·ª´ M·∫°ng 4G -\u0026gt; HASS http http://MYDOMAIN.duckdns.org -\u0026gt; t·ª± redirect https, v√†o dc HASS -\u0026gt; ko m·ªü ƒëc iframe Navidrome. T·ª´ M·∫°ng 4G -\u0026gt; HASS http http://MYDOMAIN.duckdns.org:8123 -\u0026gt; v√†o dc HASS, m·ªü ƒëc iframe Navidrome. T·ª´ m·∫°ng LAN -\u0026gt; HASS http http://MYDOMAIN.duckdns.org:8123-\u0026gt; v√†o dc HASS, m·ªü ƒëc iframe Navidrome. T·ª´ m·∫°ng 4G -\u0026gt; http://navidrome.MYDOMAIN.duckdns.org -\u0026gt; v√†o ƒëc Navidrome. T·ª´ m·∫°ng 4G -\u0026gt; https://navidrome.MYDOMAIN.duckdns.org -\u0026gt; v√†o ƒëc Navidrome.  17. Troubleshoot docker overlay https://forums.docker.com/t/some-way-to-clean-up-identify-contents-of-var-lib-docker-overlay/30604/59\nN·∫øu b·∫°n th·∫•y d√πng l∆∞·ª£ng trong /var/lib/docker/overlay2/ nhi·ªÅu 1 c√°ch b·∫•t th∆∞·ªùng, c√≥ 1 v√†i step n√™n ki·ªÉm tra:\nXem t·ªïng c·ªông file log c√≥ nhi·ªÅu ko?\ndu -shc /var/lib/docker/containers/*/*.log Xem s·ªë l∆∞·ª£ng file trong folder diff:\ndu -shc /var/lib/docker/overlay2/*/diff Sort c√°c file theo th·ª© t·ª± dung l∆∞·ª£ng gi·∫£m d·∫ßn:\ndu -s /var/lib/docker/overlay2/*/diff |sort -n -r Xem folder to nh·∫•t ƒë∆∞·ª£c d√πng cho container n√†o:\ndocker inspect $(docker container ls -q) |grep \u0026#39;FOLDER_ID_HERE\u0026#39; -B 100 -A 100 Sau ƒë√≥ b·∫°n c√≥ th·ªÉ l·ª±a ch·ªçn x√≥a container ƒë√≥ r·ªìi run l·∫°i xem sao.\nHo·∫∑c gi·ªõi h·∫°n l·∫°i dung l∆∞·ª£ng logging c·ªßa container ƒë√≥ r·ªìi run l·∫°i.\n1 s·ªë c√°ch kh√°c:\n# X√≥a WARNING! This will remove: # - all stopped containers # - all networks not used by at least one container # - all dangling images # - all dangling build cache docker system prune X√°c ƒë·ªãnh v·ªÅ folder n√†o ƒëang size to nh·∫•t, v√≠ d·ª• sau ƒë√¢y folder to nh·∫•t l√† /volumes 30G:\n/var/lib/docker# sudo du -h --max-depth=1 325M ./buildkit 4.0K ./runtimes 8.0K ./tmp 4.0K ./swarm 100K ./network 92K ./containers 4.3M ./image 1.9G ./overlay2 16K ./plugins 30G ./volumes 4.0K ./trust 33G . X√≥a c√°c volume th·ª´a ƒëi:\ndocker volume prune WARNING! This will remove all local volumes not used by at least one container. Are you sure you want to continue? [y/N] y ... Total reclaimed space: 22.61GB 18. Setup HASS.Agent on Windows machine T√¨nh hu·ªëng l√†: M√¨nh c√≥ 1 Laptop ƒëang s·ª≠ d·ª•ng Smart Plug. M√¨nh mu·ªën n·∫øu pin dc s·∫°c ƒë·∫ßy tr√™n 99% s·∫Ω t·ª± ng·∫Øt Plug, v√† n·∫øu pin d∆∞·ªõi 10% th√¨ t·ª± b·∫≠t plug.\nƒê·ªÉ l√†m ƒëi·ªÅu n√†y th√¨ Laptop c·ªßa b·∫°n c·∫ßn g·ª≠i th√¥ng tin cho HASS.-\u0026gt; B·∫°n c·∫ßn c√†i HASS.Agent tr√™n Windows.\nSau khi download v√† install xong (m√¨nh c√†i version 2022.13.0)\nSetup URL c·ªßa HASS, v√† Long-live token:\nSetup connect ƒë·∫øn MQTT server, ch·ªó n√†y c·∫ßn nh·∫≠p user/password c·ªßa MQTT server nh√©:\nSau ƒë√≥ ch·ªù HASS.Agent restart ƒë·ªÉ setup, r·ªìi v√†o dashboard -\u0026gt; ch·ªçn Local Sensors ƒë·ªÉ add sensor v√†o:\nAdd new:\nCh·ªçn Battery -\u0026gt; ·∫•n Store Sensor:\n·∫§n ti·∫øp Store and Active sensor\nXOng r·ªìi, quay l·∫°i HASS -\u0026gt; Setting -\u0026gt; Devices, s·∫Ω th·∫•y Laptop hi·ªán ra v√† c√°c battery sensor t∆∞∆°ng ·ª©ng\nGi·ªù b·∫°n c√≥ th·ªÉ vi·∫øt Automation cho c√°c sensor ƒë√≥, hi·ªán t·∫°i m√¨nh ƒëang d√πng c√°i n√†y:\n- id: \u0026#39;2hoangmnsd3sadsdsdsfd4132123f\u0026#39; alias: Turn On/Off Laptop PLug base on Battery description: Turn On/Off Laptop PLug base on Battery trigger: # platform: time_pattern # minutes: \u0026#39;/1\u0026#39; # every 1 minute - below: \u0026#39;10\u0026#39; entity_id: sensor.asusk501lb_battery_charge_remaining_percentage platform: numeric_state - above: \u0026#39;99\u0026#39; entity_id: sensor.asusk501lb_battery_charge_remaining_percentage platform: numeric_state condition: [] action: - data: {} entity_id: switch.smart_plug_01 service_template: \u0026gt;{% set p = states(\u0026#39;sensor.asusk501lb_battery_charge_remaining_percentage\u0026#39;) | int(0) %} {% if p \u0026gt;= 99 %} switch.turn_off {% elif p \u0026lt;= 10 %} switch.turn_on {% endif %} 19. Setup NFC Tag Xem 1 s·ªë clip youtube th·∫•y dc 1 s·ªë t√¨nh nƒÉng h·ªØu d·ª•ng c·ªßa NFC Tag trong nh√† m√¨nh:\n  ƒê√¥i khi ng·ªìi ƒÉn c∆°m, m√¨nh ko mu·ªën c·ª© n√≥i \u0026ldquo;Alexa, stop\u0026rdquo;, \u0026ldquo;Alexa, play news\u0026rdquo;, \u0026ldquo;Alexa music\u0026rdquo; m√£i.\n  Ho·∫∑c trong ph√≤ng ng·ªß, m√¨nh ko mu·ªën ph·∫£i g√†o l√™n \u0026ldquo;Alexa, turn off Bedroom Light\u0026rdquo; (Alexa ƒë·∫∑t ngo√†i ph√≤ng kh√°ch, m√¨nh ch∆∞a c√≥ ti·ªÅn mua th√™m Alexa trong ph√≤ng ng·ªß üòÜ).\n  Th·∫ø n√™n, M√¨nh mu·ªën c√≥ 1 NFC Tag d√°n tr√™n m·∫∑t b√†n/ƒë·∫ßu gi∆∞·ªùng, m√¨nh qu∆° ƒëi·ªán tho·∫°i l√† n√≥ s·∫Ω t·ª± ra l·ªánh lu√¥n thay m√¨nh lu√¥n (Androids v·∫´n ph·∫£i unlock ƒëi·ªán tho·∫°i tr∆∞·ªõc, Iphone ko c·∫ßn unlock nh∆∞ng ph·∫£i s√°ng m√†n h√¨nh), ko c·∫ßn g√†o l√™n, ko c·∫ßn m·ªü app tr√™n ƒëi·ªán tho·∫°i r·ªìi t√¨m ƒë·∫øn button ·∫•n n√∫t n·ªØa.\nV√† b·ªüi v√¨ trong nh√† m√¨nh c√≥ nhi·ªÅu lo·∫°i ƒëi·ªán tho·∫°i n√™n c≈©ng c·∫ßn setup ri√™ng cho t·ª´ng lo·∫°i.\nIphone th√¨ c·∫ßn k·∫øt h·ª£p v·ªõi app Shortcut (ch·ªâ h·ªó tr·ª£ NFC Tag t·ª´ iphone X tr·ªü ƒëi)\nAndroids th√¨ ko ph·∫£i ƒëi·ªán tho·∫°i n√†o c≈©ng c√≥ chip NFC, v√≠ d·ª• Vsnart Live 4 ko c√≥ NFC th√¨ ko scan ƒë∆∞·ª£c.\nVi·ªác setup th√¨ ch·ªß y·∫øu nh·ªù video n√†y l√† ƒë·ªß:\nhttps://www.youtube.com/watch?v=VsjTzm2JFJ0\u0026amp;t=374s\u0026amp;ab_channel=JuanMTech\nT√≥m t·∫Øt n·∫øu ko mu·ªën xem l·∫°i Video tr√™n:\nƒê·∫ßu ti√™n l√† l√™n Shoppee mua 1 l·ªë NFC tag v·ªÅ, gi√° kh√° r·∫ª th√¥i, lo·∫°i n√†o c≈©ng ƒë∆∞·ª£c v√¨ HA App t∆∞∆°ng th√≠ch v·ªõi m·ªçi lo·∫°i NFC Tag (m√¨nh c≈©ng ch∆∞a research ƒë·ªÉ bi·∫øt n√™n d√πng lo·∫°i NFC tag n√†o)\nSau khi ƒë√£ c√≥ Tag. C√†i Home Assistant App tr√™n ƒëi·ªán tho·∫°i.\nV√†o ph·∫ßn Setting -\u0026gt; Tags -\u0026gt; Add Tag -\u0026gt; ƒê·∫∑t t√™n cho Tag\nQu∆° ƒëi·ªán tho·∫°i tr√™n C√°i Tag m·ªõi, n√≥ s·∫Ω ghi ƒë√® auto-gen ID m·ªõi cho Tag\nXong ph·∫ßn ƒëi·ªán tho·∫°i, gi·ªù v√†o l√†m tr√™n m√°y t√≠nh, t·∫°o Automation cho Tag ƒë√≥ (Ch√∫ √Ω copy c√°i ID c·ªßa Tag tr∆∞·ªõc)\nV√≠ d·ª• Automation cho vi·ªác scan Tag c·ªßa m√¨nh:\n# Control Bedroom Light base on NFC Tag - id: \u0026#39;handle-tag-scan-bedroom-light1\u0026#39; alias: \u0026#34;Handle Tag Scan: Bedroom Light 1\u0026#34; mode: single # Hide warnings when triggered while in delay. max_exceeded: silent trigger: - platform: tag tag_id: 84df4xxxxxxxxxxxxxxxxxxxxx15f01785 condition: [] action: - service: switch.toggle data: {} target: entity_id: switch.bed_room_light Reload l·∫°i Automation. Test b·∫±ng c√°ch Qu∆° ƒëi·ªán tho·∫°i tr√™n Tag. B∆∞·ªõc n√†y ƒêi·ªán tho·∫°i Androids c·∫ßn unlock tr∆∞·ªõc.\nƒêi·ªán tho·∫°i Iphone s·∫Ω c·∫ßn unlock v√† ch·∫°m v√†o c√°i notification pop-up ra. -\u0026gt; h∆°i phi·ªÅn.\nGi·ªù fix c√°i h∆°i phi·ªÅn c·ªßa Iphone ƒë√≥:\n  Tr√™n Iphone t√¨m app Shortcut ch·ªçn Automation -\u0026gt; NFC -\u0026gt; Scan -\u0026gt; Name the Tag -\u0026gt; Click Next on top right\n  Add action -\u0026gt; search Home Assistant App -\u0026gt; select HA fire event -\u0026gt; change shortcut_event to anything (ko c√≥ space), v√≠ d·ª• change th√†nh BedroomLightTag1 -\u0026gt; Next\n  Uncheck Ask before running -\u0026gt; Uncheck Notify when run\n  S·ª≠a Automation tr√™n Home Assistant, add th√™m trigger platform: event nh∆∞ n√†y, ch√∫ √Ω event_type ch·ªçn ƒë√∫ng c√°i ƒë√£ ƒë·∫∑t t√™n BedroomLightTag1 trong App Shortcut:\n  # Control Bedroom Light base on NFC Tag - id: \u0026#39;handle-tag-scan-bedroom-light1\u0026#39; alias: \u0026#34;Handle Tag Scan: Bedroom Light 1\u0026#34; mode: single # Hide warnings when triggered while in delay. max_exceeded: silent trigger: - platform: tag tag_id: 84xxxxxxxxxxxxxxxxxxxxx85 - platform: event event_type: BedroomLightTag1 condition: [] action: - service: switch.toggle data: {} target: entity_id: switch.bed_room_light   Reload Automation tr√™n Home Assistant\n  Test l·∫°i, ko c·∫ßn unlock Iphone, nh∆∞ng c·∫ßn ch·∫°m m√†n h√¨nh s√°ng, Qu∆° Iphone tr√™n Tag -\u0026gt; Done üòò\n  ƒêi·ªán tho·∫°i n√†o c≈©ng ph·∫£i l√†m (Tuy nhi√™n ko ph·∫£i Iphone n√†o app Shortcut c≈©ng c√≥ option NFC ƒë√¢u)\n  Troubeshooting:\nƒê√¥i khi c√≥ l·ªói l√† scan NFC Tag xong Automation ko ch·∫°y, c√≥ th·ªÉ l√† do m√¨nh v·ª´a s·ª≠a Automation li√™n quan ƒë·∫øn vi·ªác Scan NFC:\nhttps://community.home-assistant.io/t/trouble-using-nfc-tags-ha-says-never-scanned/438377\n-\u0026gt; V√†o App x√≥a cache, data ƒëi, k·∫øt n·ªëi l·∫°i v·ªõi server HA v√† login l·∫°i l√† OK\nReview Qu√° tr√¨nh s·ª≠ d·ª•ng 1,2 tu·∫ßn m√¨nh g·∫∑p 1 s·ªë l·ªói:\n C·ª≠a ƒë√≥ng nh∆∞ng h·ªá th·ªëng v·∫´n b√°o l√† ƒëang m·ªü. Ko c√≤n chuy·ªÉn ƒë·ªông nh∆∞ng sensor v·∫´n ·ªü tr·∫°ng th√°i Detected motion trong v√≤ng 15p.  M√¨nh nghƒ© l√† do qos c·ªßa MQTT server ƒëang set l√† qos=0. M√¨nh ƒë√£ config l·∫°i MQTT server qos=2. Ch·ªù th√™m kho·∫£ng 1.2 tu·∫ßn n·ªØa xem c√≥ ok ko‚Ä¶\nSau kho·∫£ng 1 tu·∫ßn d√πng, th√¨ c√≥ 1 l·ªói nh∆∞ n√†y:\n Motion sensor v·∫´n ·ªü tr·∫°ng th√°i Detected motion trong v√≤ng 2 ti·∫øng, m·∫∑c d√π ko c√≥ chuy·ªÉn ƒë·ªông n√†o, sau 2 ti·∫øng th√¨ n√≥ m·ªõi chuy·ªÉn v·ªÅ tr·∫°ng th√°i Cleared.  N√≥i chung l√† qos=2 th√¨ ch·∫Øc ch·∫Øn l√† message s·∫Ω ƒë·∫øn dc MQTT server nh∆∞ng bao l√¢u m·ªõi ƒë·∫øn th√¨ ko bi·∫øt (·ªü ƒë√¢y l√† 2 ti·∫øng m·ªõi ƒë·∫øn) üòÇ\n 1 l·ªói n·ªØa l√† Motion ko ph√°t hi·ªán ƒë∆∞·ª£c chuy·ªÉn ƒë·ªông. R·ªìi 1 l√∫c sau th√¨ l·∫°i ph√°t hi·ªán chuy·ªÉn ƒë·ªông b√¨nh th∆∞·ªùng. L·ªói n√†y ƒë√¥i khi x·∫£y ra.  CREDIT https://www.jfrog.com/connect/post/install-docker-compose-on-raspberry-pi/\nhttps://www.zigbee2mqtt.io/guide/getting-started/#installation\nhttps://www.youtube.com/watch?v=cZV2OOXLtEI\u0026amp;ab_channel=HomeAutomationGuy\nhttps://www.home-assistant.io/installation/raspberrypi#install-home-assistant-container\nhttps://www.jfrog.com/connect/post/install-docker-compose-on-raspberry-pi/\nhttps://peyanski.com/how-to-install-home-assistant-community-store-hacs/#Initial_install_of_Home_Assistant_Community_Store_HACS_on_Home_Assistant_Container\nhttps://konnected.vn/home-assistant/home-assistant-thong-bao-den-dien-thoai-may-tinh-2020-05-11\nhttps://www.home-assistant.io/integrations/telegram/\nhttps://hub.docker.com/r/linuxserver/duplicati\nhttps://community.home-assistant.io/t/automation-based-on-log/337925/6?u=super318\nhttps://hub.docker.com/r/linuxserver/duckdns\nhttps://konnected.vn/home-assistant/home-assistant-truy-cap-bang-domain-va-cai-dat-chung-chi-https-2020-05-18\nhttps://www.addictedtotech.net/home-vpn-using-wireguard-docker-on-a-raspberry-pi-4/\nhttps://hub.docker.com/r/linuxserver/wireguard\nhttps://viblo.asia/p/docker-dung-vpn-server-wireguard-tren-docker-Qbq5QBOmKD8\nhttps://github.com/linuxserver/docker-wireguard\nhttps://www.youtube.com/watch?v=aGIg6N9HzSg\nhttps://gist.github.com/qdm12/4e0e4f9d1a34db9cf63ebb0997827d0d\nhttps://www.youtube.com/watch?v=XKD5vBZLKgE\nhttps://techviewleo.com/run-prometheus-and-grafana-using-docker-compose/\nhttps://easycode.page/monitoring-on-raspberry-pi-with-node-exporter-prometheus-and-grafana/\nhttps://grafana.com/grafana/dashboards/1860\nhttps://geektechstuff.com/2020/06/25/grafana-prometheus-and-node-exporter-on-raspberry-pi-via-ansible-raspberry-pi-and-linux/\nhttps://www.home-assistant.io/integrations/person/\nhttps://www.home-assistant.io/integrations/nmap_tracker/\nhttps://www.home-assistant.io/integrations/input_boolean/\nhttps://www.home-assistant.io/docs/scripts/#counted-repeat\nhttps://community.home-assistant.io/t/is-it-possible-to-use-dynamic-name-in-e-g-sensor-card/298677\nhttps://community.home-assistant.io/t/100-templatable-lovelace-configurations/105241\nhttps://github.com/iantrich/config-template-card\nhttps://www.home-assistant.io/integrations/input_select/\nhttps://community.home-assistant.io/t/hass-agent-windows-client-to-receive-notifications-use-commands-sensors-quick-actions-and-more/369094\nhttps://community.home-assistant.io/t/charge-my-android-tablet-automatically-when-battery-goes-less-than-10-stop-charging-when-100/61296/6\nhttps://www.youtube.com/watch?v=B4SnJPVbSXc\u0026amp;t=432s\u0026amp;ab_channel=EverythingSmartHome\nhttps://github.com/LAB02-Research/HASS.Agent\nhttps://superuser.com/a/1612473\nhttps://github.com/oijkn/Docker-Raspberry-PI-Monitoring\nhttps://thesmarthomejourney.com/2022/07/25/monitoring-smarthome-prometheus/\n","href":"/posts/encrypt-setup-home-assistant-on-raspberry-pi-and-addons/","title":"Setup Home Assistant on Raspberry Pi (Part 1) - Addons"},{"content":"","href":"/tags/aci/","title":"ACI"},{"content":"1. G√°n Role cho Identity = ARM B·∫°n c√≥ 1 ACI container, ƒë√£ enable System assigned identity\nB·∫°n mu·ªën g√°n Role READER cho identity tr√™n b·∫±ng ARM Quan tr·ªçng l√† b·∫°n ph·∫£i bi·∫øt c√°ch l·∫•y dc principalId c·ªßa c√°i System assigned identity.\nV√≠ d·ª• sau:\n// add Role to SystemAssigned identity of ACI { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Authorization/roleAssignments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-09-01-preview\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;aciRoleDefinitionName\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[variables(\u0026#39;aciContainerGroupName\u0026#39;)]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;[variables(\u0026#39;aciRoleDefinitionId\u0026#39;)]\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.ContainerInstance/containerGroups\u0026#39;, variables(\u0026#39;aciContainerGroupName\u0026#39;)), \u0026#39;2021-09-01\u0026#39;, \u0026#39;Full\u0026#39;).Identity.principalId]\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;[resourceGroup().id]\u0026#34;, \u0026#34;principalType\u0026#34;: \u0026#34;ServicePrincipal\u0026#34; } } 2. ACI b·ªã terminated ngay sau khi start (ko c√≥ log l·ªói) ACI b·ªã terminated ngay t·ª´ ƒë·∫ßu, ko c√≥ log l·ªói, c·ª© start l·∫°i container ACI l·∫°i b·ªã terminated ngay\n-\u0026gt; Solution:\nx√≥a to√†n b·ªô untagged images tr√™n ACR, d√πng Notepad++ format file Dockerfile v√† entry.sh th√†nh UNIX file. (Notepad++ -\u0026gt; Edit -\u0026gt; EOL Conversion -\u0026gt; Unix LF -\u0026gt; save) Build l·∫°i Docker image, push l√™n ACR.\n3. Kh√°c nhau v·ªÅ limitation gi·ªØa c√°c region Ch√∫ √Ω v·ªÅ ACI in VNET trong francecentral. ACI in VNET t·∫°o t·ª´ ARM b·ªã l·ªói:\nmessage:\u0026quot;The requested resource is not available in the location 'francecentral' at this moment. Please retry with a different resource request or in another location. Resource requested: '1' CPU '4' GB memory 'Linux' OS virtual network\u0026quot;} m·∫∑c d√π Documents ghi r√µ c√≥ support 4CPU-16GB tr√™n francencentral:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-region-availability\nNh∆∞ng b·∫°n v·∫´n s·∫Ω g·∫∑p l·ªói tr√™n khi deploy t·ª´ ARM.\nL√Ω do b√™n MS ƒë∆∞a ra l√† Do limitation: Ko th·ªÉ s·ª≠ d·ª•ng managed-identity trong 1 ACI deploy trong VNET:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-managed-identity#limitations\nM·∫∑c d√π documents ghi l√† limit nh∆∞ v·∫≠y, nh∆∞ng b·∫°n v·∫´n c√≥ th·ªÉ t·∫°o ƒë∆∞·ª£c ·ªü region westeurope. Nh∆∞ng s·∫Ω l·ªói ·ªü francecentral ü§£\nTr∆∞·ªùng h·ª£p n√†y c√≥ v√†i c√°ch:\n s·ª≠a l·∫°i th√†nh public ACI (ko ƒë·∫∑t trong VNET n·ªØa). M·∫∑c d√π ƒëi·ªÅu n√†y c√≥ th·ªÉ khi·∫øn b·∫°n c·∫ßn thay ƒë·ªìi architect, AKS ph·∫£i ƒë∆∞a ra public theo (ko secure) ko cho user deploy tr√™n francecentral ticket cho MS Support (ko nhi·ªÅu hy v·ªçng)  ","href":"/bk/encrypt-azure-aci-notes/","title":"Azure ACI notes"},{"content":"1. G√°n Role cho Identity = ARM B·∫°n c√≥ 1 ACI container, ƒë√£ enable System assigned identity\nB·∫°n mu·ªën g√°n Role READER cho identity tr√™n b·∫±ng ARM Quan tr·ªçng l√† b·∫°n ph·∫£i bi·∫øt c√°ch l·∫•y dc principalId c·ªßa c√°i System assigned identity.\nV√≠ d·ª• sau:\n// add Role to SystemAssigned identity of ACI { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Authorization/roleAssignments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-09-01-preview\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;aciRoleDefinitionName\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[variables(\u0026#39;aciContainerGroupName\u0026#39;)]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;[variables(\u0026#39;aciRoleDefinitionId\u0026#39;)]\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.ContainerInstance/containerGroups\u0026#39;, variables(\u0026#39;aciContainerGroupName\u0026#39;)), \u0026#39;2021-09-01\u0026#39;, \u0026#39;Full\u0026#39;).Identity.principalId]\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;[resourceGroup().id]\u0026#34;, \u0026#34;principalType\u0026#34;: \u0026#34;ServicePrincipal\u0026#34; } } 2. ACI b·ªã terminated ngay sau khi start (ko c√≥ log l·ªói) ACI b·ªã terminated ngay t·ª´ ƒë·∫ßu, ko c√≥ log l·ªói, c·ª© start l·∫°i container ACI l·∫°i b·ªã terminated ngay\n-\u0026gt; Solution:\nx√≥a to√†n b·ªô untagged images tr√™n ACR, d√πng Notepad++ format file Dockerfile v√† entry.sh th√†nh UNIX file. (Notepad++ -\u0026gt; Edit -\u0026gt; EOL Conversion -\u0026gt; Unix LF -\u0026gt; save) Build l·∫°i Docker image, push l√™n ACR.\n3. Kh√°c nhau v·ªÅ limitation gi·ªØa c√°c region Ch√∫ √Ω v·ªÅ ACI in VNET trong francecentral. ACI in VNET t·∫°o t·ª´ ARM b·ªã l·ªói:\nmessage:\u0026quot;The requested resource is not available in the location 'francecentral' at this moment. Please retry with a different resource request or in another location. Resource requested: '1' CPU '4' GB memory 'Linux' OS virtual network\u0026quot;} m·∫∑c d√π Documents ghi r√µ c√≥ support 4CPU-16GB tr√™n francencentral:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-region-availability\nNh∆∞ng b·∫°n v·∫´n s·∫Ω g·∫∑p l·ªói tr√™n khi deploy t·ª´ ARM.\nL√Ω do b√™n MS ƒë∆∞a ra l√† Do limitation: Ko th·ªÉ s·ª≠ d·ª•ng managed-identity trong 1 ACI deploy trong VNET:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-managed-identity#limitations\nM·∫∑c d√π documents ghi l√† limit nh∆∞ v·∫≠y, nh∆∞ng b·∫°n v·∫´n c√≥ th·ªÉ t·∫°o ƒë∆∞·ª£c ·ªü region westeurope. Nh∆∞ng s·∫Ω l·ªói ·ªü francecentral ü§£\nTr∆∞·ªùng h·ª£p n√†y c√≥ v√†i c√°ch:\n s·ª≠a l·∫°i th√†nh public ACI (ko ƒë·∫∑t trong VNET n·ªØa). M·∫∑c d√π ƒëi·ªÅu n√†y c√≥ th·ªÉ khi·∫øn b·∫°n c·∫ßn thay ƒë·ªìi architect, AKS ph·∫£i ƒë∆∞a ra public theo (ko secure) ko cho user deploy tr√™n francecentral ticket cho MS Support (ko nhi·ªÅu hy v·ªçng)  ","href":"/posts/encrypt-azure-aci-notes/","title":"Azure ACI notes"},{"content":"V·∫•n ƒë·ªÅ Khi c√°c b·∫°n t·∫°o Azure Synapse b·∫±ng ARM template. B·∫°n mu·ªën output ra Workspace name, SQL endpoint, Synapse Studio URL,\u0026hellip; nh∆∞ng ko bi·∫øt l√†m sao?\nHi·ªán t∆∞·ª£ng C√≥ th·ªÉ b·∫°n s·∫Ω g·∫∑p l·ªói n√†y:\n{\u0026quot;status\u0026quot;:\u0026quot;Failed\u0026quot;,\u0026quot;error\u0026quot;:{\u0026quot;code\u0026quot;:\u0026quot;DeploymentOutputEvaluationFailed\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;Unable to evaluate template outputs: 'synapseWorkspaceName'. Please see error details and deployment operations. Please see https://aka.ms/arm-debug for usage details.\u0026quot;,\u0026quot;details\u0026quot;:[{\u0026quot;code\u0026quot;:\u0026quot;DeploymentOutputEvaluationFailed\u0026quot;,\u0026quot;target\u0026quot;:\u0026quot;synapseWorkspaceName\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;The template output 'synapseWorkspaceName' is not valid: The language expression property 'name' doesn't exist, available properties are 'apiVersion, location, tags, identity, properties, condition, existing, isConditionTrue, subscriptionId, resourceGroupName, scope, resourceId, referenceApiVersion, isTemplateResource, isAction, provisioningOperation'..\u0026quot;}]}} L√Ω do L·ªói n√†y x·∫£y ra khi b·∫°n ƒë√£ t·∫°o ra Synapse Workspace r·ªìi, nh∆∞ng ph·∫ßn output th√¨ b·∫°n ƒëang code l√†:\noutputs: { synapseWorkspaceName: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Synapse/workspaces\u0026#39;, variables(\u0026#39;synapseWorkspaceName\u0026#39;)), \u0026#39;2021-06-01\u0026#39;, \u0026#39;Full\u0026#39;).name]\u0026#34; } } Nh∆∞ng b·∫°n ko bi·∫øt l√†m sao ƒë·ªÉ th·∫•y dc c√°c available properties? C√≥ th·ªÉ b·∫°n s·∫Ω nghƒ© ƒë·∫øn chuy·ªán d√πng https://resources.azure.com/ ƒë·ªÉ t√¨m available properties, nh∆∞ng ch·∫Øc ch·∫Øn s·∫Ω ko t√¨m th·∫•y v·ªõi tr∆∞·ªùng h·ª£p Azure Synapse. (ƒê∆°n gi·∫£n v√¨ n√≥ ko cung c·∫•p)\nGi·∫£i ph√°p H√£y s·ª≠a code th√†nh:\noutputs: { \u0026#34;synapseWorkspaceName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Synapse/workspaces\u0026#39;, variables(\u0026#39;synapseWorkspaceName\u0026#39;)), \u0026#39;2021-06-01\u0026#39;, \u0026#39;Full\u0026#39;)]\u0026#34; } } V·∫≠y l√† b·∫°n s·∫Ω in ra ƒë∆∞·ª£c 1 object ch·ª©a to√†n available properties ch·ªâ vi·ªác t√¨m c√°i n√†o l√† connectivityEndpoints, r·ªìi s·ª≠a code l·∫°i ƒë·ªÉ output n√≥ ra th√¥i:\noutputs: { \u0026#34;synapseWorkspaceName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Synapse/workspaces\u0026#39;, variables(\u0026#39;synapseWorkspaceName\u0026#39;)), \u0026#39;2021-06-01\u0026#39;, \u0026#39;Full\u0026#39;).properties.connectivityEndpoints.sqlOnDemand]\u0026#34; } } ü§°\n","href":"/bk/azure-notes-synapse-arm-template-output/","title":"Azure Synapse ARM Template Output properties"},{"content":"V·∫•n ƒë·ªÅ Khi c√°c b·∫°n t·∫°o Azure Synapse b·∫±ng ARM template. B·∫°n mu·ªën output ra Workspace name, SQL endpoint, Synapse Studio URL,\u0026hellip; nh∆∞ng ko bi·∫øt l√†m sao?\nHi·ªán t∆∞·ª£ng C√≥ th·ªÉ b·∫°n s·∫Ω g·∫∑p l·ªói n√†y:\n{\u0026quot;status\u0026quot;:\u0026quot;Failed\u0026quot;,\u0026quot;error\u0026quot;:{\u0026quot;code\u0026quot;:\u0026quot;DeploymentOutputEvaluationFailed\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;Unable to evaluate template outputs: 'synapseWorkspaceName'. Please see error details and deployment operations. Please see https://aka.ms/arm-debug for usage details.\u0026quot;,\u0026quot;details\u0026quot;:[{\u0026quot;code\u0026quot;:\u0026quot;DeploymentOutputEvaluationFailed\u0026quot;,\u0026quot;target\u0026quot;:\u0026quot;synapseWorkspaceName\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;The template output 'synapseWorkspaceName' is not valid: The language expression property 'name' doesn't exist, available properties are 'apiVersion, location, tags, identity, properties, condition, existing, isConditionTrue, subscriptionId, resourceGroupName, scope, resourceId, referenceApiVersion, isTemplateResource, isAction, provisioningOperation'..\u0026quot;}]}} L√Ω do L·ªói n√†y x·∫£y ra khi b·∫°n ƒë√£ t·∫°o ra Synapse Workspace r·ªìi, nh∆∞ng ph·∫ßn output th√¨ b·∫°n ƒëang code l√†:\noutputs: { synapseWorkspaceName: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Synapse/workspaces\u0026#39;, variables(\u0026#39;synapseWorkspaceName\u0026#39;)), \u0026#39;2021-06-01\u0026#39;, \u0026#39;Full\u0026#39;).name]\u0026#34; } } Nh∆∞ng b·∫°n ko bi·∫øt l√†m sao ƒë·ªÉ th·∫•y dc c√°c available properties? C√≥ th·ªÉ b·∫°n s·∫Ω nghƒ© ƒë·∫øn chuy·ªán d√πng https://resources.azure.com/ ƒë·ªÉ t√¨m available properties, nh∆∞ng ch·∫Øc ch·∫Øn s·∫Ω ko t√¨m th·∫•y v·ªõi tr∆∞·ªùng h·ª£p Azure Synapse. (ƒê∆°n gi·∫£n v√¨ n√≥ ko cung c·∫•p)\nGi·∫£i ph√°p H√£y s·ª≠a code th√†nh:\noutputs: { \u0026#34;synapseWorkspaceName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Synapse/workspaces\u0026#39;, variables(\u0026#39;synapseWorkspaceName\u0026#39;)), \u0026#39;2021-06-01\u0026#39;, \u0026#39;Full\u0026#39;)]\u0026#34; } } V·∫≠y l√† b·∫°n s·∫Ω in ra ƒë∆∞·ª£c 1 object ch·ª©a to√†n available properties ch·ªâ vi·ªác t√¨m c√°i n√†o l√† connectivityEndpoints, r·ªìi s·ª≠a code l·∫°i ƒë·ªÉ output n√≥ ra th√¥i:\noutputs: { \u0026#34;synapseWorkspaceName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[reference(resourceId(\u0026#39;Microsoft.Synapse/workspaces\u0026#39;, variables(\u0026#39;synapseWorkspaceName\u0026#39;)), \u0026#39;2021-06-01\u0026#39;, \u0026#39;Full\u0026#39;).properties.connectivityEndpoints.sqlOnDemand]\u0026#34; } } ü§°\n","href":"/posts/azure-notes-synapse-arm-template-output/","title":"Azure Synapse ARM Template Output properties"},{"content":"","href":"/tags/aws/","title":"AWS"},{"content":"","href":"/tags/iot/","title":"IoT"},{"content":"1. C√°ch l√†m 1.1. Tr√™n AWS IoT Core, t·∫°o things Go to Manage -\u0026gt; Thing -\u0026gt; Create single thing\nNote to select Auto-generate a new certificate (Recommended)\nDownload all 5 files (Certificate and Key)\nƒê·ªïi t√™n c√°c file ƒë√≥ cho d·ªÖ nh·ªõ, r·ªìi d√πng WinSCP ƒë·ªÉ transfer 5 file ƒë√≥ l√™n RPi c·ªßa b·∫°n\n1.2. Tr√™n AWS IoT Core, t·∫°o policy Goto Secure -\u0026gt; Policies -\u0026gt; Create Policy\nƒëi·ªÅn nh∆∞ h√¨nh sau:\n1.3. Tr√™n AWS IoT Core, ch·ªçn: Secure -\u0026gt; Certificates B·∫°n s·∫Ω th·∫•y ƒë√£ c√≥ s·∫µn 1 certificate nh∆∞ n√†y:\nN√≥ ƒë∆∞·ª£c t·∫°o c√πng l√∫c v·ªõi Thing m√† b·∫°n ƒë√£ t·∫°o\n  Attach policy v·ª´a t·∫°o:\n  Attach thing v·ª´a t·∫°o:\n  1.4. Tr√™n AWS Cognito, t·∫°o Managed Identity Pool Khi t·∫°o identity pool ch√∫ √Ω Enable access to unauthenticated identities:\n2. Test 2.1. Test Sucscribe topic SSH v√†o RPi, t·∫°o file demo-data-transfer.py v·ªõi content nh∆∞ sau:\nimport time from AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClient \u0026#39;\u0026#39;\u0026#39; sudo apt-get update sudo apt-get install python3-pip pip install AWSIoTPythonSDK \u0026#39;\u0026#39;\u0026#39; def helloworld (self, params, packet): print(\u0026#34;Received msg from AWS IOT Core\u0026#34;) print(\u0026#34;Topic: \u0026#34;+ packet.topic) print(\u0026#34;Payload: \u0026#34;, (packet.payload)) myMQTTClient = AWSIoTMQTTClient(\u0026#34;YOUR_THING_NAME\u0026#34;) #random key, if another connection using the same key is opened the previous one is auto closed by AWS IOT myMQTTClient.configureEndpoint(\u0026#34;YOUR_THING_ARN_ENDPOINT\u0026#34;, 8883) myMQTTClient.configureCredentials(\u0026#34;/home/pi/awsiot/AmazonRootCA1.pem\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-private.pem.key\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-certificate.pem.crt\u0026#34;) myMQTTClient.configureOfflinePublishQueueing(-1) # Infinite offline Publish queueing myMQTTClient.configureDrainingFrequency(2) # Draining: 2 Hz myMQTTClient.configureConnectDisconnectTimeout(10) # 10 sec myMQTTClient.configureMQTTOperationTimeout(5) # 5 sec print (\u0026#39;Initiating Realtime Data Transfer From Raspberry Pi...\u0026#39;) myMQTTClient.connect() myMQTTClient.subscribe(\u0026#34;home/helloworld\u0026#34;,1,helloworld) while True: time.sleep(5) # print(\u0026#34;Publish message to AWS IoT Core\u0026#34;) # myMQTTClient.publish( # topic=\u0026#34;home/helloworld2\u0026#34;, # QoS=1, # payload=\u0026#34;{\u0026#39;Message\u0026#39;: \u0026#39;Message by MyRPi\u0026#39;}\u0026#34; # ) S·ª≠a YOUR_THING_NAME th√†nh t√™n c·ªßa thing m√† b·∫°n ƒë√£ t·∫°o tr√™n AWS IoT Core.\nS·ª≠a YOUR_THING_ARN_ENDPOINT b·∫±ng c√°ch v√†o ƒë√¢y ƒë·ªÉ l·∫•y arn endpoint t∆∞∆°ng ·ª©ng v·ªõi things:\nS·ª≠a line n√†y:\nmyMQTTClient.configureCredentials(\u0026#34;/home/pi/awsiot/AmazonRootCA1.pem\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-private.pem.key\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-certificate.pem.crt\u0026#34;) th√†nh c√°c ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c ƒë·∫øn c√°c file Key c·ªßa b·∫°n\nSau khi ƒë√£ s·ª≠a xong. Run c√°c command sau ƒë·ªÉ download c√°c module c·∫ßn thi·∫øt:\nsudo apt-get update sudo apt-get install python3-pip pip install AWSIoTPythonSDK Run code ƒë·ªÉ test th√¥i:\nD√πng AWS IoT Core ƒë·ªÉ publish message Test:\nstep 1-5 b·∫°n th·ª±c hi·ªán publish 1 message l√™n topic home/helloworld\nstep 6 ch√≠nh l√† k·∫øt qu·∫£, RPi v√¨ ƒëang subscribe topic ƒë√≥ n√™n n√≥ ƒë√£ nh·∫≠n ƒë∆∞·ª£c message c·ªßa b·∫°n v√† hi·ªÉn th·ªã ra.\n2.2. Test Publish message to topic v·∫´n l√† file code tr√™n, s·ª≠a th√†nh nh∆∞ sau:\n... # myMQTTClient.subscribe(\u0026#34;home/helloworld\u0026#34;,1,helloworld) # while True: # time.sleep(5) print(\u0026#34;Publish message to AWS IoT Core\u0026#34;) myMQTTClient.publish( topic=\u0026#34;home/helloworld2\u0026#34;, QoS=1, payload=\u0026#34;{\u0026#39;Message\u0026#39;: \u0026#39;Message sent by MyRPi\u0026#39;}\u0026#34; ) V√†o AWS IoT Core Test, Subscribe topic home/helloworld2:\nTr√™n RPi, run code:\nG·∫ßn nh∆∞ ngay l·∫≠p t·ª©c, tr√™n AWS IoT Core, b·∫°n s·∫Ω th·∫•y Message g·ª≠i t·ª´ RPi:\nV·ªÅ c∆° b·∫£n th√¨ ch·ªâ c√≥ v·∫≠y th√¥i. C·∫ßn k·∫øt h·ª£p v·ªõi vi·ªác config ph·∫ßn c·ª©ng tr√™n RPi, c·∫£m bi·∫øn c√°c th·ª© n·ªØa.\nREFERENCES https://www.youtube.com/watch?v=kPLafcrng-c\n","href":"/bk/encrypt-simple-demo-data-transfer-between-raspberry-and-aws-iotcore/","title":"Simple Demo: Data transfer between Raspberry Pi and AWS IoT Core"},{"content":"1. C√°ch l√†m 1.1. Tr√™n AWS IoT Core, t·∫°o things Go to Manage -\u0026gt; Thing -\u0026gt; Create single thing\nNote to select Auto-generate a new certificate (Recommended)\nDownload all 5 files (Certificate and Key)\nƒê·ªïi t√™n c√°c file ƒë√≥ cho d·ªÖ nh·ªõ, r·ªìi d√πng WinSCP ƒë·ªÉ transfer 5 file ƒë√≥ l√™n RPi c·ªßa b·∫°n\n1.2. Tr√™n AWS IoT Core, t·∫°o policy Goto Secure -\u0026gt; Policies -\u0026gt; Create Policy\nƒëi·ªÅn nh∆∞ h√¨nh sau:\n1.3. Tr√™n AWS IoT Core, ch·ªçn: Secure -\u0026gt; Certificates B·∫°n s·∫Ω th·∫•y ƒë√£ c√≥ s·∫µn 1 certificate nh∆∞ n√†y:\nN√≥ ƒë∆∞·ª£c t·∫°o c√πng l√∫c v·ªõi Thing m√† b·∫°n ƒë√£ t·∫°o\n  Attach policy v·ª´a t·∫°o:\n  Attach thing v·ª´a t·∫°o:\n  1.4. Tr√™n AWS Cognito, t·∫°o Managed Identity Pool Khi t·∫°o identity pool ch√∫ √Ω Enable access to unauthenticated identities:\n2. Test 2.1. Test Sucscribe topic SSH v√†o RPi, t·∫°o file demo-data-transfer.py v·ªõi content nh∆∞ sau:\nimport time from AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClient \u0026#39;\u0026#39;\u0026#39; sudo apt-get update sudo apt-get install python3-pip pip install AWSIoTPythonSDK \u0026#39;\u0026#39;\u0026#39; def helloworld (self, params, packet): print(\u0026#34;Received msg from AWS IOT Core\u0026#34;) print(\u0026#34;Topic: \u0026#34;+ packet.topic) print(\u0026#34;Payload: \u0026#34;, (packet.payload)) myMQTTClient = AWSIoTMQTTClient(\u0026#34;YOUR_THING_NAME\u0026#34;) #random key, if another connection using the same key is opened the previous one is auto closed by AWS IOT myMQTTClient.configureEndpoint(\u0026#34;YOUR_THING_ARN_ENDPOINT\u0026#34;, 8883) myMQTTClient.configureCredentials(\u0026#34;/home/pi/awsiot/AmazonRootCA1.pem\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-private.pem.key\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-certificate.pem.crt\u0026#34;) myMQTTClient.configureOfflinePublishQueueing(-1) # Infinite offline Publish queueing myMQTTClient.configureDrainingFrequency(2) # Draining: 2 Hz myMQTTClient.configureConnectDisconnectTimeout(10) # 10 sec myMQTTClient.configureMQTTOperationTimeout(5) # 5 sec print (\u0026#39;Initiating Realtime Data Transfer From Raspberry Pi...\u0026#39;) myMQTTClient.connect() myMQTTClient.subscribe(\u0026#34;home/helloworld\u0026#34;,1,helloworld) while True: time.sleep(5) # print(\u0026#34;Publish message to AWS IoT Core\u0026#34;) # myMQTTClient.publish( # topic=\u0026#34;home/helloworld2\u0026#34;, # QoS=1, # payload=\u0026#34;{\u0026#39;Message\u0026#39;: \u0026#39;Message by MyRPi\u0026#39;}\u0026#34; # ) S·ª≠a YOUR_THING_NAME th√†nh t√™n c·ªßa thing m√† b·∫°n ƒë√£ t·∫°o tr√™n AWS IoT Core.\nS·ª≠a YOUR_THING_ARN_ENDPOINT b·∫±ng c√°ch v√†o ƒë√¢y ƒë·ªÉ l·∫•y arn endpoint t∆∞∆°ng ·ª©ng v·ªõi things:\nS·ª≠a line n√†y:\nmyMQTTClient.configureCredentials(\u0026#34;/home/pi/awsiot/AmazonRootCA1.pem\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-private.pem.key\u0026#34;, \u0026#34;/home/pi/awsiot/YOUR_THING_NAME-certificate.pem.crt\u0026#34;) th√†nh c√°c ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c ƒë·∫øn c√°c file Key c·ªßa b·∫°n\nSau khi ƒë√£ s·ª≠a xong. Run c√°c command sau ƒë·ªÉ download c√°c module c·∫ßn thi·∫øt:\nsudo apt-get update sudo apt-get install python3-pip pip install AWSIoTPythonSDK Run code ƒë·ªÉ test th√¥i:\nD√πng AWS IoT Core ƒë·ªÉ publish message Test:\nstep 1-5 b·∫°n th·ª±c hi·ªán publish 1 message l√™n topic home/helloworld\nstep 6 ch√≠nh l√† k·∫øt qu·∫£, RPi v√¨ ƒëang subscribe topic ƒë√≥ n√™n n√≥ ƒë√£ nh·∫≠n ƒë∆∞·ª£c message c·ªßa b·∫°n v√† hi·ªÉn th·ªã ra.\n2.2. Test Publish message to topic v·∫´n l√† file code tr√™n, s·ª≠a th√†nh nh∆∞ sau:\n... # myMQTTClient.subscribe(\u0026#34;home/helloworld\u0026#34;,1,helloworld) # while True: # time.sleep(5) print(\u0026#34;Publish message to AWS IoT Core\u0026#34;) myMQTTClient.publish( topic=\u0026#34;home/helloworld2\u0026#34;, QoS=1, payload=\u0026#34;{\u0026#39;Message\u0026#39;: \u0026#39;Message sent by MyRPi\u0026#39;}\u0026#34; ) V√†o AWS IoT Core Test, Subscribe topic home/helloworld2:\nTr√™n RPi, run code:\nG·∫ßn nh∆∞ ngay l·∫≠p t·ª©c, tr√™n AWS IoT Core, b·∫°n s·∫Ω th·∫•y Message g·ª≠i t·ª´ RPi:\nV·ªÅ c∆° b·∫£n th√¨ ch·ªâ c√≥ v·∫≠y th√¥i. C·∫ßn k·∫øt h·ª£p v·ªõi vi·ªác config ph·∫ßn c·ª©ng tr√™n RPi, c·∫£m bi·∫øn c√°c th·ª© n·ªØa.\nREFERENCES https://www.youtube.com/watch?v=kPLafcrng-c\n","href":"/posts/encrypt-simple-demo-data-transfer-between-raspberry-and-aws-iotcore/","title":"Simple Demo: Data transfer between Raspberry Pi and AWS IoT Core"},{"content":"M√†y m√≤ v·ªçc v·∫°ch v·ªÅ Raspberry Pi 4B/8GB 1 ch√∫t.\nV·∫•n ƒë·ªÅ Ban ƒë·∫ßu m√¨nh mu·ªën connect wifi cho RPi th√¨ ph·∫£i flash OS v√†o th·∫ª microSD, c·∫Øm b√†n ph√≠m, m√†n h√¨nh, r·ªìi c·∫Øm d√¢y ngu·ªìn.\nSau ƒë√≥ v√†o s·ª≠a 1 s·ªë file config ƒë·ªÉ connect wifi. ƒêi·ªÅu n√†y khi·∫øn m√¨nh th·∫•y lo·∫±ng ngo·∫±ng v√† t·ªën th·ªùi gian.\nM·ª•c ti√™u V·ªõi nhu c·∫ßu b·∫£n th√¢n ch·ªâ c·∫ßn remote t·ª´ laptop SSH v√†o RPi (via Putty), ko c·∫ßn RPi c√≥ desktop. Th·∫ø n√™n m√¨nh mu·ªën m·ªói khi m√¨nh flash OS v√†o th·∫ª microSD, th·∫ª SD ƒë√≥ s·∫Ω config h·∫øt m·ªçi th·ª© s·∫µn. M√¨nh ch·ªâ c·∫ßn c·∫Øm ngu·ªìn v√†o RPi, sau ƒë√≥ l√† m√¨nh c√≥ th·ªÉ t·ª´ laptop SSH v√†o RPi ƒë∆∞·ª£c lu√¥n (via Putty).\nC√°ch l√†m 1. Ubuntu Server OS 20.04 LTS (no desktop environment) Vi·ªác setup cho connect wifi tr√™n Raspian OS th√¨ c√≥ l·∫Ω ƒë√£ c√≥ nhi·ªÅu b√†i vi·∫øt tr√™n m·∫°ng r·ªìi, nh∆∞ng d√†nh cho Ubuntu OS th√¨ c≈©ng c√≥ nh∆∞ng m√¨nh l√†m theo th√¨ h·∫ßu nh∆∞ kh√¥ng th√†nh c√¥ng.\nM√¨nh vi·∫øt b√†i n√†y ƒë·ªÉ note l·∫°i c√°c steps c·∫ßn l√†m v√† ch√∫ √Ω trong qu√° tr√¨nh setup.\n1.1. Format microSd by Raspberry Pi Imager version \u0026gt;= 1.7.1 link download: https://www.raspberrypi.com/software/\nChu·∫©n b·ªã 1 th·∫ª nh·ªõ microSD, c·∫Øm v√†o Laptop.\nD√πng ch·ª©c nƒÉng ERASE ƒë·ªÉ format th·∫ª nh·ªõ sang fat32 tr∆∞·ªõc:\n1.2. Select OS Sau khi format xong, quay l·∫°i ch·ªçn OS Ubuntu Server 20.04 LTS, 64 bit.\n1.3. Config Advanced options Ch·ªçn Advanced Option, set hostname, enable SSH, set ssh public key (ƒë·ªÉ cho secure th√¨ n√™n disable ch·ª©c nƒÉng authen via password), configure WIFI, password wifi, wifi country\u0026hellip;\n1.4. Process Click button Write ƒë·ªÉ ti·∫øn h√†nh Flash OS v√†o th·∫ª microSD\n1.5. Re-insert microSD to Laptop Sau khi RP Imager done, th√°o th·∫ª microSD ra. C·∫Øm l·∫°i v√†o laptop.\n1.6. Edit user-data file T√¨m ƒë·∫øn directory c·ªßa microSD: system-boot.B·∫°n s·∫Ω t√¨m 2 file sau: user-data, v√† network-config\n2 file ƒë∆∞·ª£c generated b·ªüi nh∆∞ng th√¥ng tin m√† b·∫°n ƒë√£ input v√†o Raspberry Pi Imager ·ªü step 3\nN·ªôi dung file network-config s·∫Ω t∆∞∆°ng t·ª± nh∆∞ n√†y:\nversion: 2 wifis: renderer: networkd wlan0: dhcp4: true optional: true access-points: \u0026quot;okThisIsMyWifi\u0026quot;: password: \u0026quot;ced8123jka92312j332kj4b1j4b4jk14hbg3a6181234nkjn\u0026quot; -\u0026gt; b·∫°n ko c·∫ßn ph·∫£i s·ª≠a g√¨ file network-config n√†y. Password c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c m√£ h√≥a.\nN·ªôi dung file user-data s·∫Ω t∆∞∆°ng t·ª± nh∆∞ n√†y:\n#cloud-config hostname: ubuntu-rpi manage_etc_hosts: true packages: - avahi-daemon apt: conf: | Acquire { Check-Date \u0026quot;false\u0026quot;; }; users: - name: ubuntu groups: users,adm,dialout,audio,netdev,video,plugdev,cdrom,games,input,gpio,spi,i2c,render,sudo shell: /bin/bash lock_passwd: true ssh_authorized_keys: - ssh-rsa AAAABkjasbjkfkjdnvlvnjkdncsdn+w3Y0g24ieofnodfcndlkcfnmefjewnfd/ldmfjkwldsmvkvnwdlvn+pakovnweojfnwefojwefnjkenfdoeD9= sudo: ALL=(ALL) NOPASSWD:ALL runcmd: - sed -i 's/^s*REGDOMAIN=S*/REGDOMAIN=VN/' /etc/default/crda || true -\u0026gt; b·∫°n c·∫ßn s·ª≠a file user-data n√†y, add th√™m d√≤ng sau:\n.... runcmd: - sed -i 's/^s*REGDOMAIN=S*/REGDOMAIN=VN/' /etc/default/crda || true - netplan apply # Reboot after cloud-init completes power_state: mode: reboot Nh∆∞ b·∫°n th·∫•y, m√¨nh add th√™m command netplan apply v√† ph·∫ßn reboot ƒë·ªÉ RPi c·ªßa m√¨nh s·∫Ω reboot ·ªü step cu·ªëi c√πng.\nV·∫≠y l√† xong ph·∫ßn chu·∫©n b·ªã cho th·∫ª microSD. Eject n√≥ ra kh·ªèi Laptop, gi·ªù b·∫°n c√≥ th·ªÉ c·∫Øm n√≥ v√†o RPi r·ªìi.\n1.7. Setup RPi C·∫Øm th·∫ª nh·ªõ microSD v√†o RPi, c·∫Øm HDMI (c√°i n√†y t√πy b·∫°n, nh∆∞ng c·∫Øm HDMI th√¨ b·∫°n s·∫Ω theo d√µi ƒë∆∞·ª£c qu√° tr√¨nh bootstrap c·ªßa OS, n·∫øu ko theo d√µi th√¨ b·∫°n ch·ªù 3-4 ph√∫t r·ªìi quay l·∫°i)\nC·∫Øm d√¢y ngu·ªìn v√†o RPi.\nM√†n h√¨nh c·ªßa RPi m√† hi·ªán nh∆∞ n√†y nghƒ©a l√† ƒë√£ xong:\n1.8. SSH to RPi Gi·ªù b·∫°n ch·ªâ c·∫ßn t√¨m xem ip c·ªßa RPi l√† g√¨, r·ªìi d√πng Putty SSH v√†o th√¥i. Th∆∞·ªùng th√¨ ƒë·ªÉ check ip c·ªßa RPi m√¨nh v√†o 192.168.1.1 ƒë·ªÉ ƒëƒÉng nh·∫≠p v√†o giao di·ªán c·ªßa modem internet.\n1.9. Ch√∫ √Ω  Ngo√†i ra, n·∫øu b·∫°n ko mu·ªën d√πng wifi, m·ªõi ƒë·∫ßu Ethernet b·ªã disable (ko c√≥ ƒë√®n xanh), ph·∫£i run command sau: \u0026ldquo;sudo dhclient -v\u0026rdquo;. Th√¨ s·∫Ω th·∫•y c·ªïng Ethernet s√°ng ƒë√®n xanh. B·∫°n c√≥ th·ªÉ add command ƒë√≥ v√†o file user-data ƒë·ªÉ n√≥ enable Ethernet ngay t·ª´ ƒë·∫ßu  2. Raspian OS (no desktop environment) 2.1. Initial setup V·ªõi OS n√†y th√¨ ƒë∆°n gi·∫£n h∆°n v√¨ t√†i li·ªáu tr√™n m·∫°ng r·∫•t ƒë·∫ßy ƒë·ªß\n Sau khi flash image Raspian OS without desktop v√†o microSD, th√°o ra insert l·∫°i v√†o Laptop T·∫°o 2 file sau: SSH v√† wpa_supplicant.conf trong root directory c·ªßa microSD File SSH ko c√≥ content File wpa_supplicant.conf c√≥ content nh∆∞ sau:  country=vn update_config=1 ctrl_interface=/var/run/wpa_supplicant network={ scan_ssid=1 ssid=\u0026quot;okThisIsMyWifi\u0026quot; psk=\u0026quot;12345678\u0026quot; }  C·∫Øm microSD v√†o RPi, t√¨m IP c·ªßa n√≥, r·ªìi SSH b·∫±ng Putty v√†o v·ªõi user m·∫∑c ƒë·ªãnh: pi, pass: raspberry. SSH dc r·ªìi th√¨ ƒë·ªïi pass ƒëi nh√©.  2.2. Setup static IP B√¨nh th∆∞·ªùng m·ªói khi m·∫•t m·∫°ng, stop/start RPi kh·∫£ nƒÉng RPi s·∫Ω b·ªã thay ƒë·ªïi internal IP\nB·∫°n mu·ªën c·ªë ƒë·ªãnh IP c·ªßa RPi l√† 192.168.1.200 m·ªói khi nh∆∞ v·∫≠y c·∫ßn SSH v√†o RPi r·ªìi s·ª≠a ·ªü ƒë√¢y /etc/dhcpcd.conf:\n... # Example static IP configuration: ... # Hoang: set static IP BEGIN # setting for ethernet (WIFI) interface eth0 static ip_address=192.168.1.200/24 static routers=192.168.1.1 static domain_name_servers=192.168.1.1 8.8.8.8 # setting for wired (LAN) interface wlan0 static ip_address=192.168.1.200/24 static routers=192.168.1.1 static domain_name_servers=192.168.1.1 8.8.8.8 # Hoang: END ... ch√∫ √Ω c√≥ ph·∫ßn s·ª≠a:\neth0 l√† config khi d√πng m·∫°ng WIFI (Ethernet ko d√¢y)\nwlan0l√† config khi d√πng m·∫°ng d√¢y (Wired)\nstatic routers= l√† config IP c·ªßa Router nh√† b·∫°n (th∆∞·ªùng l√† 192.168.1.1)\nGi·ªù reboot RPi ƒë·ªÉ apply thay ƒë·ªïi:\nsudo reboot Done, SSH to 192.168.1.200 to check again!\nCh√∫ √Ω Trong tr∆∞·ªùng h·ª£p b·∫°n c·∫ßn remote (d√πng VNC) v√†o desktop c·ªßa RPi ch·∫°y Raspian OS with desptop environment:\n N√™n enable SSH tr∆∞·ªõc D√πng Putty SSH v√†o rPi, run command vncserver ƒë·ªÉ enable VNC server S·∫Ω nh√¨n th·∫•y output c·ªßa command tr√™n ki·ªÉu:  New desktop is raspberrypi:1 (192.168.1.4:1)  T·∫£i v√† c√†i ƒë·∫∑t tr√™n laptop ph·∫ßn m·ªÅm: VNC viewer D√πng VNC connect v√†o ip: 192.168.1.4:1  Bonus Kh√¥ng n√™n r√∫t d√¢y ngu·ªìn c·ªßa RPi ƒë·ªôt ng·ªôt, m√† n√™n shutdown OS tr∆∞·ªõc:\nCommand to shutdown safely via SSH, Raspian OS:\nsudo shutdown -h now sudo shutdown -h 2 # Reboot: sudo shutdown -r now Check model of RPi on Raspian OS:\npi@raspberrypi:~ $ cat /proc/cpuinfo | grep Model References https://sergejskozlovics.medium.com/how-to-set-up-a-wireless-ubuntu-server-on-raspberry-pi-89b84dca34d2\nhttps://forums.raspberrypi.com/viewtopic.php?t=258832\nhttps://raspberrypi.vn/thiet-lap-dia-chi-ip-tinh-cho-raspberry-147.pi\n","href":"/bk/setup-auto-connect-wifi-raspberrypi-ubuntu-raspian-os/","title":"Raspberry Pi: Setup auto connect wifi at first boots for Ubuntu OS, Raspian OS"},{"content":"M√†y m√≤ v·ªçc v·∫°ch v·ªÅ Raspberry Pi 4B/8GB 1 ch√∫t.\nV·∫•n ƒë·ªÅ Ban ƒë·∫ßu m√¨nh mu·ªën connect wifi cho RPi th√¨ ph·∫£i flash OS v√†o th·∫ª microSD, c·∫Øm b√†n ph√≠m, m√†n h√¨nh, r·ªìi c·∫Øm d√¢y ngu·ªìn.\nSau ƒë√≥ v√†o s·ª≠a 1 s·ªë file config ƒë·ªÉ connect wifi. ƒêi·ªÅu n√†y khi·∫øn m√¨nh th·∫•y lo·∫±ng ngo·∫±ng v√† t·ªën th·ªùi gian.\nM·ª•c ti√™u V·ªõi nhu c·∫ßu b·∫£n th√¢n ch·ªâ c·∫ßn remote t·ª´ laptop SSH v√†o RPi (via Putty), ko c·∫ßn RPi c√≥ desktop. Th·∫ø n√™n m√¨nh mu·ªën m·ªói khi m√¨nh flash OS v√†o th·∫ª microSD, th·∫ª SD ƒë√≥ s·∫Ω config h·∫øt m·ªçi th·ª© s·∫µn. M√¨nh ch·ªâ c·∫ßn c·∫Øm ngu·ªìn v√†o RPi, sau ƒë√≥ l√† m√¨nh c√≥ th·ªÉ t·ª´ laptop SSH v√†o RPi ƒë∆∞·ª£c lu√¥n (via Putty).\nC√°ch l√†m 1. Ubuntu Server OS 20.04 LTS (no desktop environment) Vi·ªác setup cho connect wifi tr√™n Raspian OS th√¨ c√≥ l·∫Ω ƒë√£ c√≥ nhi·ªÅu b√†i vi·∫øt tr√™n m·∫°ng r·ªìi, nh∆∞ng d√†nh cho Ubuntu OS th√¨ c≈©ng c√≥ nh∆∞ng m√¨nh l√†m theo th√¨ h·∫ßu nh∆∞ kh√¥ng th√†nh c√¥ng.\nM√¨nh vi·∫øt b√†i n√†y ƒë·ªÉ note l·∫°i c√°c steps c·∫ßn l√†m v√† ch√∫ √Ω trong qu√° tr√¨nh setup.\n1.1. Format microSd by Raspberry Pi Imager version \u0026gt;= 1.7.1 link download: https://www.raspberrypi.com/software/\nChu·∫©n b·ªã 1 th·∫ª nh·ªõ microSD, c·∫Øm v√†o Laptop.\nD√πng ch·ª©c nƒÉng ERASE ƒë·ªÉ format th·∫ª nh·ªõ sang fat32 tr∆∞·ªõc:\n1.2. Select OS Sau khi format xong, quay l·∫°i ch·ªçn OS Ubuntu Server 20.04 LTS, 64 bit.\n1.3. Config Advanced options Ch·ªçn Advanced Option, set hostname, enable SSH, set ssh public key (ƒë·ªÉ cho secure th√¨ n√™n disable ch·ª©c nƒÉng authen via password), configure WIFI, password wifi, wifi country\u0026hellip;\n1.4. Process Click button Write ƒë·ªÉ ti·∫øn h√†nh Flash OS v√†o th·∫ª microSD\n1.5. Re-insert microSD to Laptop Sau khi RP Imager done, th√°o th·∫ª microSD ra. C·∫Øm l·∫°i v√†o laptop.\n1.6. Edit user-data file T√¨m ƒë·∫øn directory c·ªßa microSD: system-boot.B·∫°n s·∫Ω t√¨m 2 file sau: user-data, v√† network-config\n2 file ƒë∆∞·ª£c generated b·ªüi nh∆∞ng th√¥ng tin m√† b·∫°n ƒë√£ input v√†o Raspberry Pi Imager ·ªü step 3\nN·ªôi dung file network-config s·∫Ω t∆∞∆°ng t·ª± nh∆∞ n√†y:\nversion: 2 wifis: renderer: networkd wlan0: dhcp4: true optional: true access-points: \u0026quot;okThisIsMyWifi\u0026quot;: password: \u0026quot;ced8123jka92312j332kj4b1j4b4jk14hbg3a6181234nkjn\u0026quot; -\u0026gt; b·∫°n ko c·∫ßn ph·∫£i s·ª≠a g√¨ file network-config n√†y. Password c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c m√£ h√≥a.\nN·ªôi dung file user-data s·∫Ω t∆∞∆°ng t·ª± nh∆∞ n√†y:\n#cloud-config hostname: ubuntu-rpi manage_etc_hosts: true packages: - avahi-daemon apt: conf: | Acquire { Check-Date \u0026quot;false\u0026quot;; }; users: - name: ubuntu groups: users,adm,dialout,audio,netdev,video,plugdev,cdrom,games,input,gpio,spi,i2c,render,sudo shell: /bin/bash lock_passwd: true ssh_authorized_keys: - ssh-rsa AAAABkjasbjkfkjdnvlvnjkdncsdn+w3Y0g24ieofnodfcndlkcfnmefjewnfd/ldmfjkwldsmvkvnwdlvn+pakovnweojfnwefojwefnjkenfdoeD9= sudo: ALL=(ALL) NOPASSWD:ALL runcmd: - sed -i 's/^s*REGDOMAIN=S*/REGDOMAIN=VN/' /etc/default/crda || true -\u0026gt; b·∫°n c·∫ßn s·ª≠a file user-data n√†y, add th√™m d√≤ng sau:\n.... runcmd: - sed -i 's/^s*REGDOMAIN=S*/REGDOMAIN=VN/' /etc/default/crda || true - netplan apply # Reboot after cloud-init completes power_state: mode: reboot Nh∆∞ b·∫°n th·∫•y, m√¨nh add th√™m command netplan apply v√† ph·∫ßn reboot ƒë·ªÉ RPi c·ªßa m√¨nh s·∫Ω reboot ·ªü step cu·ªëi c√πng.\nV·∫≠y l√† xong ph·∫ßn chu·∫©n b·ªã cho th·∫ª microSD. Eject n√≥ ra kh·ªèi Laptop, gi·ªù b·∫°n c√≥ th·ªÉ c·∫Øm n√≥ v√†o RPi r·ªìi.\n1.7. Setup RPi C·∫Øm th·∫ª nh·ªõ microSD v√†o RPi, c·∫Øm HDMI (c√°i n√†y t√πy b·∫°n, nh∆∞ng c·∫Øm HDMI th√¨ b·∫°n s·∫Ω theo d√µi ƒë∆∞·ª£c qu√° tr√¨nh bootstrap c·ªßa OS, n·∫øu ko theo d√µi th√¨ b·∫°n ch·ªù 3-4 ph√∫t r·ªìi quay l·∫°i)\nC·∫Øm d√¢y ngu·ªìn v√†o RPi.\nM√†n h√¨nh c·ªßa RPi m√† hi·ªán nh∆∞ n√†y nghƒ©a l√† ƒë√£ xong:\n1.8. SSH to RPi Gi·ªù b·∫°n ch·ªâ c·∫ßn t√¨m xem ip c·ªßa RPi l√† g√¨, r·ªìi d√πng Putty SSH v√†o th√¥i. Th∆∞·ªùng th√¨ ƒë·ªÉ check ip c·ªßa RPi m√¨nh v√†o 192.168.1.1 ƒë·ªÉ ƒëƒÉng nh·∫≠p v√†o giao di·ªán c·ªßa modem internet.\n1.9. Ch√∫ √Ω  Ngo√†i ra, n·∫øu b·∫°n ko mu·ªën d√πng wifi, m·ªõi ƒë·∫ßu Ethernet b·ªã disable (ko c√≥ ƒë√®n xanh), ph·∫£i run command sau: \u0026ldquo;sudo dhclient -v\u0026rdquo;. Th√¨ s·∫Ω th·∫•y c·ªïng Ethernet s√°ng ƒë√®n xanh. B·∫°n c√≥ th·ªÉ add command ƒë√≥ v√†o file user-data ƒë·ªÉ n√≥ enable Ethernet ngay t·ª´ ƒë·∫ßu  2. Raspian OS (no desktop environment) 2.1. Initial setup V·ªõi OS n√†y th√¨ ƒë∆°n gi·∫£n h∆°n v√¨ t√†i li·ªáu tr√™n m·∫°ng r·∫•t ƒë·∫ßy ƒë·ªß\n Sau khi flash image Raspian OS without desktop v√†o microSD, th√°o ra insert l·∫°i v√†o Laptop T·∫°o 2 file sau: SSH v√† wpa_supplicant.conf trong root directory c·ªßa microSD File SSH ko c√≥ content File wpa_supplicant.conf c√≥ content nh∆∞ sau:  country=vn update_config=1 ctrl_interface=/var/run/wpa_supplicant network={ scan_ssid=1 ssid=\u0026quot;okThisIsMyWifi\u0026quot; psk=\u0026quot;12345678\u0026quot; }  C·∫Øm microSD v√†o RPi, t√¨m IP c·ªßa n√≥, r·ªìi SSH b·∫±ng Putty v√†o v·ªõi user m·∫∑c ƒë·ªãnh: pi, pass: raspberry. SSH dc r·ªìi th√¨ ƒë·ªïi pass ƒëi nh√©.  2.2. Setup static IP B√¨nh th∆∞·ªùng m·ªói khi m·∫•t m·∫°ng, stop/start RPi kh·∫£ nƒÉng RPi s·∫Ω b·ªã thay ƒë·ªïi internal IP\nB·∫°n mu·ªën c·ªë ƒë·ªãnh IP c·ªßa RPi l√† 192.168.1.200 m·ªói khi nh∆∞ v·∫≠y c·∫ßn SSH v√†o RPi r·ªìi s·ª≠a ·ªü ƒë√¢y /etc/dhcpcd.conf:\n... # Example static IP configuration: ... # Hoang: set static IP BEGIN # setting for ethernet (WIFI) interface eth0 static ip_address=192.168.1.200/24 static routers=192.168.1.1 static domain_name_servers=192.168.1.1 8.8.8.8 # setting for wired (LAN) interface wlan0 static ip_address=192.168.1.200/24 static routers=192.168.1.1 static domain_name_servers=192.168.1.1 8.8.8.8 # Hoang: END ... ch√∫ √Ω c√≥ ph·∫ßn s·ª≠a:\neth0 l√† config khi d√πng m·∫°ng WIFI (Ethernet ko d√¢y)\nwlan0l√† config khi d√πng m·∫°ng d√¢y (Wired)\nstatic routers= l√† config IP c·ªßa Router nh√† b·∫°n (th∆∞·ªùng l√† 192.168.1.1)\nGi·ªù reboot RPi ƒë·ªÉ apply thay ƒë·ªïi:\nsudo reboot Done, SSH to 192.168.1.200 to check again!\nCh√∫ √Ω Trong tr∆∞·ªùng h·ª£p b·∫°n c·∫ßn remote (d√πng VNC) v√†o desktop c·ªßa RPi ch·∫°y Raspian OS with desptop environment:\n N√™n enable SSH tr∆∞·ªõc D√πng Putty SSH v√†o rPi, run command vncserver ƒë·ªÉ enable VNC server S·∫Ω nh√¨n th·∫•y output c·ªßa command tr√™n ki·ªÉu:  New desktop is raspberrypi:1 (192.168.1.4:1)  T·∫£i v√† c√†i ƒë·∫∑t tr√™n laptop ph·∫ßn m·ªÅm: VNC viewer D√πng VNC connect v√†o ip: 192.168.1.4:1  Bonus Kh√¥ng n√™n r√∫t d√¢y ngu·ªìn c·ªßa RPi ƒë·ªôt ng·ªôt, m√† n√™n shutdown OS tr∆∞·ªõc:\nCommand to shutdown safely via SSH, Raspian OS:\nsudo shutdown -h now sudo shutdown -h 2 # Reboot: sudo shutdown -r now Check model of RPi on Raspian OS:\npi@raspberrypi:~ $ cat /proc/cpuinfo | grep Model References https://sergejskozlovics.medium.com/how-to-set-up-a-wireless-ubuntu-server-on-raspberry-pi-89b84dca34d2\nhttps://forums.raspberrypi.com/viewtopic.php?t=258832\nhttps://raspberrypi.vn/thiet-lap-dia-chi-ip-tinh-cho-raspberry-147.pi\n","href":"/posts/setup-auto-connect-wifi-raspberrypi-ubuntu-raspian-os/","title":"Raspberry Pi: Setup auto connect wifi at first boots for Ubuntu OS, Raspian OS"},{"content":"","href":"/tags/bayes/","title":"Bayes"},{"content":"ƒê·ªÅ b√†i  c√≥ 1 cƒÉn b·ªánh c√≥ x√°c su·∫•t 1%. C·ª© 100 ng∆∞·ªùi th√¨ c√≥ 1 ng∆∞·ªùi m·∫Øc b·ªánh. c√≥ 1 lo·∫°i k√≠t x√©t nghi·ªám A c√≥ x√°c su·∫•t l√† 95% d∆∞∆°ng t√≠nh n·∫øu ng∆∞·ªùi x√©t nghi·ªám m·∫Øc b·ªánh. T·ª®c l√† c·ª© 100 ng∆∞·ªùi b·ªã b·ªánh th√¨ c√°i k√≠t n√†y s·∫Ω b√°o d∆∞∆°ng t√≠nh 95 ng∆∞·ªùi. k√≠t A c√≤n c√≥ x√°c su·∫•t l√† 90% √¢m t√≠nh n·∫øu ng∆∞·ªùi x√©t nghi·ªám ko b·ªánh. T·ª©c l√† c·ª© 100 ng∆∞·ªùi kh·ªèe m·∫°nh th√¨ k√≠t s·∫Ω b√°o √¢m t√≠nh 90 ng∆∞·ªùi. c√¢u h·ªèi l√† v·∫≠y th√¨ n·∫øu 1 ng∆∞·ªùi m√† k√≠t x√©t nghi·ªám d∆∞∆°ng t√≠nh th√¨ ng∆∞·ªùi ƒë√≥ c√≥ bao nhi√™u ph·∫ßn trƒÉm b·ªã b·ªánh th·ª±c s·ª±?  T∆∞·ªüng l√† nhi·ªÅu nh∆∞ng s·ª± th·ª±c th√¨ ch·ªâ c√≥ 9% ng∆∞·ªùi ƒë√≥ b·ªã b·ªánh m√† th√¥i.\nGi·∫£i th√≠ch  c√≥ 1 cƒÉn b·ªánh c√≥ x√°c su·∫•t 1%. C·ª© 100 ng∆∞·ªùi th√¨ c√≥ 1 ng∆∞·ªùi m·∫Øc b·ªánh. =\u0026gt; P(A) = 1% c·ª© 100 ng∆∞·ªùi b·ªã b·ªánh th√¨ c√°i k√≠t n√†y s·∫Ω b√°o d∆∞∆°ng t√≠nh 95 ng∆∞·ªùi =\u0026gt; P(B|A) = 95% c·∫ßn t√¨m P(A|B) k·∫øt qu·∫£ cu·ªëi c√πng l√† 9%  CREDIT https://www.youtube.com/watch?v=7DD0MPjlcfI\u0026amp;ab_channel=B%C3%A0iH%E1%BB%8Dc10Ph%C3%BAt\n","href":"/secrets/encrypt-theorem-of-bayes/","title":"ƒê·ªãnh l√Ω Bayes - Credit Youtube: B√†i H·ªçc 10 Ph√∫t"},{"content":"Trong qu√° tr√¨nh config c√†i c·∫Øm, c√≥ th·ªÉ b·∫°n s·∫Ω mu·ªën t·∫°o snapshot/ƒë√≥ng image cho 1 VM n√†o ƒë√≥. Sau n√†y b·∫°n ch·ªâ c·∫ßn t·∫°o l·∫°i VM t·ª´ snapshot ƒë√≥ l√† xong.\nAzure c√≥ t√≠nh nƒÉng Capture - create an image, nh∆∞ng N·∫øu capture b·∫°n s·∫Ω ph·∫£i stop VM, v√† s·∫Ω ko th·ªÉ s·ª≠ d·ª•ng VM ƒë∆∞·ª£c n·ªØa. (qu√° tr√¨nh Capture ƒë√≤i h·ªèi generalise VM v√† x√≥a c√°c th√¥ng tin v·ªÅ user)\nAzure c√≤n c√≥ ch·ª©c nƒÉng Snapshot, b·∫°n ch·ªâ c·∫ßn l√†m tr√™n giao di·ªán l√† c√≥ th·ªÉ snapshot ƒë∆∞·ª£c 1 ·ªï disk b·∫•t k√¨. Vi·ªác n√†y ki·ªÉu nh∆∞ b·∫°n c√≥ 1 b·∫£n backup point-in-time c·ªßa VM hi·ªán c√≥ m√† ko c·∫ßn ph·∫£i stop VM.\n-\u0026gt; Th·∫ø n√™n tr∆∞·ªùng h·ª£p n√†y m√¨nh s·∫Ω s·ª≠ d·ª•ng Snapshot.\nSau khi snapshot 1 VM, c√≥ th·ªÉ b·∫°n s·∫Ω mu·ªën copy snapshot ƒë√≥ l√™n storage account. ƒê·ªÉ t·∫≠p trung 1 ch·ªó (n·∫øu ƒë·ªÉ snapshot c√≥ th·ªÉ b·ªã x√≥a do v√¥ √Ω)\nQu√° tr√¨nh copy n√†y m√¨nh s·∫Ω d√πng az-cli: ch·∫°y tr√™n az-cli 2.32.0 c·ªßa WSL Ubuntu18.04 b·ªã l·ªói Invalid return character or leading space in header: x-ms-copy-source\nPh·∫£i chuy·ªÉn sang ch·∫°y tr√™n 1 Azure VM, c√†i az-cli 2.37.0 r·ªìi ch·∫°y th√¨ ko b·ªã l·ªói n·ªØa. C√°ch ƒë·ªÉ c√†i az-cli 2.37.0 th√¨ nh∆∞ sau:\napt-get update apt-get install ca-certificates curl apt-transport-https lsb-release gnupg -y curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg \u0026gt; /dev/null AZ_REPO=$(lsb_release -cs) echo \u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPOmain\u0026#34; | sudo tee /etc/apt/sources.list.d/azure-cli.list apt-get update apt-get install azure-cli=2.37.0-1~bionic R·ªìi copy l√™n storage account (nh·ªõ chu·∫©n b·ªã storage account tr∆∞·ªõc):\n#Provide the subscription Id where snapshot is created subscriptionId=\u0026#34;YOUR SUBSCRPTION ID\u0026#34; #Provide the name of your resource group where snapshot is created resourceGroupName=\u0026#34;YOUR RG NAME\u0026#34; #Provide the snapshot name  snapshotName=\u0026#34;YOUR EXPECT SNAPSHOT NAME\u0026#34; #Provide Shared Access Signature (SAS) expiry duration in seconds e.g. 3600. #Know more about SAS here: https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-shared-access-signature-part-1 sasExpiryDuration=3600 #Provide storage account name where you want to copy the snapshot.  storageAccountName=\u0026#34;YOUR STORAGE ACCOUNT\u0026#34; #Name of the storage container where the downloaded snapshot will be stored storageContainerName=\u0026#34;YOUR CONTAINER INSIDE STORAGE ACCOUNT\u0026#34; #Provide the key of the storage account where you want to copy snapshot.  storageAccountKey=\u0026#34;STORAGE ACCOUNT KEY\u0026#34; #Provide the name of the VHD file to which snapshot will be copied. destinationVHDFileName=\u0026#34;YOUR EXPECTED DESTINATION FILE NAME\u0026#34;.vhd az account set --subscription $subscriptionId sas=$(az snapshot grant-access --resource-group $resourceGroupName --name $snapshotName --duration-in-seconds $sasExpiryDuration --query [accessSas] -o tsv) az storage blob copy start --destination-blob $destinationVHDFileName --destination-container $storageContainerName --account-name $storageAccountName --account-key $storageAccountKey --source-uri $sas Sau khi c√≥ file VHD r·ªìi, 1 l√∫c n√†o ƒë√≥ b·∫°n mu·ªën t·∫°o ra VM t·ª´ file ƒë√≥ th√¨ c√≥ th·ªÉ tham kh·∫£o file ARM template sau:\n-S·∫º ADD SAU- CREDIT https://docs.microsoft.com/en-us/azure/virtual-machines/scripts/copy-snapshot-to-storage-account\nhttps://stackoverflow.com/questions/63340139/why-does-capturing-an-image-of-a-vm-in-azure-prevent-the-vm-from-being-used\n","href":"/bk/encrypt-azure-copy-snapshot-to-storage-account/","title":"Azure: Copy Snapshot to Storage Account"},{"content":"Trong qu√° tr√¨nh config c√†i c·∫Øm, c√≥ th·ªÉ b·∫°n s·∫Ω mu·ªën t·∫°o snapshot/ƒë√≥ng image cho 1 VM n√†o ƒë√≥. Sau n√†y b·∫°n ch·ªâ c·∫ßn t·∫°o l·∫°i VM t·ª´ snapshot ƒë√≥ l√† xong.\nAzure c√≥ t√≠nh nƒÉng Capture - create an image, nh∆∞ng N·∫øu capture b·∫°n s·∫Ω ph·∫£i stop VM, v√† s·∫Ω ko th·ªÉ s·ª≠ d·ª•ng VM ƒë∆∞·ª£c n·ªØa. (qu√° tr√¨nh Capture ƒë√≤i h·ªèi generalise VM v√† x√≥a c√°c th√¥ng tin v·ªÅ user)\nAzure c√≤n c√≥ ch·ª©c nƒÉng Snapshot, b·∫°n ch·ªâ c·∫ßn l√†m tr√™n giao di·ªán l√† c√≥ th·ªÉ snapshot ƒë∆∞·ª£c 1 ·ªï disk b·∫•t k√¨. Vi·ªác n√†y ki·ªÉu nh∆∞ b·∫°n c√≥ 1 b·∫£n backup point-in-time c·ªßa VM hi·ªán c√≥ m√† ko c·∫ßn ph·∫£i stop VM.\n-\u0026gt; Th·∫ø n√™n tr∆∞·ªùng h·ª£p n√†y m√¨nh s·∫Ω s·ª≠ d·ª•ng Snapshot.\nSau khi snapshot 1 VM, c√≥ th·ªÉ b·∫°n s·∫Ω mu·ªën copy snapshot ƒë√≥ l√™n storage account. ƒê·ªÉ t·∫≠p trung 1 ch·ªó (n·∫øu ƒë·ªÉ snapshot c√≥ th·ªÉ b·ªã x√≥a do v√¥ √Ω)\nQu√° tr√¨nh copy n√†y m√¨nh s·∫Ω d√πng az-cli: ch·∫°y tr√™n az-cli 2.32.0 c·ªßa WSL Ubuntu18.04 b·ªã l·ªói Invalid return character or leading space in header: x-ms-copy-source\nPh·∫£i chuy·ªÉn sang ch·∫°y tr√™n 1 Azure VM, c√†i az-cli 2.37.0 r·ªìi ch·∫°y th√¨ ko b·ªã l·ªói n·ªØa. C√°ch ƒë·ªÉ c√†i az-cli 2.37.0 th√¨ nh∆∞ sau:\napt-get update apt-get install ca-certificates curl apt-transport-https lsb-release gnupg -y curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg \u0026gt; /dev/null AZ_REPO=$(lsb_release -cs) echo \u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPOmain\u0026#34; | sudo tee /etc/apt/sources.list.d/azure-cli.list apt-get update apt-get install azure-cli=2.37.0-1~bionic R·ªìi copy l√™n storage account (nh·ªõ chu·∫©n b·ªã storage account tr∆∞·ªõc):\n#Provide the subscription Id where snapshot is created subscriptionId=\u0026#34;YOUR SUBSCRPTION ID\u0026#34; #Provide the name of your resource group where snapshot is created resourceGroupName=\u0026#34;YOUR RG NAME\u0026#34; #Provide the snapshot name  snapshotName=\u0026#34;YOUR EXPECT SNAPSHOT NAME\u0026#34; #Provide Shared Access Signature (SAS) expiry duration in seconds e.g. 3600. #Know more about SAS here: https://docs.microsoft.com/en-us/azure/storage/storage-dotnet-shared-access-signature-part-1 sasExpiryDuration=3600 #Provide storage account name where you want to copy the snapshot.  storageAccountName=\u0026#34;YOUR STORAGE ACCOUNT\u0026#34; #Name of the storage container where the downloaded snapshot will be stored storageContainerName=\u0026#34;YOUR CONTAINER INSIDE STORAGE ACCOUNT\u0026#34; #Provide the key of the storage account where you want to copy snapshot.  storageAccountKey=\u0026#34;STORAGE ACCOUNT KEY\u0026#34; #Provide the name of the VHD file to which snapshot will be copied. destinationVHDFileName=\u0026#34;YOUR EXPECTED DESTINATION FILE NAME\u0026#34;.vhd az account set --subscription $subscriptionId sas=$(az snapshot grant-access --resource-group $resourceGroupName --name $snapshotName --duration-in-seconds $sasExpiryDuration --query [accessSas] -o tsv) az storage blob copy start --destination-blob $destinationVHDFileName --destination-container $storageContainerName --account-name $storageAccountName --account-key $storageAccountKey --source-uri $sas Sau khi c√≥ file VHD r·ªìi, 1 l√∫c n√†o ƒë√≥ b·∫°n mu·ªën t·∫°o ra VM t·ª´ file ƒë√≥ th√¨ c√≥ th·ªÉ tham kh·∫£o file ARM template sau:\n-S·∫º ADD SAU- CREDIT https://docs.microsoft.com/en-us/azure/virtual-machines/scripts/copy-snapshot-to-storage-account\nhttps://stackoverflow.com/questions/63340139/why-does-capturing-an-image-of-a-vm-in-azure-prevent-the-vm-from-being-used\n","href":"/posts/encrypt-azure-copy-snapshot-to-storage-account/","title":"Azure: Copy Snapshot to Storage Account"},{"content":"","href":"/tags/git/","title":"Git"},{"content":"C√¢u h·ªèi v·ªÅ So s√°nh git rebase v√† git merge ƒë√¥i khi ƒë∆∞·ª£c h·ªèi trong ph·ªèng v·∫•n. B·∫°n n√™n n·∫Øm ƒë∆∞·ª£c s·ª± kh√°c nhau ƒë·ªÉ t·ª± tin h∆°n khi tr·∫£ l·ªùi.\n1. Intro ƒê√¥i khi ch√∫ng ta th·∫•y c√°c Developer d√πng git rebase ƒë·ªÉ merge/l·∫•y c√°c thay ƒë·ªïi t·ª´ nh√°nh kh√°c v·ªÅ.\nNh∆∞ng ch·ªâ n√™n d√πng git rebase tr√™n 1 nh√°nh c·ªßa ri√™ng b·∫°n (ho·∫∑c ch·ªâ m√¨nh b·∫°n l√†m vi·ªác tr√™n nh√°nh ƒë√≥),\nƒë·ª´ng n√™n d√πng git rebase tr√™n 1 nh√°nh m√† c√≥ nhi·ªÅu ng∆∞·ªùi c√πng l√†m vi·ªác nh√©. T·∫°i sao th√¨ ch√∫ng ta s·∫Ω c√πng theo d√µi v√≠ d·ª• sau.\n2. Demo 2.1. Git Merge Tr√™n Gitlab, t·∫°o project git-rebase-merge-test\nTr√™n nh√°nh main t·∫°o file \u0026ldquo;main01\u0026rdquo;:\nCommit log c·ªßa nh√°nh main s·∫Ω nh∆∞ n√†y:\nHi·ªán t·∫°i c√°c b·∫°n ƒëang ·ªü ch·ªó b√¥i s√°ng c·ªßa diagram sau:\nT·∫°i m√°y local, t·∫°o branch m·ªõi l√† dev (git checkout -b dev), t·∫°o file dev01 r·ªìi push l√™n remote:\nV√¨ nh√°nh dev ƒë∆∞·ª£c t·∫°o ra t·ª´ nh√°nh main n√™n n√≥ m·∫∑c ƒë·ªãnh c√≥ 3 commit ban ƒë·∫ßu c·ªßa main, t·∫°i th·ªùi ƒëi·ªÉm n√†y b·∫°n c√≥ th√™m commit \u0026ldquo;add dev01\u0026rdquo;: Tr√™n giao di·ªán GitLab, tr√™n nh√°nh main t·∫°o file main02:\nT·∫°i th·ªùi ƒëi·ªÉm n√†y ·ªü local m√°y b·∫°n c·∫ßn sang nh√°nh main pull code latest v·ªÅ, b·∫°n s·∫Ω th·∫•y nh√°nh main ƒë√£ c√≥ file main02:\ngi·ªù switch sang nh√°nh dev, b·∫°n ti·∫øp t·ª•c edit file README.md r·ªìi push l√™n remote v·ªõi commit \u0026ldquo;dev edit 1 readme\u0026rdquo;:\nTr√™n diagram, b·∫°n ƒëang ·ªü v√πng n√†y:\nGi·ªù b·∫°n ƒë·ª©ng ·ªü nh√°nh dev, th·ª±c hi·ªán merge v·ªõi nh√°nh main b·∫±ng command git merge main (git pull origin main c≈©ng s·∫Ω l√†m nhi·ªám v·ª• t∆∞∆°ng t·ª±):\nb·∫°n mu·ªën file main02 c·ªßa nh√°nh main ƒë∆∞·ª£c xu·∫•t hi·ªán trong nh√°nh dev c·ªßa b·∫°n ƒë√∫ng ko n√†o?\nFile main02 xu·∫•t hi·ªán:\nV·∫´n ƒëang tr√™n nh√°nh dev, b·∫°n t·∫°o th√™m 1 file dev02 r·ªìi push l√™n remote\nCh√∫ √Ω nh√©\nB·∫°n s·∫Ω th·∫•y 1 commit ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông l√† \u0026ldquo;Merge branch main into dev\u0026rdquo; -\u0026gt; ƒë√≥ l√† commit ƒë∆∞·ª£c t·∫°o ra b·ªõi command git merge main:\nTr√™n diagram, b·∫°n ƒëang ·ªü v√πng n√†y:\nV·∫≠y git merge ƒë√£ l√†m g√¨?\n-\u0026gt; git merge ƒë∆∞a ƒë√∫ng c√°c commit v√†o v·ªã tr√≠ c·ªßa n√≥ theo th·ªùi gian:\n Commit \u0026ldquo;Add new file main02\u0026rdquo; ƒë∆∞·ª£c ƒë∆∞a v√†o ƒë√∫ng v·ªã tr√≠ l√† sau \u0026ldquo;add dev01\u0026rdquo;. T·∫•t c·∫£ commit hash id c·ªßa nh√°nh dev ko thay ƒë·ªïi g√¨. T·∫°o th√™m 1 commit (box m√†u h·ªìng) l√†: \u0026ldquo;Merge branch main into dev\u0026rdquo;.  Ti·∫øp theo ta t√¨m hi·ªÉu git rebase s·∫Ω ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o\u0026hellip;\n2.2. Git Rebase Gi·ªù tr√™n giao di·ªán Gitlab, nh√°nh main t·∫°o 1 file m·ªõi \u0026ldquo;main03\u0026rdquo;:\nT·∫°i local, tr√™n nh√°nh dev, t·∫°o file \u0026ldquo;dev03\u0026rdquo;, push l√™n remote. Nh√°nh dev ƒëang ·ªü v√πng s√°ng n√†y trong diagram: edit file README.md, push l√™n remote v·ªõi commit l√† \u0026ldquo;edit readme 2\u0026rdquo;:\nTr√™n m√°y local, switch sang nh√°nh main, pull code latest v·ªÅ\nR·ªìi, gi·ªù switch sang dev, ƒë·ª©ng nh√°nh dev run git rebase main:\nRebase xong b·∫°n s·∫Ω c·∫ßn push thay ƒë·ªïi l√™n nh√°nh dev:\nn·∫øu b·∫°n git push origin dev -\u0026gt; s·∫Ω b√°o l·ªói ngay\nth·∫ø n√™n b·∫°n ph·∫£i push force nh∆∞ sau:\ngit push origin dev -f log commit s·∫Ω nh∆∞ sau:\nB·∫°n nh√¨n log ƒë√≥ h∆°i l·∫° ƒë√∫ng ko, s·ª± thay ƒë·ªïi ƒë∆∞·ª£c m√¥ t·∫£ l·∫°i tr√™n diagram:\nV·∫≠y Git Rebase ƒë√£ l√†m g√¨?\nB·∫°n s·∫Ω th·∫•y d√π ko c√≤n c√°i commit t·ª± ƒë·ªông (b√¥i h·ªìng) ƒë∆∞·ª£c th√™m v√†o n·ªØa nh∆∞ng\n-\u0026gt; git rebase ƒë√£ l√†m:\n ƒê∆∞a c√°c commit c·ªßa nh√°nh dev l√™n tr√™n ƒë·∫ßu, commit c·ªßa nh√°nh main ƒë·∫©y xu·ªëng d∆∞·ªõi. Commit hash id c·ªßa nh√°nh dev b·ªã thay ƒë·ªïi h·∫øt (-\u0026gt; ƒë√¢y l√† l√Ω do b·∫°n c·∫ßn push force).  =\u0026gt; N·∫øu b·∫°n l√†m vi·ªác tr√™n 1 nh√°nh ri√™ng bi·ªát c·ªßa m√¨nh b·∫°n, vi·ªác thay ƒë·ªïi commit hash id n√†y ko v·∫•n ƒë·ªÅ g√¨,\nnh∆∞ng n·∫øu b·∫°n l√†m vi·ªác tr√™n 1 nh√°nh m√† c√≥ nhi·ªÅu ng∆∞·ªùi c√πng l√†m vi·ªác, vi·ªác n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn l√†m m·∫•t code c·ªßa ng∆∞·ªùi kh√°c.\nB√π l·∫°i, commit log c·ªßa nh√°nh dev nh√¨n s·∫Ω clear h∆°n cho b·∫°n, ko c√≥ commit t·ª± ƒë·ªông xen v√†o gi·ªØa.\nDiagram t·ªïng k·∫øt ·ªü ƒë√¢y:\nQuan ƒëi·ªÉm c·ªßa ri√™ng m√¨nh: Ch·ªâ d√πng git rebase tr√™n nh√°nh ri√™ng c·ªßa b·∫°n ƒë·ªÉ l√†m ƒë·∫πp commit log.\nB·ªüi git rebase ghi ƒë√® l·ªãch s·ª≠ commit n√™n n·∫øu b·∫°n t√¥n tr·ªçng l·ªãch s·ª≠ th√¨ ƒë·ª´ng n√™n c·ªë g·∫Øng thay ƒë·ªïi n√≥. H√£y d√πng git merge.\nHy v·ªçng b√†i vi·∫øt v√† diagram tr√™n gi√∫p b·∫°n hi·ªÉu r√µ s·ª± kh√°c bi·ªát ‚úå\nCREDIT https://stackoverflow.com/a/66813076/9922066\n","href":"/bk/git-different-rebase-merge/","title":"Git - Difference between Rebase and Merge"},{"content":"C√¢u h·ªèi v·ªÅ So s√°nh git rebase v√† git merge ƒë√¥i khi ƒë∆∞·ª£c h·ªèi trong ph·ªèng v·∫•n. B·∫°n n√™n n·∫Øm ƒë∆∞·ª£c s·ª± kh√°c nhau ƒë·ªÉ t·ª± tin h∆°n khi tr·∫£ l·ªùi.\n1. Intro ƒê√¥i khi ch√∫ng ta th·∫•y c√°c Developer d√πng git rebase ƒë·ªÉ merge/l·∫•y c√°c thay ƒë·ªïi t·ª´ nh√°nh kh√°c v·ªÅ.\nNh∆∞ng ch·ªâ n√™n d√πng git rebase tr√™n 1 nh√°nh c·ªßa ri√™ng b·∫°n (ho·∫∑c ch·ªâ m√¨nh b·∫°n l√†m vi·ªác tr√™n nh√°nh ƒë√≥),\nƒë·ª´ng n√™n d√πng git rebase tr√™n 1 nh√°nh m√† c√≥ nhi·ªÅu ng∆∞·ªùi c√πng l√†m vi·ªác nh√©. T·∫°i sao th√¨ ch√∫ng ta s·∫Ω c√πng theo d√µi v√≠ d·ª• sau.\n2. Demo 2.1. Git Merge Tr√™n Gitlab, t·∫°o project git-rebase-merge-test\nTr√™n nh√°nh main t·∫°o file \u0026ldquo;main01\u0026rdquo;:\nCommit log c·ªßa nh√°nh main s·∫Ω nh∆∞ n√†y:\nHi·ªán t·∫°i c√°c b·∫°n ƒëang ·ªü ch·ªó b√¥i s√°ng c·ªßa diagram sau:\nT·∫°i m√°y local, t·∫°o branch m·ªõi l√† dev (git checkout -b dev), t·∫°o file dev01 r·ªìi push l√™n remote:\nV√¨ nh√°nh dev ƒë∆∞·ª£c t·∫°o ra t·ª´ nh√°nh main n√™n n√≥ m·∫∑c ƒë·ªãnh c√≥ 3 commit ban ƒë·∫ßu c·ªßa main, t·∫°i th·ªùi ƒëi·ªÉm n√†y b·∫°n c√≥ th√™m commit \u0026ldquo;add dev01\u0026rdquo;: Tr√™n giao di·ªán GitLab, tr√™n nh√°nh main t·∫°o file main02:\nT·∫°i th·ªùi ƒëi·ªÉm n√†y ·ªü local m√°y b·∫°n c·∫ßn sang nh√°nh main pull code latest v·ªÅ, b·∫°n s·∫Ω th·∫•y nh√°nh main ƒë√£ c√≥ file main02:\ngi·ªù switch sang nh√°nh dev, b·∫°n ti·∫øp t·ª•c edit file README.md r·ªìi push l√™n remote v·ªõi commit \u0026ldquo;dev edit 1 readme\u0026rdquo;:\nTr√™n diagram, b·∫°n ƒëang ·ªü v√πng n√†y:\nGi·ªù b·∫°n ƒë·ª©ng ·ªü nh√°nh dev, th·ª±c hi·ªán merge v·ªõi nh√°nh main b·∫±ng command git merge main (git pull origin main c≈©ng s·∫Ω l√†m nhi·ªám v·ª• t∆∞∆°ng t·ª±):\nb·∫°n mu·ªën file main02 c·ªßa nh√°nh main ƒë∆∞·ª£c xu·∫•t hi·ªán trong nh√°nh dev c·ªßa b·∫°n ƒë√∫ng ko n√†o?\nFile main02 xu·∫•t hi·ªán:\nV·∫´n ƒëang tr√™n nh√°nh dev, b·∫°n t·∫°o th√™m 1 file dev02 r·ªìi push l√™n remote\nCh√∫ √Ω nh√©\nB·∫°n s·∫Ω th·∫•y 1 commit ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông l√† \u0026ldquo;Merge branch main into dev\u0026rdquo; -\u0026gt; ƒë√≥ l√† commit ƒë∆∞·ª£c t·∫°o ra b·ªõi command git merge main:\nTr√™n diagram, b·∫°n ƒëang ·ªü v√πng n√†y:\nV·∫≠y git merge ƒë√£ l√†m g√¨?\n-\u0026gt; git merge ƒë∆∞a ƒë√∫ng c√°c commit v√†o v·ªã tr√≠ c·ªßa n√≥ theo th·ªùi gian:\n Commit \u0026ldquo;Add new file main02\u0026rdquo; ƒë∆∞·ª£c ƒë∆∞a v√†o ƒë√∫ng v·ªã tr√≠ l√† sau \u0026ldquo;add dev01\u0026rdquo;. T·∫•t c·∫£ commit hash id c·ªßa nh√°nh dev ko thay ƒë·ªïi g√¨. T·∫°o th√™m 1 commit (box m√†u h·ªìng) l√†: \u0026ldquo;Merge branch main into dev\u0026rdquo;.  Ti·∫øp theo ta t√¨m hi·ªÉu git rebase s·∫Ω ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o\u0026hellip;\n2.2. Git Rebase Gi·ªù tr√™n giao di·ªán Gitlab, nh√°nh main t·∫°o 1 file m·ªõi \u0026ldquo;main03\u0026rdquo;:\nT·∫°i local, tr√™n nh√°nh dev, t·∫°o file \u0026ldquo;dev03\u0026rdquo;, push l√™n remote. Nh√°nh dev ƒëang ·ªü v√πng s√°ng n√†y trong diagram: edit file README.md, push l√™n remote v·ªõi commit l√† \u0026ldquo;edit readme 2\u0026rdquo;:\nTr√™n m√°y local, switch sang nh√°nh main, pull code latest v·ªÅ\nR·ªìi, gi·ªù switch sang dev, ƒë·ª©ng nh√°nh dev run git rebase main:\nRebase xong b·∫°n s·∫Ω c·∫ßn push thay ƒë·ªïi l√™n nh√°nh dev:\nn·∫øu b·∫°n git push origin dev -\u0026gt; s·∫Ω b√°o l·ªói ngay\nth·∫ø n√™n b·∫°n ph·∫£i push force nh∆∞ sau:\ngit push origin dev -f log commit s·∫Ω nh∆∞ sau:\nB·∫°n nh√¨n log ƒë√≥ h∆°i l·∫° ƒë√∫ng ko, s·ª± thay ƒë·ªïi ƒë∆∞·ª£c m√¥ t·∫£ l·∫°i tr√™n diagram:\nV·∫≠y Git Rebase ƒë√£ l√†m g√¨?\nB·∫°n s·∫Ω th·∫•y d√π ko c√≤n c√°i commit t·ª± ƒë·ªông (b√¥i h·ªìng) ƒë∆∞·ª£c th√™m v√†o n·ªØa nh∆∞ng\n-\u0026gt; git rebase ƒë√£ l√†m:\n ƒê∆∞a c√°c commit c·ªßa nh√°nh dev l√™n tr√™n ƒë·∫ßu, commit c·ªßa nh√°nh main ƒë·∫©y xu·ªëng d∆∞·ªõi. Commit hash id c·ªßa nh√°nh dev b·ªã thay ƒë·ªïi h·∫øt (-\u0026gt; ƒë√¢y l√† l√Ω do b·∫°n c·∫ßn push force).  =\u0026gt; N·∫øu b·∫°n l√†m vi·ªác tr√™n 1 nh√°nh ri√™ng bi·ªát c·ªßa m√¨nh b·∫°n, vi·ªác thay ƒë·ªïi commit hash id n√†y ko v·∫•n ƒë·ªÅ g√¨,\nnh∆∞ng n·∫øu b·∫°n l√†m vi·ªác tr√™n 1 nh√°nh m√† c√≥ nhi·ªÅu ng∆∞·ªùi c√πng l√†m vi·ªác, vi·ªác n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn l√†m m·∫•t code c·ªßa ng∆∞·ªùi kh√°c.\nB√π l·∫°i, commit log c·ªßa nh√°nh dev nh√¨n s·∫Ω clear h∆°n cho b·∫°n, ko c√≥ commit t·ª± ƒë·ªông xen v√†o gi·ªØa.\nDiagram t·ªïng k·∫øt ·ªü ƒë√¢y:\nQuan ƒëi·ªÉm c·ªßa ri√™ng m√¨nh: Ch·ªâ d√πng git rebase tr√™n nh√°nh ri√™ng c·ªßa b·∫°n ƒë·ªÉ l√†m ƒë·∫πp commit log.\nB·ªüi git rebase ghi ƒë√® l·ªãch s·ª≠ commit n√™n n·∫øu b·∫°n t√¥n tr·ªçng l·ªãch s·ª≠ th√¨ ƒë·ª´ng n√™n c·ªë g·∫Øng thay ƒë·ªïi n√≥. H√£y d√πng git merge.\nHy v·ªçng b√†i vi·∫øt v√† diagram tr√™n gi√∫p b·∫°n hi·ªÉu r√µ s·ª± kh√°c bi·ªát ‚úå\nCREDIT https://stackoverflow.com/a/66813076/9922066\n","href":"/posts/git-different-rebase-merge/","title":"Git - Difference between Rebase and Merge"},{"content":"V·∫•n ƒë·ªÅ T√¨m hi·ªÉu v·ªÅ Monty Hall problem gi√∫p b·∫°n th·∫•y ƒë∆∞·ª£c s·ª©c m·∫°nh c·ªßa X√°c su·∫•t th·ªëng k√™.\nMonty Hall problem n√≥i v·ªÅ 1 tr√≤ ch∆°i nh∆∞ sau:\n Monty ƒë∆∞a cho b·∫°n 3 c√°nh c·ª≠a (A,B,C), ƒë·∫±ng sau ƒë√≥ l√† 2 con d√™ v√† 1 c√°i √¥ t√¥. B·∫°n ch·ªçn 1 c·ª≠a (gi·∫£ s·ª≠ l√† A). B·∫°n nghƒ© r·∫±ng sau c√°nh c·ª≠a ƒë√≥ l√† c√°i √¥ t√¥. Monty ki·ªÉm tra 2 c√°nh c·ª≠a c√≤n l·∫°i (B,C), v√† m·ªü 1 c√°nh c·ª≠a c√≥ con d√™. C√¢u h·ªèi l√† b·∫°n v·∫´n ch·ªçn c√°nh c·ª≠a A nh∆∞ ban ƒë·∫ßu, hay ƒë·ªïi sang 1 c√°nh c·ª≠a kh√°c? X√°c su·∫•t c√≥ kh√°c nhau ko?  \u0026hellip;\nC√¢u tr·∫£ l·ªùi c√≥ th·ªÉ s·∫Ω l√†m 1 s·ªë ng∆∞·ªùi ng·∫°c nhi√™n l√† x√°c su·∫•t ko ph·∫£i l√† 50-50. N·∫øu b·∫°n ƒë·ªïi sang 1 c√°nh c·ª≠a kh√°c. X√°c su·∫•t b·∫°n ch·ªçn ƒë√∫ng c·ª≠a c√≥ √¥ t√¥ l√† 66 %\nNghe chuy√™n gia gi·∫£i th√≠ch Chuy√™n gia gi·∫£i th√≠ch r·∫±ng:\nN·∫øu b·∫°n ch·ªçn c√°nh c·ª≠a A t·ª´ ƒë·∫ßu, b·∫°n c√≥ 1/3 c∆° h·ªôi ƒë√∫ng.\nNh∆∞ng n·∫øu b·∫°n gi·ªØ nguy√™n l·ª±a ch·ªçn ƒë√≥ sau khi Monty lo·∫°i tr·ª´ 1 c√°nh c·ª≠a sai, b·∫°n s·∫Ω ko th·ªÉ c·∫£i thi·ªán x√°c su·∫•t c·ªßa m√¨nh.\nKhi Monty ch∆∞a lo·∫°i tr·ª´, x√°c su·∫•t c·ªßa 2 c√°nh c·ª≠a c√≤n l·∫°i (B,C) l√† 2/3.\nMonty ki·ªÉm tra ch√∫ng v√† b·ªè 1 c√°nh c·ª≠a sai (gi·∫£ s·ª≠ B), th√¨ c√°nh c·ª≠a c√≤n l·∫°i (C) s·∫Ω c√≥ x√°c su·∫•t l√† 2/3.\nKhi ƒë√≥ vi·ªác ƒë·ªïi sang C s·∫Ω gi√∫p b·∫°n tƒÉng x√°c su·∫•t t·ª´ 1/3 l√™n 2/3.\nB·∫°n hi·ªÉu ch∆∞a?\nD∆∞·ªõi 1 g√≥c nh√¨n kh√°c:\n Gi·∫£ s·ª≠ c√≥ 100 c√°nh c·ª≠a thay v√¨ 3. B·∫°n ch·ªçn 1. Monty ki·ªÉm tra 99 c√°nh c·ª≠a c√≤n l·∫°i, √¥ng ·∫•y m·ªü 98 c√°nh c·ª≠a c√≥ d√™ v√† ch·ªâ ƒë·ªÉ l·∫°i 1 c·ª≠a. B·∫°n v·∫´n gi·ªØ l·ª±a ch·ªçn ban ƒë·∫ßu (v·ªõi c∆° h·ªôi 1/100) hay l√† chuy·ªÉn sang c√°nh c·ª≠a c√≤n l·∫°i (c√°i m√† ƒë√£ ƒë∆∞·ª£c l·ªçc ra trong 99 c√°i) ?  ƒê·∫øn ƒë√¢y c√≥ l·∫Ω b·∫°n ƒë√£ hi·ªÉu r√µ h∆°n. H√†nh ƒë·ªông l·ªçc c·ªßa Monty ƒë√£ gi√∫p b·∫°n. √îng ·∫•y cho b·∫°n ch·ªçn gi·ªØa \u0026ldquo;l·ª±a ch·ªçn ho√†n to√†n ng·∫´u nhi√™n v√† \u0026ldquo;l·ª±a ch·ªçn sau 1 l·∫ßn ch·ªçn l·ªçc\u0026rdquo;.\nL·ª±a ch·ªçn sau 1 l·∫ßn l·ªçc r√µ r√†ng t·ªët h∆°n.\nV·∫≠y khi n√†o x√°c su·∫•t l·ª±a ch·ªçn l√† 50-50 ? ƒê√≥ l√† khi b·∫°n kh√¥ng h·ªÅ c√≥ th√¥ng tin v·ªÅ h√†nh ƒë·ªông c·ªßa Monty.\nT∆∞·ªüng t∆∞·ª£ng r·∫±ng:\n Monty ƒë∆∞a cho b·∫°n 3 c√°nh c·ª≠a (A,B,C), ƒë·∫±ng sau ƒë√≥ l√† 2 con d√™ v√† 1 c√°i √¥ t√¥. B·∫°n ch·ªçn 1 c·ª≠a (gi·∫£ s·ª≠ l√† A). B·∫°n nghƒ© r·∫±ng sau c√°nh c·ª≠a ƒë√≥ l√† c√°i √¥ t√¥. Monty ki·ªÉm tra 2 c√°nh c·ª≠a c√≤n l·∫°i (B,C), v√† m·ªü 1 c√°nh c·ª≠a c√≥ con d√™. ƒê√∫ng l√∫c n√†y 1 ng∆∞·ªùi b·∫°n c·ªßa b·∫°n ƒëi ƒë·∫øn, nh√¨n th·∫•y 2 c√°nh c·ª≠a v√† ko bi·∫øt Monty v·ª´a l√†m g√¨. Anh ta ph·∫£i ch·ªçn 1, x√°c su·∫•t c·ªßa anh ta ch·ªçn ƒë√∫ng l√† 50 %. Nh∆∞ng x√°c su·∫•t n·∫øu b·∫°n ƒë·ªïi c√°nh c·ª≠a sang C l·∫°i l√† 66 %  =\u0026gt; K·∫øt lu·∫≠n l√†: Th√¥ng tin s·∫Ω ·∫£nh h∆∞·ªüng r·∫•t nhi·ªÅu ƒë·∫øn x√°c su·∫•t c·ªßa l·ª±a ch·ªçn\nDemo Khi b·∫°n v·∫´n ph√¢n v√¢n ko hi·ªÉu v·ªÅ b√†i to√°n v√† l·ªùi gi·∫£i tr√™n, c√°ch ƒë∆°n gi·∫£n nh·∫•t ƒë·ªÉ ch·ª©ng minh l√† th·ª±c nghi·ªám.\nH√£y d√πng 1 ƒëo·∫°n code python ƒë·ªÉ th·ª±c nghi·ªám xem x√°c su·∫•t c√≥ ƒë√∫ng l√† tƒÉng l√™n ko?\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; an demostration of Monty Hall problem \u0026#34;\u0026#34;\u0026#34; import random import sys def main(): COUNT = int(sys.argv[1]) success_count_of_switch = 0 success_count_not_switch = 0 time_count = 0 while(time_count \u0026lt; COUNT): # DOOR DOOR = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;] # only one DOOR cover the CAR result = random.choice(DOOR) print(\u0026#34;\u0026#34;) print(\u0026#34;hidden result is: \u0026#34;, result) # player make a first choice my_first_choice = random.choice(DOOR) print(\u0026#34;my first choice is: \u0026#34;, my_first_choice) # round 1, reveal a DOOR that cover a goat reveal1 = random.choice(DOOR) # make sure that reveal is not the door cover a car and not my_first_choice while((reveal1 == result) or (reveal1 == my_first_choice)): reveal1 = random.choice(DOOR) print(\u0026#34;first reveal: \u0026#34;, reveal1) # now DOOR will be subjected  DOOR.remove(reveal1) # player must switch the choice my_second_choice = random.choice(DOOR) # make sure that player switch to another DOOR while(my_second_choice == my_first_choice): my_second_choice = random.choice(DOOR) print(\u0026#34;my second choice is: \u0026#34;, my_second_choice) if (my_second_choice == result): print(\u0026#34;Switch is CORRECT decision!\u0026#34;) success_count_of_switch += 1 elif (my_first_choice == result): print(\u0026#34;Switch is wrong decision!\u0026#34;) success_count_not_switch += 1 time_count += 1 print(\u0026#34;Success percentage of second choice: %d/ %d\u0026#34; % (success_count_of_switch, time_count)) print(\u0026#34;Success percentage of first choice : %d/ %d\u0026#34; % (success_count_not_switch, time_count)) if __name__ == \u0026#39;__main__\u0026#39;: main() Run kho·∫£ng 1000 l·∫ßn, b·∫°n s·∫Ω th·∫•y x√°c su·∫•t kh√° ch√≠nh x√°c:\n$ python3 monty-hall-demo.py 1000 Success percentage of second choice: 665 / 1000 Success percentage of first choice : 335 / 1000 ƒêi xa h∆°n 1 ch√∫t H√£y nghƒ© v·ªÅ 1 ho√†n c·∫£nh c√≥ th·ªÉ kh√°c:\n B·∫°n ch∆°i \u0026ldquo;Ai L√† Tri·ªáu Ph√∫\u0026rdquo; B·∫°n ko bi·∫øt g√¨, ko c√≥ b·∫•t c·ª© 1 th√¥ng tin g√¨ ƒë·ªÉ ph√¢n v√¢n, Nh∆∞ng b·∫°n ch·ªçn s·∫Ω n√≥i v·ªõi MC l√† b·∫°n nghi√™ng v·ªÅ ƒë√°p √°n A. B·∫°n d√πng s·ª± tr·ª£ gi√∫p 50-50 T·∫°i ƒë√¢y gi·∫£ s·ª≠ h·ªç b·ªè ƒëi 2 ƒë√°p √°n sai v√† gi·ªØ l·∫°i ƒë√°p √°n A. Gi·ªù b·∫°n c√≥ n√™n ƒë·ªïi s·ª± l·ª±a ch·ªçn sang ƒë√°p √°n c√≤n l·∫°i ko?  H√£y th·ª±c nghi·ªám v·ªõi 1 ƒëo·∫°n code (s·ª≠a ƒëo·∫°n code tr√™n 1 ch√∫t):\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; an demostration of Monty Hall problem \u0026#34;\u0026#34;\u0026#34; import random import sys def main(): COUNT = int(sys.argv[1]) success_count_of_switch = 0 success_count_not_switch = 0 time_count = 0 while(time_count \u0026lt; COUNT): # DOOR DOOR = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;] # only one DOOR cover the CAR result = random.choice(DOOR) print(\u0026#34;\u0026#34;) print(\u0026#34;hidden result is: \u0026#34;, result) # player make a first choice my_first_choice = random.choice(DOOR) print(\u0026#34;my first choice is: \u0026#34;, my_first_choice) # round 1, reveal a DOOR that cover a goat reveal1 = random.choice(DOOR) # make sure that reveal is not the door cover a car and not my_first_choice while((reveal1 == result) or (reveal1 == my_first_choice)): reveal1 = random.choice(DOOR) print(\u0026#34;first reveal: \u0026#34;, reveal1) # now DOOR will be subjected  DOOR.remove(reveal1) # round 2, reveal a DOOR that cover a goat reveal2 = random.choice(DOOR) # make sure that reveal is not the door cover a car and not my_first_choice while((reveal2 == result) or (reveal2 == my_first_choice)): reveal2 = random.choice(DOOR) print(\u0026#34;second reveal: \u0026#34;, reveal2) # now DOOR will be subjected  DOOR.remove(reveal2) # player must switch the choice my_second_choice = random.choice(DOOR) # make sure that player switch to another DOOR while(my_second_choice == my_first_choice): my_second_choice = random.choice(DOOR) print(\u0026#34;my second choice is: \u0026#34;, my_second_choice) if (my_second_choice == result): print(\u0026#34;Switch is CORRECT decision!\u0026#34;) success_count_of_switch += 1 elif (my_first_choice == result): print(\u0026#34;Switch is wrong decision!\u0026#34;) success_count_not_switch += 1 time_count += 1 print(\u0026#34;Success percentage of second choice: %d/ %d\u0026#34; % (success_count_of_switch, time_count)) print(\u0026#34;Success percentage of first choice : %d/ %d\u0026#34; % (success_count_not_switch, time_count)) if __name__ == \u0026#39;__main__\u0026#39;: main() K·∫øt qu·∫£ l√†:\n$ python3 monty-hall-demo.py 1000 Success percentage of second choice: 754 / 1000 Success percentage of first choice : 246 / 1000 Vi·ªác ƒë·ªïi l·ª±a ch·ªçn r√µ r√†ng ƒëem l·∫°i kh·∫£ nƒÉng ƒë√∫ng cao h∆°n.\nNh∆∞ng h√£y ch√∫ √Ω r·∫±ng ƒë·∫•y l√† tr∆∞·ªùng h·ª£p ch√∫ng ta ƒëang gi·∫£ s·ª≠: \u0026ldquo;gi·∫£ s·ª≠ h·ªç b·ªè ƒëi 2 ƒë√°p √°n sai v√† gi·ªØ l·∫°i ƒë√°p √°n A.\u0026rdquo;\nTrong th·ª±c t·∫ø MC s·∫Ω ko n√≥i r·∫±ng MC s·∫Ω gi·ªØ l·∫°i ƒë√°p √°n A, nhi·ªÅu khi h·ªç b·ªè n√≥ lu√¥n.\nL√∫c ·∫•y ƒëo·∫°n code n√™n thay ƒë·ªïi:\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; an demostration of Monty Hall problem \u0026#34;\u0026#34;\u0026#34; import random import sys def main(): COUNT = int(sys.argv[1]) success_count_of_switch = 0 success_count_not_switch = 0 time_count = 0 while(time_count \u0026lt; COUNT): # DOOR DOOR = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;] # only one DOOR cover the CAR result = random.choice(DOOR) print(\u0026#34;\u0026#34;) print(\u0026#34;hidden result is: \u0026#34;, result) # player make a first choice my_first_choice = random.choice(DOOR) print(\u0026#34;my first choice is: \u0026#34;, my_first_choice) # round 1, reveal a DOOR that cover a goat reveal1 = random.choice(DOOR) # make sure that reveal is not the door cover a car while(reveal1 == result): reveal1 = random.choice(DOOR) print(\u0026#34;first reveal: \u0026#34;, reveal1) # now DOOR will be subjected  DOOR.remove(reveal1) # round 2, reveal a DOOR that cover a goat reveal2 = random.choice(DOOR) # make sure that reveal is not the door cover a car while(reveal2 == result): reveal2 = random.choice(DOOR) print(\u0026#34;second reveal: \u0026#34;, reveal2) # now DOOR will be subjected  DOOR.remove(reveal2) # player must switch the choice my_second_choice = random.choice(DOOR) # make sure that player switch to another DOOR while(my_second_choice == my_first_choice): my_second_choice = random.choice(DOOR) print(\u0026#34;my second choice is: \u0026#34;, my_second_choice) if (my_second_choice == result): print(\u0026#34;Switch is CORRECT decision!\u0026#34;) success_count_of_switch += 1 elif (my_first_choice == result): print(\u0026#34;Switch is wrong decision!\u0026#34;) success_count_not_switch += 1 time_count += 1 print(\u0026#34;Success percentage of second choice: %d/ %d\u0026#34; % (success_count_of_switch, time_count)) print(\u0026#34;Success percentage of first choice : %d/ %d\u0026#34; % (success_count_not_switch, time_count)) if __name__ == \u0026#39;__main__\u0026#39;: main() L√∫c n√†y l·∫ßn l·ª±a ch·ªçn th·ª© 2 c·ªßa b·∫°n c√≥ x√°c su·∫•t l√† 50 %\nSuccess percentage of second choice: 509 / 1000 Success percentage of first choice : 250 / 1000 H√£y t·ª± nghƒ© ra th√™m kh·∫£ nƒÉng v√† th·ª±c nghi·ªám ƒë·ªÉ xem k·∫øt qu·∫£ nh√© !\nV·∫≠y l√† b·∫°n c√≥ th·ªÉ th·∫•y s·ª©c m·∫°nh c·ªßa X√°c su·∫•t th·ªëng k√™ gi√∫p b·∫°n ƒë·ª° t·ªën c√¥ng th·ª±c nghi·ªám r·∫•t nhi·ªÅu.\nThay v√¨ ph·∫£i vi·∫øt code ƒë·ªÉ t√¨m hi·ªÉu nh∆∞ m√¨nh, b·∫°n ch·ªâ c·∫ßn l√†m v√†i ph√©p t√≠nh X√°c su·∫•t l√† c√≥ th·ªÉ t√¨m ra l·ªùi gi·∫£i r·∫•t nhanh\u0026hellip;\nTh·∫ø n√™n h√£y nghi√™m t√∫c khi h·ªçc nh√© ü§£\nCREDIT https://betterexplained.com/articles/understanding-the-monty-hall-problem/\n","href":"/bk/monty-hall-problem-demo/","title":"Monty Hall Problem Demo"},{"content":"V·∫•n ƒë·ªÅ T√¨m hi·ªÉu v·ªÅ Monty Hall problem gi√∫p b·∫°n th·∫•y ƒë∆∞·ª£c s·ª©c m·∫°nh c·ªßa X√°c su·∫•t th·ªëng k√™.\nMonty Hall problem n√≥i v·ªÅ 1 tr√≤ ch∆°i nh∆∞ sau:\n Monty ƒë∆∞a cho b·∫°n 3 c√°nh c·ª≠a (A,B,C), ƒë·∫±ng sau ƒë√≥ l√† 2 con d√™ v√† 1 c√°i √¥ t√¥. B·∫°n ch·ªçn 1 c·ª≠a (gi·∫£ s·ª≠ l√† A). B·∫°n nghƒ© r·∫±ng sau c√°nh c·ª≠a ƒë√≥ l√† c√°i √¥ t√¥. Monty ki·ªÉm tra 2 c√°nh c·ª≠a c√≤n l·∫°i (B,C), v√† m·ªü 1 c√°nh c·ª≠a c√≥ con d√™. C√¢u h·ªèi l√† b·∫°n v·∫´n ch·ªçn c√°nh c·ª≠a A nh∆∞ ban ƒë·∫ßu, hay ƒë·ªïi sang 1 c√°nh c·ª≠a kh√°c? X√°c su·∫•t c√≥ kh√°c nhau ko?  \u0026hellip;\nC√¢u tr·∫£ l·ªùi c√≥ th·ªÉ s·∫Ω l√†m 1 s·ªë ng∆∞·ªùi ng·∫°c nhi√™n l√† x√°c su·∫•t ko ph·∫£i l√† 50-50. N·∫øu b·∫°n ƒë·ªïi sang 1 c√°nh c·ª≠a kh√°c. X√°c su·∫•t b·∫°n ch·ªçn ƒë√∫ng c·ª≠a c√≥ √¥ t√¥ l√† 66 %\nNghe chuy√™n gia gi·∫£i th√≠ch Chuy√™n gia gi·∫£i th√≠ch r·∫±ng:\nN·∫øu b·∫°n ch·ªçn c√°nh c·ª≠a A t·ª´ ƒë·∫ßu, b·∫°n c√≥ 1/3 c∆° h·ªôi ƒë√∫ng.\nNh∆∞ng n·∫øu b·∫°n gi·ªØ nguy√™n l·ª±a ch·ªçn ƒë√≥ sau khi Monty lo·∫°i tr·ª´ 1 c√°nh c·ª≠a sai, b·∫°n s·∫Ω ko th·ªÉ c·∫£i thi·ªán x√°c su·∫•t c·ªßa m√¨nh.\nKhi Monty ch∆∞a lo·∫°i tr·ª´, x√°c su·∫•t c·ªßa 2 c√°nh c·ª≠a c√≤n l·∫°i (B,C) l√† 2/3.\nMonty ki·ªÉm tra ch√∫ng v√† b·ªè 1 c√°nh c·ª≠a sai (gi·∫£ s·ª≠ B), th√¨ c√°nh c·ª≠a c√≤n l·∫°i (C) s·∫Ω c√≥ x√°c su·∫•t l√† 2/3.\nKhi ƒë√≥ vi·ªác ƒë·ªïi sang C s·∫Ω gi√∫p b·∫°n tƒÉng x√°c su·∫•t t·ª´ 1/3 l√™n 2/3.\nB·∫°n hi·ªÉu ch∆∞a?\nD∆∞·ªõi 1 g√≥c nh√¨n kh√°c:\n Gi·∫£ s·ª≠ c√≥ 100 c√°nh c·ª≠a thay v√¨ 3. B·∫°n ch·ªçn 1. Monty ki·ªÉm tra 99 c√°nh c·ª≠a c√≤n l·∫°i, √¥ng ·∫•y m·ªü 98 c√°nh c·ª≠a c√≥ d√™ v√† ch·ªâ ƒë·ªÉ l·∫°i 1 c·ª≠a. B·∫°n v·∫´n gi·ªØ l·ª±a ch·ªçn ban ƒë·∫ßu (v·ªõi c∆° h·ªôi 1/100) hay l√† chuy·ªÉn sang c√°nh c·ª≠a c√≤n l·∫°i (c√°i m√† ƒë√£ ƒë∆∞·ª£c l·ªçc ra trong 99 c√°i) ?  ƒê·∫øn ƒë√¢y c√≥ l·∫Ω b·∫°n ƒë√£ hi·ªÉu r√µ h∆°n. H√†nh ƒë·ªông l·ªçc c·ªßa Monty ƒë√£ gi√∫p b·∫°n. √îng ·∫•y cho b·∫°n ch·ªçn gi·ªØa \u0026ldquo;l·ª±a ch·ªçn ho√†n to√†n ng·∫´u nhi√™n v√† \u0026ldquo;l·ª±a ch·ªçn sau 1 l·∫ßn ch·ªçn l·ªçc\u0026rdquo;.\nL·ª±a ch·ªçn sau 1 l·∫ßn l·ªçc r√µ r√†ng t·ªët h∆°n.\nV·∫≠y khi n√†o x√°c su·∫•t l·ª±a ch·ªçn l√† 50-50 ? ƒê√≥ l√† khi b·∫°n kh√¥ng h·ªÅ c√≥ th√¥ng tin v·ªÅ h√†nh ƒë·ªông c·ªßa Monty.\nT∆∞·ªüng t∆∞·ª£ng r·∫±ng:\n Monty ƒë∆∞a cho b·∫°n 3 c√°nh c·ª≠a (A,B,C), ƒë·∫±ng sau ƒë√≥ l√† 2 con d√™ v√† 1 c√°i √¥ t√¥. B·∫°n ch·ªçn 1 c·ª≠a (gi·∫£ s·ª≠ l√† A). B·∫°n nghƒ© r·∫±ng sau c√°nh c·ª≠a ƒë√≥ l√† c√°i √¥ t√¥. Monty ki·ªÉm tra 2 c√°nh c·ª≠a c√≤n l·∫°i (B,C), v√† m·ªü 1 c√°nh c·ª≠a c√≥ con d√™. ƒê√∫ng l√∫c n√†y 1 ng∆∞·ªùi b·∫°n c·ªßa b·∫°n ƒëi ƒë·∫øn, nh√¨n th·∫•y 2 c√°nh c·ª≠a v√† ko bi·∫øt Monty v·ª´a l√†m g√¨. Anh ta ph·∫£i ch·ªçn 1, x√°c su·∫•t c·ªßa anh ta ch·ªçn ƒë√∫ng l√† 50 %. Nh∆∞ng x√°c su·∫•t n·∫øu b·∫°n ƒë·ªïi c√°nh c·ª≠a sang C l·∫°i l√† 66 %  =\u0026gt; K·∫øt lu·∫≠n l√†: Th√¥ng tin s·∫Ω ·∫£nh h∆∞·ªüng r·∫•t nhi·ªÅu ƒë·∫øn x√°c su·∫•t c·ªßa l·ª±a ch·ªçn\nDemo Khi b·∫°n v·∫´n ph√¢n v√¢n ko hi·ªÉu v·ªÅ b√†i to√°n v√† l·ªùi gi·∫£i tr√™n, c√°ch ƒë∆°n gi·∫£n nh·∫•t ƒë·ªÉ ch·ª©ng minh l√† th·ª±c nghi·ªám.\nH√£y d√πng 1 ƒëo·∫°n code python ƒë·ªÉ th·ª±c nghi·ªám xem x√°c su·∫•t c√≥ ƒë√∫ng l√† tƒÉng l√™n ko?\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; an demostration of Monty Hall problem \u0026#34;\u0026#34;\u0026#34; import random import sys def main(): COUNT = int(sys.argv[1]) success_count_of_switch = 0 success_count_not_switch = 0 time_count = 0 while(time_count \u0026lt; COUNT): # DOOR DOOR = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;] # only one DOOR cover the CAR result = random.choice(DOOR) print(\u0026#34;\u0026#34;) print(\u0026#34;hidden result is: \u0026#34;, result) # player make a first choice my_first_choice = random.choice(DOOR) print(\u0026#34;my first choice is: \u0026#34;, my_first_choice) # round 1, reveal a DOOR that cover a goat reveal1 = random.choice(DOOR) # make sure that reveal is not the door cover a car and not my_first_choice while((reveal1 == result) or (reveal1 == my_first_choice)): reveal1 = random.choice(DOOR) print(\u0026#34;first reveal: \u0026#34;, reveal1) # now DOOR will be subjected  DOOR.remove(reveal1) # player must switch the choice my_second_choice = random.choice(DOOR) # make sure that player switch to another DOOR while(my_second_choice == my_first_choice): my_second_choice = random.choice(DOOR) print(\u0026#34;my second choice is: \u0026#34;, my_second_choice) if (my_second_choice == result): print(\u0026#34;Switch is CORRECT decision!\u0026#34;) success_count_of_switch += 1 elif (my_first_choice == result): print(\u0026#34;Switch is wrong decision!\u0026#34;) success_count_not_switch += 1 time_count += 1 print(\u0026#34;Success percentage of second choice: %d/ %d\u0026#34; % (success_count_of_switch, time_count)) print(\u0026#34;Success percentage of first choice : %d/ %d\u0026#34; % (success_count_not_switch, time_count)) if __name__ == \u0026#39;__main__\u0026#39;: main() Run kho·∫£ng 1000 l·∫ßn, b·∫°n s·∫Ω th·∫•y x√°c su·∫•t kh√° ch√≠nh x√°c:\n$ python3 monty-hall-demo.py 1000 Success percentage of second choice: 665 / 1000 Success percentage of first choice : 335 / 1000 ƒêi xa h∆°n 1 ch√∫t H√£y nghƒ© v·ªÅ 1 ho√†n c·∫£nh c√≥ th·ªÉ kh√°c:\n B·∫°n ch∆°i \u0026ldquo;Ai L√† Tri·ªáu Ph√∫\u0026rdquo; B·∫°n ko bi·∫øt g√¨, ko c√≥ b·∫•t c·ª© 1 th√¥ng tin g√¨ ƒë·ªÉ ph√¢n v√¢n, Nh∆∞ng b·∫°n ch·ªçn s·∫Ω n√≥i v·ªõi MC l√† b·∫°n nghi√™ng v·ªÅ ƒë√°p √°n A. B·∫°n d√πng s·ª± tr·ª£ gi√∫p 50-50 T·∫°i ƒë√¢y gi·∫£ s·ª≠ h·ªç b·ªè ƒëi 2 ƒë√°p √°n sai v√† gi·ªØ l·∫°i ƒë√°p √°n A. Gi·ªù b·∫°n c√≥ n√™n ƒë·ªïi s·ª± l·ª±a ch·ªçn sang ƒë√°p √°n c√≤n l·∫°i ko?  H√£y th·ª±c nghi·ªám v·ªõi 1 ƒëo·∫°n code (s·ª≠a ƒëo·∫°n code tr√™n 1 ch√∫t):\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; an demostration of Monty Hall problem \u0026#34;\u0026#34;\u0026#34; import random import sys def main(): COUNT = int(sys.argv[1]) success_count_of_switch = 0 success_count_not_switch = 0 time_count = 0 while(time_count \u0026lt; COUNT): # DOOR DOOR = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;] # only one DOOR cover the CAR result = random.choice(DOOR) print(\u0026#34;\u0026#34;) print(\u0026#34;hidden result is: \u0026#34;, result) # player make a first choice my_first_choice = random.choice(DOOR) print(\u0026#34;my first choice is: \u0026#34;, my_first_choice) # round 1, reveal a DOOR that cover a goat reveal1 = random.choice(DOOR) # make sure that reveal is not the door cover a car and not my_first_choice while((reveal1 == result) or (reveal1 == my_first_choice)): reveal1 = random.choice(DOOR) print(\u0026#34;first reveal: \u0026#34;, reveal1) # now DOOR will be subjected  DOOR.remove(reveal1) # round 2, reveal a DOOR that cover a goat reveal2 = random.choice(DOOR) # make sure that reveal is not the door cover a car and not my_first_choice while((reveal2 == result) or (reveal2 == my_first_choice)): reveal2 = random.choice(DOOR) print(\u0026#34;second reveal: \u0026#34;, reveal2) # now DOOR will be subjected  DOOR.remove(reveal2) # player must switch the choice my_second_choice = random.choice(DOOR) # make sure that player switch to another DOOR while(my_second_choice == my_first_choice): my_second_choice = random.choice(DOOR) print(\u0026#34;my second choice is: \u0026#34;, my_second_choice) if (my_second_choice == result): print(\u0026#34;Switch is CORRECT decision!\u0026#34;) success_count_of_switch += 1 elif (my_first_choice == result): print(\u0026#34;Switch is wrong decision!\u0026#34;) success_count_not_switch += 1 time_count += 1 print(\u0026#34;Success percentage of second choice: %d/ %d\u0026#34; % (success_count_of_switch, time_count)) print(\u0026#34;Success percentage of first choice : %d/ %d\u0026#34; % (success_count_not_switch, time_count)) if __name__ == \u0026#39;__main__\u0026#39;: main() K·∫øt qu·∫£ l√†:\n$ python3 monty-hall-demo.py 1000 Success percentage of second choice: 754 / 1000 Success percentage of first choice : 246 / 1000 Vi·ªác ƒë·ªïi l·ª±a ch·ªçn r√µ r√†ng ƒëem l·∫°i kh·∫£ nƒÉng ƒë√∫ng cao h∆°n.\nNh∆∞ng h√£y ch√∫ √Ω r·∫±ng ƒë·∫•y l√† tr∆∞·ªùng h·ª£p ch√∫ng ta ƒëang gi·∫£ s·ª≠: \u0026ldquo;gi·∫£ s·ª≠ h·ªç b·ªè ƒëi 2 ƒë√°p √°n sai v√† gi·ªØ l·∫°i ƒë√°p √°n A.\u0026rdquo;\nTrong th·ª±c t·∫ø MC s·∫Ω ko n√≥i r·∫±ng MC s·∫Ω gi·ªØ l·∫°i ƒë√°p √°n A, nhi·ªÅu khi h·ªç b·ªè n√≥ lu√¥n.\nL√∫c ·∫•y ƒëo·∫°n code n√™n thay ƒë·ªïi:\n#!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; an demostration of Monty Hall problem \u0026#34;\u0026#34;\u0026#34; import random import sys def main(): COUNT = int(sys.argv[1]) success_count_of_switch = 0 success_count_not_switch = 0 time_count = 0 while(time_count \u0026lt; COUNT): # DOOR DOOR = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;] # only one DOOR cover the CAR result = random.choice(DOOR) print(\u0026#34;\u0026#34;) print(\u0026#34;hidden result is: \u0026#34;, result) # player make a first choice my_first_choice = random.choice(DOOR) print(\u0026#34;my first choice is: \u0026#34;, my_first_choice) # round 1, reveal a DOOR that cover a goat reveal1 = random.choice(DOOR) # make sure that reveal is not the door cover a car while(reveal1 == result): reveal1 = random.choice(DOOR) print(\u0026#34;first reveal: \u0026#34;, reveal1) # now DOOR will be subjected  DOOR.remove(reveal1) # round 2, reveal a DOOR that cover a goat reveal2 = random.choice(DOOR) # make sure that reveal is not the door cover a car while(reveal2 == result): reveal2 = random.choice(DOOR) print(\u0026#34;second reveal: \u0026#34;, reveal2) # now DOOR will be subjected  DOOR.remove(reveal2) # player must switch the choice my_second_choice = random.choice(DOOR) # make sure that player switch to another DOOR while(my_second_choice == my_first_choice): my_second_choice = random.choice(DOOR) print(\u0026#34;my second choice is: \u0026#34;, my_second_choice) if (my_second_choice == result): print(\u0026#34;Switch is CORRECT decision!\u0026#34;) success_count_of_switch += 1 elif (my_first_choice == result): print(\u0026#34;Switch is wrong decision!\u0026#34;) success_count_not_switch += 1 time_count += 1 print(\u0026#34;Success percentage of second choice: %d/ %d\u0026#34; % (success_count_of_switch, time_count)) print(\u0026#34;Success percentage of first choice : %d/ %d\u0026#34; % (success_count_not_switch, time_count)) if __name__ == \u0026#39;__main__\u0026#39;: main() L√∫c n√†y l·∫ßn l·ª±a ch·ªçn th·ª© 2 c·ªßa b·∫°n c√≥ x√°c su·∫•t l√† 50 %\nSuccess percentage of second choice: 509 / 1000 Success percentage of first choice : 250 / 1000 H√£y t·ª± nghƒ© ra th√™m kh·∫£ nƒÉng v√† th·ª±c nghi·ªám ƒë·ªÉ xem k·∫øt qu·∫£ nh√© !\nV·∫≠y l√† b·∫°n c√≥ th·ªÉ th·∫•y s·ª©c m·∫°nh c·ªßa X√°c su·∫•t th·ªëng k√™ gi√∫p b·∫°n ƒë·ª° t·ªën c√¥ng th·ª±c nghi·ªám r·∫•t nhi·ªÅu.\nThay v√¨ ph·∫£i vi·∫øt code ƒë·ªÉ t√¨m hi·ªÉu nh∆∞ m√¨nh, b·∫°n ch·ªâ c·∫ßn l√†m v√†i ph√©p t√≠nh X√°c su·∫•t l√† c√≥ th·ªÉ t√¨m ra l·ªùi gi·∫£i r·∫•t nhanh\u0026hellip;\nTh·∫ø n√™n h√£y nghi√™m t√∫c khi h·ªçc nh√© ü§£\nCREDIT https://betterexplained.com/articles/understanding-the-monty-hall-problem/\n","href":"/posts/monty-hall-problem-demo/","title":"Monty Hall Problem Demo"},{"content":"Khi s·ª≠ d·ª•ng Git d·∫ßn d·∫ßn tr·ªü n√™n th∆∞·ªùng xuy√™n th√¨ vi·ªác hi·ªÉu v·ªÅ c√°c c√¢u l·ªánh n√¢ng cao c·ªßa n√≥ c√†ng tr·ªü n√™n c·∫ßn thi·∫øt.\nS·ª≠ d·ª•ng nhi·ªÅu th√¨ d·∫´n ƒë·∫øn commit nhi·ªÅu, commit nhi·ªÅu th√¨ vi·ªác commit sai file, commit sai message l√† chuy·ªán d·ªÖ hi·ªÉu.\nT·ª´ ƒë√≥ d·∫´n ƒë·∫øn nhu c·∫ßu s·ª≠a l·∫°i commit message, commit sai mu·ªën revert l·∫°i commit tr∆∞·ªõc ƒë√≥, g·ªôp c√°c commit l·∫°i cho d·ªÖ nh√¨n, cho trace log trong t∆∞∆°ng lai, v√† \u0026hellip; nh√¨n log commit cho ƒë·∫πp.\nƒê√≥ l√† l√∫c b·∫°n c·∫ßn t√¨m hi·ªÉu nhi·ªÅu h∆°n v·ªÅ c√°c c√¢u l·ªánh c·ªßa Git.\n1 s·ªë Git command hay s·ª≠ d·ª•ng nh·∫•t l√†:\n git clone, git add, git commit, git remote set-url, git pull, git push, git merge, \u0026hellip;  B√†i n√†y n√≥i v·ªÅ git rebase interactive, git stash, git reset.\n1. Git Rebase interactive C·∫ßn ch√∫ √Ω r·∫±ng kh√¥ng n√™n s·ª≠ d·ª•ng git rebase tr√™n nh√°nh master, v√¨ n√≥ d·∫´n ƒë·∫øn kh·∫£ nƒÉng b·∫°n s·∫Ω l√†m m·∫•t code c·ªßa ng∆∞·ªùi kh√°c.\nCh·ªâ n√™n d√πng tr√™n nh√°nh feature c·ªßa b·∫°n m√† ch·ªâ c√≥ b·∫°n ƒëang l√†m vi·ªác, ho·∫∑c b·∫°n bi·∫øt ch·∫Øc m·∫•t code c·ªßa ng∆∞·ªùi kh√°c c≈©ng ko v·∫•n ƒë·ªÅ.\n1.1. V√≠ d·ª• 1 B·∫Øt ƒë·∫ßu v·ªõi v√≠ d·ª• nh∆∞ n√†y:\nM√¨nh ƒëang tr√™n nh√°nh testing. ƒê√£ edit file README.md 6 l·∫ßn. L·∫ßn g·∫ßn nh·∫•t l√† \u0026ldquo;edit readme 6\u0026rdquo;.\nM·ªói l·∫ßn edit m√¨nh add th√™m 1 s·ªë v√†o file README.md. Sau 6 l·∫ßn th√¨ n·ªôi dung n√≥ l√†:\nhello 1 2 3 4 5 6 run command sau:\ngit rebase -i HEAD~6 ƒë√¢y l√† h√¨nh ·∫£nh m√†n h√¨nh editor c·ªßa git rebase -i HEAD~6\ngi·ªù m√¨nh mu·ªën g·ªôp 2 commit \u0026ldquo;edit readme 5\u0026rdquo; v√† \u0026ldquo;edit readme 4\u0026rdquo; l·∫°i, ch·ªâ gi·ªØ c√°i \u0026ldquo;edit readme 4\u0026rdquo; th√¥i.\ntrong m√†n h√¨nh editor c·ªßa git rebase, thay ch·ªØ pick c·ªßa commit 5 th√†nh s (vi·∫øt t·∫Øt c·ªßa squash)\nch·ªØ s n√†y c√≥ nghƒ©a b·∫°n v·∫´n d√πng commit n√†y nh∆∞ng g·ªôp n√≥ v√†o commit tr∆∞·ªõc ƒë√≥ (commit 4) sau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nngay sau ƒë√≥ 1 editor m·ªõi ƒë∆∞·ª£c b·∫≠t l√™n:\n·ªü ƒë√¢y b·∫°n ch·ªâ mu·ªën gi·ªØ commit message \u0026ldquo;edit readme 4\u0026rdquo; th√¥i ƒë√∫ng ko, h√£y add # v√†o tr∆∞·ªõc \u0026ldquo;edit readme 5\u0026rdquo; ƒë·ªÉ ignore n√≥:\nsau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nm√†n h√¨nh nh∆∞ sau l√† ok.\nƒë·∫øn ƒë√¢y ch∆∞a xong, b·∫°n c·∫ßn push c√°c thay ƒë·ªïi v·ª´a xong l√™n remote branch ƒë·ªÉ check:\ngit push origin testing -f l√™n Git repository xem log commit:\nn·ªôi dung c·ªßa commit 4 ƒë√£ bao g·ªìm c·∫£ thay ƒë·ªïi c·ªßa commit 5.\nnh∆∞ v·∫≠y l√† ƒë√∫ng √Ω ch√∫ng ta r·ªìi.\n1.2. V√≠ d·ª• 2 Gi·ªù b·∫°n mu·ªën g·ªôp 3 commit b√¥i v√†ng l·∫°i, v√† commit message m·ªõi s·∫Ω l√† \u0026lsquo;hello 123\u0026rsquo; th√¨ sao?\ngit rebase -i HEAD~5 trong m√†n h√¨nh editor c·ªßa git rebase, thay ch·ªØ pick c·ªßa commit 3 v√† commit 2 th√†nh s (vi·∫øt t·∫Øt c·ªßa squash)\nsau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nngay sau ƒë√≥ 1 editor m·ªõi ƒë∆∞·ª£c b·∫≠t l√™n:\nH√£y comment out 2 commit message \u0026ldquo;edit readme 2\u0026rdquo; v√† \u0026ldquo;edit readme 3\u0026rdquo;, s·ª≠a \u0026ldquo;edit readme 1\u0026rdquo; th√†nh \u0026ldquo;hello 123\u0026rdquo; nh∆∞ √Ω mu·ªën ban ƒë·∫ßu sau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nƒë·∫øn ƒë√¢y ch∆∞a xong, b·∫°n c·∫ßn push c√°c thay ƒë·ªïi v·ª´a xong l√™n remote branch ƒë·ªÉ check:\ngit push origin testing -f l√™n Git repository xem log commit:\nnh∆∞ v·∫≠y l√† ƒë√£ ƒë√∫ng √Ω ch√∫ng ta r·ªìi üòç\nGit rebase interactive c√≥ nhi·ªÅu option nh∆∞ squash, edit, reword, fixup n·∫øu b·∫°n ƒë·ªÉ √Ω m√†n h√¨nh editor c·ªßa Git rebase.\n \u0026ldquo;reword\u0026rdquo; allows you to change ONLY the commit message, NOT the commit contents.\n  \u0026ldquo;edit\u0026rdquo; allows you to change BOTH commit contents AND commit message. By replacing the command \u0026ldquo;pick\u0026rdquo; with the command \u0026ldquo;edit\u0026rdquo;, you can tell git rebase to stop after applying that commit, so that you can edit the files and/or the commit message, amend the commit, and continue rebasing.\n Nh∆∞ng theo m√¨nh ch·ªâ c·∫ßn bi·∫øt squash l√† ƒë·ªß üòÇ\n2. Git stash T∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang ph√°t tri·ªÉn tr√™n 1 branch testing, nh∆∞ng s·∫øp b·∫Øt fix bug tr√™n 1 nh√°nh kh√°c (master) n√™n b·∫°n ph·∫£i nh·∫£y qua nh√°nh master ƒë√≥ ƒë·ªÉ fix.\nNh∆∞ng mu·ªën checkout master th√¨ c·∫ßn ph·∫£i commit c√°c thay ƒë·ªïi hi·ªán t·∫°i ƒë√£\u0026hellip;\nNh∆∞ng b·∫°n ch∆∞a mu·ªën commit c√°c code d·ªü dang l√™n branch testing lu√¥n (v√¨ n√≥ s·∫Ω x·∫•u commit log, n√≥ s·∫Ω push nh·ªØng sensitve info l√™n, n√≥ s·∫Ω trigger pipeline ko c·∫ßn thi·∫øt, \u0026hellip; etc)\n-\u0026gt; ƒê√¢y l√† l√∫c b·∫°n mu·ªën d√πng git stash\ngi·∫£ s·ª≠ file README.md c·ªßa b·∫°n ·ªü nh√°nh testing nh∆∞ sau:\nb·∫°n mu·ªën save l·∫°i session l√†m vi·ªác hi·ªán t·∫°i, run command sau:\ngit stash save --include-untracked doing crazy things command tr√™n c√≥ nghƒ©a l√† b·∫°n save session hi·ªán t·∫°i v·ªõi message l√† \u0026ldquo;doing crazy things\u0026rdquo;\nm·ªçi th·ª© s·∫Ω clean ƒëi, quay tr·ªü v·ªÅ nh√°nh testing ban ƒë·∫ßu.\ngi·ªù b·∫°n c√≥ th·ªÉ d·ªÖ d√†ng checkout nh√°nh master ƒë·ªÉ fix bug cho s·∫øp.\nSau khi fix xong b·∫°n quay tr·ªü l·∫°i nh√°nh testing, l√¥i nh·ªØng ƒëo·∫°n code d·ªü dang ra v√† ti·∫øp t·ª•c\u0026hellip;\n$ git stash list stash@{0}: On testing: doing crazy things B·∫°n l√¥i code d·ªü dang b·∫±ng command sau:\ngit stash pop stash@{0} Tuy·ªát v·ªùi, l·∫°i ti·∫øp t·ª•c c√¥ng vi·ªác th√¥i! ü•±\nM·∫∑c d√π git stash c√≤n c√≥ 1 s·ªë option nh∆∞:\ngit stash apply, git stash drop, git stash clear, git stash show\nNh∆∞ng m√¨nh nghƒ© ch·ªâ c·∫ßn bi·∫øt 3 c√°i l√† ƒë·ªß r·ªìi üòÇ:\n git stash save --include-untracked what-you-are-doing-message, git stash pop, git stash list  3. Git reset Gi·∫£ s·ª≠ b·∫°n ƒëang b·∫°n ƒëang ·ªü tr√™n nh√°nh feature/A, b·∫°n ƒë√£ th·ª±c hi·ªán nhi·ªÅu commit nh∆∞ sau:\nnh∆∞ng b·∫°n ph√°t hi·ªán ra l√† b·∫°n ƒë√£ l·ª° tay commit nhi·ªÅu c√°i linh tinh qu√°, pipeline ch·∫°y l·ªói t√πm lum r·ªìi, b·∫°n mu·ªën quay l·∫°i commit \u0026ldquo;edit readme 3\u0026rdquo;, x√≥a h·∫øt 3 commit m·ªõi nh·∫•t ƒëi. L√†m sao nh·ªâ?\nNh∆∞ n√†y, ƒë·∫ßu ti√™n check git log ƒë·ªÉ l·∫•y commit_id:\ngit log --decorate --oneline B·∫°n mu·ªën quay l·∫°i commit \u0026ldquo;edit readme 3\u0026rdquo; th√¨ commit_id ch√≠nh l√† 543a35a\nGi·ªù run command git reset --hard \u0026lt;commit_id\u0026gt;:\ngit reset --hard 543a35a HEAD is now at 543a35a edit readme 3 Gi·ªù file README.md ·ªü local c·ªßa b·∫°n ƒë√£ tr·ªü v·ªÅ th·ªùi ƒëi·ªÉm commit \u0026ldquo;edit readme 3\u0026rdquo;\nNh∆∞ng ch∆∞a ƒë·ªß, b·∫°n c·∫ßn push thay ƒë·ªïi ·ªü local l√™n remote Git n·ªØa\n# force clean, remove all object and sub directories git clean -f -d # download all object and refs from all remotes to local git fetch --all # git push to only branch feature/A, set it upstream git push -u origin +feature/A X√°c nh·∫≠n ko c√≥ l·ªói g√¨ khi run c√°c command tr√™n, quay l·∫°i xem log git commit tr√™n remote th√¥i.\nV·∫≠y l√† ƒë√∫ng √Ω ch√∫ng ta r·ªìi, ti·∫øp t·ª•c edit code cho c·∫©n th·∫≠n r·ªìi h√£y commit l√™n nh√©\n4. Pull code t·ª´ 1 repository kh√°c gi·ªØ nguy√™n history commit https://stackoverflow.com/a/46289324/9922066\nGi·∫£ s·ª≠ b·∫°n c√≥ 1 repo A. B·∫°n mu·ªën archive repo A v√† chuy·ªÉn h·∫øt code sang repo B. Gi·ªØ nguy√™n history commit.\n-\u0026gt; T·∫°o repo B, gi·∫£ s·ª≠ branch m·∫∑c ƒë·ªãnh l√† master.\n-\u0026gt; B·∫°n t·∫°o branch m·ªõi (develop) tr√™n repo B m√† ko c√≥ content g√¨ c·∫£ b·∫±ng c√°ch:\ngit checkout --orphan develop git reset --hard git pull [REPO_A_URL] [REPO_A_BRANCH] Nh∆∞ v·∫≠y repo B (branch develop) ƒë√£ c√≥ t·∫•t c·∫£ history commit c·ªßa repo A (branch REPO_A_BRANCH)\n5. Delete all commit history https://stackoverflow.com/questions/13716658/how-to-delete-all-commit-history-in-github\nGi·∫£ s·ª≠ b·∫°n mu·ªën x√≥a h·∫øt commit history th√¨ l√†m nh∆∞ c√°c command sau:\n# Create new empty branch git checkout --orphan latest_branch # Add all the files git add -A # Commit the changes git commit -am \u0026#34;commit message\u0026#34; # Delete the branch git branch -D master # Rename the current branch to master git branch -m master # Finally, force update your repository git push -f origin master CREDIT https://thoughtbot.com/blog/git-interactive-rebase-squash-amend-rewriting-history\nhttps://kipalog.com/posts/Viet-lai-lich-su-cua-git-bang-cach-su-dung-interactive-rebase\nhttps://git-scm.com/docs/git-push\nhttps://git-scm.com/docs/git-fetch\nhttps://git-scm.com/docs/git-clean\nhttps://stackoverflow.com/a/46289324/9922066\n","href":"/bk/git-tutor-rebase-stash-reset/","title":"Amateur tutor Git commands (rebase interactive, stash, reset)"},{"content":"Khi s·ª≠ d·ª•ng Git d·∫ßn d·∫ßn tr·ªü n√™n th∆∞·ªùng xuy√™n th√¨ vi·ªác hi·ªÉu v·ªÅ c√°c c√¢u l·ªánh n√¢ng cao c·ªßa n√≥ c√†ng tr·ªü n√™n c·∫ßn thi·∫øt.\nS·ª≠ d·ª•ng nhi·ªÅu th√¨ d·∫´n ƒë·∫øn commit nhi·ªÅu, commit nhi·ªÅu th√¨ vi·ªác commit sai file, commit sai message l√† chuy·ªán d·ªÖ hi·ªÉu.\nT·ª´ ƒë√≥ d·∫´n ƒë·∫øn nhu c·∫ßu s·ª≠a l·∫°i commit message, commit sai mu·ªën revert l·∫°i commit tr∆∞·ªõc ƒë√≥, g·ªôp c√°c commit l·∫°i cho d·ªÖ nh√¨n, cho trace log trong t∆∞∆°ng lai, v√† \u0026hellip; nh√¨n log commit cho ƒë·∫πp.\nƒê√≥ l√† l√∫c b·∫°n c·∫ßn t√¨m hi·ªÉu nhi·ªÅu h∆°n v·ªÅ c√°c c√¢u l·ªánh c·ªßa Git.\n1 s·ªë Git command hay s·ª≠ d·ª•ng nh·∫•t l√†:\n git clone, git add, git commit, git remote set-url, git pull, git push, git merge, \u0026hellip;  B√†i n√†y n√≥i v·ªÅ git rebase interactive, git stash, git reset.\n1. Git Rebase interactive C·∫ßn ch√∫ √Ω r·∫±ng kh√¥ng n√™n s·ª≠ d·ª•ng git rebase tr√™n nh√°nh master, v√¨ n√≥ d·∫´n ƒë·∫øn kh·∫£ nƒÉng b·∫°n s·∫Ω l√†m m·∫•t code c·ªßa ng∆∞·ªùi kh√°c.\nCh·ªâ n√™n d√πng tr√™n nh√°nh feature c·ªßa b·∫°n m√† ch·ªâ c√≥ b·∫°n ƒëang l√†m vi·ªác, ho·∫∑c b·∫°n bi·∫øt ch·∫Øc m·∫•t code c·ªßa ng∆∞·ªùi kh√°c c≈©ng ko v·∫•n ƒë·ªÅ.\n1.1. V√≠ d·ª• 1 B·∫Øt ƒë·∫ßu v·ªõi v√≠ d·ª• nh∆∞ n√†y:\nM√¨nh ƒëang tr√™n nh√°nh testing. ƒê√£ edit file README.md 6 l·∫ßn. L·∫ßn g·∫ßn nh·∫•t l√† \u0026ldquo;edit readme 6\u0026rdquo;.\nM·ªói l·∫ßn edit m√¨nh add th√™m 1 s·ªë v√†o file README.md. Sau 6 l·∫ßn th√¨ n·ªôi dung n√≥ l√†:\nhello 1 2 3 4 5 6 run command sau:\ngit rebase -i HEAD~6 ƒë√¢y l√† h√¨nh ·∫£nh m√†n h√¨nh editor c·ªßa git rebase -i HEAD~6\ngi·ªù m√¨nh mu·ªën g·ªôp 2 commit \u0026ldquo;edit readme 5\u0026rdquo; v√† \u0026ldquo;edit readme 4\u0026rdquo; l·∫°i, ch·ªâ gi·ªØ c√°i \u0026ldquo;edit readme 4\u0026rdquo; th√¥i.\ntrong m√†n h√¨nh editor c·ªßa git rebase, thay ch·ªØ pick c·ªßa commit 5 th√†nh s (vi·∫øt t·∫Øt c·ªßa squash)\nch·ªØ s n√†y c√≥ nghƒ©a b·∫°n v·∫´n d√πng commit n√†y nh∆∞ng g·ªôp n√≥ v√†o commit tr∆∞·ªõc ƒë√≥ (commit 4) sau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nngay sau ƒë√≥ 1 editor m·ªõi ƒë∆∞·ª£c b·∫≠t l√™n:\n·ªü ƒë√¢y b·∫°n ch·ªâ mu·ªën gi·ªØ commit message \u0026ldquo;edit readme 4\u0026rdquo; th√¥i ƒë√∫ng ko, h√£y add # v√†o tr∆∞·ªõc \u0026ldquo;edit readme 5\u0026rdquo; ƒë·ªÉ ignore n√≥:\nsau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nm√†n h√¨nh nh∆∞ sau l√† ok.\nƒë·∫øn ƒë√¢y ch∆∞a xong, b·∫°n c·∫ßn push c√°c thay ƒë·ªïi v·ª´a xong l√™n remote branch ƒë·ªÉ check:\ngit push origin testing -f l√™n Git repository xem log commit:\nn·ªôi dung c·ªßa commit 4 ƒë√£ bao g·ªìm c·∫£ thay ƒë·ªïi c·ªßa commit 5.\nnh∆∞ v·∫≠y l√† ƒë√∫ng √Ω ch√∫ng ta r·ªìi.\n1.2. V√≠ d·ª• 2 Gi·ªù b·∫°n mu·ªën g·ªôp 3 commit b√¥i v√†ng l·∫°i, v√† commit message m·ªõi s·∫Ω l√† \u0026lsquo;hello 123\u0026rsquo; th√¨ sao?\ngit rebase -i HEAD~5 trong m√†n h√¨nh editor c·ªßa git rebase, thay ch·ªØ pick c·ªßa commit 3 v√† commit 2 th√†nh s (vi·∫øt t·∫Øt c·ªßa squash)\nsau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nngay sau ƒë√≥ 1 editor m·ªõi ƒë∆∞·ª£c b·∫≠t l√™n:\nH√£y comment out 2 commit message \u0026ldquo;edit readme 2\u0026rdquo; v√† \u0026ldquo;edit readme 3\u0026rdquo;, s·ª≠a \u0026ldquo;edit readme 1\u0026rdquo; th√†nh \u0026ldquo;hello 123\u0026rdquo; nh∆∞ √Ω mu·ªën ban ƒë·∫ßu sau ƒë√≥ Ctr+X, Yes ƒë·ªÉ save editor l·∫°i\nƒë·∫øn ƒë√¢y ch∆∞a xong, b·∫°n c·∫ßn push c√°c thay ƒë·ªïi v·ª´a xong l√™n remote branch ƒë·ªÉ check:\ngit push origin testing -f l√™n Git repository xem log commit:\nnh∆∞ v·∫≠y l√† ƒë√£ ƒë√∫ng √Ω ch√∫ng ta r·ªìi üòç\nGit rebase interactive c√≥ nhi·ªÅu option nh∆∞ squash, edit, reword, fixup n·∫øu b·∫°n ƒë·ªÉ √Ω m√†n h√¨nh editor c·ªßa Git rebase.\n \u0026ldquo;reword\u0026rdquo; allows you to change ONLY the commit message, NOT the commit contents.\n  \u0026ldquo;edit\u0026rdquo; allows you to change BOTH commit contents AND commit message. By replacing the command \u0026ldquo;pick\u0026rdquo; with the command \u0026ldquo;edit\u0026rdquo;, you can tell git rebase to stop after applying that commit, so that you can edit the files and/or the commit message, amend the commit, and continue rebasing.\n Nh∆∞ng theo m√¨nh ch·ªâ c·∫ßn bi·∫øt squash l√† ƒë·ªß üòÇ\n2. Git stash T∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang ph√°t tri·ªÉn tr√™n 1 branch testing, nh∆∞ng s·∫øp b·∫Øt fix bug tr√™n 1 nh√°nh kh√°c (master) n√™n b·∫°n ph·∫£i nh·∫£y qua nh√°nh master ƒë√≥ ƒë·ªÉ fix.\nNh∆∞ng mu·ªën checkout master th√¨ c·∫ßn ph·∫£i commit c√°c thay ƒë·ªïi hi·ªán t·∫°i ƒë√£\u0026hellip;\nNh∆∞ng b·∫°n ch∆∞a mu·ªën commit c√°c code d·ªü dang l√™n branch testing lu√¥n (v√¨ n√≥ s·∫Ω x·∫•u commit log, n√≥ s·∫Ω push nh·ªØng sensitve info l√™n, n√≥ s·∫Ω trigger pipeline ko c·∫ßn thi·∫øt, \u0026hellip; etc)\n-\u0026gt; ƒê√¢y l√† l√∫c b·∫°n mu·ªën d√πng git stash\ngi·∫£ s·ª≠ file README.md c·ªßa b·∫°n ·ªü nh√°nh testing nh∆∞ sau:\nb·∫°n mu·ªën save l·∫°i session l√†m vi·ªác hi·ªán t·∫°i, run command sau:\ngit stash save --include-untracked doing crazy things command tr√™n c√≥ nghƒ©a l√† b·∫°n save session hi·ªán t·∫°i v·ªõi message l√† \u0026ldquo;doing crazy things\u0026rdquo;\nm·ªçi th·ª© s·∫Ω clean ƒëi, quay tr·ªü v·ªÅ nh√°nh testing ban ƒë·∫ßu.\ngi·ªù b·∫°n c√≥ th·ªÉ d·ªÖ d√†ng checkout nh√°nh master ƒë·ªÉ fix bug cho s·∫øp.\nSau khi fix xong b·∫°n quay tr·ªü l·∫°i nh√°nh testing, l√¥i nh·ªØng ƒëo·∫°n code d·ªü dang ra v√† ti·∫øp t·ª•c\u0026hellip;\n$ git stash list stash@{0}: On testing: doing crazy things B·∫°n l√¥i code d·ªü dang b·∫±ng command sau:\ngit stash pop stash@{0} Tuy·ªát v·ªùi, l·∫°i ti·∫øp t·ª•c c√¥ng vi·ªác th√¥i! ü•±\nM·∫∑c d√π git stash c√≤n c√≥ 1 s·ªë option nh∆∞:\ngit stash apply, git stash drop, git stash clear, git stash show\nNh∆∞ng m√¨nh nghƒ© ch·ªâ c·∫ßn bi·∫øt 3 c√°i l√† ƒë·ªß r·ªìi üòÇ:\n git stash save --include-untracked what-you-are-doing-message, git stash pop, git stash list  3. Git reset Gi·∫£ s·ª≠ b·∫°n ƒëang b·∫°n ƒëang ·ªü tr√™n nh√°nh feature/A, b·∫°n ƒë√£ th·ª±c hi·ªán nhi·ªÅu commit nh∆∞ sau:\nnh∆∞ng b·∫°n ph√°t hi·ªán ra l√† b·∫°n ƒë√£ l·ª° tay commit nhi·ªÅu c√°i linh tinh qu√°, pipeline ch·∫°y l·ªói t√πm lum r·ªìi, b·∫°n mu·ªën quay l·∫°i commit \u0026ldquo;edit readme 3\u0026rdquo;, x√≥a h·∫øt 3 commit m·ªõi nh·∫•t ƒëi. L√†m sao nh·ªâ?\nNh∆∞ n√†y, ƒë·∫ßu ti√™n check git log ƒë·ªÉ l·∫•y commit_id:\ngit log --decorate --oneline B·∫°n mu·ªën quay l·∫°i commit \u0026ldquo;edit readme 3\u0026rdquo; th√¨ commit_id ch√≠nh l√† 543a35a\nGi·ªù run command git reset --hard \u0026lt;commit_id\u0026gt;:\ngit reset --hard 543a35a HEAD is now at 543a35a edit readme 3 Gi·ªù file README.md ·ªü local c·ªßa b·∫°n ƒë√£ tr·ªü v·ªÅ th·ªùi ƒëi·ªÉm commit \u0026ldquo;edit readme 3\u0026rdquo;\nNh∆∞ng ch∆∞a ƒë·ªß, b·∫°n c·∫ßn push thay ƒë·ªïi ·ªü local l√™n remote Git n·ªØa\n# force clean, remove all object and sub directories git clean -f -d # download all object and refs from all remotes to local git fetch --all # git push to only branch feature/A, set it upstream git push -u origin +feature/A X√°c nh·∫≠n ko c√≥ l·ªói g√¨ khi run c√°c command tr√™n, quay l·∫°i xem log git commit tr√™n remote th√¥i.\nV·∫≠y l√† ƒë√∫ng √Ω ch√∫ng ta r·ªìi, ti·∫øp t·ª•c edit code cho c·∫©n th·∫≠n r·ªìi h√£y commit l√™n nh√©\n4. Pull code t·ª´ 1 repository kh√°c gi·ªØ nguy√™n history commit https://stackoverflow.com/a/46289324/9922066\nGi·∫£ s·ª≠ b·∫°n c√≥ 1 repo A. B·∫°n mu·ªën archive repo A v√† chuy·ªÉn h·∫øt code sang repo B. Gi·ªØ nguy√™n history commit.\n-\u0026gt; T·∫°o repo B, gi·∫£ s·ª≠ branch m·∫∑c ƒë·ªãnh l√† master.\n-\u0026gt; B·∫°n t·∫°o branch m·ªõi (develop) tr√™n repo B m√† ko c√≥ content g√¨ c·∫£ b·∫±ng c√°ch:\ngit checkout --orphan develop git reset --hard git pull [REPO_A_URL] [REPO_A_BRANCH] Nh∆∞ v·∫≠y repo B (branch develop) ƒë√£ c√≥ t·∫•t c·∫£ history commit c·ªßa repo A (branch REPO_A_BRANCH)\n5. Delete all commit history https://stackoverflow.com/questions/13716658/how-to-delete-all-commit-history-in-github\nGi·∫£ s·ª≠ b·∫°n mu·ªën x√≥a h·∫øt commit history th√¨ l√†m nh∆∞ c√°c command sau:\n# Create new empty branch git checkout --orphan latest_branch # Add all the files git add -A # Commit the changes git commit -am \u0026#34;commit message\u0026#34; # Delete the branch git branch -D master # Rename the current branch to master git branch -m master # Finally, force update your repository git push -f origin master CREDIT https://thoughtbot.com/blog/git-interactive-rebase-squash-amend-rewriting-history\nhttps://kipalog.com/posts/Viet-lai-lich-su-cua-git-bang-cach-su-dung-interactive-rebase\nhttps://git-scm.com/docs/git-push\nhttps://git-scm.com/docs/git-fetch\nhttps://git-scm.com/docs/git-clean\nhttps://stackoverflow.com/a/46289324/9922066\n","href":"/posts/git-tutor-rebase-stash-reset/","title":"Amateur tutor Git commands (rebase interactive, stash, reset)"},{"content":"","href":"/tags/argocd/","title":"Argocd"},{"content":"","href":"/tags/gitlab/","title":"Gitlab"},{"content":"1. Intro Demo about ArgoCD with private Gitlab CI repository, private ACR 2. Prerequisites  Basic knowledge about Azure, ssh-keygen, K8S, Gitlab, Docker commands Workspace: run commands on Azure Cloudshell Prepare an AKS cluster (Standard_DS2_v2, kubernetes version 1.21.2) Prepare an ACR (Azure container registry) with credential  export ACR_SERVER=\u0026#34;YOUR_ACR_NAME.azurecr.io\u0026#34; export ACR_USER=\u0026#34;YOUR_ACR_USER_NAME\u0026#34; export ACR_PASSWORD=\u0026#34;YOUR_ACR_PASSWORD\u0026#34; Clone 2 projects to your workspace:\n https://gitlab.com/argocd-demo2801/gitops-chart https://gitlab.com/argocd-demo2801/webapp  3. Setup 3.1. Config on Webapp repository https://gitlab.com/argocd-demo2801/webapp\nGenerate an SSH keypair by ssh-keygen command, you\u0026rsquo;ll have a pair of SSH_PRIVATE_KEY and SSH_PUBLIC_KEY\nSet variables (Setting -\u0026gt; CI/CD -\u0026gt; Variables): CI_REGISTRY_PASSWORD, CI_REGISTRY_URL, CI_REGISTRY_USER, SSH_PRIVATE_KEY\n3.2. Config on GitOps repository https://gitlab.com/argocd-demo2801/gitops-chart\n  Go to Setting -\u0026gt; Repository -\u0026gt; Deploy Key, set Deploy Key: paste the SSH_PUBLIC_KEY value (grant write permission to it)   Edit gitops-chart/argocd/argocd-apps/webapp-python.yml file, Update 3 places of \u0026lt;YOUR_ACR_NAME\u0026gt;:\n  and push it to master branch again.\n3.3. Confirm the Pipeline Try to run the CI to confirm everything OK, Gitlab Pipeline of \u0026ldquo;https://gitlab.com/argocd-demo2801/webapp\u0026quot; should finish successful.\nYou should push some small changes to dev branch, and master branch to see.\n3.4. Config on Cluster Prepare namespaces:\n#Create namespaces for dev,staging,prod environment kubectl create namespace dev kubectl create namespace staging kubectl create namespace prod Create k8s secret to store ACR credential:\nkubectl create secret docker-registry regcred \\  --docker-server=$ACR_SERVER \\  --docker-username=$ACR_USER \\  --docker-password=$ACR_PASSWORD \\  --namespace=dev kubectl create secret docker-registry regcred \\  --docker-server=$ACR_SERVER \\  --docker-username=$ACR_USER \\  --docker-password=$ACR_PASSWORD \\  --namespace=staging kubectl create secret docker-registry regcred \\  --docker-server=$ACR_SERVER \\  --docker-username=$ACR_USER \\  --docker-password=$ACR_PASSWORD \\  --namespace=prod Install ArgoCD:\nkubectl create namespace argocd helm repo add argo https://argoproj.github.io/argo-helm helm repo update cd gitops-chart/argocd/argocd-install/ helm install argocd -n argocd argo/argo-cd -f values-override.yml Get ArgoCD public IP:\nkubectl get svc -n argocd Get ArgoCD login password, update it (username: admin):\nARGOCD_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) argocd login \u0026lt;ARGOCD_SERVER_IP\u0026gt; --username admin --password $ARGOCD_PWD # change password argocd account update-password Add gitops-chart repository to ArgoCD:\nargocd repo add https://gitlab.com/argocd-demo2801/gitops-chart.git --username \u0026lt;YOUR_GITLAB_USER\u0026gt; --password \u0026lt;Gitlab personal access token\u0026gt; If above command failed for some reason, you can add by using UI, just ensure that it\u0026rsquo;s working as the image below on ArgoCD Admin UI:\n4. Understand the file structure File gitops-chart/argocd/argocd-install/values-override.yml at step \u0026ldquo;Install ArgoCD\u0026rdquo; create:\n 2 Apps: argocd-appprojects and argocd-apps. (They have responsibility to sync template in argocd/argocd-apps and argocd/argocd-appprojects/. ) 1 special Project: argocdprojects. (It contains 2 above applications argocd-appprojects and argocd-apps.)  File gitops-chart/argocd/argocd-install/argocd-appprojects/prj-1.yml create:\n 1 Project named prj-1. It can access to (dev,staging,prod) namespaces only (it\u0026rsquo;s syncing)  3 folders in gitops-chart/charts/webapp-python create:\n 3 Apps named webapp-python-dev , webapp-python-staging, webapp-python-prod. They belong to prj-1. (They\u0026rsquo;re syncing)  5. Understand the CICD Flow The gitlab-ci.yml file described everything:\nWe have 3 envs: Dev, Staging, Prod. 2 branchs: dev and master\nChanges made to branch dev reflect to Dev env only.\nChanges made to branch master reflect to Staging env first, and you need to approve maunally later to reflect on Prod env.\n6. Test 6.1. Commit some changes Now you can test by push some changes to project https://gitlab.com/argocd-demo2801/webapp.git in branch dev:\nChanges will reflect to the Dev environment.\nWhen you merge branch dev to master:\nChanges will reflect to the Staging environment first, then you need to manual approve to trigger deploy to Prod environment.\n6.2. Make some changes Make some changes on argocd/argocd-appprojects/, or add new yml file then push to master branch: It will reflect to K8S automatically\nMake some changes on argocd/argocd-apps/, or add new yml file then push to master branch: It will reflect to K8S automatically\nFor instance:\n You can create new environment named QA for the webapp-python by edit file webapp-python.yml and charts/webapp-python  6.3. If your new app dont have Helm chart If your new app dont have Helm chart (just manifest yaml files). Follow this demo to see. I will create a new spring-maven-postgres-docker-k8s app, it belongs to prj-2 project. It\u0026rsquo;s in dev environment only.\n Create new prj-2.yml file in folder argocd/argocd-appprojects:  # this AppProject can access only to \u0026#34;dev,prod\u0026#34; namespaces apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: prj-2 namespace: argocd spec: clusterResourceWhitelist: - group: \u0026#39;*\u0026#39; kind: \u0026#39;*\u0026#39; destinations: - namespace: dev server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc orphanedResources: warn: false sourceRepos: - \u0026#39;*\u0026#39;  Create new spring-maven-postgres-docker-k8s.yml file in folder argocd/argocd-appps:  --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: spring-maven-postgres-docker-k8s-dev namespace: argocd spec: project: prj-2 source: path: charts/spring-maven-postgres-docker-k8s/dev repoURL: https://gitlab.com/argocd-demo2801/gitops-chart.git targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: dev syncPolicy: automated: prune: true  Create new directory to store manifest charts/spring-maven-postgres-docker-k8s/dev, it includes 4 files:   First file: docker_postgres-deployment.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: docker-postgres labels: app: docker-postgres spec: selector: matchLabels: app: docker-postgres replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-postgres spec: containers: - image: hoangmnsd/docker_postgres env: - name: POSTGRES_USER value: dbuser - name: POSTGRES_PASSWORD value: password - name: POSTGRES_DB value: store imagePullPolicy: Always name: docker-postgres ports: - containerPort: 5432  Second file: docker_postgres-service.yaml  apiVersion: v1 kind: Service metadata: name: docker-postgres spec: ports: - port: 5432 protocol: TCP targetPort: 5432 selector: app: docker-postgres  Third file: docker_spring-boot-containers-deployment.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: docker-spring-boot-containers labels: app: docker-spring-boot-containers spec: selector: matchLabels: app: docker-spring-boot-containers replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-spring-boot-containers spec: containers: - image: hoangmnsd/docker_spring-boot-containers imagePullPolicy: Always name: docker-spring-boot-containers ports: - containerPort: 12345  Fourth file: docker_spring-boot-containers-service.yaml  apiVersion: v1 kind: Service metadata: name: docker-spring-boot-containers spec: type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 12345 selector: app: docker-spring-boot-containers  Push them to master branch to see changes apply to ArgoCD\n  7. Open Issue In file /argocd_demo2801/gitops-chart/argocd/argocd-apps/webapp-python.yml,\nI am hardcoding the image.repository parameter when create ArgoCD Application kind, because there is no way to secure it from local environment variable. The issue\u0026rsquo;s reported here:\nhttps://github.com/argoproj/argo-cd/issues/1786\nI heard the same complaint from other people with them ending up choosing Flux over Argo for this sole reason.\nThere are also some workaround to get over this, but not solve problem completely.\nI want to use envsubst to workaround in this tutorial but Azure Cloudshell doesn\u0026rsquo;t support envsubst command.\nFor someone who want to see answer about envsubst: https://stackoverflow.com/a/68936302/9922066\n8. Clean up Notes: When you delete files on gitops-chart/argocd/argocd-appprojects, gitops-chart/argocd/argocd-apps, and push changes to master branch, the applications and project will be remove on ArgoCD (it makes sense and that\u0026rsquo;s what we think).\nBut the deployment/pod/svc are not removed from K8S Cluster at all. They remain until we execute this command:\nkubectl delete svc docker-spring-boot-containers -n dev kubectl delete deployment docker-spring-boot-containers -n dev Not sure it is an issue or just a feature of ArgoCD? ü§î\n# delete helm argocd release helm uninstall argocd -n argocd CREDITS https://medium.com/devopsturkiye/self-managed-argo-cd-app-of-everything-a226eb100cf0\nhttps://levelup.gitconnected.com/gitops-in-kubernetes-with-gitlab-ci-and-argocd-9e20b5d3b55b\nhttps://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6\nhttps://github.com/kurtburak/argocd\n","href":"/bk/encrypt-playaround-w-argocd-and-private-gitlab-ci-acr/","title":"GitOps: Playaround with ArgoCD and private Gitlab CI, ACR"},{"content":"1. Intro Demo about ArgoCD with private Gitlab CI repository, private ACR 2. Prerequisites  Basic knowledge about Azure, ssh-keygen, K8S, Gitlab, Docker commands Workspace: run commands on Azure Cloudshell Prepare an AKS cluster (Standard_DS2_v2, kubernetes version 1.21.2) Prepare an ACR (Azure container registry) with credential  export ACR_SERVER=\u0026#34;YOUR_ACR_NAME.azurecr.io\u0026#34; export ACR_USER=\u0026#34;YOUR_ACR_USER_NAME\u0026#34; export ACR_PASSWORD=\u0026#34;YOUR_ACR_PASSWORD\u0026#34; Clone 2 projects to your workspace:\n https://gitlab.com/argocd-demo2801/gitops-chart https://gitlab.com/argocd-demo2801/webapp  3. Setup 3.1. Config on Webapp repository https://gitlab.com/argocd-demo2801/webapp\nGenerate an SSH keypair by ssh-keygen command, you\u0026rsquo;ll have a pair of SSH_PRIVATE_KEY and SSH_PUBLIC_KEY\nSet variables (Setting -\u0026gt; CI/CD -\u0026gt; Variables): CI_REGISTRY_PASSWORD, CI_REGISTRY_URL, CI_REGISTRY_USER, SSH_PRIVATE_KEY\n3.2. Config on GitOps repository https://gitlab.com/argocd-demo2801/gitops-chart\n  Go to Setting -\u0026gt; Repository -\u0026gt; Deploy Key, set Deploy Key: paste the SSH_PUBLIC_KEY value (grant write permission to it)   Edit gitops-chart/argocd/argocd-apps/webapp-python.yml file, Update 3 places of \u0026lt;YOUR_ACR_NAME\u0026gt;:\n  and push it to master branch again.\n3.3. Confirm the Pipeline Try to run the CI to confirm everything OK, Gitlab Pipeline of \u0026ldquo;https://gitlab.com/argocd-demo2801/webapp\u0026quot; should finish successful.\nYou should push some small changes to dev branch, and master branch to see.\n3.4. Config on Cluster Prepare namespaces:\n#Create namespaces for dev,staging,prod environment kubectl create namespace dev kubectl create namespace staging kubectl create namespace prod Create k8s secret to store ACR credential:\nkubectl create secret docker-registry regcred \\  --docker-server=$ACR_SERVER \\  --docker-username=$ACR_USER \\  --docker-password=$ACR_PASSWORD \\  --namespace=dev kubectl create secret docker-registry regcred \\  --docker-server=$ACR_SERVER \\  --docker-username=$ACR_USER \\  --docker-password=$ACR_PASSWORD \\  --namespace=staging kubectl create secret docker-registry regcred \\  --docker-server=$ACR_SERVER \\  --docker-username=$ACR_USER \\  --docker-password=$ACR_PASSWORD \\  --namespace=prod Install ArgoCD:\nkubectl create namespace argocd helm repo add argo https://argoproj.github.io/argo-helm helm repo update cd gitops-chart/argocd/argocd-install/ helm install argocd -n argocd argo/argo-cd -f values-override.yml Get ArgoCD public IP:\nkubectl get svc -n argocd Get ArgoCD login password, update it (username: admin):\nARGOCD_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) argocd login \u0026lt;ARGOCD_SERVER_IP\u0026gt; --username admin --password $ARGOCD_PWD # change password argocd account update-password Add gitops-chart repository to ArgoCD:\nargocd repo add https://gitlab.com/argocd-demo2801/gitops-chart.git --username \u0026lt;YOUR_GITLAB_USER\u0026gt; --password \u0026lt;Gitlab personal access token\u0026gt; If above command failed for some reason, you can add by using UI, just ensure that it\u0026rsquo;s working as the image below on ArgoCD Admin UI:\n4. Understand the file structure File gitops-chart/argocd/argocd-install/values-override.yml at step \u0026ldquo;Install ArgoCD\u0026rdquo; create:\n 2 Apps: argocd-appprojects and argocd-apps. (They have responsibility to sync template in argocd/argocd-apps and argocd/argocd-appprojects/. ) 1 special Project: argocdprojects. (It contains 2 above applications argocd-appprojects and argocd-apps.)  File gitops-chart/argocd/argocd-install/argocd-appprojects/prj-1.yml create:\n 1 Project named prj-1. It can access to (dev,staging,prod) namespaces only (it\u0026rsquo;s syncing)  3 folders in gitops-chart/charts/webapp-python create:\n 3 Apps named webapp-python-dev , webapp-python-staging, webapp-python-prod. They belong to prj-1. (They\u0026rsquo;re syncing)  5. Understand the CICD Flow The gitlab-ci.yml file described everything:\nWe have 3 envs: Dev, Staging, Prod. 2 branchs: dev and master\nChanges made to branch dev reflect to Dev env only.\nChanges made to branch master reflect to Staging env first, and you need to approve maunally later to reflect on Prod env.\n6. Test 6.1. Commit some changes Now you can test by push some changes to project https://gitlab.com/argocd-demo2801/webapp.git in branch dev:\nChanges will reflect to the Dev environment.\nWhen you merge branch dev to master:\nChanges will reflect to the Staging environment first, then you need to manual approve to trigger deploy to Prod environment.\n6.2. Make some changes Make some changes on argocd/argocd-appprojects/, or add new yml file then push to master branch: It will reflect to K8S automatically\nMake some changes on argocd/argocd-apps/, or add new yml file then push to master branch: It will reflect to K8S automatically\nFor instance:\n You can create new environment named QA for the webapp-python by edit file webapp-python.yml and charts/webapp-python  6.3. If your new app dont have Helm chart If your new app dont have Helm chart (just manifest yaml files). Follow this demo to see. I will create a new spring-maven-postgres-docker-k8s app, it belongs to prj-2 project. It\u0026rsquo;s in dev environment only.\n Create new prj-2.yml file in folder argocd/argocd-appprojects:  # this AppProject can access only to \u0026#34;dev,prod\u0026#34; namespaces apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: prj-2 namespace: argocd spec: clusterResourceWhitelist: - group: \u0026#39;*\u0026#39; kind: \u0026#39;*\u0026#39; destinations: - namespace: dev server: https://kubernetes.default.svc - namespace: prod server: https://kubernetes.default.svc orphanedResources: warn: false sourceRepos: - \u0026#39;*\u0026#39;  Create new spring-maven-postgres-docker-k8s.yml file in folder argocd/argocd-appps:  --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: spring-maven-postgres-docker-k8s-dev namespace: argocd spec: project: prj-2 source: path: charts/spring-maven-postgres-docker-k8s/dev repoURL: https://gitlab.com/argocd-demo2801/gitops-chart.git targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: dev syncPolicy: automated: prune: true  Create new directory to store manifest charts/spring-maven-postgres-docker-k8s/dev, it includes 4 files:   First file: docker_postgres-deployment.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: docker-postgres labels: app: docker-postgres spec: selector: matchLabels: app: docker-postgres replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-postgres spec: containers: - image: hoangmnsd/docker_postgres env: - name: POSTGRES_USER value: dbuser - name: POSTGRES_PASSWORD value: password - name: POSTGRES_DB value: store imagePullPolicy: Always name: docker-postgres ports: - containerPort: 5432  Second file: docker_postgres-service.yaml  apiVersion: v1 kind: Service metadata: name: docker-postgres spec: ports: - port: 5432 protocol: TCP targetPort: 5432 selector: app: docker-postgres  Third file: docker_spring-boot-containers-deployment.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: docker-spring-boot-containers labels: app: docker-spring-boot-containers spec: selector: matchLabels: app: docker-spring-boot-containers replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-spring-boot-containers spec: containers: - image: hoangmnsd/docker_spring-boot-containers imagePullPolicy: Always name: docker-spring-boot-containers ports: - containerPort: 12345  Fourth file: docker_spring-boot-containers-service.yaml  apiVersion: v1 kind: Service metadata: name: docker-spring-boot-containers spec: type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 12345 selector: app: docker-spring-boot-containers  Push them to master branch to see changes apply to ArgoCD\n  7. Open Issue In file /argocd_demo2801/gitops-chart/argocd/argocd-apps/webapp-python.yml,\nI am hardcoding the image.repository parameter when create ArgoCD Application kind, because there is no way to secure it from local environment variable. The issue\u0026rsquo;s reported here:\nhttps://github.com/argoproj/argo-cd/issues/1786\nI heard the same complaint from other people with them ending up choosing Flux over Argo for this sole reason.\nThere are also some workaround to get over this, but not solve problem completely.\nI want to use envsubst to workaround in this tutorial but Azure Cloudshell doesn\u0026rsquo;t support envsubst command.\nFor someone who want to see answer about envsubst: https://stackoverflow.com/a/68936302/9922066\n8. Clean up Notes: When you delete files on gitops-chart/argocd/argocd-appprojects, gitops-chart/argocd/argocd-apps, and push changes to master branch, the applications and project will be remove on ArgoCD (it makes sense and that\u0026rsquo;s what we think).\nBut the deployment/pod/svc are not removed from K8S Cluster at all. They remain until we execute this command:\nkubectl delete svc docker-spring-boot-containers -n dev kubectl delete deployment docker-spring-boot-containers -n dev Not sure it is an issue or just a feature of ArgoCD? ü§î\n# delete helm argocd release helm uninstall argocd -n argocd CREDITS https://medium.com/devopsturkiye/self-managed-argo-cd-app-of-everything-a226eb100cf0\nhttps://levelup.gitconnected.com/gitops-in-kubernetes-with-gitlab-ci-and-argocd-9e20b5d3b55b\nhttps://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6\nhttps://github.com/kurtburak/argocd\n","href":"/posts/encrypt-playaround-w-argocd-and-private-gitlab-ci-acr/","title":"GitOps: Playaround with ArgoCD and private Gitlab CI, ACR"},{"content":"You cannot move a VM from a Vnet/subnet to different Vnet/subnet without re-creating the VM. But in some specific cases, you may not have to move VM to another Vnet/subnet, it will be easier when you just change the private IP of Vnet/subnet/VM/Load balancer\u0026hellip;\nBackground Assume that you\u0026rsquo;re having a Virtual machine with private IP is: 10.20.1.8 and you want to change it to: 10.25.2.9.\nSee this picture:\nNote that this article is not about moving VM to a different Vnet/Subnet. This article is about changing the private IP of Vnet, Subnet, VM, etc\u0026hellip;\nIf you try to change directly in Network interface portal, you may encounter this error:\nIt means you have to change the Subnet IP range, Vnet IP range also.\nSteps 1. Create new address space for VNet (optional) if your Vnet is peering with other Vnet, you should delete the peering first\n2. Create new subnet 3. Config on Network interface (NIC) Change IP configuration from Static to Dynamic:\nChange the subnet of NIC: After above step, your VM is already placed in the new Subnet: On the NIC IP configuration, change VM private IP from Dynamic ‚Üí Static and set it 10.25.2.9: Now your VM is already have your desired private IP: 4. Delete redundancy resources  Delete the old Subnet Delete the old address space of Vnet  Now you can re-create the Vnet Peering, then try to access the VM with new private IP to test connection.\nAbout Load balancer private IP In case you want to change Load balancer private IP, try these steps:\n Create new frontend LB configuration Take note the LB rule, health probe config Delete old frontend LB configuration Re-create new LB rule, health probe config  ","href":"/bk/azure-update-private-ip-vnet-subnet-vm-loadbalancer/","title":"Azure: How to change private IP of VM, Vnet, Subnet, Load balancer"},{"content":"You cannot move a VM from a Vnet/subnet to different Vnet/subnet without re-creating the VM. But in some specific cases, you may not have to move VM to another Vnet/subnet, it will be easier when you just change the private IP of Vnet/subnet/VM/Load balancer\u0026hellip;\nBackground Assume that you\u0026rsquo;re having a Virtual machine with private IP is: 10.20.1.8 and you want to change it to: 10.25.2.9.\nSee this picture:\nNote that this article is not about moving VM to a different Vnet/Subnet. This article is about changing the private IP of Vnet, Subnet, VM, etc\u0026hellip;\nIf you try to change directly in Network interface portal, you may encounter this error:\nIt means you have to change the Subnet IP range, Vnet IP range also.\nSteps 1. Create new address space for VNet (optional) if your Vnet is peering with other Vnet, you should delete the peering first\n2. Create new subnet 3. Config on Network interface (NIC) Change IP configuration from Static to Dynamic:\nChange the subnet of NIC: After above step, your VM is already placed in the new Subnet: On the NIC IP configuration, change VM private IP from Dynamic ‚Üí Static and set it 10.25.2.9: Now your VM is already have your desired private IP: 4. Delete redundancy resources  Delete the old Subnet Delete the old address space of Vnet  Now you can re-create the Vnet Peering, then try to access the VM with new private IP to test connection.\nAbout Load balancer private IP In case you want to change Load balancer private IP, try these steps:\n Create new frontend LB configuration Take note the LB rule, health probe config Delete old frontend LB configuration Re-create new LB rule, health probe config  ","href":"/posts/azure-update-private-ip-vnet-subnet-vm-loadbalancer/","title":"Azure: How to change private IP of VM, Vnet, Subnet, Load balancer"},{"content":"Gi·ªëng nh∆∞ IAM Role cho EC2 tr√™n AWS, th√¨ tr√™n Azure c≈©ng c√≥ feature t∆∞∆°ng t·ª± ƒë√≥ l√† Managed identity (System assgined identity ho·∫∑c User assigned identity)\nPrerequisites B·∫°n ƒë√£ t·∫°o User-Assgined identity, VM ƒë√£ enable identity.\nStory Theo Azure official document, v√≠ d·ª• ƒë·ªÉ l·∫•y access token d√πng Azure Resource manager th√¨ t·ª´ trong VM c·∫ßn run command nh∆∞ n√†y:\nresponse=$(curl \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https%3A%2F%2Fmanagement.azure.com%2F\u0026#39; -H Metadata:true -s) access_token=$(echo $response | python -c \u0026#39;import sys, json; print (json.load(sys.stdin)[\u0026#34;access_token\u0026#34;])\u0026#39;) echo The managed identities for Azure resources access token is $access_token Nh∆∞ng n·∫øu mu·ªën l·∫•y access token d√πng ƒë·ªÉ truy c·∫≠p \u0026ldquo;Azure Digital Twins\u0026rdquo;, th√¨ link c·∫ßn thay ƒë·ªïi 1 ch√∫t (ko c√≥ %2F ·ªü cu·ªëi url n·ªØa -\u0026gt; m√¨nh ƒë√£ m·∫•t 1 ng√†y ƒë·ªÉ nh·∫≠n ra ƒëi·ªÅu n√†y üò´ -\u0026gt; l√Ω do vi·∫øt b√†i n√†y):\ncurl \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https%3A%2F%2Fdigitaltwins.azure.net\u0026#39; -H Metadata:true -s response=$(curl \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https%3A%2F%2Fdigitaltwins.azure.net\u0026#39; -H Metadata:true -s) access_token=$(echo $response | python -c \u0026#39;import sys, json; print (json.load(sys.stdin)[\u0026#34;access_token\u0026#34;])\u0026#39;) echo The managed identities for Azure resources access token is $access_token CREDIT:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#get-a-token-using-curl\n","href":"/bk/azure-request-accesstoken-to-digital-twins-managed-identity/","title":"Azure: Request Access Token to Digital Twins use User-Assigned Identity"},{"content":"Gi·ªëng nh∆∞ IAM Role cho EC2 tr√™n AWS, th√¨ tr√™n Azure c≈©ng c√≥ feature t∆∞∆°ng t·ª± ƒë√≥ l√† Managed identity (System assgined identity ho·∫∑c User assigned identity)\nPrerequisites B·∫°n ƒë√£ t·∫°o User-Assgined identity, VM ƒë√£ enable identity.\nStory Theo Azure official document, v√≠ d·ª• ƒë·ªÉ l·∫•y access token d√πng Azure Resource manager th√¨ t·ª´ trong VM c·∫ßn run command nh∆∞ n√†y:\nresponse=$(curl \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https%3A%2F%2Fmanagement.azure.com%2F\u0026#39; -H Metadata:true -s) access_token=$(echo $response | python -c \u0026#39;import sys, json; print (json.load(sys.stdin)[\u0026#34;access_token\u0026#34;])\u0026#39;) echo The managed identities for Azure resources access token is $access_token Nh∆∞ng n·∫øu mu·ªën l·∫•y access token d√πng ƒë·ªÉ truy c·∫≠p \u0026ldquo;Azure Digital Twins\u0026rdquo;, th√¨ link c·∫ßn thay ƒë·ªïi 1 ch√∫t (ko c√≥ %2F ·ªü cu·ªëi url n·ªØa -\u0026gt; m√¨nh ƒë√£ m·∫•t 1 ng√†y ƒë·ªÉ nh·∫≠n ra ƒëi·ªÅu n√†y üò´ -\u0026gt; l√Ω do vi·∫øt b√†i n√†y):\ncurl \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https%3A%2F%2Fdigitaltwins.azure.net\u0026#39; -H Metadata:true -s response=$(curl \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https%3A%2F%2Fdigitaltwins.azure.net\u0026#39; -H Metadata:true -s) access_token=$(echo $response | python -c \u0026#39;import sys, json; print (json.load(sys.stdin)[\u0026#34;access_token\u0026#34;])\u0026#39;) echo The managed identities for Azure resources access token is $access_token CREDIT:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#get-a-token-using-curl\n","href":"/posts/azure-request-accesstoken-to-digital-twins-managed-identity/","title":"Azure: Request Access Token to Digital Twins use User-Assigned Identity"},{"content":"This is just my notes when playaround with python-selenium, not a tutorial\n1. Workspace 1: Windows, GitBash terminal, PowerShell, Docker for Windows, Firefox Prepare:\n  Install Python, selenium follow this docs:\nhttps://selenium-python.readthedocs.io/installation.html\nOpen GitBash:\nsh pip install selenium\nDownload geckodriver, extract to exe file, update PATH environment\n  Run Selenium webdriver follow this docs:\nhttps://github.com/SeleniumHQ/docker-selenium#quick-start\nRun on Powershell:\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\u0026#34;2g\u0026#34; selenium/standalone-firefox:4.0.0-rc-2-20210930   After all, you can run your python script in GitBash\n  Open the Selenium Webdriver to see what happen when your code execute\n  Script:\nThis is a sample script for particular website, you should change the code arcording to your site:\n# import unittest from selenium import webdriver from selenium.webdriver.common.keys import Keys import logging, os from time import sleep \u0026#34;\u0026#34;\u0026#34; Get particular monthly fee from https://xxxx.com You should prepair some Evironment Variables as below sample: USER_NAME: user name to login PASSWORD: password to login This script has no required argument \u0026#34;\u0026#34;\u0026#34; # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) MONTH_TO_SEARCH = \u0026#34;10/2021\u0026#34; # get env variables USER = os.environ.get(\u0026#39;USER_NAME\u0026#39;) PASSWORD = os.environ.get(\u0026#39;PASSWORD\u0026#39;) # setup driver = webdriver.Firefox() driver.implicitly_wait(2) driver.get(\u0026#34;https://xxxx.com/login\u0026#34;) # login user = driver.find_element_by_name(\u0026#34;Username\u0026#34;) user.clear() user.send_keys(USER) passw = driver.find_element_by_name(\u0026#34;Password\u0026#34;) passw.clear() passw.send_keys(PASSWORD) driver.find_element_by_xpath(\u0026#39;//button[@type=\u0026#34;submit\u0026#34;]\u0026#39;).click() sleep(5) # direct to other, then get data driver.get(\u0026#34;https://xxxx.com/ThanhToanTrucTuyen/LichSuThanhToan\u0026#34;) # input start month start_month = driver.find_element_by_id(\u0026#34;thangBatDau\u0026#34;) start_month.send_keys(Keys.DELETE, MONTH_TO_SEARCH) # input end month end_month = driver.find_element_by_id(\u0026#34;thangKetThuc\u0026#34;) end_month.send_keys(Keys.DELETE, MONTH_TO_SEARCH) # click button to get data driver.find_element_by_id(\u0026#34;btnSearch\u0026#34;).click() sleep(5) # wait for getting data month_id_raw = driver.find_elements_by_xpath(\u0026#34;/html/body/div[1]/main/div[3]/div/div/div/div[4]/div/table/tbody/tr/td[1]\u0026#34;) print(\u0026#34;number of records: \u0026#34; + str(len(month_id_raw))) if len(month_id_raw) == 0: print(\u0026#34;Have no data of %s\u0026#34; % MONTH_TO_SEARCH) if len(month_id_raw) == 1: print(\u0026#34;Month: %s\u0026#34; % month_id_raw[0].text) total_fee_raw = driver.find_elements_by_xpath(\u0026#34;/html/body/div[1]/main/div[3]/div/div/div/div[4]/div/table/tbody/tr/td[8]\u0026#34;) print(\u0026#34;Total of %s is: %s VND\u0026#34; % (MONTH_TO_SEARCH, total_fee_raw[0].text)) # close sleep(5) driver.close() 2. Workspace 2: Raspberry Pi 4B 8G, Raspion OS Lite without desktop Prepare:\npip3 install selenium sudo apt-get install chromium-chromedriver -y docker run --name selenium -d -p 4444:4444 --shm-size=\u0026#34;2g\u0026#34; kynetiv/selenium-standalone-chromium-pi:latest Script:\n# import unittest from selenium import webdriver from selenium.webdriver.common.keys import Keys import logging, os from time import sleep from selenium.webdriver.chrome.options import Options \u0026#34;\u0026#34;\u0026#34; Get particular monthly fee from https://xxxx.com You should prepair some Evironment Variables as below sample: USER_NAME: user name to login PASSWORD: password to login This script has no required argument. Test successfully on Raspian OS (Raspberry Pi 4B). To prepare environment on Raspian OS, follow this topic: https://hoangmnsd.github.io/posts/encrypt-play-around-w-selenium/ \u0026#34;\u0026#34;\u0026#34; # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) MONTH_TO_SEARCH = \u0026#34;05/2022\u0026#34; # get env variables USER = os.environ.get(\u0026#39;USER_NAME\u0026#39;) PASSWORD = os.environ.get(\u0026#39;PASSWORD\u0026#39;) # setup chrome_options = Options() chrome_options.add_argument(\u0026#34;--headless\u0026#34;) chrome_options.add_argument(\u0026#39;ignore-certificate-errors\u0026#39;) # if you are want to access a not valid cert website (insecure) driver = webdriver.Chrome(executable_path = \u0026#39;/usr/lib/chromium-browser/chromedriver\u0026#39;,options=chrome_options) driver.get(\u0026#34;https://xxxx.com/login\u0026#34;) # html = driver.page_source # DEBUG # print(html) # DEBUG ","href":"/bk/encrypt-play-around-w-selenium/","title":"Play around with Python-Selenium"},{"content":"This is just my notes when playaround with python-selenium, not a tutorial\n1. Workspace 1: Windows, GitBash terminal, PowerShell, Docker for Windows, Firefox Prepare:\n  Install Python, selenium follow this docs:\nhttps://selenium-python.readthedocs.io/installation.html\nOpen GitBash:\nsh pip install selenium\nDownload geckodriver, extract to exe file, update PATH environment\n  Run Selenium webdriver follow this docs:\nhttps://github.com/SeleniumHQ/docker-selenium#quick-start\nRun on Powershell:\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\u0026#34;2g\u0026#34; selenium/standalone-firefox:4.0.0-rc-2-20210930   After all, you can run your python script in GitBash\n  Open the Selenium Webdriver to see what happen when your code execute\n  Script:\nThis is a sample script for particular website, you should change the code arcording to your site:\n# import unittest from selenium import webdriver from selenium.webdriver.common.keys import Keys import logging, os from time import sleep \u0026#34;\u0026#34;\u0026#34; Get particular monthly fee from https://xxxx.com You should prepair some Evironment Variables as below sample: USER_NAME: user name to login PASSWORD: password to login This script has no required argument \u0026#34;\u0026#34;\u0026#34; # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) MONTH_TO_SEARCH = \u0026#34;10/2021\u0026#34; # get env variables USER = os.environ.get(\u0026#39;USER_NAME\u0026#39;) PASSWORD = os.environ.get(\u0026#39;PASSWORD\u0026#39;) # setup driver = webdriver.Firefox() driver.implicitly_wait(2) driver.get(\u0026#34;https://xxxx.com/login\u0026#34;) # login user = driver.find_element_by_name(\u0026#34;Username\u0026#34;) user.clear() user.send_keys(USER) passw = driver.find_element_by_name(\u0026#34;Password\u0026#34;) passw.clear() passw.send_keys(PASSWORD) driver.find_element_by_xpath(\u0026#39;//button[@type=\u0026#34;submit\u0026#34;]\u0026#39;).click() sleep(5) # direct to other, then get data driver.get(\u0026#34;https://xxxx.com/ThanhToanTrucTuyen/LichSuThanhToan\u0026#34;) # input start month start_month = driver.find_element_by_id(\u0026#34;thangBatDau\u0026#34;) start_month.send_keys(Keys.DELETE, MONTH_TO_SEARCH) # input end month end_month = driver.find_element_by_id(\u0026#34;thangKetThuc\u0026#34;) end_month.send_keys(Keys.DELETE, MONTH_TO_SEARCH) # click button to get data driver.find_element_by_id(\u0026#34;btnSearch\u0026#34;).click() sleep(5) # wait for getting data month_id_raw = driver.find_elements_by_xpath(\u0026#34;/html/body/div[1]/main/div[3]/div/div/div/div[4]/div/table/tbody/tr/td[1]\u0026#34;) print(\u0026#34;number of records: \u0026#34; + str(len(month_id_raw))) if len(month_id_raw) == 0: print(\u0026#34;Have no data of %s\u0026#34; % MONTH_TO_SEARCH) if len(month_id_raw) == 1: print(\u0026#34;Month: %s\u0026#34; % month_id_raw[0].text) total_fee_raw = driver.find_elements_by_xpath(\u0026#34;/html/body/div[1]/main/div[3]/div/div/div/div[4]/div/table/tbody/tr/td[8]\u0026#34;) print(\u0026#34;Total of %s is: %s VND\u0026#34; % (MONTH_TO_SEARCH, total_fee_raw[0].text)) # close sleep(5) driver.close() 2. Workspace 2: Raspberry Pi 4B 8G, Raspion OS Lite without desktop Prepare:\npip3 install selenium sudo apt-get install chromium-chromedriver -y docker run --name selenium -d -p 4444:4444 --shm-size=\u0026#34;2g\u0026#34; kynetiv/selenium-standalone-chromium-pi:latest Script:\n# import unittest from selenium import webdriver from selenium.webdriver.common.keys import Keys import logging, os from time import sleep from selenium.webdriver.chrome.options import Options \u0026#34;\u0026#34;\u0026#34; Get particular monthly fee from https://xxxx.com You should prepair some Evironment Variables as below sample: USER_NAME: user name to login PASSWORD: password to login This script has no required argument. Test successfully on Raspian OS (Raspberry Pi 4B). To prepare environment on Raspian OS, follow this topic: https://hoangmnsd.github.io/posts/encrypt-play-around-w-selenium/ \u0026#34;\u0026#34;\u0026#34; # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) MONTH_TO_SEARCH = \u0026#34;05/2022\u0026#34; # get env variables USER = os.environ.get(\u0026#39;USER_NAME\u0026#39;) PASSWORD = os.environ.get(\u0026#39;PASSWORD\u0026#39;) # setup chrome_options = Options() chrome_options.add_argument(\u0026#34;--headless\u0026#34;) chrome_options.add_argument(\u0026#39;ignore-certificate-errors\u0026#39;) # if you are want to access a not valid cert website (insecure) driver = webdriver.Chrome(executable_path = \u0026#39;/usr/lib/chromium-browser/chromedriver\u0026#39;,options=chrome_options) driver.get(\u0026#34;https://xxxx.com/login\u0026#34;) # html = driver.page_source # DEBUG # print(html) # DEBUG ","href":"/posts/encrypt-play-around-w-selenium/","title":"Play around with Python-Selenium"},{"content":"","href":"/tags/selenium/","title":"Selenium"},{"content":"1. Gi·ªõi thi·ªáu Vi·ªác copy 1 file t·ª´ storage account n√†y sang 1 storage acccount kh√°c tenant kh√¥ng h·∫≥n l√† kh√≥ khƒÉn, nh∆∞ng c≈©ng ƒë√≤i h·ªèi 1 ch√∫t ch√∫ √Ω ƒë·ªÉ tr√°nh m·∫•t th·ªùi gian\n2. Y√™u c·∫ßu  D√πng WSL Download AzCopy tool, refer: https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10  3. C√°ch l√†m 3.1. Copy a file from a tenant to another ƒê·∫ßu ti√™n c·∫ßn n·∫Øm ƒë∆∞·ª£c syntax:\nCopy a single blob to another blob by using a SAS token.\n azcopy cp \u0026ldquo;https://[srcaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026rdquo; \u0026ldquo;https://[destaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026rdquo;  Copy a single blob to another blob by using a SAS token and an OAuth token. You have to use a SAS token at the end of the source account URL, but the destination account doesn\u0026rsquo;t need one if you log into AzCopy by using the azcopy login command.\n azcopy cp \u0026ldquo;https://[srcaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026rdquo; \u0026ldquo;https://[destaccount].blob.core.windows.net/[container]/[path/to/blob]\u0026rdquo;  Nh∆∞ v·∫≠y, ch√∫ng ta ƒëi theo h∆∞·ªõng d√πng syntax s·ªë 1, ƒë·ªÉ ko ph·∫£i login v√†o AzCopy tool:\nazcopy cp \u0026#34;https://[srcaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026#34; \u0026#34;https://[destaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026#34; ƒê·∫ßu ti√™n l√† l·∫•y SAS URL c·ªßa Source Account file, b·∫°n ch·ªâ c·∫ßn ƒë·ªÉ quy·ªÅn READ l√† ƒë·ªß: Th·ª© 2 l√† l·∫•y SAS URL c·ªßa Destination Account container, ch·ªó b·∫°n s·∫Ω paste file v√†o, th√¨ c·∫ßn c·∫£ quy·ªÅn WRITE n·ªØa (nhi·ªÅu ng ko ƒë·ªÉ √Ω th√¨ khi copy s·∫Ω b·ªã l·ªói): GI·ªù run command:\nazcopy copy \u0026#39;https://abcd12345678.blob.core.windows.net/container1/abcd123455666.zip?sp=r\u0026amp;st=2020-10-08T15:32:23Z\u0026amp;se=2020-10-09T23:32:23Z\u0026amp;spr=https\u0026amp;sv=2020-08-04\u0026amp;sr=b\u0026amp;sig=UP7bXNoIbXNoIbXNoIbXNoIbXNoIbXNoIbXNoIw%3D\u0026#39; \u0026#39;https://abc1234.blob.core.windows.net/container2?sp=racwl\u0026amp;st=2020-10-09T16:11:19Z\u0026amp;se=2020-10-12T00:11:19Z\u0026amp;spr=https\u0026amp;sv=2020-08-04\u0026amp;sr=c\u0026amp;sig=idW348Um3AQ348Um3AQj348Um3AQj348Um3AQjjxe4%3D\u0026#39; INFO: Scanning... INFO: Failed to create one or more destination container(s). Your transfers may still succeed if the container already exists. INFO: Any empty folders will not be processed, because source and/or destination doesn\u0026#39;t have full folder support Job e4304ef7-e5eb-b24e-5477-6a55642e9341 has started Log file is located at: 100.0 %, 0 Done, 0 Failed, 1 Pending, 0 Skipped, 1 Total, Job q1234ef7-xxxxxxxxxxxxx-2e9842 summary Elapsed Time (Minutes): 1.234 Number of File Transfers: 1 Number of Folder Property Transfers: 0 Total Number of Transfers: 1 Number of Transfers Completed: 1 Number of Transfers Failed: 0 Number of Transfers Skipped: 0 TotalBytesTransferred: 32213303808 Final Job Status: Completed Command run xong nh∆∞ng job COPY v·∫´n ƒëang ch·∫°y nha c√°c b·∫°n. N·∫øu nh√¨n th·∫•y h√¨nh tr√™n c√≥ nghƒ©a l√† v·∫´n ch∆∞a copy xong ƒë√¢u.\nH√£y ki√™n nh·∫´n ch·ªù bao gi·ªù copy xong n√≥ s·∫Ω nh∆∞ th·∫ø n√†y: 3.2. Copy all content in a container to another https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy\nRun azcopy --help ƒë·ªÉ xem format command copy 1 container t·ª´ b√™n n√†y sang b√™n kia\nStep 1: T·∫°o tr∆∞·ªõc 1 container b√™n Destination Storage account (ko nh·∫•t thi·∫øt ph·∫£i gi·ªëng t√™n v·ªõi source container).\nStep 2: L·∫•y SAS token c·ªßa container b√™n Source Storage account, c·∫ßn quy·ªÅn READ v√† LIST\nStep 3: L·∫•y SAS token c·ªßa container b√™n Destination Storage account, c·∫ßn quy·ªÅn WRITE, ADD..\nStep 4: run az cp command\nN·∫øu b·ªã l·ªói n√†y:\nfailed to perform copy command due to error: cannot start job due to error: cannot list files due to reason -\u0026gt; github.com/Azure/azure-storage-blob-go/azblob.newStorageError, /home/vsts/go/pkg/mod/github.com/!azure/azure-storage-blob-go@v0.15.0/azblob/zc_storage_error.go:42 ===== RESPONSE ERROR (ServiceCode=AuthorizationPermissionMismatch) ===== Description=This request is not authorized to perform this operation using this permission. -\u0026gt; Nghƒ©a l√† V√¨ m√¨nh ƒëang copy All content t·ª´ 1 container n√™n Source SAS c·∫ßn quy·ªÅn LIST, c·∫ßn t·∫°o l·∫°i SAS token\nCh√∫c c√°c b·∫°n th√†nh c√¥ng!! ‚úå\nCREDITS https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy?toc=/azure/storage/blobs/toc.json\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy\n","href":"/bk/azure-copy-blob-from-a-tenant-to-another/","title":"Azure: Copy a blob from a tenant to another tenant account"},{"content":"1. Gi·ªõi thi·ªáu Vi·ªác copy 1 file t·ª´ storage account n√†y sang 1 storage acccount kh√°c tenant kh√¥ng h·∫≥n l√† kh√≥ khƒÉn, nh∆∞ng c≈©ng ƒë√≤i h·ªèi 1 ch√∫t ch√∫ √Ω ƒë·ªÉ tr√°nh m·∫•t th·ªùi gian\n2. Y√™u c·∫ßu  D√πng WSL Download AzCopy tool, refer: https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10  3. C√°ch l√†m 3.1. Copy a file from a tenant to another ƒê·∫ßu ti√™n c·∫ßn n·∫Øm ƒë∆∞·ª£c syntax:\nCopy a single blob to another blob by using a SAS token.\n azcopy cp \u0026ldquo;https://[srcaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026rdquo; \u0026ldquo;https://[destaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026rdquo;  Copy a single blob to another blob by using a SAS token and an OAuth token. You have to use a SAS token at the end of the source account URL, but the destination account doesn\u0026rsquo;t need one if you log into AzCopy by using the azcopy login command.\n azcopy cp \u0026ldquo;https://[srcaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026rdquo; \u0026ldquo;https://[destaccount].blob.core.windows.net/[container]/[path/to/blob]\u0026rdquo;  Nh∆∞ v·∫≠y, ch√∫ng ta ƒëi theo h∆∞·ªõng d√πng syntax s·ªë 1, ƒë·ªÉ ko ph·∫£i login v√†o AzCopy tool:\nazcopy cp \u0026#34;https://[srcaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026#34; \u0026#34;https://[destaccount].blob.core.windows.net/[container]/[path/to/blob]?[SAS]\u0026#34; ƒê·∫ßu ti√™n l√† l·∫•y SAS URL c·ªßa Source Account file, b·∫°n ch·ªâ c·∫ßn ƒë·ªÉ quy·ªÅn READ l√† ƒë·ªß: Th·ª© 2 l√† l·∫•y SAS URL c·ªßa Destination Account container, ch·ªó b·∫°n s·∫Ω paste file v√†o, th√¨ c·∫ßn c·∫£ quy·ªÅn WRITE n·ªØa (nhi·ªÅu ng ko ƒë·ªÉ √Ω th√¨ khi copy s·∫Ω b·ªã l·ªói): GI·ªù run command:\nazcopy copy \u0026#39;https://abcd12345678.blob.core.windows.net/container1/abcd123455666.zip?sp=r\u0026amp;st=2020-10-08T15:32:23Z\u0026amp;se=2020-10-09T23:32:23Z\u0026amp;spr=https\u0026amp;sv=2020-08-04\u0026amp;sr=b\u0026amp;sig=UP7bXNoIbXNoIbXNoIbXNoIbXNoIbXNoIbXNoIw%3D\u0026#39; \u0026#39;https://abc1234.blob.core.windows.net/container2?sp=racwl\u0026amp;st=2020-10-09T16:11:19Z\u0026amp;se=2020-10-12T00:11:19Z\u0026amp;spr=https\u0026amp;sv=2020-08-04\u0026amp;sr=c\u0026amp;sig=idW348Um3AQ348Um3AQj348Um3AQj348Um3AQjjxe4%3D\u0026#39; INFO: Scanning... INFO: Failed to create one or more destination container(s). Your transfers may still succeed if the container already exists. INFO: Any empty folders will not be processed, because source and/or destination doesn\u0026#39;t have full folder support Job e4304ef7-e5eb-b24e-5477-6a55642e9341 has started Log file is located at: 100.0 %, 0 Done, 0 Failed, 1 Pending, 0 Skipped, 1 Total, Job q1234ef7-xxxxxxxxxxxxx-2e9842 summary Elapsed Time (Minutes): 1.234 Number of File Transfers: 1 Number of Folder Property Transfers: 0 Total Number of Transfers: 1 Number of Transfers Completed: 1 Number of Transfers Failed: 0 Number of Transfers Skipped: 0 TotalBytesTransferred: 32213303808 Final Job Status: Completed Command run xong nh∆∞ng job COPY v·∫´n ƒëang ch·∫°y nha c√°c b·∫°n. N·∫øu nh√¨n th·∫•y h√¨nh tr√™n c√≥ nghƒ©a l√† v·∫´n ch∆∞a copy xong ƒë√¢u.\nH√£y ki√™n nh·∫´n ch·ªù bao gi·ªù copy xong n√≥ s·∫Ω nh∆∞ th·∫ø n√†y: 3.2. Copy all content in a container to another https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy\nRun azcopy --help ƒë·ªÉ xem format command copy 1 container t·ª´ b√™n n√†y sang b√™n kia\nStep 1: T·∫°o tr∆∞·ªõc 1 container b√™n Destination Storage account (ko nh·∫•t thi·∫øt ph·∫£i gi·ªëng t√™n v·ªõi source container).\nStep 2: L·∫•y SAS token c·ªßa container b√™n Source Storage account, c·∫ßn quy·ªÅn READ v√† LIST\nStep 3: L·∫•y SAS token c·ªßa container b√™n Destination Storage account, c·∫ßn quy·ªÅn WRITE, ADD..\nStep 4: run az cp command\nN·∫øu b·ªã l·ªói n√†y:\nfailed to perform copy command due to error: cannot start job due to error: cannot list files due to reason -\u0026gt; github.com/Azure/azure-storage-blob-go/azblob.newStorageError, /home/vsts/go/pkg/mod/github.com/!azure/azure-storage-blob-go@v0.15.0/azblob/zc_storage_error.go:42 ===== RESPONSE ERROR (ServiceCode=AuthorizationPermissionMismatch) ===== Description=This request is not authorized to perform this operation using this permission. -\u0026gt; Nghƒ©a l√† V√¨ m√¨nh ƒëang copy All content t·ª´ 1 container n√™n Source SAS c·∫ßn quy·ªÅn LIST, c·∫ßn t·∫°o l·∫°i SAS token\nCh√∫c c√°c b·∫°n th√†nh c√¥ng!! ‚úå\nCREDITS https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy?toc=/azure/storage/blobs/toc.json\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy\n","href":"/posts/azure-copy-blob-from-a-tenant-to-another/","title":"Azure: Copy a blob from a tenant to another tenant account"},{"content":"","href":"/tags/aks/","title":"AKS"},{"content":"B·∫°n mu·ªën deploy Azure AKS Cluster b·∫±ng ARM v√† Azure Container Instance, sau ƒë√≥ deploy helm chart l√™n AKS cluster ƒë√≥.\n1. Gi·ªõi Thi·ªáu CNAB l√† g√¨? https://cnab.io/\nCNAB l√† Cloud Native Application Bundle. N√≥ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ bundling, installing, managing c√°c distributed app.\nN√≥ ƒë∆∞·ª£c design b·ªüi MS, Docker, Bitami, Hashicorp, Pivotal, codefresh.\n1 CNAB bao g·ªìm 3 th√†nh ph·∫ßn: Application Image, Invocation Image, Bundle descriptor.\nT√°c d·ª•ng m√† CNAB ƒëem l·∫°i: Package to√†n b·ªô app c·ªßa b·∫°n, ko c·∫ßn c·∫•u tr√∫c ph·ª©c t·∫°p,\u0026hellip;\nPorter l√† g√¨? https://porter.sh/\nTrong c√°c CNAB tools, ƒë·ªÉ t·∫°o ra CNAB th√¨ c√≥ 1 s·ªë tool ƒë∆∞·ª£c s·ª≠ d·ª•ng: Porter, Duffle, Docker App\n-\u0026gt; Nh∆∞ v·∫≠y Porter l√† 1 tools ƒë·ªÉ t·∫°o ra CNAB\n2. Y√™u C·∫ßu 2.1. Service Principal credential 2.1.a. Create Service principal H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ c√≥ 1 Azure Service Principal Credential, v√† add permission cho n√≥ v√†o Subscription m√† b·∫°n ƒë·ªãnh deploy. N·∫øu ch∆∞a c√≥ th√¨ l√†m theo c√°c step sau:\nC√°ch 1 l√† run command, m√¨nh t·∫°o 1 credential t√™n l√† CNAB_PORTER_APP:\naz ad sp create-for-rbac -n \u0026#34;CNAB_PORTER_APP\u0026#34; --query \u0026#34;{ client_id: appId, client_secret: password, tenant_id: tenant }\u0026#34; b·∫°n nh·∫≠n dc k·∫øt qu·∫£ tr·∫£ v·ªÅ l√† 1 ƒëo·∫°n json ki·ªÉu sau, h√£y gi·ªØ n√≥ b√≠ m·∫≠t:\n{ \u0026#34;client_id\u0026#34;: \u0026#34;xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;yyyyyyyy_yyyyyyyyyy\u0026#34;, \u0026#34;tenant_id\u0026#34;: \u0026#34;zzzz-zzzz-zzzz-zzzz-zzzzzzzzzzz\u0026#34; } C√°ch 2 n·∫øu b·∫°n ko mu·ªën run command th√¨ v√†o giao Azure Portal ƒë·ªÉ t·∫°o, t·ª´ Home, ch·ªçn Azure Active Directory AAD:\nm√†n h√¨nh sau ƒë√≥ nh·∫≠p t√™n App, ch·ªçn Accounts in this organizational directory only (ho·∫∑c t√πy √Ω b·∫°n n·∫øu hi·ªÉu) -\u0026gt; Register\nCh·ªçn c√°i App m√† b·∫°n v·ª´a t·∫°o, ch·ªçn tab Certificates \u0026amp; secrets, t·∫°o 1 secrets:\nH√£y ƒë·ªÉ √Ω tab Overview, Client ID c·ªßa App ·ªü ƒë√¢y:\nT·∫•t c·∫£ nh·ªØng g√¨ b·∫°n c·∫ßn ghi l·∫°i, save l·∫°i l√† Client ID v√† Client secret m√† b·∫°n v·ª´a t·∫°o ra.\nCh√∫ng s·∫Ω c√≥ √≠ch v·ªÅ sau.\n2.1.b. Grant permission Service principal on Subscription Ti·∫øp theo h√£y g√°n quy·ªÅn cho c√°i App b·∫°n v·ª´a t·∫°o, ch√∫ng c·∫ßn quy·ªÅn tr√™n to√†n Subscription m√† b·∫°n s·∫Ω s·ª≠ d·ª•ng. Nh∆∞ h√¨nh sau, h√£y ch·∫Øc ch·∫Øn App c·ªßa b·∫°n n·∫±m trong List Role assignments v·ªõi role Contributors:\nN·∫øu ko ph·∫£i, ho·∫∑c ch∆∞a ƒë√∫ng, h√£y ch·ªçn n√∫t khoanh ƒë·ªè Add ƒë·ªÉ add role assignment cho App c·ªßa b·∫°n.\n2.2. ACR Sau n√†y b·∫°n s·∫Ω mu·ªën c√°c images ch·ª©a code c·ªßa m√¨nh ƒë∆∞·ª£c gi·ªØ ·ªü Private Registry c·ªßa ri√™ng m√¨nh.\nH√£y t·∫°o cho m√¨nh 1 Azure Container Registry, n∆°i s·∫Ω l∆∞u gi·ªØ c√°c Porter Images c·ªßa b·∫°n. Ch√∫ √Ω enable Admin User v√¨ sau n√†y b·∫°n s·∫Ω c·∫ßn.\n3. C√°ch l√†m 3.1. C√†i ƒë·∫∑t Theo official docs: https://porter.sh/install/\nM√¨nh d√πng linux n√™n ch·ªâ c·∫ßn run command sau:\ncurl -L https://cdn.porter.sh/latest/install-linux.sh | bash 3.2. porter.yaml T·∫°i workspace c·ªßa b·∫°n, t·∫°o 1 file t√™n l√† porter.yaml:\nch√∫ √Ω thay YOUR_ACR_NAME b·∫±ng t√™n ACR c·ªßa ri√™ng b·∫°n.\nname: aks version: 0.1.6 description: \u0026#34;Azure Kubernetes Service (AKS)\u0026#34; registry: YOUR_ACR_NAME.azurecr.io/porter credentials: - name: azure_client_id env: AZURE_CLIENT_ID description: AAD Client ID for Azure account authentication - used for AKS Cluster SPN details and for authentication to azure to get KubeConfig - name: azure_tenant_id env: AZURE_TENANT_ID description: Azure AAD Tenant Id for Azure account authentication - used to authenticate to Azure to get KubeConfig  - name: azure_client_secret env: AZURE_CLIENT_SECRET description: AAD Client Secret for Azure account authentication - used for AKS Cluster SPN details and for authentication to azure to get KubeConfig - name: azure_subscription_id env: AZURE_SUBSCRIPTION_ID description: Azure Subscription Id used to set the subscription where the account has access to multiple subscriptions parameters: - name: resource_group env: CNAB_PARAM_resource_group type: string description: The name of the resource group to create the AKS Cluster in - name: cluster_name env: CNAB_PARAM_cluster_name type: string description: The name to use for the AKS Cluster - name: azure_location env: CNAB_PARAM_azure_location type: string description: The Azure location to create the resources in applyTo: - \u0026#34;install\u0026#34; - name: kubernetes_version env: CNAB_PARAM_kubernetes_version type: string description: The Kubernetes version to use default: \u0026#34;1.21.2\u0026#34; applyTo: - \u0026#34;install\u0026#34; - name: node_vm_size env: CNAB_PARAM_node_vm_size type: string description: The VM size to use for the cluster default: \u0026#34;Standard_DS2_v2\u0026#34; applyTo: - \u0026#34;install\u0026#34; - name: node_count env: CNAB_PARAM_node_count type: integer minimum: 1 description: The VM size to use for the cluster default: 1 applyTo: - \u0026#34;install\u0026#34; - name: vm_set_type env: CNAB_PARAM_vm_set_type type: string enum: - VirtualMachineScaleSets - AvailabilitySet description: Agent pool VM set type default: VirtualMachineScaleSets applyTo: - \u0026#34;install\u0026#34; - name: installation_name env: CNAB_PARAM_installation_name type: string description: Installation name for Helm deployment default: wordpress applyTo: - \u0026#34;install\u0026#34; - name: helm_chart_version env: CNAB_PARAM_helm_chart_version type: string description: Version number for the Helm chart default: 7.6.5 applyTo: - \u0026#34;install\u0026#34; - \u0026#34;upgrade\u0026#34; - name: namespace env: CNAB_PARAM_namespace type: string description: Kubernetes namespace for installation default: wordpress applyTo: - \u0026#34;install\u0026#34; mixins: - exec - az - helm install: - az: description: \u0026#34;Azure CLI login\u0026#34; arguments: - \u0026#34;login\u0026#34; flags: service-principal: username: \u0026#34;{{ bundle.credentials.azure_client_id}}\u0026#34; password: \u0026#34;{{ bundle.credentials.azure_client_secret}}\u0026#34; tenant: \u0026#34;{{ bundle.credentials.azure_tenant_id}}\u0026#34; - az: description: \u0026#34;Azure set subscription Id\u0026#34; arguments: - \u0026#34;account\u0026#34; - \u0026#34;set\u0026#34; flags: subscription: \u0026#34;{{ bundle.credentials.azure_subscription_id}}\u0026#34; - az: description: \u0026#34;Create resource group if not exists\u0026#34; arguments: - \u0026#34;group\u0026#34; - \u0026#34;create\u0026#34; flags: name: \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; location: \u0026#34;{{ bundle.parameters.azure_location }}\u0026#34; - exec: description: \u0026#34;Create AKS if not exists\u0026#34; command: \u0026#34;bash\u0026#34; arguments: - \u0026#34;aks.sh\u0026#34; - \u0026#34;create-aks\u0026#34; - \u0026#34;{{ bundle.parameters.cluster_name }}\u0026#34; - \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; - \u0026#34;{{ bundle.parameters.kubernetes_version }}\u0026#34; - \u0026#34;{{ bundle.parameters.node_vm_size }}\u0026#34; - \u0026#34;{{ bundle.parameters.node_count }}\u0026#34; - \u0026#34;{{ bundle.credentials.azure_client_id}}\u0026#34; - \u0026#34;{{ bundle.credentials.azure_client_secret}}\u0026#34; - \u0026#34;{{ bundle.parameters.azure_location }}\u0026#34; - \u0026#34;{{ bundle.parameters.vm_set_type }}\u0026#34; - az: description: \u0026#34;Azure CLI AKS get-credentials\u0026#34; arguments: - \u0026#34;aks\u0026#34; - \u0026#34;get-credentials\u0026#34; flags: resource-group: \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; name: \u0026#34;{{ bundle.parameters.cluster_name }}\u0026#34; - helm: description: Install wordpress name: \u0026#39;{{ bundle.parameters.installation_name }}\u0026#39; chart: stable/wordpress version: \u0026#39;{{ bundle.parameters.helm_chart_version }}\u0026#39; namespace: \u0026#39;{{ bundle.parameters.namespace }}\u0026#39; replace: true uninstall: - az: description: \u0026#34;Azure CLI login\u0026#34; arguments: - \u0026#34;login\u0026#34; flags: service-principal: username: \u0026#34;{{ bundle.credentials.azure_client_id }}\u0026#34; password: \u0026#34;{{ bundle.credentials.azure_client_secret }}\u0026#34; tenant: \u0026#34;{{ bundle.credentials.azure_tenant_id }}\u0026#34; - az: description: \u0026#34;Azure set subscription Id\u0026#34; arguments: - \u0026#34;account\u0026#34; - \u0026#34;set\u0026#34; flags: subscription: \u0026#34;{{ bundle.credentials.azure_subscription_id }}\u0026#34; - exec: description: \u0026#34;Delete AKS\u0026#34; command: bash arguments: - \u0026#34;aks.sh\u0026#34; - \u0026#34;delete-aks\u0026#34; - \u0026#34;{{ bundle.parameters.cluster_name }}\u0026#34; - \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; File porter.yaml n√†y m√¥ t·∫£ t·∫•t c·∫£ c√°c step s·∫Ω ƒë∆∞·ª£c ch·∫°y khi ACI c·ªßa b·∫°n start.\nN√≥ gi·ªëng nh∆∞ 1 ki·ªÉu wrapper l√™n az-cli, run c√°c command t·ª´ az login, r·ªìi az create aks, helm\u0026hellip;etc\nFile b√™n tr√™n v·ªõi action install, n√≥ s·∫Ω login v√†o az b·∫±ng service principal m√† b·∫°n cung c·∫•p, t·∫°o AKS cluster n·∫øu ch∆∞a c√≥, deploy Helm chart c·ªßa Wordpress. End! R·∫•t ƒë∆°n gi·∫£n th·∫ø th√¥i.\nFile ti·∫øp theo b·∫°n c·∫ßn chu·∫©n b·ªã l√† file shell aks.sh:\nfunction create-aks { if [[ -z $(az aks show --name $1 --resource-group $2 2\u0026gt; /dev/null) ]] then az aks create \\  --name $1 \\  --resource-group $2 \\  --kubernetes-version $3 \\  --node-vm-size $4 \\  --node-count $5 \\  --service-principal $6 \\  --client-secret $7 \\  --location $8 \\  --vm-set-type $9 \\  --generate-ssh-keys else echo \u0026#34;AKS cluster already exists in specified resource group with specified name\u0026#34; fi } function delete-aks { az aks delete --name $1 --resource-group $2 --yes } \u0026#34;$@\u0026#34; Gi·ªù l√†m sau ƒë·ªÉ ƒë√≥ng g√≥i h·∫øt c√°c step ƒë√≥ v√†o 1 Docker image, r·ªìi publish n√≥ l√™n ACR?\n3.3. Build images Run command sau ƒë·ªÉ build Docker image:\nch√∫ √Ω tr·ªè ƒë·∫øn n∆°i b·∫°n ƒëang ch·ª©a file porter.yaml v√† aks.sh\nporter build N√≥ s·∫Ω t·∫°o ra 1 Docker image g·ªçi l√† invocation image ·ªü local m√°y b·∫°n.\nb·∫°n th·ª≠ run command docker images s·∫Ω th·∫•y n√≥:\n3.4. Publish images \u0026amp; bundle Gi·ªù l√†m sao ƒë·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c n√≥?\nCh·∫Øc b·∫°n nghƒ© r·∫±ng ch·ªâ c·∫ßn docker push ... l√† images s·∫Ω l√™n ACR c·ªßa m√¨nh r·ªìi ƒë√∫ng ko? - Kh√¥ng ƒë∆°n gi·∫£n th·∫ø\nƒê·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c n√≥, ACR c·ªßa b·∫°n c·∫ßn ch·ª©a images v√† 1 th·ª© g·ªçi l√† bundle. N·∫øu b·∫°n ch·ªâ push m·ªói image l√™n th√¨ v√¥ d·ª•ng. V√¨ image n√†y do Porter t·∫°o ra, n√™n h√£y push n√≥ l√™n ACR b·∫±ng command c·ªßa porter.\nporter publish C·∫£ image v√† bundle s·∫Ω ƒë∆∞·ª£c push l√™n ACR m√† b·∫°n ƒë√£ ch·ªâ ƒë·ªãnh trong file porter.yaml\nnh∆∞ h√¨nh tr√™n b·∫°n ƒë√£ th·∫•y images l√† /porter/aks-installer:v0.1.6\nv√† bundle l√† /porter/aks:v0.1.6. ƒê·ªÅu n·∫±m tr√™n ACR c·ªßa b·∫°n.\n3.5. Write ARM L√†m sao ƒë·ªÉ s·ª≠ d·ª•ng images ch·ª©a c√°c step tr√™n trong ACI ?\nTi·∫øp theo s·∫Ω c·∫ßn vi·∫øt 1 ARM template ƒë·ªÉ deploy ra 1 ACI, n∆°i s·∫Ω run c√°c step m√¨nh ƒë√£ define trong porter.yaml\nt·∫°o 1 file ARM template t√™n t√πy √Ω: deploy-aks-by-aci-cnab.json\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;cluster_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;akstest001\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name to use for the AKS Cluster\u0026#34; } }, \u0026#34;cnab_action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;install\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name of the action to be performed on the application instance.\u0026#34; } }, \u0026#34;cnab_azure_client_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;AAD Client ID for Azure account authentication - used to authenticate to Azure using Service Principal for ACI creation.\u0026#34; } }, \u0026#34;cnab_azure_client_secret\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;AAD Client Secret for Azure account authentication - used to authenticate to Azure using Service Principal for ACI creation.\u0026#34; } }, \u0026#34;kubernetes_version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;1.21.2\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The Kubernetes version to use\u0026#34; } }, \u0026#34;node_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;defaultValue\u0026#34;: 1, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The VM size to use for the cluster\u0026#34; }, \u0026#34;minValue\u0026#34;: 1 }, \u0026#34;node_vm_size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_DS2_v2\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The VM size to use for the cluster\u0026#34; } }, \u0026#34;resource_group\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;cluster_name\u0026#39;),\u0026#39;-RG\u0026#39;)]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name of the resource group to create the AKS Cluster in\u0026#34; } }, \u0026#34;vm_set_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;VirtualMachineScaleSets\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;VirtualMachineScaleSets\u0026#34;, \u0026#34;AvailabilitySet\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Agent pool VM set type\u0026#34; } }, \u0026#34;registry_server\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY server\u0026#34; } }, \u0026#34;registry_username\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY username\u0026#34; } }, \u0026#34;registry_password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY password\u0026#34; } }, \u0026#34;installation_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY username\u0026#34; } }, \u0026#34;helm_chart_version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;7.6.5\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;helm chart version\u0026#34; } }, \u0026#34;namespace\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;namespace\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;aci_location\u0026#34;: \u0026#34;[resourceGroup().Location]\u0026#34;, \u0026#34;cnab_action\u0026#34;: \u0026#34;[parameters(\u0026#39;cnab_action\u0026#39;)]\u0026#34;, \u0026#34;cnab_azure_client_id\u0026#34;: \u0026#34;[parameters(\u0026#39;cnab_azure_client_id\u0026#39;)]\u0026#34;, \u0026#34;cnab_azure_client_secret\u0026#34;: \u0026#34;[parameters(\u0026#39;cnab_azure_client_secret\u0026#39;)]\u0026#34;, \u0026#34;cnab_azure_location\u0026#34;: \u0026#34;[resourceGroup().Location]\u0026#34;, \u0026#34;cnab_azure_subscription_id\u0026#34;: \u0026#34;[subscription().subscriptionId]\u0026#34;, \u0026#34;cnab_azure_tenant_id\u0026#34;: \u0026#34;[subscription().tenantId]\u0026#34;, \u0026#34;cnab_installation_name\u0026#34;: \u0026#34;aks\u0026#34;, \u0026#34;containerGroupName\u0026#34;: \u0026#34;[concat(\u0026#39;cg-\u0026#39;,uniqueString(resourceGroup().id, \u0026#39;aks\u0026#39;))]\u0026#34;, \u0026#34;containerName\u0026#34;: \u0026#34;[concat(\u0026#39;cn-\u0026#39;,uniqueString(resourceGroup().id, \u0026#39;aks\u0026#39;))]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.ContainerInstance/containerGroups\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;containerGroupName\u0026#39;)]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[variables(\u0026#39;aci_location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;imageRegistryCredentials\u0026#34;: [ { \u0026#34;server\u0026#34;: \u0026#34;[parameters(\u0026#39;registry_server\u0026#39;)]\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;[parameters(\u0026#39;registry_username\u0026#39;)]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;[parameters(\u0026#39;registry_password\u0026#39;)]\u0026#34; } ], \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;containerName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;YOUR_ACR_NAME.azurecr.io/porter/aks-installer:v0.1.6\u0026#34;, \u0026#34;resources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: 1.0, \u0026#34;memoryInGb\u0026#34;: 1.5 } }, \u0026#34;environmentVariables\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;CNAB_ACTION\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_action\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_INSTALLATION_NAME\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_installation_name\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_LOCATION\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_location\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_CLIENT_ID\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_client_id\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_CLIENT_SECRET\u0026#34;, \u0026#34;secureValue\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_client_secret\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_SUBSCRIPTION_ID\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_subscription_id\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_TENANT_ID\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_tenant_id\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;VERBOSE\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;false\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_BUNDLE_NAME\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;aks\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_BUNDLE_TAG\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;YOUR_ACR_NAME.azurecr.io/porter/aks:v0.1.6\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_azure_location\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_location\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_cluster_name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;cluster_name\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_kubernetes_version\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;kubernetes_version\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_node_count\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;node_count\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_node_vm_size\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;node_vm_size\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_resource_group\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;resource_group\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_vm_set_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;vm_set_type\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_installation_name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;installation_name\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_helm_chart_version\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;helm_chart_version\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;namespace\u0026#39;)]\u0026#34; } ] } } ], \u0026#34;osType\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;restartPolicy\u0026#34;: \u0026#34;Never\u0026#34; } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;CNAB Package Action Logs Command\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[concat(\u0026#39;az container logs -g \u0026#39;,resourceGroup().name,\u0026#39; -n \u0026#39;,variables(\u0026#39;containerGroupName\u0026#39;),\u0026#39; --container-name \u0026#39;,variables(\u0026#39;containerName\u0026#39;), \u0026#39; --follow\u0026#39;)]\u0026#34; } } } t√¨m 2 ch·ªó YOUR_ACR_NAME v√† s·ª≠a b·∫±ng ACR name c·ªßa b·∫°n\nb·∫°n s·∫Ω th·∫•y Bundle c·∫ßn ƒë·∫∑t ·ªü ƒë√¢u v√† Images c·∫ßn ƒë·∫∑t ·ªü ƒë√¢u.\nM√¨nh s·∫Ω ko ƒëi s√¢u v√†o logic code, c√°c b·∫°n l√†m theo ƒë√∫ng v√† ch·∫°y ƒë∆∞·ª£c OK ƒë√£, sau ƒë√≥ h√£y quay l·∫°i ƒë·ªÉ t√¨m hi·ªÉu logic th√¨ s·∫Ω d·ªÖ hi·ªÉu h∆°n. Trong qu√° tr√¨nh fix cho code ch·∫°y ƒë∆∞·ª£c, c√°c b·∫°n c≈©ng s·∫Ω hi·ªÉu logic h∆°n l√† ng·ªìi ƒë·ªçc chay.\n3.6. Deploy ARM template V√†o link sau ƒë·ªÉ deploy ARM template m√¨nh v·ª´a vi·∫øt:\nhttps://portal.azure.com/#create/Microsoft.Template\nCh·ªçn h√¨nh b√∫t ch√¨ Build your own template in the editor\nCopy to√†n b·ªô n·ªôi dung ARM v·ª´a vi·∫øt paste ƒë√® l√™n -\u0026gt; ·∫•n n√∫t Save\nT·∫°i m√†n h√¨nh ti·∫øp theo, H√£y nh·∫≠p Client ID, Client secret, Registry server/username/password (ch√∫ √Ω b√¥i v√†ng):\n·∫•n Review and create\nCh·ªù ƒë·∫øn khi m√†n h√¨nh n√†y hi·ªán ra, nghƒ©a l√† ACI c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c t·∫°o ra: Tuy nhi√™n ƒë·∫øn ƒë√¢y th√¨ c√°c command/step c·ªßa b·∫°n v·∫´n ƒëang ch·∫°y trong ACI, h√£y xem log c·ªßa n√≥ l√† success hay fail: V√†o Azure Container Instance, ch·ªçn Instance m·ªõi do b·∫°n t·∫°o ra, -\u0026gt; v√†o Container -\u0026gt; Log,\nN·∫øu m√†n h√¨nh log nh∆∞ sau, c√≥ nghƒ©a l√† n√≥ ƒë√£ qua h·∫øt c√°c step t·ª´ AZ login, AZ create AKS, Helm chart ƒë·ªÉ cu·ªëi c√πng c√≥ 1 trang Wordpress cho b·∫°n:\nT·∫•t nhi√™n b·∫°n c√≥ th·ªÉ login v√†o AKS cluster xem c√°c pod, c√°c namespace, services ƒë∆∞·ª£c r·ªìi\nDone!\nCh√∫c b·∫°n th√†nh c√¥ng üòú\n4. Troubleshooting 4.1. ERROR: (AuthorizationFailed) N·∫øu b·∫°n g·∫∑p l·ªói n√†y khi xem Logs c·ªßa Container:\nAzure set subscription Id Create resource group if not exists ERROR: (AuthorizationFailed) The client 'xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx' with object id 'xxxxxxx-xxxx-xxxx-yyyy-yyyyy' does not have authorization to perform action 'Microsoft.Resources/subscriptions/resourcegroups/write' over scope '/subscriptions/*******/resourcegroups/akstest001-RG' or the scope is invalid. If access was recently granted, please refresh your credentials. err: error running command /cnab/app az group create --location westeurope --name akstest001-RG --output json: exit status 1 -\u0026gt; C√≥ nghƒ©a l√† App Client Id/Secret c·ªßa b·∫°n ƒëang ko c√≥ quy·ªÅn t·∫°o Resource Group. B·∫°n h√£y ch√∫ √Ω App Registration c·ªßa b·∫°n c·∫ßn dc c√≥ quy·ªÅn COntributor tr√™n scope l√† Subscription, ko ph·∫£i ch·ªâ 1 Resource group n√†o ƒë√≥. (Xem k·ªπ l·∫°i step 2.1.b.)\n5. T√¨m hi·ªÉu th√™m v·ªÅ Porter Trong qu√° tr√¨nh ph√°t tri·ªÉn, b·∫°n s·∫Ω kh√¥ng mu·ªën m·ªói l·∫ßn s·ª≠a code mu·ªën test th√¨ l·∫°i ph·∫£i porter publish r·ªìi l·∫°i deploy ARM template. Nh∆∞ v·∫≠y s·∫Ω r·∫•t t·ªën th·ªùi gian (M·ªói l·∫ßn porter publish ph·∫£i m·∫•t kho·∫£ng 10-15 ph√∫t ƒë·ªÉ n√≥ upload ƒë∆∞·ª£c images x·∫•p x·ªâ 2Gb l√™n Registry c·ªßa b·∫°n)\nTh·∫ø n√™n sau b∆∞·ªõc porter build, b·∫°n c√≥ th·ªÉ test lu√¥n image tr∆∞·ªõc khi publish n√≥ b·∫±ng c√°ch d√πng c√°c command porter install\nNh∆∞ng ƒë·ªÉ d√πng th√¨ b·∫°n c·∫ßn truy·ªÅn c√°c credential v√†o tr∆∞·ªõc ƒë√£:\nporter credentials generate [YOUR_CRED_NAME] 1 list c√°c l·ª±a ch·ªçn hi·ªán ra, sau ƒë√≥ b·∫°n insert c√°c gi√° tr·ªã credential (Service Principal Client ID, secret, registry user name, password, \u0026hellip;etc)\n(Optional) B·∫°n c≈©ng c√≥ th·ªÉ t·∫°o c√°c Parameter set:\nporter parameters generate [YOUR_PARAM_SET_NAME] Cu·ªëi c√πng th√¨ c√≥ th·ªÉ test Images:\nporter install --cred [YOUR_CRED_NAME] --parameter-set [YOUR_PARAM_SET_NAME] C√¢u l·ªánh tr√™n s·∫Ω t·∫°o ra container ngay tr√™n m√°y local c·ªßa b·∫°n, gi√∫p b·∫°n nhanh ch√≥ng debug n·∫øu c√≥ v·∫•n ƒë·ªÅ.\nList c√°c command c·ªßa Porter ·ªü ƒë√¢y: https://porter.sh/cli/porter/#see-also\nCREDITS https://porter.sh/\nhttps://github.com/Azure/azure-cnab-quickstarts\nhttps://www.youtube.com/watch?v=z1lnQfaAVeg\u0026amp;ab_channel=endjin\n","href":"/bk/azure-create-akscluster-using-arm-aci-cnab-porter/","title":"Azure: Create AKS Cluster by ARM and ACI (using CNAB, porter.sh)"},{"content":"B·∫°n mu·ªën deploy Azure AKS Cluster b·∫±ng ARM v√† Azure Container Instance, sau ƒë√≥ deploy helm chart l√™n AKS cluster ƒë√≥.\n1. Gi·ªõi Thi·ªáu CNAB l√† g√¨? https://cnab.io/\nCNAB l√† Cloud Native Application Bundle. N√≥ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ bundling, installing, managing c√°c distributed app.\nN√≥ ƒë∆∞·ª£c design b·ªüi MS, Docker, Bitami, Hashicorp, Pivotal, codefresh.\n1 CNAB bao g·ªìm 3 th√†nh ph·∫ßn: Application Image, Invocation Image, Bundle descriptor.\nT√°c d·ª•ng m√† CNAB ƒëem l·∫°i: Package to√†n b·ªô app c·ªßa b·∫°n, ko c·∫ßn c·∫•u tr√∫c ph·ª©c t·∫°p,\u0026hellip;\nPorter l√† g√¨? https://porter.sh/\nTrong c√°c CNAB tools, ƒë·ªÉ t·∫°o ra CNAB th√¨ c√≥ 1 s·ªë tool ƒë∆∞·ª£c s·ª≠ d·ª•ng: Porter, Duffle, Docker App\n-\u0026gt; Nh∆∞ v·∫≠y Porter l√† 1 tools ƒë·ªÉ t·∫°o ra CNAB\n2. Y√™u C·∫ßu 2.1. Service Principal credential 2.1.a. Create Service principal H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ c√≥ 1 Azure Service Principal Credential, v√† add permission cho n√≥ v√†o Subscription m√† b·∫°n ƒë·ªãnh deploy. N·∫øu ch∆∞a c√≥ th√¨ l√†m theo c√°c step sau:\nC√°ch 1 l√† run command, m√¨nh t·∫°o 1 credential t√™n l√† CNAB_PORTER_APP:\naz ad sp create-for-rbac -n \u0026#34;CNAB_PORTER_APP\u0026#34; --query \u0026#34;{ client_id: appId, client_secret: password, tenant_id: tenant }\u0026#34; b·∫°n nh·∫≠n dc k·∫øt qu·∫£ tr·∫£ v·ªÅ l√† 1 ƒëo·∫°n json ki·ªÉu sau, h√£y gi·ªØ n√≥ b√≠ m·∫≠t:\n{ \u0026#34;client_id\u0026#34;: \u0026#34;xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;yyyyyyyy_yyyyyyyyyy\u0026#34;, \u0026#34;tenant_id\u0026#34;: \u0026#34;zzzz-zzzz-zzzz-zzzz-zzzzzzzzzzz\u0026#34; } C√°ch 2 n·∫øu b·∫°n ko mu·ªën run command th√¨ v√†o giao Azure Portal ƒë·ªÉ t·∫°o, t·ª´ Home, ch·ªçn Azure Active Directory AAD:\nm√†n h√¨nh sau ƒë√≥ nh·∫≠p t√™n App, ch·ªçn Accounts in this organizational directory only (ho·∫∑c t√πy √Ω b·∫°n n·∫øu hi·ªÉu) -\u0026gt; Register\nCh·ªçn c√°i App m√† b·∫°n v·ª´a t·∫°o, ch·ªçn tab Certificates \u0026amp; secrets, t·∫°o 1 secrets:\nH√£y ƒë·ªÉ √Ω tab Overview, Client ID c·ªßa App ·ªü ƒë√¢y:\nT·∫•t c·∫£ nh·ªØng g√¨ b·∫°n c·∫ßn ghi l·∫°i, save l·∫°i l√† Client ID v√† Client secret m√† b·∫°n v·ª´a t·∫°o ra.\nCh√∫ng s·∫Ω c√≥ √≠ch v·ªÅ sau.\n2.1.b. Grant permission Service principal on Subscription Ti·∫øp theo h√£y g√°n quy·ªÅn cho c√°i App b·∫°n v·ª´a t·∫°o, ch√∫ng c·∫ßn quy·ªÅn tr√™n to√†n Subscription m√† b·∫°n s·∫Ω s·ª≠ d·ª•ng. Nh∆∞ h√¨nh sau, h√£y ch·∫Øc ch·∫Øn App c·ªßa b·∫°n n·∫±m trong List Role assignments v·ªõi role Contributors:\nN·∫øu ko ph·∫£i, ho·∫∑c ch∆∞a ƒë√∫ng, h√£y ch·ªçn n√∫t khoanh ƒë·ªè Add ƒë·ªÉ add role assignment cho App c·ªßa b·∫°n.\n2.2. ACR Sau n√†y b·∫°n s·∫Ω mu·ªën c√°c images ch·ª©a code c·ªßa m√¨nh ƒë∆∞·ª£c gi·ªØ ·ªü Private Registry c·ªßa ri√™ng m√¨nh.\nH√£y t·∫°o cho m√¨nh 1 Azure Container Registry, n∆°i s·∫Ω l∆∞u gi·ªØ c√°c Porter Images c·ªßa b·∫°n. Ch√∫ √Ω enable Admin User v√¨ sau n√†y b·∫°n s·∫Ω c·∫ßn.\n3. C√°ch l√†m 3.1. C√†i ƒë·∫∑t Theo official docs: https://porter.sh/install/\nM√¨nh d√πng linux n√™n ch·ªâ c·∫ßn run command sau:\ncurl -L https://cdn.porter.sh/latest/install-linux.sh | bash 3.2. porter.yaml T·∫°i workspace c·ªßa b·∫°n, t·∫°o 1 file t√™n l√† porter.yaml:\nch√∫ √Ω thay YOUR_ACR_NAME b·∫±ng t√™n ACR c·ªßa ri√™ng b·∫°n.\nname: aks version: 0.1.6 description: \u0026#34;Azure Kubernetes Service (AKS)\u0026#34; registry: YOUR_ACR_NAME.azurecr.io/porter credentials: - name: azure_client_id env: AZURE_CLIENT_ID description: AAD Client ID for Azure account authentication - used for AKS Cluster SPN details and for authentication to azure to get KubeConfig - name: azure_tenant_id env: AZURE_TENANT_ID description: Azure AAD Tenant Id for Azure account authentication - used to authenticate to Azure to get KubeConfig  - name: azure_client_secret env: AZURE_CLIENT_SECRET description: AAD Client Secret for Azure account authentication - used for AKS Cluster SPN details and for authentication to azure to get KubeConfig - name: azure_subscription_id env: AZURE_SUBSCRIPTION_ID description: Azure Subscription Id used to set the subscription where the account has access to multiple subscriptions parameters: - name: resource_group env: CNAB_PARAM_resource_group type: string description: The name of the resource group to create the AKS Cluster in - name: cluster_name env: CNAB_PARAM_cluster_name type: string description: The name to use for the AKS Cluster - name: azure_location env: CNAB_PARAM_azure_location type: string description: The Azure location to create the resources in applyTo: - \u0026#34;install\u0026#34; - name: kubernetes_version env: CNAB_PARAM_kubernetes_version type: string description: The Kubernetes version to use default: \u0026#34;1.21.2\u0026#34; applyTo: - \u0026#34;install\u0026#34; - name: node_vm_size env: CNAB_PARAM_node_vm_size type: string description: The VM size to use for the cluster default: \u0026#34;Standard_DS2_v2\u0026#34; applyTo: - \u0026#34;install\u0026#34; - name: node_count env: CNAB_PARAM_node_count type: integer minimum: 1 description: The VM size to use for the cluster default: 1 applyTo: - \u0026#34;install\u0026#34; - name: vm_set_type env: CNAB_PARAM_vm_set_type type: string enum: - VirtualMachineScaleSets - AvailabilitySet description: Agent pool VM set type default: VirtualMachineScaleSets applyTo: - \u0026#34;install\u0026#34; - name: installation_name env: CNAB_PARAM_installation_name type: string description: Installation name for Helm deployment default: wordpress applyTo: - \u0026#34;install\u0026#34; - name: helm_chart_version env: CNAB_PARAM_helm_chart_version type: string description: Version number for the Helm chart default: 7.6.5 applyTo: - \u0026#34;install\u0026#34; - \u0026#34;upgrade\u0026#34; - name: namespace env: CNAB_PARAM_namespace type: string description: Kubernetes namespace for installation default: wordpress applyTo: - \u0026#34;install\u0026#34; mixins: - exec - az - helm install: - az: description: \u0026#34;Azure CLI login\u0026#34; arguments: - \u0026#34;login\u0026#34; flags: service-principal: username: \u0026#34;{{ bundle.credentials.azure_client_id}}\u0026#34; password: \u0026#34;{{ bundle.credentials.azure_client_secret}}\u0026#34; tenant: \u0026#34;{{ bundle.credentials.azure_tenant_id}}\u0026#34; - az: description: \u0026#34;Azure set subscription Id\u0026#34; arguments: - \u0026#34;account\u0026#34; - \u0026#34;set\u0026#34; flags: subscription: \u0026#34;{{ bundle.credentials.azure_subscription_id}}\u0026#34; - az: description: \u0026#34;Create resource group if not exists\u0026#34; arguments: - \u0026#34;group\u0026#34; - \u0026#34;create\u0026#34; flags: name: \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; location: \u0026#34;{{ bundle.parameters.azure_location }}\u0026#34; - exec: description: \u0026#34;Create AKS if not exists\u0026#34; command: \u0026#34;bash\u0026#34; arguments: - \u0026#34;aks.sh\u0026#34; - \u0026#34;create-aks\u0026#34; - \u0026#34;{{ bundle.parameters.cluster_name }}\u0026#34; - \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; - \u0026#34;{{ bundle.parameters.kubernetes_version }}\u0026#34; - \u0026#34;{{ bundle.parameters.node_vm_size }}\u0026#34; - \u0026#34;{{ bundle.parameters.node_count }}\u0026#34; - \u0026#34;{{ bundle.credentials.azure_client_id}}\u0026#34; - \u0026#34;{{ bundle.credentials.azure_client_secret}}\u0026#34; - \u0026#34;{{ bundle.parameters.azure_location }}\u0026#34; - \u0026#34;{{ bundle.parameters.vm_set_type }}\u0026#34; - az: description: \u0026#34;Azure CLI AKS get-credentials\u0026#34; arguments: - \u0026#34;aks\u0026#34; - \u0026#34;get-credentials\u0026#34; flags: resource-group: \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; name: \u0026#34;{{ bundle.parameters.cluster_name }}\u0026#34; - helm: description: Install wordpress name: \u0026#39;{{ bundle.parameters.installation_name }}\u0026#39; chart: stable/wordpress version: \u0026#39;{{ bundle.parameters.helm_chart_version }}\u0026#39; namespace: \u0026#39;{{ bundle.parameters.namespace }}\u0026#39; replace: true uninstall: - az: description: \u0026#34;Azure CLI login\u0026#34; arguments: - \u0026#34;login\u0026#34; flags: service-principal: username: \u0026#34;{{ bundle.credentials.azure_client_id }}\u0026#34; password: \u0026#34;{{ bundle.credentials.azure_client_secret }}\u0026#34; tenant: \u0026#34;{{ bundle.credentials.azure_tenant_id }}\u0026#34; - az: description: \u0026#34;Azure set subscription Id\u0026#34; arguments: - \u0026#34;account\u0026#34; - \u0026#34;set\u0026#34; flags: subscription: \u0026#34;{{ bundle.credentials.azure_subscription_id }}\u0026#34; - exec: description: \u0026#34;Delete AKS\u0026#34; command: bash arguments: - \u0026#34;aks.sh\u0026#34; - \u0026#34;delete-aks\u0026#34; - \u0026#34;{{ bundle.parameters.cluster_name }}\u0026#34; - \u0026#34;{{ bundle.parameters.resource_group }}\u0026#34; File porter.yaml n√†y m√¥ t·∫£ t·∫•t c·∫£ c√°c step s·∫Ω ƒë∆∞·ª£c ch·∫°y khi ACI c·ªßa b·∫°n start.\nN√≥ gi·ªëng nh∆∞ 1 ki·ªÉu wrapper l√™n az-cli, run c√°c command t·ª´ az login, r·ªìi az create aks, helm\u0026hellip;etc\nFile b√™n tr√™n v·ªõi action install, n√≥ s·∫Ω login v√†o az b·∫±ng service principal m√† b·∫°n cung c·∫•p, t·∫°o AKS cluster n·∫øu ch∆∞a c√≥, deploy Helm chart c·ªßa Wordpress. End! R·∫•t ƒë∆°n gi·∫£n th·∫ø th√¥i.\nFile ti·∫øp theo b·∫°n c·∫ßn chu·∫©n b·ªã l√† file shell aks.sh:\nfunction create-aks { if [[ -z $(az aks show --name $1 --resource-group $2 2\u0026gt; /dev/null) ]] then az aks create \\  --name $1 \\  --resource-group $2 \\  --kubernetes-version $3 \\  --node-vm-size $4 \\  --node-count $5 \\  --service-principal $6 \\  --client-secret $7 \\  --location $8 \\  --vm-set-type $9 \\  --generate-ssh-keys else echo \u0026#34;AKS cluster already exists in specified resource group with specified name\u0026#34; fi } function delete-aks { az aks delete --name $1 --resource-group $2 --yes } \u0026#34;$@\u0026#34; Gi·ªù l√†m sau ƒë·ªÉ ƒë√≥ng g√≥i h·∫øt c√°c step ƒë√≥ v√†o 1 Docker image, r·ªìi publish n√≥ l√™n ACR?\n3.3. Build images Run command sau ƒë·ªÉ build Docker image:\nch√∫ √Ω tr·ªè ƒë·∫øn n∆°i b·∫°n ƒëang ch·ª©a file porter.yaml v√† aks.sh\nporter build N√≥ s·∫Ω t·∫°o ra 1 Docker image g·ªçi l√† invocation image ·ªü local m√°y b·∫°n.\nb·∫°n th·ª≠ run command docker images s·∫Ω th·∫•y n√≥:\n3.4. Publish images \u0026amp; bundle Gi·ªù l√†m sao ƒë·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c n√≥?\nCh·∫Øc b·∫°n nghƒ© r·∫±ng ch·ªâ c·∫ßn docker push ... l√† images s·∫Ω l√™n ACR c·ªßa m√¨nh r·ªìi ƒë√∫ng ko? - Kh√¥ng ƒë∆°n gi·∫£n th·∫ø\nƒê·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c n√≥, ACR c·ªßa b·∫°n c·∫ßn ch·ª©a images v√† 1 th·ª© g·ªçi l√† bundle. N·∫øu b·∫°n ch·ªâ push m·ªói image l√™n th√¨ v√¥ d·ª•ng. V√¨ image n√†y do Porter t·∫°o ra, n√™n h√£y push n√≥ l√™n ACR b·∫±ng command c·ªßa porter.\nporter publish C·∫£ image v√† bundle s·∫Ω ƒë∆∞·ª£c push l√™n ACR m√† b·∫°n ƒë√£ ch·ªâ ƒë·ªãnh trong file porter.yaml\nnh∆∞ h√¨nh tr√™n b·∫°n ƒë√£ th·∫•y images l√† /porter/aks-installer:v0.1.6\nv√† bundle l√† /porter/aks:v0.1.6. ƒê·ªÅu n·∫±m tr√™n ACR c·ªßa b·∫°n.\n3.5. Write ARM L√†m sao ƒë·ªÉ s·ª≠ d·ª•ng images ch·ª©a c√°c step tr√™n trong ACI ?\nTi·∫øp theo s·∫Ω c·∫ßn vi·∫øt 1 ARM template ƒë·ªÉ deploy ra 1 ACI, n∆°i s·∫Ω run c√°c step m√¨nh ƒë√£ define trong porter.yaml\nt·∫°o 1 file ARM template t√™n t√πy √Ω: deploy-aks-by-aci-cnab.json\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;cluster_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;akstest001\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name to use for the AKS Cluster\u0026#34; } }, \u0026#34;cnab_action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;install\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name of the action to be performed on the application instance.\u0026#34; } }, \u0026#34;cnab_azure_client_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;AAD Client ID for Azure account authentication - used to authenticate to Azure using Service Principal for ACI creation.\u0026#34; } }, \u0026#34;cnab_azure_client_secret\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;AAD Client Secret for Azure account authentication - used to authenticate to Azure using Service Principal for ACI creation.\u0026#34; } }, \u0026#34;kubernetes_version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;1.21.2\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The Kubernetes version to use\u0026#34; } }, \u0026#34;node_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;defaultValue\u0026#34;: 1, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The VM size to use for the cluster\u0026#34; }, \u0026#34;minValue\u0026#34;: 1 }, \u0026#34;node_vm_size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_DS2_v2\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The VM size to use for the cluster\u0026#34; } }, \u0026#34;resource_group\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;cluster_name\u0026#39;),\u0026#39;-RG\u0026#39;)]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The name of the resource group to create the AKS Cluster in\u0026#34; } }, \u0026#34;vm_set_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;VirtualMachineScaleSets\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;VirtualMachineScaleSets\u0026#34;, \u0026#34;AvailabilitySet\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Agent pool VM set type\u0026#34; } }, \u0026#34;registry_server\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY server\u0026#34; } }, \u0026#34;registry_username\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY username\u0026#34; } }, \u0026#34;registry_password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY password\u0026#34; } }, \u0026#34;installation_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;REGISTRY username\u0026#34; } }, \u0026#34;helm_chart_version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;7.6.5\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;helm chart version\u0026#34; } }, \u0026#34;namespace\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;namespace\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;aci_location\u0026#34;: \u0026#34;[resourceGroup().Location]\u0026#34;, \u0026#34;cnab_action\u0026#34;: \u0026#34;[parameters(\u0026#39;cnab_action\u0026#39;)]\u0026#34;, \u0026#34;cnab_azure_client_id\u0026#34;: \u0026#34;[parameters(\u0026#39;cnab_azure_client_id\u0026#39;)]\u0026#34;, \u0026#34;cnab_azure_client_secret\u0026#34;: \u0026#34;[parameters(\u0026#39;cnab_azure_client_secret\u0026#39;)]\u0026#34;, \u0026#34;cnab_azure_location\u0026#34;: \u0026#34;[resourceGroup().Location]\u0026#34;, \u0026#34;cnab_azure_subscription_id\u0026#34;: \u0026#34;[subscription().subscriptionId]\u0026#34;, \u0026#34;cnab_azure_tenant_id\u0026#34;: \u0026#34;[subscription().tenantId]\u0026#34;, \u0026#34;cnab_installation_name\u0026#34;: \u0026#34;aks\u0026#34;, \u0026#34;containerGroupName\u0026#34;: \u0026#34;[concat(\u0026#39;cg-\u0026#39;,uniqueString(resourceGroup().id, \u0026#39;aks\u0026#39;))]\u0026#34;, \u0026#34;containerName\u0026#34;: \u0026#34;[concat(\u0026#39;cn-\u0026#39;,uniqueString(resourceGroup().id, \u0026#39;aks\u0026#39;))]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.ContainerInstance/containerGroups\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;containerGroupName\u0026#39;)]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[variables(\u0026#39;aci_location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;imageRegistryCredentials\u0026#34;: [ { \u0026#34;server\u0026#34;: \u0026#34;[parameters(\u0026#39;registry_server\u0026#39;)]\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;[parameters(\u0026#39;registry_username\u0026#39;)]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;[parameters(\u0026#39;registry_password\u0026#39;)]\u0026#34; } ], \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;containerName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;YOUR_ACR_NAME.azurecr.io/porter/aks-installer:v0.1.6\u0026#34;, \u0026#34;resources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: 1.0, \u0026#34;memoryInGb\u0026#34;: 1.5 } }, \u0026#34;environmentVariables\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;CNAB_ACTION\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_action\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_INSTALLATION_NAME\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_installation_name\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_LOCATION\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_location\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_CLIENT_ID\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_client_id\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_CLIENT_SECRET\u0026#34;, \u0026#34;secureValue\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_client_secret\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_SUBSCRIPTION_ID\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_subscription_id\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;AZURE_TENANT_ID\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_tenant_id\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;VERBOSE\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;false\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_BUNDLE_NAME\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;aks\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_BUNDLE_TAG\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;YOUR_ACR_NAME.azurecr.io/porter/aks:v0.1.6\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_azure_location\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;cnab_azure_location\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_cluster_name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;cluster_name\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_kubernetes_version\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;kubernetes_version\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_node_count\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;node_count\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_node_vm_size\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;node_vm_size\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_resource_group\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;resource_group\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_vm_set_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;vm_set_type\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_installation_name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;installation_name\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_helm_chart_version\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;helm_chart_version\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;CNAB_PARAM_namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;namespace\u0026#39;)]\u0026#34; } ] } } ], \u0026#34;osType\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;restartPolicy\u0026#34;: \u0026#34;Never\u0026#34; } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;CNAB Package Action Logs Command\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[concat(\u0026#39;az container logs -g \u0026#39;,resourceGroup().name,\u0026#39; -n \u0026#39;,variables(\u0026#39;containerGroupName\u0026#39;),\u0026#39; --container-name \u0026#39;,variables(\u0026#39;containerName\u0026#39;), \u0026#39; --follow\u0026#39;)]\u0026#34; } } } t√¨m 2 ch·ªó YOUR_ACR_NAME v√† s·ª≠a b·∫±ng ACR name c·ªßa b·∫°n\nb·∫°n s·∫Ω th·∫•y Bundle c·∫ßn ƒë·∫∑t ·ªü ƒë√¢u v√† Images c·∫ßn ƒë·∫∑t ·ªü ƒë√¢u.\nM√¨nh s·∫Ω ko ƒëi s√¢u v√†o logic code, c√°c b·∫°n l√†m theo ƒë√∫ng v√† ch·∫°y ƒë∆∞·ª£c OK ƒë√£, sau ƒë√≥ h√£y quay l·∫°i ƒë·ªÉ t√¨m hi·ªÉu logic th√¨ s·∫Ω d·ªÖ hi·ªÉu h∆°n. Trong qu√° tr√¨nh fix cho code ch·∫°y ƒë∆∞·ª£c, c√°c b·∫°n c≈©ng s·∫Ω hi·ªÉu logic h∆°n l√† ng·ªìi ƒë·ªçc chay.\n3.6. Deploy ARM template V√†o link sau ƒë·ªÉ deploy ARM template m√¨nh v·ª´a vi·∫øt:\nhttps://portal.azure.com/#create/Microsoft.Template\nCh·ªçn h√¨nh b√∫t ch√¨ Build your own template in the editor\nCopy to√†n b·ªô n·ªôi dung ARM v·ª´a vi·∫øt paste ƒë√® l√™n -\u0026gt; ·∫•n n√∫t Save\nT·∫°i m√†n h√¨nh ti·∫øp theo, H√£y nh·∫≠p Client ID, Client secret, Registry server/username/password (ch√∫ √Ω b√¥i v√†ng):\n·∫•n Review and create\nCh·ªù ƒë·∫øn khi m√†n h√¨nh n√†y hi·ªán ra, nghƒ©a l√† ACI c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c t·∫°o ra: Tuy nhi√™n ƒë·∫øn ƒë√¢y th√¨ c√°c command/step c·ªßa b·∫°n v·∫´n ƒëang ch·∫°y trong ACI, h√£y xem log c·ªßa n√≥ l√† success hay fail: V√†o Azure Container Instance, ch·ªçn Instance m·ªõi do b·∫°n t·∫°o ra, -\u0026gt; v√†o Container -\u0026gt; Log,\nN·∫øu m√†n h√¨nh log nh∆∞ sau, c√≥ nghƒ©a l√† n√≥ ƒë√£ qua h·∫øt c√°c step t·ª´ AZ login, AZ create AKS, Helm chart ƒë·ªÉ cu·ªëi c√πng c√≥ 1 trang Wordpress cho b·∫°n:\nT·∫•t nhi√™n b·∫°n c√≥ th·ªÉ login v√†o AKS cluster xem c√°c pod, c√°c namespace, services ƒë∆∞·ª£c r·ªìi\nDone!\nCh√∫c b·∫°n th√†nh c√¥ng üòú\n4. Troubleshooting 4.1. ERROR: (AuthorizationFailed) N·∫øu b·∫°n g·∫∑p l·ªói n√†y khi xem Logs c·ªßa Container:\nAzure set subscription Id Create resource group if not exists ERROR: (AuthorizationFailed) The client 'xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx' with object id 'xxxxxxx-xxxx-xxxx-yyyy-yyyyy' does not have authorization to perform action 'Microsoft.Resources/subscriptions/resourcegroups/write' over scope '/subscriptions/*******/resourcegroups/akstest001-RG' or the scope is invalid. If access was recently granted, please refresh your credentials. err: error running command /cnab/app az group create --location westeurope --name akstest001-RG --output json: exit status 1 -\u0026gt; C√≥ nghƒ©a l√† App Client Id/Secret c·ªßa b·∫°n ƒëang ko c√≥ quy·ªÅn t·∫°o Resource Group. B·∫°n h√£y ch√∫ √Ω App Registration c·ªßa b·∫°n c·∫ßn dc c√≥ quy·ªÅn COntributor tr√™n scope l√† Subscription, ko ph·∫£i ch·ªâ 1 Resource group n√†o ƒë√≥. (Xem k·ªπ l·∫°i step 2.1.b.)\n5. T√¨m hi·ªÉu th√™m v·ªÅ Porter Trong qu√° tr√¨nh ph√°t tri·ªÉn, b·∫°n s·∫Ω kh√¥ng mu·ªën m·ªói l·∫ßn s·ª≠a code mu·ªën test th√¨ l·∫°i ph·∫£i porter publish r·ªìi l·∫°i deploy ARM template. Nh∆∞ v·∫≠y s·∫Ω r·∫•t t·ªën th·ªùi gian (M·ªói l·∫ßn porter publish ph·∫£i m·∫•t kho·∫£ng 10-15 ph√∫t ƒë·ªÉ n√≥ upload ƒë∆∞·ª£c images x·∫•p x·ªâ 2Gb l√™n Registry c·ªßa b·∫°n)\nTh·∫ø n√™n sau b∆∞·ªõc porter build, b·∫°n c√≥ th·ªÉ test lu√¥n image tr∆∞·ªõc khi publish n√≥ b·∫±ng c√°ch d√πng c√°c command porter install\nNh∆∞ng ƒë·ªÉ d√πng th√¨ b·∫°n c·∫ßn truy·ªÅn c√°c credential v√†o tr∆∞·ªõc ƒë√£:\nporter credentials generate [YOUR_CRED_NAME] 1 list c√°c l·ª±a ch·ªçn hi·ªán ra, sau ƒë√≥ b·∫°n insert c√°c gi√° tr·ªã credential (Service Principal Client ID, secret, registry user name, password, \u0026hellip;etc)\n(Optional) B·∫°n c≈©ng c√≥ th·ªÉ t·∫°o c√°c Parameter set:\nporter parameters generate [YOUR_PARAM_SET_NAME] Cu·ªëi c√πng th√¨ c√≥ th·ªÉ test Images:\nporter install --cred [YOUR_CRED_NAME] --parameter-set [YOUR_PARAM_SET_NAME] C√¢u l·ªánh tr√™n s·∫Ω t·∫°o ra container ngay tr√™n m√°y local c·ªßa b·∫°n, gi√∫p b·∫°n nhanh ch√≥ng debug n·∫øu c√≥ v·∫•n ƒë·ªÅ.\nList c√°c command c·ªßa Porter ·ªü ƒë√¢y: https://porter.sh/cli/porter/#see-also\nCREDITS https://porter.sh/\nhttps://github.com/Azure/azure-cnab-quickstarts\nhttps://www.youtube.com/watch?v=z1lnQfaAVeg\u0026amp;ab_channel=endjin\n","href":"/posts/azure-create-akscluster-using-arm-aci-cnab-porter/","title":"Azure: Create AKS Cluster by ARM and ACI (using CNAB, porter.sh)"},{"content":"","href":"/tags/cnab/","title":"CNAB"},{"content":"Gi·ªõi thi·ªáu Ch·∫Øc c√°c b·∫°n h·∫≥n ƒë√£ t·ª´ng g·∫∑p l·ªói n√†y khi d√πng Ubuntu subsystem tr√™n Windows:\n The command \u0026lsquo;docker\u0026rsquo; could not be found in this WSL1 distro. We recommend to convert this distro into WSL 2 and activate the WSL integration in Docker Desktop settings.\n V·∫≠y c·∫ßn l√†m g√¨ ƒë·ªÉ convert WSL 1 sang WSL 2?\nY√™u c·∫ßu  Ph·∫£i l√† Windows 10 B·∫≠t CMD, run winver ƒë·ªÉ xem version Windows c·ªßa b·∫°n ƒëang d√πng l√† g√¨? version nh·ªè h∆°n 18362 l√† ko l√†m ƒë∆∞·ª£c ƒë√¢u nh√©.   Builds lower than 18362 do not support WSL 2.\n C√°ch l√†m Download latest package from this step: https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package\nIn short, download from this link and install it: https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi\nB·∫≠t PowerShell, Run command sau ƒë·ªÉ set WSL version 2 cho Ubuntu-18.04:\nwsl --set-version Ubuntu-18.04 2\nFor information on key differences with WSL 2 please visit https://aka.ms/wsl2 PS C:\\Users\\abc\u0026gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 1 docker-desktop Running 2 docker-desktop-data Running 2 PS C:\\Users\\abc\u0026gt; wsl --set-version Ubuntu-18.04 2 Conversion in progress, this may take a few minutes... For information on key differences with WSL 2 please visit https://aka.ms/wsl2 Conversion complete. PS C:\\Users\\abc\u0026gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 2 docker-desktop Running 2 docker-desktop-data Running 2 PS C:\\Users\\abc\u0026gt; Nh∆∞ ƒëo·∫°n log m√†n h√¨nh tr√™n m√† c√°c b·∫°n th·∫•y, ban ƒë·∫ßu m√¨nh list c√°c wsl hi·ªán c√≥ th√¨ Ubuntu-18.04 ƒëang ch·∫°y Version 1.\nSau khi setting l·∫°i th√¨ n√≥ chuy·ªÉn sang version 2.\nCh∆∞a h·∫øt b·∫°n c·∫ßn setting trong Docker Windows App n·ªØa, v√†o SETTING -\u0026gt; RESOURCES -\u0026gt; WSL Integration, ch·ªçn nh∆∞ h√¨nh n√†y:\nXong r·ªìi, v√†o Ubuntu subsystem ƒë·ªÉ g√µ docker th√¥i.\nCh√∫c b·∫°n th√†nh c√¥ng!\nCREDITS https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package\n","href":"/bk/enable-docker-command-for-ubuntu-subsytem-wsl/","title":"Run docker command from Ubuntu Subsystem of Windows (WSL)"},{"content":"Gi·ªõi thi·ªáu Ch·∫Øc c√°c b·∫°n h·∫≥n ƒë√£ t·ª´ng g·∫∑p l·ªói n√†y khi d√πng Ubuntu subsystem tr√™n Windows:\n The command \u0026lsquo;docker\u0026rsquo; could not be found in this WSL1 distro. We recommend to convert this distro into WSL 2 and activate the WSL integration in Docker Desktop settings.\n V·∫≠y c·∫ßn l√†m g√¨ ƒë·ªÉ convert WSL 1 sang WSL 2?\nY√™u c·∫ßu  Ph·∫£i l√† Windows 10 B·∫≠t CMD, run winver ƒë·ªÉ xem version Windows c·ªßa b·∫°n ƒëang d√πng l√† g√¨? version nh·ªè h∆°n 18362 l√† ko l√†m ƒë∆∞·ª£c ƒë√¢u nh√©.   Builds lower than 18362 do not support WSL 2.\n C√°ch l√†m Download latest package from this step: https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package\nIn short, download from this link and install it: https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi\nB·∫≠t PowerShell, Run command sau ƒë·ªÉ set WSL version 2 cho Ubuntu-18.04:\nwsl --set-version Ubuntu-18.04 2\nFor information on key differences with WSL 2 please visit https://aka.ms/wsl2 PS C:\\Users\\abc\u0026gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 1 docker-desktop Running 2 docker-desktop-data Running 2 PS C:\\Users\\abc\u0026gt; wsl --set-version Ubuntu-18.04 2 Conversion in progress, this may take a few minutes... For information on key differences with WSL 2 please visit https://aka.ms/wsl2 Conversion complete. PS C:\\Users\\abc\u0026gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 2 docker-desktop Running 2 docker-desktop-data Running 2 PS C:\\Users\\abc\u0026gt; Nh∆∞ ƒëo·∫°n log m√†n h√¨nh tr√™n m√† c√°c b·∫°n th·∫•y, ban ƒë·∫ßu m√¨nh list c√°c wsl hi·ªán c√≥ th√¨ Ubuntu-18.04 ƒëang ch·∫°y Version 1.\nSau khi setting l·∫°i th√¨ n√≥ chuy·ªÉn sang version 2.\nCh∆∞a h·∫øt b·∫°n c·∫ßn setting trong Docker Windows App n·ªØa, v√†o SETTING -\u0026gt; RESOURCES -\u0026gt; WSL Integration, ch·ªçn nh∆∞ h√¨nh n√†y:\nXong r·ªìi, v√†o Ubuntu subsystem ƒë·ªÉ g√µ docker th√¥i.\nCh√∫c b·∫°n th√†nh c√¥ng!\nCREDITS https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package\n","href":"/posts/enable-docker-command-for-ubuntu-subsytem-wsl/","title":"Run docker command from Ubuntu Subsystem of Windows (WSL)"},{"content":"","href":"/tags/wsl/","title":"WSL"},{"content":"","href":"/tags/apigateway/","title":"APIGateway"},{"content":"","href":"/tags/cognito/","title":"Cognito"},{"content":"Gi·ªõi thi·ªáu B√†i n√†y gi·∫£i th√≠ch v·ªÅ c√°ch secure API Gateway enpoint b·∫±ng c√°ch d√πng Cognito User Pool Authorizer.\nFrontend trong b√†i n√†y s·ª≠ d·ª•ng Reacjs nh√©.\nAPI Gateway c·ªßa AWS th√¨ c√≥ 2 lo·∫°i chia theo protocol: HTTP API v√† REST API (B√†i n√†y n√≥i v·ªÅ REST API).\nY√™u c·∫ßu C√°c b·∫°n c·∫ßn c√≥ base ki·∫øn th·ª©c v·ªÅ ReactJS 1 ch√∫t, bi·∫øt l√†m th·∫ø n√†o ƒë·ªÉ vi·∫øt 1 component, export n√≥ ra nh∆∞ n√†o, l√†m sao ƒë·ªÉ apply authenticate v√†o Gatsby, c√°ch vi·∫øt 1 component, create pages, pass props qua c√°c page, export page \u0026hellip;\nƒê√£ t·∫°o 1 Cognito User Pool, c√°c b·∫°n n√™n ho√†n th√†nh ho·∫∑c ƒë·ªçc qua b√†i n√†y tr∆∞·ªõc ƒë·ªÉ bi·∫øt c√°ch integrate AWS Cognito v√†o Gatsby website:\nIntegrate AWS Cognito to Gatsby Website\nB·∫°n c≈©ng c·∫ßn ƒë√£ bi·∫øt v·ªÅ c√°ch config API Gateway REST, ƒë√£ t·∫°o ƒë∆∞·ª£c RESOURCES, METHOD v√† integrate ƒë∆∞·ª£c n√≥ v·ªõi Lambda function.\nC√°ch l√†m Backend c·ªßa ch√∫ng ta chu·∫©n b·ªã 1 h√†m Lambda ƒë∆°n gi·∫£n nh∆∞ n√†y:\nConfig trong API Gateway REST:\nT·∫°o 1 resource /interact v√† method POST cho n√≥\nmethod OPTION ƒë∆∞·ª£c t·∫°o ra 1 c√°ch t·ª± ƒë·ªông n√™n b·∫°n ko c·∫ßn ƒë·ªông ƒë·∫øn n√≥ l√†m g√¨ nh√©.\nSau ƒë√≥ integrate n√≥ v·ªõi h√†m Lambda m√† ch√∫ng ta ƒë√£ chu·∫©n b·ªã ·ªü tr√™n V√†o tab Authorizers, t·∫°o 1 c√°i Authorizer, nh·ªõ ch·ªçn Cognito User Pool m√† b·∫°n ƒë√£ c√≥, ch√∫ √Ω ch·ªó Token Source ƒëi·ªÅn Authorization nh√©:\nQuay l·∫°i tab Resources, ch·ªçn Method POST -\u0026gt; Method Request, ph·∫ßn Authorization h√£y ch·ªçn c√°i Authorizer m√† b·∫°n v·ª´a t·∫°o ·ªü tr√™n:\nSave xong th√¨ quay l·∫°i, m√†n h√¨nh c·ªßa resource /interact n√≥ s·∫Ω nh∆∞ n√†y:\nEnable CORS v√† deploy l·∫°i API Gateway:\nTrong code c·ªßa Frontend th√¨ c·∫ßn ƒë∆∞a token v√†o trong header nh∆∞ sau, d∆∞·ªõi ƒë√¢y l√† code cho 1 component. T√™n file l√† InteractRestApiGW.js:\nimport React, { useState } from \u0026#39;react\u0026#39;; import { useForm } from \u0026#39;react-hook-form\u0026#39;; const GATEWAY_URL = \u0026#34;https://xxxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/interact\u0026#34;; // REST API Gateway (API has applied Authorizer already) var msgFromLambda = \u0026#34;Nothing, something go wrong..\u0026#34;; const InteractRestApiGW = ({ token }) =\u0026gt; { const [submitted, setSubmitted] = useState(false); const { register, handleSubmit, setError, reset, formState: { errors, isSubmitting } } = useForm(); const onSubmit = async data =\u0026gt; { const payload = { \u0026#34;message\u0026#34;: { \u0026#34;name\u0026#34;: data.myName, } } try { const response = await fetch(GATEWAY_URL, { method: \u0026#34;POST\u0026#34;, mode: \u0026#34;cors\u0026#34;, contentType: \u0026#34;application/json\u0026#34;, body: JSON.stringify(payload), headers: { \u0026#34;Content-type\u0026#34;: \u0026#34;application/json; charset=UTF-8\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: token, } }); const result = await response.json(); msgFromLambda = result.text setSubmitted(true); reset(); } catch (error) { setError( \u0026#34;submit\u0026#34;, \u0026#34;submitError\u0026#34;, `Oops! There seems to be an issue! ${error.message}` ); } }; const showThankYou = ( \u0026lt;div className=\u0026#34;msg-confirm\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Awesome! Your command was sent. The response is: \u0026lt;/p\u0026gt;  \u0026lt;p style={{fontWeight: \u0026#34;bold\u0026#34;}}\u0026gt;{msgFromLambda}\u0026lt;/p\u0026gt; \u0026lt;p style={{fontSize: \u0026#39;13px\u0026#39;}}\u0026gt;\u0026lt;i\u0026gt;If the response is a Hello your name from Lambda, it means Backend recognized you. Congras! Otherwise, please debug it for more detail...\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;button className=\u0026#34;btn btn-info my-3\u0026#34; type=\u0026#34;button\u0026#34; onClick={() =\u0026gt; setSubmitted(false)}\u0026gt; Send another command? \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); const showSubmitError = msg =\u0026gt; \u0026lt;p className=\u0026#34;msg-error\u0026#34;\u0026gt;{msg}\u0026lt;/p\u0026gt;; const showForm = ( \u0026lt;form onSubmit={handleSubmit(onSubmit)} method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;p\u0026gt;This text box send command to REST API Gateway URL (which is applied Cognito User Pool Authorizer already. It means if someone has URL, they still can not send requests to it unless they authenticated this page): \u0026lt;/p\u0026gt; \u0026lt;label htmlFor=\u0026#34;myName\u0026#34;\u0026gt; \u0026lt;input className=\u0026#34;form-control\u0026#34; type=\u0026#34;myName\u0026#34; name=\u0026#34;myName\u0026#34; id=\u0026#34;myName\u0026#34; placeholder=\u0026#34;Your name\u0026#34; {...register(\u0026#34;myName\u0026#34;, { required: \u0026#34;Dont forget your name!\u0026#34;, })} disabled={isSubmitting} /\u0026gt; {errors.myName \u0026amp;\u0026amp; \u0026lt;div className=\u0026#34;msg-error\u0026#34;\u0026gt;{errors.myName.message}\u0026lt;/div\u0026gt;} \u0026lt;/label\u0026gt; \u0026lt;div className=\u0026#34;submit-wrapper\u0026#34;\u0026gt; \u0026lt;button className=\u0026#34;btn btn-info my-3\u0026#34; type=\u0026#34;submit\u0026#34; disabled={isSubmitting}\u0026gt;Send command\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; ); return ( \u0026lt;div className=\u0026#34;container my-4\u0026#34;\u0026gt; {errors \u0026amp;\u0026amp; errors.submit \u0026amp;\u0026amp; showSubmitError(errors.submit.message)} \u0026lt;div className=\u0026#34;form-side\u0026#34;\u0026gt;{submitted ? showThankYou : showForm}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ) } export default InteractRestApiGW C√≥ th·ªÉ c√°c b·∫°n s·∫Ω t·ª± h·ªèi token l·∫•y ƒë√¢u ra?\nTr·∫£ l·ªùi: Th√¨ ƒë·ªÉ c√≥ token n√†y b·∫°n c≈©ng c·∫ßn s·ª≠a n∆°i c√°c b·∫°n apply Authentication 1 ch√∫t.\nGi·∫£ s·ª≠ ƒë√¢y l√† code c·ªßa Home Page (index.js) c·ªßa b·∫°n, nh∆∞ b·∫°n th·∫•y ch√∫ng ta ƒë√£ import component InteractRestApiGW v·ª´a t·∫°o ra ·ªü tr√™n, s·ª≠ d·ª•ng n√≥ g·∫ßn cu·ªëi ƒëo·∫°n code, v√† truy·ªÅn bi·∫øn token l·∫•y t·ª´ props c·ªßa qu√° tr√¨nh authen ƒë·∫øn Cognito:\nimport React from \u0026#39;react\u0026#39; import { SignIn, SignOut } from \u0026#34;@hoangmnsd/gatsby-theme-amplify-cognito\u0026#34;; import InteractRestApiGW from \u0026#39;../components/InteractRestApiGW\u0026#39;; const PrivateSpace = props =\u0026gt; { return ( (props.authState !== \u0026#34;signedIn\u0026#34;) ? \u0026lt;SignIn authState={props.authState} /\u0026gt; : \u0026lt;SignOut /\u0026gt; \u0026lt;div className=\u0026#34;container about my-5\u0026#34;\u0026gt; \u0026lt;h1 className=\u0026#34;font-weight-bold\u0026#34;\u0026gt;Hello my friend,\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;This space requires an authenticated user\u0026lt;/h2\u0026gt; \u0026lt;InteractRestApiGW token={props.authData.signInUserSession.idToken.jwtToken} /\u0026gt; \u0026lt;/div\u0026gt; ) } export default PrivateSpace Ph·∫ßn UI c·ªßa ƒëo·∫°n code tr√™n nh∆∞ sau:\nKhi b·∫°n nh·∫≠p t√™n v√†o ch·ªó Your name, App s·∫Ω g·ª≠i request ƒë·∫øn API Gateway, trong header c·ªßa request ƒë√≥ c√≥ token c·ªßa b·∫°n, token n√†y b·∫°n l·∫•y ƒë∆∞·ª£c sau khi login b·∫±ng Cognito identity. N·∫øu ko c√≥ token ƒë√≥, API Gateway s·∫Ω ko tr·∫£ v·ªÅ k·∫øt qu·∫£. Nh∆∞ v·∫≠y l√† secure r·ªìi ƒë√∫ng ko?\nAPI g·ª≠i t√™n b·∫°n (v·ª´a nh·∫≠p v√†o) ƒë·∫øn backend cho Lambda x·ª≠ l√Ω, Lambda tr·∫£ v·ªÅ respone v√† g·ª≠i l·∫°i k·∫øt qu·∫£ cho APIGateway, t·ª´ ƒë√≥ tr·∫£ v·ªÅ cho b·∫°n n·ªôi dung \u0026ldquo;Hello from Lambda\u0026rdquo; nh∆∞ h√¨nh sau:\nN√≥i th√¨ c√≥ v·∫ª kh√≥ hi·ªÉu, nh∆∞ng nh√¨n h√¨nh sau c√°c b·∫°n ch·∫Øc s·∫Ω th·∫•y ƒë∆°n gi·∫£n:\nDone!\nCREDITS https://seifi.org/reactjs/build-a-contact-form-in-gatsby-part-2-react-hook-form.html\nhttps://seifi.org/aws/build-a-contact-form-in-gatsby-part-1-aws-lambda-simple-email-service-and-api-gateway.html\nhttps://github.com/bengladwell/inside-story\nhttps://www.bengladwell.com/gatsby-site-with-authentication/\n","href":"/bk/secure-api-gateway-by-cognito-user-pool-gatsbyjs/","title":"Secure API Gateway by AWS Cognito User Pool (in ReactJS based Gatsby App)"},{"content":"Gi·ªõi thi·ªáu B√†i n√†y gi·∫£i th√≠ch v·ªÅ c√°ch secure API Gateway enpoint b·∫±ng c√°ch d√πng Cognito User Pool Authorizer.\nFrontend trong b√†i n√†y s·ª≠ d·ª•ng Reacjs nh√©.\nAPI Gateway c·ªßa AWS th√¨ c√≥ 2 lo·∫°i chia theo protocol: HTTP API v√† REST API (B√†i n√†y n√≥i v·ªÅ REST API).\nY√™u c·∫ßu C√°c b·∫°n c·∫ßn c√≥ base ki·∫øn th·ª©c v·ªÅ ReactJS 1 ch√∫t, bi·∫øt l√†m th·∫ø n√†o ƒë·ªÉ vi·∫øt 1 component, export n√≥ ra nh∆∞ n√†o, l√†m sao ƒë·ªÉ apply authenticate v√†o Gatsby, c√°ch vi·∫øt 1 component, create pages, pass props qua c√°c page, export page \u0026hellip;\nƒê√£ t·∫°o 1 Cognito User Pool, c√°c b·∫°n n√™n ho√†n th√†nh ho·∫∑c ƒë·ªçc qua b√†i n√†y tr∆∞·ªõc ƒë·ªÉ bi·∫øt c√°ch integrate AWS Cognito v√†o Gatsby website:\nIntegrate AWS Cognito to Gatsby Website\nB·∫°n c≈©ng c·∫ßn ƒë√£ bi·∫øt v·ªÅ c√°ch config API Gateway REST, ƒë√£ t·∫°o ƒë∆∞·ª£c RESOURCES, METHOD v√† integrate ƒë∆∞·ª£c n√≥ v·ªõi Lambda function.\nC√°ch l√†m Backend c·ªßa ch√∫ng ta chu·∫©n b·ªã 1 h√†m Lambda ƒë∆°n gi·∫£n nh∆∞ n√†y:\nConfig trong API Gateway REST:\nT·∫°o 1 resource /interact v√† method POST cho n√≥\nmethod OPTION ƒë∆∞·ª£c t·∫°o ra 1 c√°ch t·ª± ƒë·ªông n√™n b·∫°n ko c·∫ßn ƒë·ªông ƒë·∫øn n√≥ l√†m g√¨ nh√©.\nSau ƒë√≥ integrate n√≥ v·ªõi h√†m Lambda m√† ch√∫ng ta ƒë√£ chu·∫©n b·ªã ·ªü tr√™n V√†o tab Authorizers, t·∫°o 1 c√°i Authorizer, nh·ªõ ch·ªçn Cognito User Pool m√† b·∫°n ƒë√£ c√≥, ch√∫ √Ω ch·ªó Token Source ƒëi·ªÅn Authorization nh√©:\nQuay l·∫°i tab Resources, ch·ªçn Method POST -\u0026gt; Method Request, ph·∫ßn Authorization h√£y ch·ªçn c√°i Authorizer m√† b·∫°n v·ª´a t·∫°o ·ªü tr√™n:\nSave xong th√¨ quay l·∫°i, m√†n h√¨nh c·ªßa resource /interact n√≥ s·∫Ω nh∆∞ n√†y:\nEnable CORS v√† deploy l·∫°i API Gateway:\nTrong code c·ªßa Frontend th√¨ c·∫ßn ƒë∆∞a token v√†o trong header nh∆∞ sau, d∆∞·ªõi ƒë√¢y l√† code cho 1 component. T√™n file l√† InteractRestApiGW.js:\nimport React, { useState } from \u0026#39;react\u0026#39;; import { useForm } from \u0026#39;react-hook-form\u0026#39;; const GATEWAY_URL = \u0026#34;https://xxxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/interact\u0026#34;; // REST API Gateway (API has applied Authorizer already) var msgFromLambda = \u0026#34;Nothing, something go wrong..\u0026#34;; const InteractRestApiGW = ({ token }) =\u0026gt; { const [submitted, setSubmitted] = useState(false); const { register, handleSubmit, setError, reset, formState: { errors, isSubmitting } } = useForm(); const onSubmit = async data =\u0026gt; { const payload = { \u0026#34;message\u0026#34;: { \u0026#34;name\u0026#34;: data.myName, } } try { const response = await fetch(GATEWAY_URL, { method: \u0026#34;POST\u0026#34;, mode: \u0026#34;cors\u0026#34;, contentType: \u0026#34;application/json\u0026#34;, body: JSON.stringify(payload), headers: { \u0026#34;Content-type\u0026#34;: \u0026#34;application/json; charset=UTF-8\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: token, } }); const result = await response.json(); msgFromLambda = result.text setSubmitted(true); reset(); } catch (error) { setError( \u0026#34;submit\u0026#34;, \u0026#34;submitError\u0026#34;, `Oops! There seems to be an issue! ${error.message}` ); } }; const showThankYou = ( \u0026lt;div className=\u0026#34;msg-confirm\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Awesome! Your command was sent. The response is: \u0026lt;/p\u0026gt;  \u0026lt;p style={{fontWeight: \u0026#34;bold\u0026#34;}}\u0026gt;{msgFromLambda}\u0026lt;/p\u0026gt; \u0026lt;p style={{fontSize: \u0026#39;13px\u0026#39;}}\u0026gt;\u0026lt;i\u0026gt;If the response is a Hello your name from Lambda, it means Backend recognized you. Congras! Otherwise, please debug it for more detail...\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;button className=\u0026#34;btn btn-info my-3\u0026#34; type=\u0026#34;button\u0026#34; onClick={() =\u0026gt; setSubmitted(false)}\u0026gt; Send another command? \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); const showSubmitError = msg =\u0026gt; \u0026lt;p className=\u0026#34;msg-error\u0026#34;\u0026gt;{msg}\u0026lt;/p\u0026gt;; const showForm = ( \u0026lt;form onSubmit={handleSubmit(onSubmit)} method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;p\u0026gt;This text box send command to REST API Gateway URL (which is applied Cognito User Pool Authorizer already. It means if someone has URL, they still can not send requests to it unless they authenticated this page): \u0026lt;/p\u0026gt; \u0026lt;label htmlFor=\u0026#34;myName\u0026#34;\u0026gt; \u0026lt;input className=\u0026#34;form-control\u0026#34; type=\u0026#34;myName\u0026#34; name=\u0026#34;myName\u0026#34; id=\u0026#34;myName\u0026#34; placeholder=\u0026#34;Your name\u0026#34; {...register(\u0026#34;myName\u0026#34;, { required: \u0026#34;Dont forget your name!\u0026#34;, })} disabled={isSubmitting} /\u0026gt; {errors.myName \u0026amp;\u0026amp; \u0026lt;div className=\u0026#34;msg-error\u0026#34;\u0026gt;{errors.myName.message}\u0026lt;/div\u0026gt;} \u0026lt;/label\u0026gt; \u0026lt;div className=\u0026#34;submit-wrapper\u0026#34;\u0026gt; \u0026lt;button className=\u0026#34;btn btn-info my-3\u0026#34; type=\u0026#34;submit\u0026#34; disabled={isSubmitting}\u0026gt;Send command\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; ); return ( \u0026lt;div className=\u0026#34;container my-4\u0026#34;\u0026gt; {errors \u0026amp;\u0026amp; errors.submit \u0026amp;\u0026amp; showSubmitError(errors.submit.message)} \u0026lt;div className=\u0026#34;form-side\u0026#34;\u0026gt;{submitted ? showThankYou : showForm}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ) } export default InteractRestApiGW C√≥ th·ªÉ c√°c b·∫°n s·∫Ω t·ª± h·ªèi token l·∫•y ƒë√¢u ra?\nTr·∫£ l·ªùi: Th√¨ ƒë·ªÉ c√≥ token n√†y b·∫°n c≈©ng c·∫ßn s·ª≠a n∆°i c√°c b·∫°n apply Authentication 1 ch√∫t.\nGi·∫£ s·ª≠ ƒë√¢y l√† code c·ªßa Home Page (index.js) c·ªßa b·∫°n, nh∆∞ b·∫°n th·∫•y ch√∫ng ta ƒë√£ import component InteractRestApiGW v·ª´a t·∫°o ra ·ªü tr√™n, s·ª≠ d·ª•ng n√≥ g·∫ßn cu·ªëi ƒëo·∫°n code, v√† truy·ªÅn bi·∫øn token l·∫•y t·ª´ props c·ªßa qu√° tr√¨nh authen ƒë·∫øn Cognito:\nimport React from \u0026#39;react\u0026#39; import { SignIn, SignOut } from \u0026#34;@hoangmnsd/gatsby-theme-amplify-cognito\u0026#34;; import InteractRestApiGW from \u0026#39;../components/InteractRestApiGW\u0026#39;; const PrivateSpace = props =\u0026gt; { return ( (props.authState !== \u0026#34;signedIn\u0026#34;) ? \u0026lt;SignIn authState={props.authState} /\u0026gt; : \u0026lt;SignOut /\u0026gt; \u0026lt;div className=\u0026#34;container about my-5\u0026#34;\u0026gt; \u0026lt;h1 className=\u0026#34;font-weight-bold\u0026#34;\u0026gt;Hello my friend,\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;This space requires an authenticated user\u0026lt;/h2\u0026gt; \u0026lt;InteractRestApiGW token={props.authData.signInUserSession.idToken.jwtToken} /\u0026gt; \u0026lt;/div\u0026gt; ) } export default PrivateSpace Ph·∫ßn UI c·ªßa ƒëo·∫°n code tr√™n nh∆∞ sau:\nKhi b·∫°n nh·∫≠p t√™n v√†o ch·ªó Your name, App s·∫Ω g·ª≠i request ƒë·∫øn API Gateway, trong header c·ªßa request ƒë√≥ c√≥ token c·ªßa b·∫°n, token n√†y b·∫°n l·∫•y ƒë∆∞·ª£c sau khi login b·∫±ng Cognito identity. N·∫øu ko c√≥ token ƒë√≥, API Gateway s·∫Ω ko tr·∫£ v·ªÅ k·∫øt qu·∫£. Nh∆∞ v·∫≠y l√† secure r·ªìi ƒë√∫ng ko?\nAPI g·ª≠i t√™n b·∫°n (v·ª´a nh·∫≠p v√†o) ƒë·∫øn backend cho Lambda x·ª≠ l√Ω, Lambda tr·∫£ v·ªÅ respone v√† g·ª≠i l·∫°i k·∫øt qu·∫£ cho APIGateway, t·ª´ ƒë√≥ tr·∫£ v·ªÅ cho b·∫°n n·ªôi dung \u0026ldquo;Hello from Lambda\u0026rdquo; nh∆∞ h√¨nh sau:\nN√≥i th√¨ c√≥ v·∫ª kh√≥ hi·ªÉu, nh∆∞ng nh√¨n h√¨nh sau c√°c b·∫°n ch·∫Øc s·∫Ω th·∫•y ƒë∆°n gi·∫£n:\nDone!\nCREDITS https://seifi.org/reactjs/build-a-contact-form-in-gatsby-part-2-react-hook-form.html\nhttps://seifi.org/aws/build-a-contact-form-in-gatsby-part-1-aws-lambda-simple-email-service-and-api-gateway.html\nhttps://github.com/bengladwell/inside-story\nhttps://www.bengladwell.com/gatsby-site-with-authentication/\n","href":"/posts/secure-api-gateway-by-cognito-user-pool-gatsbyjs/","title":"Secure API Gateway by AWS Cognito User Pool (in ReactJS based Gatsby App)"},{"content":"1. Gi·ªõi thi·ªáu G·∫ßn ƒë√¢y khi t√¨m hi·ªÉu v·ªÅ frontend th√¨ m√¨nh bi·∫øt r·∫±ng Hugo kh√¥ng th·ªÉ cung c·∫•p t√≠nh nƒÉng Authenticate cho website c·ªßa m√¨nh. V√¨ b·∫£n th√¢n Hugo l√† 1 Static Site Generator (SSG), n√≥ ko c√≥ tr√°ch nhi·ªám bi·∫øn website c·ªßa b·∫°n th√†nh dynamic.\nTi·∫øp t·ª•c t√¨m hi·ªÉu th√¨ m√¨nh bi·∫øt th√™m v·ªÅ GatsbyJS, c≈©ng l√† 1 Static Site Generator, nh∆∞ng n√≥ based tr√™n ReactJS v√† c√≥ nhi·ªÅu plugin ƒë·ªÉ bi·∫øn trang web th√†nh dynamic.\nV√≠ d·ª• nh∆∞ m√¨nh c√≥ th·ªÉ t√≠ch h·ª£p t√≠nh nƒÉng Login/Logout v√†o cho 1 website ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng Gatsby 1 c√°ch nhanh ch√≥ng.\nM√¨nh h∆∞·ªõng t·ªõi vi·ªác x√¢y d·ª±ng nh·ªØng website Serverless ho√†n to√†n, t·ªën √≠t c√¥ng s·ª©c t√¨m hi·ªÉu v·ªÅ Framework m·ªõi, n√™n s·∫Ω s·ª≠ d·ª•ng thirdparty authenticator nh∆∞ AWS Cognito.\nC≈©ng nh∆∞ Hugo, Gatsby c√≥ 1 c·ªông ƒë·ªìng r·ªông l·ªõn, h·ªç ƒë√≥ng g√≥p nhi·ªÅu nh·ªØng theme ƒë·∫πp v√† free cho m·ªçi ng∆∞·ªùi. B·∫°n c√≥ th·ªÉ tham kh·∫£o t·∫°i ƒë√¢y:\nhttps://www.gatsbyjs.com/starters/?\nhttps://gatsbytemplates.io/free/\nhttps://themejam.gatsbyjs.org/showcase\nhttps://jamtemplates.com/\nhttps://github.com/algokun/gatsby_starter_portfolio\n(224) https://jamstackthemes.dev/ssg/gatsby/ https://github.com/stackbit/jamstackthemes\n2. B·∫Øt ƒë·∫ßu 2.1. Ch·ªçn Theme B√†i n√†y m√¨nh s·∫Ω ch·ªçn 1 theme free t√™n l√† Serif Gatsby Starter ƒë·ªÉ demo vi·ªác integrate v·ªõi Cognito. Github c·ªßa n√≥ ·ªü ƒë√¢y: https://github.com/zerostaticthemes/gatsby-serif-theme\nT·∫•t nhi√™n ƒë·ªÉ s·ª≠ d·ª•ng Gatsby b·∫°n c·∫ßn c√†i ƒë·∫∑t 1 s·ªë th·ª©:\nnode ‚Äìv npm -v npm install ‚Äìglobal gatsby-cli Clone source c·ªßa Theme m√† ta ƒë√£ ch·ªçn v·ªÅ:\ngit clone https://github.com/zerostaticthemes/gatsby-serif-theme cd gatsby-serif-theme 2.2. Run local Run project ·ªü local:\nnpm install npm start Gi·ªù h√£y v√†o ƒë·ªãa ch·ªâ http://localhost:8000 ƒë·ªÉ xem k·∫øt qu·∫£:\nR·∫•t nhanh ch√≥ng ph·∫£i ko c√°c b·∫°n ƒë√£ c√≥ 1 website r·∫•t chuy√™n nghi·ªáp r·ªìi. Gi·ªù ch·ªâ c·∫ßn thay ƒë·ªïi ph·∫ßn content, h√¨nh ·∫£nh n·ªØa.\nContent ƒë∆∞·ª£c ƒë·∫∑t trong /src/content/, g·ªìm to√†n c√°c file Markdown (r·∫•t quen thu·ªôc). B·∫°n thay ƒë·ªïi n·ªôi dung c·ªßa m√¨nh v√†o ƒë√≥ l√† xong.\n2.3. Integrate with AWS Cognito Gi·ªù l√†m sao ƒë·ªÉ integrate ph·∫ßn Authentication v√†o?\nS·∫Ω c√≥ l√∫c c√°c b·∫°n mu·ªën website c·ªßa m√¨nh ƒë∆∞·ª£c b·∫£o v·ªá, trang Home Page th√¨ ai c≈©ng xem ƒë∆∞·ª£c, nh∆∞ng c√°c trang kh√°c th√¨ ph·∫£i y√™u c·∫ßu ƒëƒÉng nh·∫≠p (ch·∫≥ng h·∫°n nh∆∞ trang Team, About, \u0026hellip;etc).\nƒê√¢y l√† l√∫c c·∫ßn ch·ª©c nƒÉng Authentication, nh∆∞ng ch·∫≥ng l·∫Ω l·∫°i ph·∫£i h·ªçc code ReactJS ƒë·ªÉ th√™m t√≠nh nƒÉng Login, Logout, Sign Up hay sao, r·ªìi Database User n·ªØa s·∫Ω ƒë·ªÉ ·ªü ƒë√¢u? Nghe c√≥ v·∫ª d·ªÖ nh∆∞ng v√†o l√†m s·∫Ω lo·∫≥ng ngo·∫±ng ƒë·∫•y, nh·∫•t l√† v·ªõi nh·ªØng bussiness nh·ªè mu·ªën m·ªçi th·ª© c·∫ßn ho√†n thi·ªán nhanh.\nV√¨ v·∫≠y m√† Gatsby cung c·∫•p nhi·ªÅu plugin ƒë∆∞·ª£c c·ªông ƒë·ªìng ƒë√≥ng g√≥p, ch·ªâ c·∫ßn ch·ªânh s·ª≠a 1 √≠t, b·∫°n v·∫´n ƒëem ƒë∆∞·ª£c t√≠nh nƒÉng Login/Logout v√†o trang web c·ªßa m√¨nh.\n2.3.1. B∆∞·ªõc 1 ƒë·∫ßu ti√™n install package m·ªõi:\nnpm install --save @hoangmnsd/gatsby-theme-amplify-cognito 2.3.2. B∆∞·ªõc 2 Ti·∫øp theo l√† setup AWS Cognito User Pool v√† t·∫°o App Client, Nh·ªõ l√† ƒë·ª´ng ch·ªçn Generate client secret: 2.3.3. B∆∞·ªõc 3 S·ª≠a file gatsby-config.js ƒë·ªÉ add plugin v√†o:\ngatsby-config.js\nmodule.exports = { plugins: [ { resolve: `@hoangmnsd/gatsby-theme-amplify-cognito`, options: { region: \u0026quot;us-east-1\u0026quot;, // replace with region of user pool userPoolId: 'us-east-1_OZIxeIDqs\u0026quot;,', // replace with user pool id identityPoolId: \u0026quot;23ab3gt81t2sanvfg84mh7xnpp\u0026quot;, // replace with identity pool associated with user pool userPoolWebClientId: \u0026quot;us-east-1:bc89f200-299e-4269-8fd2-7caf9e8b0547\u0026quot;, // replace with app client id // optional, array of paths that won't be authenticated doNotAuthenticate: [\u0026quot;/\u0026quot;, \u0026quot;/services/\u0026quot;], }, }, ], } Trong ƒëo·∫°n code tr√™n, gi√° tr·ªã identityPoolId l√† optional, c√°c b·∫°n c√≥ th·ªÉ ƒë·ªÉ tr·ªëng.\nGi√° tr·ªã doNotAuthenticate l√† ch·ª©a c√°c path m√† m√¨nh s·∫Ω ko √°p d·ª•ng Authenticate (v√≠ d·ª• tr√™n bao g·ªìm trang Home Page, v√† Services Page)\n2.3.4. B∆∞·ªõc 4 Gi·ªù add n√∫t SignOut v√†o Page b·∫°n mu·ªën, m√¨nh s·∫Ω s·ª≠a file src\\pages\\team.js:\n.... import { SignIn, SignOut } from \u0026quot;@hoangmnsd/gatsby-theme-amplify-cognito\u0026quot;; const Team = props =\u0026gt; { const team = props.data.team.edges; const { intro } = props.data; const introImageClasses = `intro-image ${intro.frontmatter.intro_image_absolute \u0026amp;\u0026amp; 'intro-image-absolute'} ${intro.frontmatter.intro_image_hide_on_mobile \u0026amp;\u0026amp; 'intro-image-hide-mobile'}`; return ( (props.authState !== \u0026quot;signedIn\u0026quot;) ? \u0026lt;SignIn authState={props.authState} /\u0026gt; : \u0026lt;Layout bodyClass=\u0026quot;page-teams\u0026quot;\u0026gt; \u0026lt;SignOut /\u0026gt;\u0026lt;SEO title=\u0026quot;Team\u0026quot; /\u0026gt; .... 2.3.5. B∆∞·ªõc 5 Gi·ªù test th√¥i:\nnpm start Khi b·∫°n load trang Home Page v√† Service Page v·∫´n b√¨nh th∆∞·ªùng, nh∆∞ng khi click v√†o Team Page th√¨ s·∫Ω ra m√†n h√¨nh Login nh∆∞ sau:\nVi·ªác b·∫°n Login/SignUp/CreateAccount/ForgetPassword/ResetPassword ho√†n to√†n l√† b·∫°n l√†m vi·ªác v·ªõi AWS Cognito, b·∫°n ko c·∫ßn code th√™m g√¨ c·∫£. R·∫•t ti·ªán l·ª£i ph·∫£i ko?\nKhi b·∫°n ƒë√£ login th√†nh c√¥ng, m√†n h√¨nh s·∫Ω redirect v·ªÅ trang b·∫°n ƒëang mu·ªën truy c·∫≠p:\n·ªû ƒë√¢y m√¨nh ch∆∞a t√¨m ra c√°ch ƒë·ªÉ s·ª≠a n√∫t SignOut sang 1 ch·ªó kh√°c nh∆∞ng m√¨nh nghƒ© n√≥ s·∫Ω ko ph·∫£i l√† v·∫•n ƒë·ªÅ\nV·∫≠y l√† xong r·ªìi, kh√° ƒë∆°n gi·∫£n ph·∫£i ko, c√°c b·∫°n ch·ªâ c·∫ßn t·∫≠p trung v√†o ph·∫ßn content th√¥i, c√≤n l·∫°i th√¨ Gatsby v√† Cognito ƒë√£ lo h·∫øt r·ªìiüòé\n2.4. Build artifact ƒê·ªÉ build ra artifact r·ªìi ƒë∆∞a l√™n c√°c n∆°i host website c·ªßa b·∫°n (AWS S3 ho·∫∑c b·∫•t k·ª≥ n∆°i n√†o b·∫°n mu·ªën) th√¨ h√£y d√πng command sau:\nnpm run build artifact s·∫Ω ƒë∆∞·ª£c sinh ra trong folder public nh√©.\nCREDIT Thanks to awsome works:\nhttps://github.com/trsben/gatsby-theme-amplify-cognito\nhttps://www.gatsbyjs.com/plugins/@webriq/gatsby-theme-amplify-cognito/\n","href":"/bk/integrate-aws-cognito-to-gatsby-website/","title":"Integrate AWS Cognito to Gatsby Website"},{"content":"1. Gi·ªõi thi·ªáu G·∫ßn ƒë√¢y khi t√¨m hi·ªÉu v·ªÅ frontend th√¨ m√¨nh bi·∫øt r·∫±ng Hugo kh√¥ng th·ªÉ cung c·∫•p t√≠nh nƒÉng Authenticate cho website c·ªßa m√¨nh. V√¨ b·∫£n th√¢n Hugo l√† 1 Static Site Generator (SSG), n√≥ ko c√≥ tr√°ch nhi·ªám bi·∫øn website c·ªßa b·∫°n th√†nh dynamic.\nTi·∫øp t·ª•c t√¨m hi·ªÉu th√¨ m√¨nh bi·∫øt th√™m v·ªÅ GatsbyJS, c≈©ng l√† 1 Static Site Generator, nh∆∞ng n√≥ based tr√™n ReactJS v√† c√≥ nhi·ªÅu plugin ƒë·ªÉ bi·∫øn trang web th√†nh dynamic.\nV√≠ d·ª• nh∆∞ m√¨nh c√≥ th·ªÉ t√≠ch h·ª£p t√≠nh nƒÉng Login/Logout v√†o cho 1 website ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng Gatsby 1 c√°ch nhanh ch√≥ng.\nM√¨nh h∆∞·ªõng t·ªõi vi·ªác x√¢y d·ª±ng nh·ªØng website Serverless ho√†n to√†n, t·ªën √≠t c√¥ng s·ª©c t√¨m hi·ªÉu v·ªÅ Framework m·ªõi, n√™n s·∫Ω s·ª≠ d·ª•ng thirdparty authenticator nh∆∞ AWS Cognito.\nC≈©ng nh∆∞ Hugo, Gatsby c√≥ 1 c·ªông ƒë·ªìng r·ªông l·ªõn, h·ªç ƒë√≥ng g√≥p nhi·ªÅu nh·ªØng theme ƒë·∫πp v√† free cho m·ªçi ng∆∞·ªùi. B·∫°n c√≥ th·ªÉ tham kh·∫£o t·∫°i ƒë√¢y:\nhttps://www.gatsbyjs.com/starters/?\nhttps://gatsbytemplates.io/free/\nhttps://themejam.gatsbyjs.org/showcase\nhttps://jamtemplates.com/\nhttps://github.com/algokun/gatsby_starter_portfolio\n(224) https://jamstackthemes.dev/ssg/gatsby/ https://github.com/stackbit/jamstackthemes\n2. B·∫Øt ƒë·∫ßu 2.1. Ch·ªçn Theme B√†i n√†y m√¨nh s·∫Ω ch·ªçn 1 theme free t√™n l√† Serif Gatsby Starter ƒë·ªÉ demo vi·ªác integrate v·ªõi Cognito. Github c·ªßa n√≥ ·ªü ƒë√¢y: https://github.com/zerostaticthemes/gatsby-serif-theme\nT·∫•t nhi√™n ƒë·ªÉ s·ª≠ d·ª•ng Gatsby b·∫°n c·∫ßn c√†i ƒë·∫∑t 1 s·ªë th·ª©:\nnode ‚Äìv npm -v npm install ‚Äìglobal gatsby-cli Clone source c·ªßa Theme m√† ta ƒë√£ ch·ªçn v·ªÅ:\ngit clone https://github.com/zerostaticthemes/gatsby-serif-theme cd gatsby-serif-theme 2.2. Run local Run project ·ªü local:\nnpm install npm start Gi·ªù h√£y v√†o ƒë·ªãa ch·ªâ http://localhost:8000 ƒë·ªÉ xem k·∫øt qu·∫£:\nR·∫•t nhanh ch√≥ng ph·∫£i ko c√°c b·∫°n ƒë√£ c√≥ 1 website r·∫•t chuy√™n nghi·ªáp r·ªìi. Gi·ªù ch·ªâ c·∫ßn thay ƒë·ªïi ph·∫ßn content, h√¨nh ·∫£nh n·ªØa.\nContent ƒë∆∞·ª£c ƒë·∫∑t trong /src/content/, g·ªìm to√†n c√°c file Markdown (r·∫•t quen thu·ªôc). B·∫°n thay ƒë·ªïi n·ªôi dung c·ªßa m√¨nh v√†o ƒë√≥ l√† xong.\n2.3. Integrate with AWS Cognito Gi·ªù l√†m sao ƒë·ªÉ integrate ph·∫ßn Authentication v√†o?\nS·∫Ω c√≥ l√∫c c√°c b·∫°n mu·ªën website c·ªßa m√¨nh ƒë∆∞·ª£c b·∫£o v·ªá, trang Home Page th√¨ ai c≈©ng xem ƒë∆∞·ª£c, nh∆∞ng c√°c trang kh√°c th√¨ ph·∫£i y√™u c·∫ßu ƒëƒÉng nh·∫≠p (ch·∫≥ng h·∫°n nh∆∞ trang Team, About, \u0026hellip;etc).\nƒê√¢y l√† l√∫c c·∫ßn ch·ª©c nƒÉng Authentication, nh∆∞ng ch·∫≥ng l·∫Ω l·∫°i ph·∫£i h·ªçc code ReactJS ƒë·ªÉ th√™m t√≠nh nƒÉng Login, Logout, Sign Up hay sao, r·ªìi Database User n·ªØa s·∫Ω ƒë·ªÉ ·ªü ƒë√¢u? Nghe c√≥ v·∫ª d·ªÖ nh∆∞ng v√†o l√†m s·∫Ω lo·∫≥ng ngo·∫±ng ƒë·∫•y, nh·∫•t l√† v·ªõi nh·ªØng bussiness nh·ªè mu·ªën m·ªçi th·ª© c·∫ßn ho√†n thi·ªán nhanh.\nV√¨ v·∫≠y m√† Gatsby cung c·∫•p nhi·ªÅu plugin ƒë∆∞·ª£c c·ªông ƒë·ªìng ƒë√≥ng g√≥p, ch·ªâ c·∫ßn ch·ªânh s·ª≠a 1 √≠t, b·∫°n v·∫´n ƒëem ƒë∆∞·ª£c t√≠nh nƒÉng Login/Logout v√†o trang web c·ªßa m√¨nh.\n2.3.1. B∆∞·ªõc 1 ƒë·∫ßu ti√™n install package m·ªõi:\nnpm install --save @hoangmnsd/gatsby-theme-amplify-cognito 2.3.2. B∆∞·ªõc 2 Ti·∫øp theo l√† setup AWS Cognito User Pool v√† t·∫°o App Client, Nh·ªõ l√† ƒë·ª´ng ch·ªçn Generate client secret: 2.3.3. B∆∞·ªõc 3 S·ª≠a file gatsby-config.js ƒë·ªÉ add plugin v√†o:\ngatsby-config.js\nmodule.exports = { plugins: [ { resolve: `@hoangmnsd/gatsby-theme-amplify-cognito`, options: { region: \u0026quot;us-east-1\u0026quot;, // replace with region of user pool userPoolId: 'us-east-1_OZIxeIDqs\u0026quot;,', // replace with user pool id identityPoolId: \u0026quot;23ab3gt81t2sanvfg84mh7xnpp\u0026quot;, // replace with identity pool associated with user pool userPoolWebClientId: \u0026quot;us-east-1:bc89f200-299e-4269-8fd2-7caf9e8b0547\u0026quot;, // replace with app client id // optional, array of paths that won't be authenticated doNotAuthenticate: [\u0026quot;/\u0026quot;, \u0026quot;/services/\u0026quot;], }, }, ], } Trong ƒëo·∫°n code tr√™n, gi√° tr·ªã identityPoolId l√† optional, c√°c b·∫°n c√≥ th·ªÉ ƒë·ªÉ tr·ªëng.\nGi√° tr·ªã doNotAuthenticate l√† ch·ª©a c√°c path m√† m√¨nh s·∫Ω ko √°p d·ª•ng Authenticate (v√≠ d·ª• tr√™n bao g·ªìm trang Home Page, v√† Services Page)\n2.3.4. B∆∞·ªõc 4 Gi·ªù add n√∫t SignOut v√†o Page b·∫°n mu·ªën, m√¨nh s·∫Ω s·ª≠a file src\\pages\\team.js:\n.... import { SignIn, SignOut } from \u0026quot;@hoangmnsd/gatsby-theme-amplify-cognito\u0026quot;; const Team = props =\u0026gt; { const team = props.data.team.edges; const { intro } = props.data; const introImageClasses = `intro-image ${intro.frontmatter.intro_image_absolute \u0026amp;\u0026amp; 'intro-image-absolute'} ${intro.frontmatter.intro_image_hide_on_mobile \u0026amp;\u0026amp; 'intro-image-hide-mobile'}`; return ( (props.authState !== \u0026quot;signedIn\u0026quot;) ? \u0026lt;SignIn authState={props.authState} /\u0026gt; : \u0026lt;Layout bodyClass=\u0026quot;page-teams\u0026quot;\u0026gt; \u0026lt;SignOut /\u0026gt;\u0026lt;SEO title=\u0026quot;Team\u0026quot; /\u0026gt; .... 2.3.5. B∆∞·ªõc 5 Gi·ªù test th√¥i:\nnpm start Khi b·∫°n load trang Home Page v√† Service Page v·∫´n b√¨nh th∆∞·ªùng, nh∆∞ng khi click v√†o Team Page th√¨ s·∫Ω ra m√†n h√¨nh Login nh∆∞ sau:\nVi·ªác b·∫°n Login/SignUp/CreateAccount/ForgetPassword/ResetPassword ho√†n to√†n l√† b·∫°n l√†m vi·ªác v·ªõi AWS Cognito, b·∫°n ko c·∫ßn code th√™m g√¨ c·∫£. R·∫•t ti·ªán l·ª£i ph·∫£i ko?\nKhi b·∫°n ƒë√£ login th√†nh c√¥ng, m√†n h√¨nh s·∫Ω redirect v·ªÅ trang b·∫°n ƒëang mu·ªën truy c·∫≠p:\n·ªû ƒë√¢y m√¨nh ch∆∞a t√¨m ra c√°ch ƒë·ªÉ s·ª≠a n√∫t SignOut sang 1 ch·ªó kh√°c nh∆∞ng m√¨nh nghƒ© n√≥ s·∫Ω ko ph·∫£i l√† v·∫•n ƒë·ªÅ\nV·∫≠y l√† xong r·ªìi, kh√° ƒë∆°n gi·∫£n ph·∫£i ko, c√°c b·∫°n ch·ªâ c·∫ßn t·∫≠p trung v√†o ph·∫ßn content th√¥i, c√≤n l·∫°i th√¨ Gatsby v√† Cognito ƒë√£ lo h·∫øt r·ªìiüòé\n2.4. Build artifact ƒê·ªÉ build ra artifact r·ªìi ƒë∆∞a l√™n c√°c n∆°i host website c·ªßa b·∫°n (AWS S3 ho·∫∑c b·∫•t k·ª≥ n∆°i n√†o b·∫°n mu·ªën) th√¨ h√£y d√πng command sau:\nnpm run build artifact s·∫Ω ƒë∆∞·ª£c sinh ra trong folder public nh√©.\nCREDIT Thanks to awsome works:\nhttps://github.com/trsben/gatsby-theme-amplify-cognito\nhttps://www.gatsbyjs.com/plugins/@webriq/gatsby-theme-amplify-cognito/\n","href":"/posts/integrate-aws-cognito-to-gatsby-website/","title":"Integrate AWS Cognito to Gatsby Website"},{"content":"Oracle c√≥ ti·∫øng ƒë√£ l√¢u nh∆∞ng Oracle Cloud th√¨ m·ªõi ƒë∆∞·ª£c ra kh√¥ng l√¢u. ƒê·ªÉ c·∫°nh tranh v·ªõi c√°c ƒë·ªëi th·ªß th√¨ Oracle Cloud cung c·∫•p nhi·ªÅu t√≠nh nƒÉng cho free-tier kh√° h·∫•p d·∫´n.\nSo s√°nh v·ªõi AWS ch√≠nh tr√™n trang ch·ªß c·ªßa m√¨nh lu√¥n ü§£\nKh√° h·∫•p d·∫´n ƒë·∫•y ch·ª©. N·∫øu b·∫°n ch∆∞a bi·∫øt th√¨ Google GCP c≈©ng c√≥ ch·∫ø ƒë·ªô free-tier ·ªü ƒë√¢y: https://cloud.google.com/free\n1. Sign Up Qu√° tr√¨nh ƒëƒÉng k√Ω th√¨ b·∫°n c√≥ th·ªÉ v√†o ƒë√¢y https://signup.cloud.oracle.com/\n1 s·ªë ƒëi·ªÉm c·∫ßn l∆∞u √Ω:\n H·ªç s·∫Ω b·∫Øt b·∫°n ch·ªçn Home Region (c√°i n√†y s·∫Ω ko th·ªÉ thay ƒë·ªïi sau n√†y, n√™n h√£y ch·ªçn nh·ªØng region ph√π h·ª£p v·ªõi b·∫°n). Khi ƒëƒÉng k√Ω h·ªç s·∫Ω c√≥ 1 ch√∫ √Ω nh·ªè ƒë·∫°i kh√°i l√†:   Nhi·ªÅu ng∆∞·ªùi ch·ªçn c√°c region UK Lodon, Tokyo, Osaka, US qu√° n√™n s·ªë instance lo·∫°i A1 c·ªßa ch√∫ng t√¥i s·∫Ω limited ·ªü c√°c region n√†y. B·∫°n n√™n ch·ªçn region kh√°c ƒë·ªÉ c√≥ th·ªÉ ch·ªçn A1 instance tho·∫£i m√°i h∆°n \u0026hellip; ƒë·∫°i lo·∫°i v·∫≠y ü§£\n  S·∫Ω c·∫ßn add th·∫ª visa, v√† nhi·ªÅu kh·∫£ nƒÉng b·∫°n s·∫Ω g·∫∑p l·ªói n√†y:\n  M√¨nh r·∫•t l·∫•y l√†m l·∫° ko hi·ªÉu v√¨ sao l·∫°i g·∫∑p l·ªói n√†y trong khi c√πng th·∫ª ƒë√≥ m√¨nh v·∫´n thanh to√°n qu·ªëc t·∫ø ·∫ßm ·∫ßm.\nTrong tr∆∞·ªùng h·ª£p b·∫°n c≈©ng d√≠nh l·ªói n√†y, m√¨nh khuy√™n b·∫°n n√™n ·∫•n v√†o ƒë√¢y ƒë·ªÉ contact Support:\ntrong v√≤ng 24h c·ªßa ng√†y l√†m vi·ªác h·ªç s·∫Ω x·ª≠ l√Ω cho b·∫°n ngay, m√¨nh th·∫•y Support ph·∫£n h·ªìi kh√° nhanh ƒë·∫•y ch·ª© (ƒëi·ªÉm c·ªông):\nH·ªç b·∫£o m√¨nh h√£y sign-up l·∫°i 1 l·∫ßn n·ªØa, v√† m√¨nh l√†m l·∫°i th√¨ ko c√≤n l·ªói add th·∫ª kia n·ªØa.\nD√π sao th√¨ c∆° ch·∫ø add th·∫ª n√†y c·ªßa Oracle v·∫´n l√† 1 ƒëi·ªÉm tr·ª´ l√†m h·ªç gi·∫£m s·ªë ng∆∞·ªùi ti·∫øp c·∫≠n s·∫£n ph·∫©m c·ªßa h·ªç xu·ªëng (Kh√¥ng ph·∫£i ai c≈©ng s·∫Ω contact support gi·ªëng nh∆∞ m√¨nh)\n2. Enable MFA (London region 2021) Sau khi ƒë√£ sign-in th√†nh c√¥ng, nh∆∞ 1 th√≥i quen m√¨nh s·∫Ω mu·ªën enable MFA cho account c·ªßa m√¨nh ngay l·∫≠p t·ª©c. (C√°i g√¨ li√™n quan ƒë·∫øn ti·ªÅn th√¨ ƒë·ªÅu c·∫ßn ph·∫£i c·∫©n th·∫≠n h·∫øt üòÅ)\nNh∆∞ng giao di·ªán c·ªßa Oracle l√†m kh√≥ m√¨nh khi mu·ªën b·∫≠t MFA. Th·∫≠t s·ª± l√† kh√≥ v√¨ khi mu·ªën b·∫≠t MFA b·∫°n ph·∫£i nh·∫£y qu√° nhi·ªÅu m√†n h√¨nh lu√¥n. ƒêi·ªÉm tr·ª´ cho ƒë·ªô th√¢n thi·ªán UX so v·ªõi AWS.\nN√≥i qua 1 ch√∫t ƒë·ªÉ m·ªçi ng∆∞·ªùi d·ªÖ hi·ªÉu,\nSau khi b·∫°n log-in, h·ªá th·ªëng s·∫Ω t·∫°o cho b·∫°n 1 tenancy v√† add b·∫°n v√†o 1 Identity Provider t√™n l√† oracleidentitycloudservice.\nOracle c≈©ng t·∫°o cho b·∫°n th√™m 1 username n·ªØa b·∫Øt ƒë·∫ßu v·ªõi oracleidentitycloudservice/username.\nNh∆∞ h√¨nh sau l√† m√†n h√¨nh list c√°c user ban ƒë·∫ßu c·ªßa m√¨nh:\nTrong h√¨nh tr√™n th√¨ c·∫£ 2 user ƒë·ªÅu l√† c·ªßa m√¨nh.\nUser d√≤ng th·ª© 1 l√† user ƒë∆∞·ª£c federated v·ªõi oracleidentitycloudservice, n√≥ ƒë∆∞·ª£c t·∫°o 1 c√°ch t·ª± ƒë·ªông.\nUser d√≤ng th·ª© 2 l√† username m√† m√¨nh ƒëƒÉng k√Ω ban ƒë·∫ßu.\nCh√∫ng s·∫Ω d√πng chung 1 password m√† b·∫°n ƒë√£ ƒëƒÉng k√Ω ban ƒë·∫ßu nh√©. Nh∆∞ng trang login l·∫°i kh√°c nhau.\n  User d√≤ng th·ª© 2 login b·∫±ng ch·ªó b√¥i v√†ng n√†y (ƒë√¢y g·ªçi l√† OCI user - Oracle Cloud Infrastructure user):\n  User d√≤ng th·ª© 1 th√¨ login b·∫±ng ch·ªó n√†y (ƒë√¢y g·ªçi l√† IDCS user - Identity Cloud Service user):\n  2.1. Enable MFA for OCI User (London region 2021) B·∫°n c√≥ th·ªÉ d·ªÖ d√†ng Enable MFA cho OCI user, b·∫±ng c√°ch sau khi login v√†o OCI console, b·∫°n v√†o User Setting, s·∫Ω enable ƒë∆∞·ª£c:\nNh∆∞ng ƒëi·ªÅu lo·∫±ng ngo·∫±ng l√† enable MFA cho IDCS User c∆° üò•\n2.2. Enable MFA for IDCS User (London region 2021) Sau khi login v√†o b·∫±ng IDCS user, d√πng m√†n h√¨nh n√†y,\nƒê·ªÉ ch·∫Øc ch·∫Øn b·∫°n ƒëang ·ªü ƒë√∫ng m√†n h√¨nh th√¨ b·∫°n c·∫ßn x√°c nh·∫≠n l√† khi ·∫•n v√†o bi·ªÉu t∆∞·ª£ng top-right c·ªßa m√†n h√¨nh s·∫Ω th·∫•y nh∆∞ sau (c√≥ d√≤ng b√¥i v√†ng):\nr·ªìi ·∫•n v√†o Service User Console\nGo to Admin Console of Identity:\nGo to My profile on top-right screen:\nGo to Security tab and here you can enable MFA for your IDCS account:\nT·∫°i th·ªùi ƒëi·ªÉm n√†y, MFA v·∫´n ch∆∞a ƒë∆∞·ª£c enable ƒë√¢u nha, h√£y quay l·∫°i Admin console at top-right screen:\nGo to Security -\u0026gt; MFA at left side panel, check the box Mobile App Passcode then SAVE:\nGo to Security -\u0026gt; Sign-On Policies, edit the Default Policy: Edit Default Sign-on Rule:\n·ªû ƒë√¢y select nh·ªØng g√¨ t√πy √Ω b·∫°n:\nXong r·ªìi, gi·ªù h√£y Sign-out and Sign-in again. Now you can use MFA to login.\nR·∫•t lo·∫±ng ngo·∫±ng ph·∫£i ko? ƒêi·ªÉm tr·ª´ cho Oracle Cloud nh√© üòÇ (Ho·∫∑c c√≥ th·ªÉ do m√¨nh ch∆∞a hi·ªÉu ch∆∞a quen n√™n m·ªõi ch√™ n√≥ th√¥i)\n3. Enable MFA (Singapore region 2023) Sau 1 th·ªùi gian th√¨ m√¨nh t·∫°o l·∫°i account m·ªõi. L·∫ßn n√†y m√¨nh ch·ªçn Singapore.\nC√≥ v·∫ª nh∆∞ b√¢y gi·ªù Oracle tr√™n region n√†y ch·ªâ cho 1 ki·ªÉu login th√¥i.\nƒê√≥ l√† user ƒëi·ªÅn tenancy v√†o, r·ªìi ƒë·∫øn m√†n h√¨nh login v√†o tenancy ƒë√≥ (ko c√≤n ki·ªÉu 2 c√°ch login nh∆∞ region London n·ªØa)\nGi·ªù Enable MFA c√≥ v·∫ª d·ªÖ h∆°n, sau ƒë√¢y l√† c√°c step:\nC√°c b·∫°n add MFA Code b·∫±ng c√°ch th√¥ng th∆∞·ªùng, qu√©t QR code l√† s·∫Ω ƒë∆∞·ª£c nh∆∞ n√†y:\nSau b∆∞·ªõc n√†y m·ªõi ch·ªâ l√† c√°c b·∫°n ƒë√£ c√≥ m√£ code th√¥i nh√©.\nC·∫ßn l√†m c√°c b∆∞·ªõc sau n·ªØa:\nV√†o ƒë√¢y:\nCh·ªçn Configure MFA:\nCh·ªçn Sign-on Policies:\nCh·ªçn Default Policy:\nEdit Rule:\nCh·ªçn c√°c c√°i n√†y nh√©, nh·ªØng c√°i kh√°c c·ª© ƒë·ªÉ default:\nDone! Gi·ªù signout ra r·ªìi signin l·∫°i th√¥i üòé\nHy v·ªçng gi√∫p √≠ch ƒë∆∞·ª£c c√°c b·∫°n\n","href":"/bk/oracle-cloud-signup-and-enable-mfa/","title":"Oracle Cloud: Signup and Enable MFA for Identity Cloud Account"},{"content":"Oracle c√≥ ti·∫øng ƒë√£ l√¢u nh∆∞ng Oracle Cloud th√¨ m·ªõi ƒë∆∞·ª£c ra kh√¥ng l√¢u. ƒê·ªÉ c·∫°nh tranh v·ªõi c√°c ƒë·ªëi th·ªß th√¨ Oracle Cloud cung c·∫•p nhi·ªÅu t√≠nh nƒÉng cho free-tier kh√° h·∫•p d·∫´n.\nSo s√°nh v·ªõi AWS ch√≠nh tr√™n trang ch·ªß c·ªßa m√¨nh lu√¥n ü§£\nKh√° h·∫•p d·∫´n ƒë·∫•y ch·ª©. N·∫øu b·∫°n ch∆∞a bi·∫øt th√¨ Google GCP c≈©ng c√≥ ch·∫ø ƒë·ªô free-tier ·ªü ƒë√¢y: https://cloud.google.com/free\n1. Sign Up Qu√° tr√¨nh ƒëƒÉng k√Ω th√¨ b·∫°n c√≥ th·ªÉ v√†o ƒë√¢y https://signup.cloud.oracle.com/\n1 s·ªë ƒëi·ªÉm c·∫ßn l∆∞u √Ω:\n H·ªç s·∫Ω b·∫Øt b·∫°n ch·ªçn Home Region (c√°i n√†y s·∫Ω ko th·ªÉ thay ƒë·ªïi sau n√†y, n√™n h√£y ch·ªçn nh·ªØng region ph√π h·ª£p v·ªõi b·∫°n). Khi ƒëƒÉng k√Ω h·ªç s·∫Ω c√≥ 1 ch√∫ √Ω nh·ªè ƒë·∫°i kh√°i l√†:   Nhi·ªÅu ng∆∞·ªùi ch·ªçn c√°c region UK Lodon, Tokyo, Osaka, US qu√° n√™n s·ªë instance lo·∫°i A1 c·ªßa ch√∫ng t√¥i s·∫Ω limited ·ªü c√°c region n√†y. B·∫°n n√™n ch·ªçn region kh√°c ƒë·ªÉ c√≥ th·ªÉ ch·ªçn A1 instance tho·∫£i m√°i h∆°n \u0026hellip; ƒë·∫°i lo·∫°i v·∫≠y ü§£\n  S·∫Ω c·∫ßn add th·∫ª visa, v√† nhi·ªÅu kh·∫£ nƒÉng b·∫°n s·∫Ω g·∫∑p l·ªói n√†y:\n  M√¨nh r·∫•t l·∫•y l√†m l·∫° ko hi·ªÉu v√¨ sao l·∫°i g·∫∑p l·ªói n√†y trong khi c√πng th·∫ª ƒë√≥ m√¨nh v·∫´n thanh to√°n qu·ªëc t·∫ø ·∫ßm ·∫ßm.\nTrong tr∆∞·ªùng h·ª£p b·∫°n c≈©ng d√≠nh l·ªói n√†y, m√¨nh khuy√™n b·∫°n n√™n ·∫•n v√†o ƒë√¢y ƒë·ªÉ contact Support:\ntrong v√≤ng 24h c·ªßa ng√†y l√†m vi·ªác h·ªç s·∫Ω x·ª≠ l√Ω cho b·∫°n ngay, m√¨nh th·∫•y Support ph·∫£n h·ªìi kh√° nhanh ƒë·∫•y ch·ª© (ƒëi·ªÉm c·ªông):\nH·ªç b·∫£o m√¨nh h√£y sign-up l·∫°i 1 l·∫ßn n·ªØa, v√† m√¨nh l√†m l·∫°i th√¨ ko c√≤n l·ªói add th·∫ª kia n·ªØa.\nD√π sao th√¨ c∆° ch·∫ø add th·∫ª n√†y c·ªßa Oracle v·∫´n l√† 1 ƒëi·ªÉm tr·ª´ l√†m h·ªç gi·∫£m s·ªë ng∆∞·ªùi ti·∫øp c·∫≠n s·∫£n ph·∫©m c·ªßa h·ªç xu·ªëng (Kh√¥ng ph·∫£i ai c≈©ng s·∫Ω contact support gi·ªëng nh∆∞ m√¨nh)\n2. Enable MFA (London region 2021) Sau khi ƒë√£ sign-in th√†nh c√¥ng, nh∆∞ 1 th√≥i quen m√¨nh s·∫Ω mu·ªën enable MFA cho account c·ªßa m√¨nh ngay l·∫≠p t·ª©c. (C√°i g√¨ li√™n quan ƒë·∫øn ti·ªÅn th√¨ ƒë·ªÅu c·∫ßn ph·∫£i c·∫©n th·∫≠n h·∫øt üòÅ)\nNh∆∞ng giao di·ªán c·ªßa Oracle l√†m kh√≥ m√¨nh khi mu·ªën b·∫≠t MFA. Th·∫≠t s·ª± l√† kh√≥ v√¨ khi mu·ªën b·∫≠t MFA b·∫°n ph·∫£i nh·∫£y qu√° nhi·ªÅu m√†n h√¨nh lu√¥n. ƒêi·ªÉm tr·ª´ cho ƒë·ªô th√¢n thi·ªán UX so v·ªõi AWS.\nN√≥i qua 1 ch√∫t ƒë·ªÉ m·ªçi ng∆∞·ªùi d·ªÖ hi·ªÉu,\nSau khi b·∫°n log-in, h·ªá th·ªëng s·∫Ω t·∫°o cho b·∫°n 1 tenancy v√† add b·∫°n v√†o 1 Identity Provider t√™n l√† oracleidentitycloudservice.\nOracle c≈©ng t·∫°o cho b·∫°n th√™m 1 username n·ªØa b·∫Øt ƒë·∫ßu v·ªõi oracleidentitycloudservice/username.\nNh∆∞ h√¨nh sau l√† m√†n h√¨nh list c√°c user ban ƒë·∫ßu c·ªßa m√¨nh:\nTrong h√¨nh tr√™n th√¨ c·∫£ 2 user ƒë·ªÅu l√† c·ªßa m√¨nh.\nUser d√≤ng th·ª© 1 l√† user ƒë∆∞·ª£c federated v·ªõi oracleidentitycloudservice, n√≥ ƒë∆∞·ª£c t·∫°o 1 c√°ch t·ª± ƒë·ªông.\nUser d√≤ng th·ª© 2 l√† username m√† m√¨nh ƒëƒÉng k√Ω ban ƒë·∫ßu.\nCh√∫ng s·∫Ω d√πng chung 1 password m√† b·∫°n ƒë√£ ƒëƒÉng k√Ω ban ƒë·∫ßu nh√©. Nh∆∞ng trang login l·∫°i kh√°c nhau.\n  User d√≤ng th·ª© 2 login b·∫±ng ch·ªó b√¥i v√†ng n√†y (ƒë√¢y g·ªçi l√† OCI user - Oracle Cloud Infrastructure user):\n  User d√≤ng th·ª© 1 th√¨ login b·∫±ng ch·ªó n√†y (ƒë√¢y g·ªçi l√† IDCS user - Identity Cloud Service user):\n  2.1. Enable MFA for OCI User (London region 2021) B·∫°n c√≥ th·ªÉ d·ªÖ d√†ng Enable MFA cho OCI user, b·∫±ng c√°ch sau khi login v√†o OCI console, b·∫°n v√†o User Setting, s·∫Ω enable ƒë∆∞·ª£c:\nNh∆∞ng ƒëi·ªÅu lo·∫±ng ngo·∫±ng l√† enable MFA cho IDCS User c∆° üò•\n2.2. Enable MFA for IDCS User (London region 2021) Sau khi login v√†o b·∫±ng IDCS user, d√πng m√†n h√¨nh n√†y,\nƒê·ªÉ ch·∫Øc ch·∫Øn b·∫°n ƒëang ·ªü ƒë√∫ng m√†n h√¨nh th√¨ b·∫°n c·∫ßn x√°c nh·∫≠n l√† khi ·∫•n v√†o bi·ªÉu t∆∞·ª£ng top-right c·ªßa m√†n h√¨nh s·∫Ω th·∫•y nh∆∞ sau (c√≥ d√≤ng b√¥i v√†ng):\nr·ªìi ·∫•n v√†o Service User Console\nGo to Admin Console of Identity:\nGo to My profile on top-right screen:\nGo to Security tab and here you can enable MFA for your IDCS account:\nT·∫°i th·ªùi ƒëi·ªÉm n√†y, MFA v·∫´n ch∆∞a ƒë∆∞·ª£c enable ƒë√¢u nha, h√£y quay l·∫°i Admin console at top-right screen:\nGo to Security -\u0026gt; MFA at left side panel, check the box Mobile App Passcode then SAVE:\nGo to Security -\u0026gt; Sign-On Policies, edit the Default Policy: Edit Default Sign-on Rule:\n·ªû ƒë√¢y select nh·ªØng g√¨ t√πy √Ω b·∫°n:\nXong r·ªìi, gi·ªù h√£y Sign-out and Sign-in again. Now you can use MFA to login.\nR·∫•t lo·∫±ng ngo·∫±ng ph·∫£i ko? ƒêi·ªÉm tr·ª´ cho Oracle Cloud nh√© üòÇ (Ho·∫∑c c√≥ th·ªÉ do m√¨nh ch∆∞a hi·ªÉu ch∆∞a quen n√™n m·ªõi ch√™ n√≥ th√¥i)\n3. Enable MFA (Singapore region 2023) Sau 1 th·ªùi gian th√¨ m√¨nh t·∫°o l·∫°i account m·ªõi. L·∫ßn n√†y m√¨nh ch·ªçn Singapore.\nC√≥ v·∫ª nh∆∞ b√¢y gi·ªù Oracle tr√™n region n√†y ch·ªâ cho 1 ki·ªÉu login th√¥i.\nƒê√≥ l√† user ƒëi·ªÅn tenancy v√†o, r·ªìi ƒë·∫øn m√†n h√¨nh login v√†o tenancy ƒë√≥ (ko c√≤n ki·ªÉu 2 c√°ch login nh∆∞ region London n·ªØa)\nGi·ªù Enable MFA c√≥ v·∫ª d·ªÖ h∆°n, sau ƒë√¢y l√† c√°c step:\nC√°c b·∫°n add MFA Code b·∫±ng c√°ch th√¥ng th∆∞·ªùng, qu√©t QR code l√† s·∫Ω ƒë∆∞·ª£c nh∆∞ n√†y:\nSau b∆∞·ªõc n√†y m·ªõi ch·ªâ l√† c√°c b·∫°n ƒë√£ c√≥ m√£ code th√¥i nh√©.\nC·∫ßn l√†m c√°c b∆∞·ªõc sau n·ªØa:\nV√†o ƒë√¢y:\nCh·ªçn Configure MFA:\nCh·ªçn Sign-on Policies:\nCh·ªçn Default Policy:\nEdit Rule:\nCh·ªçn c√°c c√°i n√†y nh√©, nh·ªØng c√°i kh√°c c·ª© ƒë·ªÉ default:\nDone! Gi·ªù signout ra r·ªìi signin l·∫°i th√¥i üòé\nHy v·ªçng gi√∫p √≠ch ƒë∆∞·ª£c c√°c b·∫°n\n","href":"/posts/oracle-cloud-signup-and-enable-mfa/","title":"Oracle Cloud: Signup and Enable MFA for Identity Cloud Account"},{"content":"","href":"/tags/ajax/","title":"Ajax"},{"content":"","href":"/tags/jquery/","title":"Jquery"},{"content":"","href":"/tags/lambda/","title":"Lambda"},{"content":"Ph·∫ßn 1 m√¨nh s·∫Ω n√≥i v·ªÅ c√°ch ƒë·ªÉ t·∫°o 1 Serverless Telegram bot d√πng Lambda + API Gateway\nPh·∫ßn 2 m√¨nh s·∫Ω n√≥i v·ªÅ c√°ch ƒë·ªÉ trigger Lambda t·ª´ 1 Web Browser host tr√™n S3 bucket d√πng Ajax Jquery (c≈©ng c·∫ßn c√≥ API Gateway)\n1. Sererless Telegram Bot using Lambda + API Gateway 1.1. T·∫°o Bot b·∫±ng c√°ch chat v·ªõi @BotFather ƒêƒÉng nh·∫≠p v√†o Telegram, t√¨m @BotFather v√† chat v·ªõi n√≥ ƒë·ªÉ t·∫°o bot c·ªßa b·∫°n. B·∫°n s·∫Ω ƒë∆∞·ª£c n√≥ nh·∫Øn cho 1 TELEGRAM TOKEN, h√£y keep secure token ƒë√≥ nh√©.\nGi·∫£ s·ª≠ token m√† n√≥ g·ª≠i cho b·∫°n l√† 1123123303:AAF4Kz3kTxxxxxxxxxxxxxxxxxxxFcGRs\n1.2. Create Lambda function T·∫°o 1 h√†m Lambda trong AWS c·ªßa b·∫°n (ch√∫ √Ω ch·ªçn Runtime l√† Python 3.7 tr·ªü l√™n)\n\u0026#39;\u0026#39;\u0026#39; Please config Environment variable: TELEGRAM_TOKEN From Python 3.7 it supports `requests` package already, you should use it as the Runtime \u0026#39;\u0026#39;\u0026#39; import json import os import sys here = os.path.dirname(os.path.realpath(__file__)) sys.path.append(os.path.join(here, \u0026#34;./vendored\u0026#34;)) import requests TOKEN = os.environ.get(\u0026#39;TELEGRAM_TOKEN\u0026#39;) BASE_URL = \u0026#34;https://api.telegram.org/bot{}\u0026#34;.format(TOKEN) def lambda_handler(event, context): try: print(event) data = json.loads(event[\u0026#34;body\u0026#34;]) message = str(data[\u0026#34;message\u0026#34;][\u0026#34;text\u0026#34;]) chat_id = data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;id\u0026#34;] first_name = data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;first_name\u0026#34;] response = \u0026#34;Please /start, {}\u0026#34;.format(first_name) if \u0026#34;start\u0026#34; in message: response = \u0026#34;Hello {}\u0026#34;.format(first_name) data = {\u0026#34;text\u0026#34;: response.encode(\u0026#34;utf8\u0026#34;), \u0026#34;chat_id\u0026#34;: chat_id} url = BASE_URL + \u0026#34;/sendMessage\u0026#34; requests.post(url, data) except Exception as e: print(e) return {\u0026#34;statusCode\u0026#34;: 200} Ch√∫ √Ω trong ƒëo·∫°n code tr√™n refer ƒë·∫øn bi·∫øn m√¥i tr∆∞·ªùng TELEGRAM_TOKEN,\nh√£y config TOKEN m√† b·∫°n nh·∫≠n ƒë∆∞·ª£c ·ªü b∆∞·ªõc 1.1 v√†o bi·∫øn TELEGRAM_TOKEN n√†y.\nR·ªìi deploy Lambda n√†y l√† ok xong b∆∞·ªõc n√†y.\n1.3. T·∫°o API Gateway V√†o API Gateway t·∫°o 1 API gateway c·ªßa b·∫°n, h√£y ch·ªçn HTTP protocol v√¨ n√≥ s·∫Ω r·∫ª h∆°n so v·ªõi REST Protocol\nPh·∫ßn Route m√¨nh t·∫°o 1 route /sendMsg v·ªõi method POST:\nPh·∫ßn Integration ch·ªçn ch√≠nh function Lambda m√† b·∫°n ƒë√£ t·∫°o ·ªü tr√™n:\nPh·∫ßn CORS h√£y config nh∆∞ sau:\nCh√∫ng ta ƒë∆∞·ª£c invoke URL c·ªßa Route /sendMsg c√≥ d·∫°ng ki·ªÉu nh∆∞ n√†y:\nhttps://23s2cdzxtrd.execute-api.us-east-1.amazonaws.com/dev/sendMsg\n1.4. Connect API Gateway to Telegram Bot Gi·ªù c·∫ßn l√†m sao ƒë·ªÉ Telegram Bot b·∫°n v·ª´a t·∫°o hi·ªÉu ƒë∆∞·ª£c l√† n√≥ c·∫ßn g·ª≠i request ƒë·∫øn Backend ·ªü ƒë√¢u.\nH√£y d√πng POSTMAN g·ª≠i invoke URL (nh·∫≠n ƒë∆∞·ª£c ·ªü b∆∞·ªõc 1.3.) ƒë·∫øn https://api.telegram.org/bot\u0026lt;Your Telegram TOKEN\u0026gt;/setWebhook nh√©:\nk·∫øt qu·∫£ tr·∫£ v·ªÅ ok = true, Webhook was set l√† ƒë√£ th√†nh c√¥ng!\nCheck confirm:\n G·ª≠i GET request ƒë·∫øn: https://api.telegram.org/bot\u0026lt;Your Telegram TOKEN\u0026gt;/getWebhookInfo (l√†m theo t√†i li·ªáu trong link n√†y: https://core.telegram.org/bots/api#getwebhookinfo)  1.5. Test Gi·ªù h√£y th·ª≠ chat v·ªõi con Bot c·ªßa b·∫°n ƒë·ªÉ xem n√≥ run c√°c Login code trong Lambda nh∆∞ th·∫ø n√†o nh√©:\nCon bot n√†y t∆∞∆°ng t√°c ngay l·∫≠p t·ª©c sau m·ªói command c·ªßa b·∫°n, ƒë√¢y l√† ƒëi·ªÉm hay.\n2. Trigger Lambda from Web browser using Ajax JQuery Gi·∫£ s·ª≠ b·∫°n c√≥ 1 website m√† static ƒë∆∞·ª£c host tr√™n S3. B·∫°n mu·ªën c√≥ 1 page m√† user s·∫Ω nh·∫≠p v√†o th√¥ng tin v√† ·∫•n 1 n√∫t, t·ª´ ƒë√≥ trigger h√†m lambda c·ªßa b·∫°n.\nƒê√¢y ch√≠nh l√† 1 process basic v·ªÅ Serverless web app.\n2.1. Create Lambda function T·∫°o 1 h√†m Lambda trong AWS c·ªßa b·∫°n (ch√∫ √Ω ch·ªçn Runtime l√† Python 3.7 tr·ªü l√™n), v√≠ d·ª• ·ªü ƒë√¢y m√¨nh t·∫°o function t√™n l√† invoked-by-ajax-jquery\nimport json def lambda_handler(event, context): # TODO implement data = json.loads(event[\u0026#34;body\u0026#34;]) name = str(data[\u0026#34;name\u0026#34;]) print(\u0026#34;Input name: \u0026#34; + name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#34;Hello \u0026#34; + name + \u0026#34; from Lambda!\u0026#34;) } 2.2. Create Route in API Gateway ·ªû ƒë√¢y API Gateway c·ªßa m√¨nh v·∫´n d√πng HTTP protocol nh√© (ƒë·ªÉ cho r·∫ª) T·∫°o 1 Route t√™n l√† /invokedByAjaxJq s·ª≠ d·ª•ng Method POST:\nMethod POST ƒë√≥ s·∫Ω integrate v·ªõi h√†m Lambda m√† m√¨nh ƒë√£ t·∫°o ra ·ªü b∆∞·ªõc 2.1:\nCORS v·∫´n th·∫ø ko ƒë·ªïi g√¨, nh∆∞ng ƒë·ªÉ cho secure h∆°n th√¨ sau n√†y n√™n s·ª≠a l·∫°i Origin nha Ch√∫ng ta ƒë∆∞·ª£c invoke URL c·ªßa Route /sendMsg c√≥ d·∫°ng ki·ªÉu nh∆∞ n√†y:\nhttps://23s2cdzxtrd.execute-api.us-east-1.amazonaws.com/dev/invokedByAjaxJq\n2.3. Create S3 bucket for hosting H√£y t·∫°o 1 S3 bucket v·ªõi config nh∆∞ sau ·ªü tab Permission:\nPh·∫ßn Bucket policy c·∫ßn edit b·∫±ng ƒëo·∫°n JSON code nh∆∞ sau:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowPublicRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::htmljsajaxjqlambda/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowPublicList\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::htmljsajaxjqlambda\u0026#34; } ] } Ph·∫ßn Access Control List c·∫ßn allow quy·ªÅn LIST v√† READ cho All User nh∆∞ sau:\nNote: Ph·∫ßn CORS ko c·∫ßn ƒë·ª•ng ƒë·∫øn nh√©\nQuay l·∫°i tab Properties, enable Website static hosting l√™n:\nB·∫°n ƒë√£ c√≥ ƒë∆∞·ª£c 1 URL ƒë·∫øn website nh∆∞ sau:\nhttp://htmljsajaxjqlambda.s3-website-us-east-1.amazonaws.com\nNh∆∞ng v√¨ S3 bucket c·ªßa b·∫°n ch∆∞a c√≥ file n√™n c·∫ßn ph·∫£i t·∫°o v√† upload l√™n ƒë√£:\nfile ƒë·∫ßu ti√™n l√† index.html:\n\u0026lt;head\u0026gt; \u0026lt;script src=\u0026#34;https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;./js/my.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;#\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;form id=\u0026#34;contact-form\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;Name:\u0026lt;/h4\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; style=\u0026#34;height:35px;\u0026#34; id=\u0026#34;name-input\u0026#34; placeholder=\u0026#34;Enter name here\u0026#34; class=\u0026#34;form-control\u0026#34; style=\u0026#34;width:100%;\u0026#34; /\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;div class=\u0026#34;g-recaptcha\u0026#34; data-sitekey=\u0026#34;6Lc7cVMUAAAAAM1yxf64wrmO8gvi8A1oQ_ead1ys\u0026#34; class=\u0026#34;form-control\u0026#34; style=\u0026#34;width:100%;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; onClick=\u0026#34;submitToAPI(event)\u0026#34; class=\u0026#34;btn btn-lg\u0026#34; style=\u0026#34;margin-top:20px;\u0026#34;\u0026gt;Submit\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; file th·ª© 2 l√† my.js trong folder js, ch√∫ √Ω s·ª≠a ph·∫ßn t·ª≠ url b·∫±ng link invoke URL m√† b·∫°n ƒë√£ t·∫°o ra ·ªü b∆∞·ªõc 2.2 :\nfunction submitToAPI(e) { e.preventDefault(); var Namere = /[A-Za-z]{1}[A-Za-z]/; if (!Namere.test($(\u0026#34;#name-input\u0026#34;).val())) { alert (\u0026#34;Name can not less than 2 char\u0026#34;); return; } var name = $(\u0026#34;#name-input\u0026#34;).val(); var data = { name : name, }; $.ajax({ type: \u0026#34;POST\u0026#34;, url : \u0026#34;https://23s2cdzxtrd.execute-api.us-east-1.amazonaws.com/dev/invokedByAjaxJq\u0026#34;, dataType: \u0026#34;json\u0026#34;, crossDomain: \u0026#34;true\u0026#34;, contentType: \u0026#34;application/json; charset=utf-8\u0026#34;, data: JSON.stringify(data), success: function (response) { // clear form and show a success message  console.log(\u0026#34;Response: \u0026#34;+ response); alert(\u0026#34;Success\u0026#34;); document.getElementById(\u0026#34;contact-form\u0026#34;).reset(); location.reload(); }, error: function () { // show an error message  alert(\u0026#34;Failed\u0026#34;); }}); } Upload l√™n S3:\n2.4. Test Gi·ªù truy c·∫≠p URL S3 static web hosting, v√† ·∫•n F12 ƒë·ªÉ xem log nh√©:\nTh·ª≠ nh·∫≠p t√™n v√†o v√† ·∫•n Submit.\nB·∫°n s·∫Ω th·∫•y popup Success hi·ªán ra nh∆∞ n√†y:\nNh√¨n xu·ªëng d∆∞·ªõi ph·∫ßn Console b·∫°n s·∫Ω th·∫•y Message Response: Hello Park Ji Sung from Lambda!\nƒêi·ªÅu n√†y c√≥ nghƒ©a Lambda function c·ªßa b·∫°n ƒë√£ ch·∫°y th√†nh c√¥ng v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ cho Client. üéâ\nCh√∫c b·∫°n th√†nh c√¥ng! üòÅ\nREFERENCES https://hackernoon.com/serverless-telegram-bot-on-aws-lambda-851204d4236c\nhttps://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/\n","href":"/bk/encrypt-lambda-apigw-telegram-bot-serverless-webapp/","title":"Lambda + API Gateway, Telegram Bot and Serverless Webapp"},{"content":"Ph·∫ßn 1 m√¨nh s·∫Ω n√≥i v·ªÅ c√°ch ƒë·ªÉ t·∫°o 1 Serverless Telegram bot d√πng Lambda + API Gateway\nPh·∫ßn 2 m√¨nh s·∫Ω n√≥i v·ªÅ c√°ch ƒë·ªÉ trigger Lambda t·ª´ 1 Web Browser host tr√™n S3 bucket d√πng Ajax Jquery (c≈©ng c·∫ßn c√≥ API Gateway)\n1. Sererless Telegram Bot using Lambda + API Gateway 1.1. T·∫°o Bot b·∫±ng c√°ch chat v·ªõi @BotFather ƒêƒÉng nh·∫≠p v√†o Telegram, t√¨m @BotFather v√† chat v·ªõi n√≥ ƒë·ªÉ t·∫°o bot c·ªßa b·∫°n. B·∫°n s·∫Ω ƒë∆∞·ª£c n√≥ nh·∫Øn cho 1 TELEGRAM TOKEN, h√£y keep secure token ƒë√≥ nh√©.\nGi·∫£ s·ª≠ token m√† n√≥ g·ª≠i cho b·∫°n l√† 1123123303:AAF4Kz3kTxxxxxxxxxxxxxxxxxxxFcGRs\n1.2. Create Lambda function T·∫°o 1 h√†m Lambda trong AWS c·ªßa b·∫°n (ch√∫ √Ω ch·ªçn Runtime l√† Python 3.7 tr·ªü l√™n)\n\u0026#39;\u0026#39;\u0026#39; Please config Environment variable: TELEGRAM_TOKEN From Python 3.7 it supports `requests` package already, you should use it as the Runtime \u0026#39;\u0026#39;\u0026#39; import json import os import sys here = os.path.dirname(os.path.realpath(__file__)) sys.path.append(os.path.join(here, \u0026#34;./vendored\u0026#34;)) import requests TOKEN = os.environ.get(\u0026#39;TELEGRAM_TOKEN\u0026#39;) BASE_URL = \u0026#34;https://api.telegram.org/bot{}\u0026#34;.format(TOKEN) def lambda_handler(event, context): try: print(event) data = json.loads(event[\u0026#34;body\u0026#34;]) message = str(data[\u0026#34;message\u0026#34;][\u0026#34;text\u0026#34;]) chat_id = data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;id\u0026#34;] first_name = data[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;first_name\u0026#34;] response = \u0026#34;Please /start, {}\u0026#34;.format(first_name) if \u0026#34;start\u0026#34; in message: response = \u0026#34;Hello {}\u0026#34;.format(first_name) data = {\u0026#34;text\u0026#34;: response.encode(\u0026#34;utf8\u0026#34;), \u0026#34;chat_id\u0026#34;: chat_id} url = BASE_URL + \u0026#34;/sendMessage\u0026#34; requests.post(url, data) except Exception as e: print(e) return {\u0026#34;statusCode\u0026#34;: 200} Ch√∫ √Ω trong ƒëo·∫°n code tr√™n refer ƒë·∫øn bi·∫øn m√¥i tr∆∞·ªùng TELEGRAM_TOKEN,\nh√£y config TOKEN m√† b·∫°n nh·∫≠n ƒë∆∞·ª£c ·ªü b∆∞·ªõc 1.1 v√†o bi·∫øn TELEGRAM_TOKEN n√†y.\nR·ªìi deploy Lambda n√†y l√† ok xong b∆∞·ªõc n√†y.\n1.3. T·∫°o API Gateway V√†o API Gateway t·∫°o 1 API gateway c·ªßa b·∫°n, h√£y ch·ªçn HTTP protocol v√¨ n√≥ s·∫Ω r·∫ª h∆°n so v·ªõi REST Protocol\nPh·∫ßn Route m√¨nh t·∫°o 1 route /sendMsg v·ªõi method POST:\nPh·∫ßn Integration ch·ªçn ch√≠nh function Lambda m√† b·∫°n ƒë√£ t·∫°o ·ªü tr√™n:\nPh·∫ßn CORS h√£y config nh∆∞ sau:\nCh√∫ng ta ƒë∆∞·ª£c invoke URL c·ªßa Route /sendMsg c√≥ d·∫°ng ki·ªÉu nh∆∞ n√†y:\nhttps://23s2cdzxtrd.execute-api.us-east-1.amazonaws.com/dev/sendMsg\n1.4. Connect API Gateway to Telegram Bot Gi·ªù c·∫ßn l√†m sao ƒë·ªÉ Telegram Bot b·∫°n v·ª´a t·∫°o hi·ªÉu ƒë∆∞·ª£c l√† n√≥ c·∫ßn g·ª≠i request ƒë·∫øn Backend ·ªü ƒë√¢u.\nH√£y d√πng POSTMAN g·ª≠i invoke URL (nh·∫≠n ƒë∆∞·ª£c ·ªü b∆∞·ªõc 1.3.) ƒë·∫øn https://api.telegram.org/bot\u0026lt;Your Telegram TOKEN\u0026gt;/setWebhook nh√©:\nk·∫øt qu·∫£ tr·∫£ v·ªÅ ok = true, Webhook was set l√† ƒë√£ th√†nh c√¥ng!\nCheck confirm:\n G·ª≠i GET request ƒë·∫øn: https://api.telegram.org/bot\u0026lt;Your Telegram TOKEN\u0026gt;/getWebhookInfo (l√†m theo t√†i li·ªáu trong link n√†y: https://core.telegram.org/bots/api#getwebhookinfo)  1.5. Test Gi·ªù h√£y th·ª≠ chat v·ªõi con Bot c·ªßa b·∫°n ƒë·ªÉ xem n√≥ run c√°c Login code trong Lambda nh∆∞ th·∫ø n√†o nh√©:\nCon bot n√†y t∆∞∆°ng t√°c ngay l·∫≠p t·ª©c sau m·ªói command c·ªßa b·∫°n, ƒë√¢y l√† ƒëi·ªÉm hay.\n2. Trigger Lambda from Web browser using Ajax JQuery Gi·∫£ s·ª≠ b·∫°n c√≥ 1 website m√† static ƒë∆∞·ª£c host tr√™n S3. B·∫°n mu·ªën c√≥ 1 page m√† user s·∫Ω nh·∫≠p v√†o th√¥ng tin v√† ·∫•n 1 n√∫t, t·ª´ ƒë√≥ trigger h√†m lambda c·ªßa b·∫°n.\nƒê√¢y ch√≠nh l√† 1 process basic v·ªÅ Serverless web app.\n2.1. Create Lambda function T·∫°o 1 h√†m Lambda trong AWS c·ªßa b·∫°n (ch√∫ √Ω ch·ªçn Runtime l√† Python 3.7 tr·ªü l√™n), v√≠ d·ª• ·ªü ƒë√¢y m√¨nh t·∫°o function t√™n l√† invoked-by-ajax-jquery\nimport json def lambda_handler(event, context): # TODO implement data = json.loads(event[\u0026#34;body\u0026#34;]) name = str(data[\u0026#34;name\u0026#34;]) print(\u0026#34;Input name: \u0026#34; + name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#34;Hello \u0026#34; + name + \u0026#34; from Lambda!\u0026#34;) } 2.2. Create Route in API Gateway ·ªû ƒë√¢y API Gateway c·ªßa m√¨nh v·∫´n d√πng HTTP protocol nh√© (ƒë·ªÉ cho r·∫ª) T·∫°o 1 Route t√™n l√† /invokedByAjaxJq s·ª≠ d·ª•ng Method POST:\nMethod POST ƒë√≥ s·∫Ω integrate v·ªõi h√†m Lambda m√† m√¨nh ƒë√£ t·∫°o ra ·ªü b∆∞·ªõc 2.1:\nCORS v·∫´n th·∫ø ko ƒë·ªïi g√¨, nh∆∞ng ƒë·ªÉ cho secure h∆°n th√¨ sau n√†y n√™n s·ª≠a l·∫°i Origin nha Ch√∫ng ta ƒë∆∞·ª£c invoke URL c·ªßa Route /sendMsg c√≥ d·∫°ng ki·ªÉu nh∆∞ n√†y:\nhttps://23s2cdzxtrd.execute-api.us-east-1.amazonaws.com/dev/invokedByAjaxJq\n2.3. Create S3 bucket for hosting H√£y t·∫°o 1 S3 bucket v·ªõi config nh∆∞ sau ·ªü tab Permission:\nPh·∫ßn Bucket policy c·∫ßn edit b·∫±ng ƒëo·∫°n JSON code nh∆∞ sau:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowPublicRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::htmljsajaxjqlambda/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowPublicList\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::htmljsajaxjqlambda\u0026#34; } ] } Ph·∫ßn Access Control List c·∫ßn allow quy·ªÅn LIST v√† READ cho All User nh∆∞ sau:\nNote: Ph·∫ßn CORS ko c·∫ßn ƒë·ª•ng ƒë·∫øn nh√©\nQuay l·∫°i tab Properties, enable Website static hosting l√™n:\nB·∫°n ƒë√£ c√≥ ƒë∆∞·ª£c 1 URL ƒë·∫øn website nh∆∞ sau:\nhttp://htmljsajaxjqlambda.s3-website-us-east-1.amazonaws.com\nNh∆∞ng v√¨ S3 bucket c·ªßa b·∫°n ch∆∞a c√≥ file n√™n c·∫ßn ph·∫£i t·∫°o v√† upload l√™n ƒë√£:\nfile ƒë·∫ßu ti√™n l√† index.html:\n\u0026lt;head\u0026gt; \u0026lt;script src=\u0026#34;https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;./js/my.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;#\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;form id=\u0026#34;contact-form\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;Name:\u0026lt;/h4\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; style=\u0026#34;height:35px;\u0026#34; id=\u0026#34;name-input\u0026#34; placeholder=\u0026#34;Enter name here\u0026#34; class=\u0026#34;form-control\u0026#34; style=\u0026#34;width:100%;\u0026#34; /\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;div class=\u0026#34;g-recaptcha\u0026#34; data-sitekey=\u0026#34;6Lc7cVMUAAAAAM1yxf64wrmO8gvi8A1oQ_ead1ys\u0026#34; class=\u0026#34;form-control\u0026#34; style=\u0026#34;width:100%;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; onClick=\u0026#34;submitToAPI(event)\u0026#34; class=\u0026#34;btn btn-lg\u0026#34; style=\u0026#34;margin-top:20px;\u0026#34;\u0026gt;Submit\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; file th·ª© 2 l√† my.js trong folder js, ch√∫ √Ω s·ª≠a ph·∫ßn t·ª≠ url b·∫±ng link invoke URL m√† b·∫°n ƒë√£ t·∫°o ra ·ªü b∆∞·ªõc 2.2 :\nfunction submitToAPI(e) { e.preventDefault(); var Namere = /[A-Za-z]{1}[A-Za-z]/; if (!Namere.test($(\u0026#34;#name-input\u0026#34;).val())) { alert (\u0026#34;Name can not less than 2 char\u0026#34;); return; } var name = $(\u0026#34;#name-input\u0026#34;).val(); var data = { name : name, }; $.ajax({ type: \u0026#34;POST\u0026#34;, url : \u0026#34;https://23s2cdzxtrd.execute-api.us-east-1.amazonaws.com/dev/invokedByAjaxJq\u0026#34;, dataType: \u0026#34;json\u0026#34;, crossDomain: \u0026#34;true\u0026#34;, contentType: \u0026#34;application/json; charset=utf-8\u0026#34;, data: JSON.stringify(data), success: function (response) { // clear form and show a success message  console.log(\u0026#34;Response: \u0026#34;+ response); alert(\u0026#34;Success\u0026#34;); document.getElementById(\u0026#34;contact-form\u0026#34;).reset(); location.reload(); }, error: function () { // show an error message  alert(\u0026#34;Failed\u0026#34;); }}); } Upload l√™n S3:\n2.4. Test Gi·ªù truy c·∫≠p URL S3 static web hosting, v√† ·∫•n F12 ƒë·ªÉ xem log nh√©:\nTh·ª≠ nh·∫≠p t√™n v√†o v√† ·∫•n Submit.\nB·∫°n s·∫Ω th·∫•y popup Success hi·ªán ra nh∆∞ n√†y:\nNh√¨n xu·ªëng d∆∞·ªõi ph·∫ßn Console b·∫°n s·∫Ω th·∫•y Message Response: Hello Park Ji Sung from Lambda!\nƒêi·ªÅu n√†y c√≥ nghƒ©a Lambda function c·ªßa b·∫°n ƒë√£ ch·∫°y th√†nh c√¥ng v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ cho Client. üéâ\nCh√∫c b·∫°n th√†nh c√¥ng! üòÅ\nREFERENCES https://hackernoon.com/serverless-telegram-bot-on-aws-lambda-851204d4236c\nhttps://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/\n","href":"/posts/encrypt-lambda-apigw-telegram-bot-serverless-webapp/","title":"Lambda + API Gateway, Telegram Bot and Serverless Webapp"},{"content":"N√™n s·ª≠ d·ª•ng Automation service c·ªßa Azure\nGi·∫£ s·ª≠ b·∫°n ƒë√£ t·∫°o 1 Resource Group t√™n l√†: test-auto-startstop-vm\nTrong Resource Group ƒë√≥ b·∫°n c√≥ 2 VM nh∆∞ sau (·∫£nh): Ch·ªçn Resource Group m√† b·∫°n mu·ªën apply stop/start solution v√†o, ch·ªçn Add (·∫£nh):\nSearch Automation (·∫£nh):\nCh·ªçn Create (·∫£nh):\nT·∫°o Automation Account. ƒêi·ªÅn th√¥ng tin ph√π h·ª£p v·ªõi b·∫°n v√† ti·∫øp t·ª•c ch·ªçn Create (·∫£nh):\nQuay l·∫°i portal c·ªßa Resource Group b·∫°n s·∫Ω th·∫•y c√≥ th√™m c√°c resource ƒë∆∞·ª£c t·∫°o ra. ·∫§n v√†o automation-account (·∫£nh):\n1 Automation Account v√† 3 Runbook\n·ªû m√†n h√¨nh portal c·ªßa Automation Account, k√©o xu·ªëng ch·ªçn Runbook Gallery, ch·ªçn ti·∫øp Stop-Start-AzureVM (Scheduled VM Shutdown/Startup) - Created by: Pradebban Raja. Nh·ªØng c√°i kh√°c c≈©ng l√† c·ªßa c·ªông ƒë·ªìng ƒë√≥ng g√≥p b·∫°n c√≥ th·ªÉ th·ª≠. Tuy nhi√™n m√¨nh ch·ªçn c√°i n√†y v√¨ n√≥ cung c·∫•p ƒë·ªìng th·ªùi c·∫£ Stop/Start VM, m√† c≈©ng ƒë∆∞·ª£c rate kh√° cao.\nCh·ªçn Import ƒë·ªÉ s·ª≠ d·ª•ng Runbook (·∫£nh):\nQuay l·∫°i portal c·ªßa Resource Group, b·∫°n s·∫Ω th·∫•y Runbook v·ª´a import ƒë√£ ƒë∆∞·ª£c hi·ªán ra, ·∫•n v√†o n√≥ (·∫£nh):\n·ªû m√†n h√¨nh portal c·ªßa Runbook, h√£y ch·ªçn Edit:\n·ªû ƒë√¢y h√£y paste n·ªôi dung sau v√†o terminal:\nWorkflow Stop-Start-AzureVM { Param ( [Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] [String] $AzureSubscriptionId, [Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] [String] $AzureVMList=\u0026#34;All\u0026#34;, [Parameter(Mandatory=$true)][ValidateSet(\u0026#34;Start\u0026#34;,\u0026#34;Stop\u0026#34;)] [String] $Action ) $connectionName = \u0026#34;AzureRunAsConnection\u0026#34; try { # Get the connection \u0026#34;AzureRunAsConnection \u0026#34; $servicePrincipalConnection=Get-AutomationConnection -Name $connectionName \u0026#34;Logging in to Azure...\u0026#34; Add-AzureRmAccount ` -ServicePrincipal ` -TenantId $servicePrincipalConnection.TenantId ` -ApplicationId $servicePrincipalConnection.ApplicationId ` -CertificateThumbprint $servicePrincipalConnection.CertificateThumbprint } catch { if (!$servicePrincipalConnection) { $ErrorMessage = \u0026#34;Connection $connectionNamenot found.\u0026#34; throw $ErrorMessage } else{ Write-Error -Message $_.Exception throw $_.Exception } } if($AzureVMList -ne \u0026#34;All\u0026#34;) { $AzureVMs = $AzureVMList.Split(\u0026#34;,\u0026#34;) [System.Collections.ArrayList]$AzureVMsToHandle = $AzureVMs } else { $AzureVMs = (Get-AzureRmVM).Name [System.Collections.ArrayList]$AzureVMsToHandle = $AzureVMs } foreach($AzureVM in $AzureVMsToHandle) { if(!(Get-AzureRmVM | ? {$_.Name -eq $AzureVM})) { throw \u0026#34; AzureVM : [$AzureVM] - Does not exist! - Check your inputs \u0026#34; } } if($Action -eq \u0026#34;Stop\u0026#34;) { Write-Output \u0026#34;Stopping VMs\u0026#34;; foreach -parallel ($AzureVM in $AzureVMsToHandle) { Get-AzureRmVM | ? {$_.Name -eq $AzureVM} | Stop-AzureRmVM -Force } } else { Write-Output \u0026#34;Starting VMs\u0026#34;; foreach -parallel ($AzureVM in $AzureVMsToHandle) { Get-AzureRmVM | ? {$_.Name -eq $AzureVM} | Start-AzureRmVM } } } Sau ƒë√≥ ch·ªçn Save v√† Publish nh√©\nƒê·∫øn ƒë√¢y c√≥ th·ªÉ c√°c b·∫°n s·∫Ω t·ª± h·ªèi t·∫°i sao ph·∫£i update code, ko d√πng c√°i c√≥ s·∫µn c·ªßa Runbook √†?\nGi·∫£i th√≠ch: ƒê√∫ng v·∫≠y, ch·∫°y Code c√≥ s·∫µn c√°c b·∫°n r·∫•t nhi·ªÅu kh·∫£ nƒÉng s·∫Ω b·ªã d√≠nh l·ªói sau ƒë√¢y:\nkh·∫£ nƒÉng l√† do ƒëo·∫°n code Login v√†o Azure c·ªßa n√≥ c√≥ v·∫•n ƒë·ªÅ, th·∫ø n√™n c·∫ßn s·ª≠ d·ª•ng ƒëo·∫°n code m√† m√¨nh thay th·∫ø ·ªü b√™n tr√™n\nQuay l·∫°i portal c·ªßa Runbook, ch·ªçn Link to schedule:\nClick v√†o ph·∫ßn setting Schedule tr∆∞·ªõc r·ªìi Create new schedule: T·∫°o 1 schedule s·∫Ω run v√†o l√∫c n√†o ƒë√≥ t√πy b·∫°n, ·ªü ƒë√¢y m√¨nh l·∫•y v√≠ d·ª• l√† 11:10PM:\nTi·∫øp t·ª•c ch·ªçn ph·∫ßn config Parameter nh∆∞ h√¨nh n√†y:\n·ªû ƒë√¢y b·∫°n c·∫ßn ƒëi·ªÅn th√¥ng tin:\nAZURESUBSCRIPTIONID: subscription id,\nAZUREVMLIST: list c√°c VM s·∫Ω stop (ngƒÉn c√°ch b·∫±ng d·∫•u ph·∫©y, ko space),\nACTION: ch·ªâ ƒëc ghi Stop ho·∫∑c Start. ·ªû v√≠ d·ª• n√†y m√¨nh s·∫Ω ƒëi·ªÅn Stop ti·∫øp t·ª•c ch·ªçn OK, ·ªü m√†n h√¨nh portal Runbook b·∫°n ch·ªçn tab Schedule ·ªü panel tay tr√°i, s·∫Ω th·∫•y Schedule ƒë√£ ƒë∆∞·ª£c l√™n l·ªãch, s·∫Ω run v√†o l√∫c 11h10PM:\nCh·ªù ƒë·∫øn 11h10PM, c√°c b·∫°n c√≥ th·ªÉ chuy·ªÉn sang tab Job ƒë·ªÉ xem tr·∫°ng th√°i Job c·ªßa b·∫°n ch·∫°y th√†nh c√¥ng hay th·∫•t b·∫°i:\nN·∫øu th√†nh c√¥ng s·∫Ω l√† Completed, n·∫øu l·ªói th√¨ s·∫Ω l√† Suspended Xem log th·∫•y nh∆∞ n√†y l√† OK, ko c√≥ ERROR, WARNING g√¨ c·∫£:\nQuay l·∫°i portal c·ªßa Virtual Machine s·∫Ω th·∫•y c√°c VMs ƒë√£ ƒë∆∞·ª£c stop. Gi·ªù c√°c b·∫°n c√≥ th·ªÉ t·ª± t·∫°o Schedule Auto Start VM n·ªØa nh∆∞ sau:\nƒêi ng·ªß v√† t·∫≠n h∆∞·ªüng th√†nh qu·∫£ th√¥i ^^\nCh√∫ √Ω   ƒê·ªÉ d√πng l√¢u d√†i v√† th∆∞·ªùng xuy√™n th√¨ PowerShell script r·∫•t kh√¥ng ·ªïn ƒë·ªãnh, nhi·ªÅu l√∫c b·ªã l·ªói ko th·ªÉ start/stop VM ƒë∆∞·ª£c, v√¨ th·∫ø m√¨nh khuy√™n c√°c b·∫°n n√™n t·ª± vi·∫øt c√°c h√†m python ƒë·ªÉ thay th·∫ø cho PowerShell\n  Ch√∫ √Ω khi t·∫°o Automation Account, n√≥ c√≥ auto t·∫°o certificate ko. N·∫øu c√≥ th√¨ cert ·∫•y s·∫Ω t·ª± ƒë·ªông h·∫øt h·∫°n sau 1 nƒÉm. C·∫ßn c√≥ gi·∫£i ph√°p: ho·∫∑c l√† try-catch ch·ªó authenticate code c·ªßa Runbook n·∫øu l·ªói th√¨ raise l√™n cho m√¨nh v√†o renew cert. Ho·∫∑c l√† s·ª≠a code th√†nh authen v√†o Azure b·∫±ng identity. Ho·∫∑c l√† 1 nƒÉm sau nh·ªõ v√†o gia h·∫°n.\n  Script python sample:\n\u0026#34;\u0026#34;\u0026#34; Automation runbook for Stop/Start VMs on Azure Parameters - Required: [PARAMETER 1] : stop | start [PARAMETER 2] : VM name - separated by comma (,) [PARAMETER 3] : Resource group name that contains VM This runbook will stop/start all VMs specified in [PARAMETER 2] \u0026#34;\u0026#34;\u0026#34; import azure.mgmt.resource import automationassets from msrestazure.azure_cloud import AZURE_PUBLIC_CLOUD import sys from azure.mgmt.compute import ComputeManagementClient from msrestazure.azure_exceptions import CloudError import requests, json def get_automation_runas_credential(runas_connection, resource_url, authority_url): \u0026#34;\u0026#34;\u0026#34; Returns credentials to authenticate against Azure resoruce manager \u0026#34;\u0026#34;\u0026#34; from OpenSSL import crypto from msrestazure import azure_active_directory import adal # Get the Azure Automation RunAs service principal certificate cert = automationassets.get_automation_certificate(\u0026#34;AzureRunAsCertificate\u0026#34;) pks12_cert = crypto.load_pkcs12(cert) pem_pkey = crypto.dump_privatekey(crypto.FILETYPE_PEM, pks12_cert.get_privatekey()) # Get run as connection information for the Azure Automation service principal application_id = runas_connection[\u0026#34;ApplicationId\u0026#34;] thumbprint = runas_connection[\u0026#34;CertificateThumbprint\u0026#34;] tenant_id = runas_connection[\u0026#34;TenantId\u0026#34;] # Authenticate with service principal certificate authority_full_url = (authority_url + \u0026#39;/\u0026#39; + tenant_id) context = adal.AuthenticationContext(authority_full_url) return azure_active_directory.AdalAuthentication( lambda: context.acquire_token_with_client_certificate( resource_url, application_id, pem_pkey, thumbprint) ) def start_vm(vm, resource_group, compute_client): try: print(\u0026#34;Try to start vm [%s]\u0026#34; % vm) async_vm_start = compute_client.virtual_machines.start(resource_group, vm) async_vm_start.wait() print(\u0026#34;vm [%s] was successfully started\u0026#34; % vm) return True except CloudError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): print(\u0026#34;Start command throttled, automatically retrying...\u0026#34;) start_vm(vm, resource_group, compute_client) else: print(\u0026#34;Start command failed!\\n%s\u0026#34; % str(err)) return False except: raise def stop_vm(vm, resource_group, compute_client): try: print(\u0026#34;Try to stop vm [%s]\u0026#34; % vm) async_vm_stop = compute_client.virtual_machines.power_off(resource_group, vm) async_vm_stop.wait() print(\u0026#34;vm [%s] was successfully stopped\u0026#34; % vm) return True except CloudError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): print(\u0026#34;Stop command throttled, automatically retrying...\u0026#34;) stop_vm(vm, resource_group, compute_client) else: print(\u0026#34;Stop command failed!\\n%s\u0026#34; % str(err)) return False except: raise def alert_on_slack(vm, resource_group): try: hook_url = \u0026#34;https://hooks.slack.com/services/XXXX/YYYY/ZZZZZ\u0026#34; payload={\u0026#34;text\u0026#34;: \u0026#34;Something wrong when stop/start vm [%s] [%s].\\nPlease go to Azure portal to take action.\u0026#34; % (vm, resource_group)} response = requests.post( hook_url, data = json.dumps(payload), headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} ) print(\u0026#34;Alert!\u0026#34;) if response.status_code != 200: raise ValueError( \u0026#39;Request to slack returned an error %s, the response is:\\n%s\u0026#39; % (response.status_code, response.text) ) return response except: raise def main(): if len(sys.argv) \u0026gt;= 3: action = str(sys.argv[1]) vm_list = str(sys.argv[2]) resource_group = str(sys.argv[3]) # need some regex to validate argument variables else: error_msg = \u0026#34;Positional parameters action, vm_list and resource_group are required...\u0026#34; print(error_msg) raise Exception(error_msg) vm_list = vm_list.split(\u0026#34;,\u0026#34;) # authenticate to Azure Cloud runas_connection = automationassets.get_automation_connection(\u0026#34;AzureRunAsConnection\u0026#34;) resource_url = AZURE_PUBLIC_CLOUD.endpoints.active_directory_resource_id authority_url = AZURE_PUBLIC_CLOUD.endpoints.active_directory resourceManager_url = AZURE_PUBLIC_CLOUD.endpoints.resource_manager azure_credential = get_automation_runas_credential(runas_connection, resource_url, authority_url) compute_client = ComputeManagementClient( azure_credential, str(runas_connection[\u0026#34;SubscriptionId\u0026#34;]) ) # stop/start VM base on action if action.lower() == \u0026#34;start\u0026#34;: for vm in vm_list: response = start_vm(vm, resource_group, compute_client) if response != True: alert_on_slack(vm, resource_group) elif action.lower() == \u0026#34;stop\u0026#34;: for vm in vm_list: response = stop_vm(vm, resource_group, compute_client) if response != True: alert_on_slack(vm, resource_group) if __name__ == \u0026#39;__main__\u0026#39;: main() ","href":"/bk/encrypt-azure-setup-auto-stop-start-virtual-machines/","title":"Azure: Setup Auto Stop/Start on schedule for Virtual Machines"},{"content":"N√™n s·ª≠ d·ª•ng Automation service c·ªßa Azure\nGi·∫£ s·ª≠ b·∫°n ƒë√£ t·∫°o 1 Resource Group t√™n l√†: test-auto-startstop-vm\nTrong Resource Group ƒë√≥ b·∫°n c√≥ 2 VM nh∆∞ sau (·∫£nh): Ch·ªçn Resource Group m√† b·∫°n mu·ªën apply stop/start solution v√†o, ch·ªçn Add (·∫£nh):\nSearch Automation (·∫£nh):\nCh·ªçn Create (·∫£nh):\nT·∫°o Automation Account. ƒêi·ªÅn th√¥ng tin ph√π h·ª£p v·ªõi b·∫°n v√† ti·∫øp t·ª•c ch·ªçn Create (·∫£nh):\nQuay l·∫°i portal c·ªßa Resource Group b·∫°n s·∫Ω th·∫•y c√≥ th√™m c√°c resource ƒë∆∞·ª£c t·∫°o ra. ·∫§n v√†o automation-account (·∫£nh):\n1 Automation Account v√† 3 Runbook\n·ªû m√†n h√¨nh portal c·ªßa Automation Account, k√©o xu·ªëng ch·ªçn Runbook Gallery, ch·ªçn ti·∫øp Stop-Start-AzureVM (Scheduled VM Shutdown/Startup) - Created by: Pradebban Raja. Nh·ªØng c√°i kh√°c c≈©ng l√† c·ªßa c·ªông ƒë·ªìng ƒë√≥ng g√≥p b·∫°n c√≥ th·ªÉ th·ª≠. Tuy nhi√™n m√¨nh ch·ªçn c√°i n√†y v√¨ n√≥ cung c·∫•p ƒë·ªìng th·ªùi c·∫£ Stop/Start VM, m√† c≈©ng ƒë∆∞·ª£c rate kh√° cao.\nCh·ªçn Import ƒë·ªÉ s·ª≠ d·ª•ng Runbook (·∫£nh):\nQuay l·∫°i portal c·ªßa Resource Group, b·∫°n s·∫Ω th·∫•y Runbook v·ª´a import ƒë√£ ƒë∆∞·ª£c hi·ªán ra, ·∫•n v√†o n√≥ (·∫£nh):\n·ªû m√†n h√¨nh portal c·ªßa Runbook, h√£y ch·ªçn Edit:\n·ªû ƒë√¢y h√£y paste n·ªôi dung sau v√†o terminal:\nWorkflow Stop-Start-AzureVM { Param ( [Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] [String] $AzureSubscriptionId, [Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] [String] $AzureVMList=\u0026#34;All\u0026#34;, [Parameter(Mandatory=$true)][ValidateSet(\u0026#34;Start\u0026#34;,\u0026#34;Stop\u0026#34;)] [String] $Action ) $connectionName = \u0026#34;AzureRunAsConnection\u0026#34; try { # Get the connection \u0026#34;AzureRunAsConnection \u0026#34; $servicePrincipalConnection=Get-AutomationConnection -Name $connectionName \u0026#34;Logging in to Azure...\u0026#34; Add-AzureRmAccount ` -ServicePrincipal ` -TenantId $servicePrincipalConnection.TenantId ` -ApplicationId $servicePrincipalConnection.ApplicationId ` -CertificateThumbprint $servicePrincipalConnection.CertificateThumbprint } catch { if (!$servicePrincipalConnection) { $ErrorMessage = \u0026#34;Connection $connectionNamenot found.\u0026#34; throw $ErrorMessage } else{ Write-Error -Message $_.Exception throw $_.Exception } } if($AzureVMList -ne \u0026#34;All\u0026#34;) { $AzureVMs = $AzureVMList.Split(\u0026#34;,\u0026#34;) [System.Collections.ArrayList]$AzureVMsToHandle = $AzureVMs } else { $AzureVMs = (Get-AzureRmVM).Name [System.Collections.ArrayList]$AzureVMsToHandle = $AzureVMs } foreach($AzureVM in $AzureVMsToHandle) { if(!(Get-AzureRmVM | ? {$_.Name -eq $AzureVM})) { throw \u0026#34; AzureVM : [$AzureVM] - Does not exist! - Check your inputs \u0026#34; } } if($Action -eq \u0026#34;Stop\u0026#34;) { Write-Output \u0026#34;Stopping VMs\u0026#34;; foreach -parallel ($AzureVM in $AzureVMsToHandle) { Get-AzureRmVM | ? {$_.Name -eq $AzureVM} | Stop-AzureRmVM -Force } } else { Write-Output \u0026#34;Starting VMs\u0026#34;; foreach -parallel ($AzureVM in $AzureVMsToHandle) { Get-AzureRmVM | ? {$_.Name -eq $AzureVM} | Start-AzureRmVM } } } Sau ƒë√≥ ch·ªçn Save v√† Publish nh√©\nƒê·∫øn ƒë√¢y c√≥ th·ªÉ c√°c b·∫°n s·∫Ω t·ª± h·ªèi t·∫°i sao ph·∫£i update code, ko d√πng c√°i c√≥ s·∫µn c·ªßa Runbook √†?\nGi·∫£i th√≠ch: ƒê√∫ng v·∫≠y, ch·∫°y Code c√≥ s·∫µn c√°c b·∫°n r·∫•t nhi·ªÅu kh·∫£ nƒÉng s·∫Ω b·ªã d√≠nh l·ªói sau ƒë√¢y:\nkh·∫£ nƒÉng l√† do ƒëo·∫°n code Login v√†o Azure c·ªßa n√≥ c√≥ v·∫•n ƒë·ªÅ, th·∫ø n√™n c·∫ßn s·ª≠ d·ª•ng ƒëo·∫°n code m√† m√¨nh thay th·∫ø ·ªü b√™n tr√™n\nQuay l·∫°i portal c·ªßa Runbook, ch·ªçn Link to schedule:\nClick v√†o ph·∫ßn setting Schedule tr∆∞·ªõc r·ªìi Create new schedule: T·∫°o 1 schedule s·∫Ω run v√†o l√∫c n√†o ƒë√≥ t√πy b·∫°n, ·ªü ƒë√¢y m√¨nh l·∫•y v√≠ d·ª• l√† 11:10PM:\nTi·∫øp t·ª•c ch·ªçn ph·∫ßn config Parameter nh∆∞ h√¨nh n√†y:\n·ªû ƒë√¢y b·∫°n c·∫ßn ƒëi·ªÅn th√¥ng tin:\nAZURESUBSCRIPTIONID: subscription id,\nAZUREVMLIST: list c√°c VM s·∫Ω stop (ngƒÉn c√°ch b·∫±ng d·∫•u ph·∫©y, ko space),\nACTION: ch·ªâ ƒëc ghi Stop ho·∫∑c Start. ·ªû v√≠ d·ª• n√†y m√¨nh s·∫Ω ƒëi·ªÅn Stop ti·∫øp t·ª•c ch·ªçn OK, ·ªü m√†n h√¨nh portal Runbook b·∫°n ch·ªçn tab Schedule ·ªü panel tay tr√°i, s·∫Ω th·∫•y Schedule ƒë√£ ƒë∆∞·ª£c l√™n l·ªãch, s·∫Ω run v√†o l√∫c 11h10PM:\nCh·ªù ƒë·∫øn 11h10PM, c√°c b·∫°n c√≥ th·ªÉ chuy·ªÉn sang tab Job ƒë·ªÉ xem tr·∫°ng th√°i Job c·ªßa b·∫°n ch·∫°y th√†nh c√¥ng hay th·∫•t b·∫°i:\nN·∫øu th√†nh c√¥ng s·∫Ω l√† Completed, n·∫øu l·ªói th√¨ s·∫Ω l√† Suspended Xem log th·∫•y nh∆∞ n√†y l√† OK, ko c√≥ ERROR, WARNING g√¨ c·∫£:\nQuay l·∫°i portal c·ªßa Virtual Machine s·∫Ω th·∫•y c√°c VMs ƒë√£ ƒë∆∞·ª£c stop. Gi·ªù c√°c b·∫°n c√≥ th·ªÉ t·ª± t·∫°o Schedule Auto Start VM n·ªØa nh∆∞ sau:\nƒêi ng·ªß v√† t·∫≠n h∆∞·ªüng th√†nh qu·∫£ th√¥i ^^\nCh√∫ √Ω   ƒê·ªÉ d√πng l√¢u d√†i v√† th∆∞·ªùng xuy√™n th√¨ PowerShell script r·∫•t kh√¥ng ·ªïn ƒë·ªãnh, nhi·ªÅu l√∫c b·ªã l·ªói ko th·ªÉ start/stop VM ƒë∆∞·ª£c, v√¨ th·∫ø m√¨nh khuy√™n c√°c b·∫°n n√™n t·ª± vi·∫øt c√°c h√†m python ƒë·ªÉ thay th·∫ø cho PowerShell\n  Ch√∫ √Ω khi t·∫°o Automation Account, n√≥ c√≥ auto t·∫°o certificate ko. N·∫øu c√≥ th√¨ cert ·∫•y s·∫Ω t·ª± ƒë·ªông h·∫øt h·∫°n sau 1 nƒÉm. C·∫ßn c√≥ gi·∫£i ph√°p: ho·∫∑c l√† try-catch ch·ªó authenticate code c·ªßa Runbook n·∫øu l·ªói th√¨ raise l√™n cho m√¨nh v√†o renew cert. Ho·∫∑c l√† s·ª≠a code th√†nh authen v√†o Azure b·∫±ng identity. Ho·∫∑c l√† 1 nƒÉm sau nh·ªõ v√†o gia h·∫°n.\n  Script python sample:\n\u0026#34;\u0026#34;\u0026#34; Automation runbook for Stop/Start VMs on Azure Parameters - Required: [PARAMETER 1] : stop | start [PARAMETER 2] : VM name - separated by comma (,) [PARAMETER 3] : Resource group name that contains VM This runbook will stop/start all VMs specified in [PARAMETER 2] \u0026#34;\u0026#34;\u0026#34; import azure.mgmt.resource import automationassets from msrestazure.azure_cloud import AZURE_PUBLIC_CLOUD import sys from azure.mgmt.compute import ComputeManagementClient from msrestazure.azure_exceptions import CloudError import requests, json def get_automation_runas_credential(runas_connection, resource_url, authority_url): \u0026#34;\u0026#34;\u0026#34; Returns credentials to authenticate against Azure resoruce manager \u0026#34;\u0026#34;\u0026#34; from OpenSSL import crypto from msrestazure import azure_active_directory import adal # Get the Azure Automation RunAs service principal certificate cert = automationassets.get_automation_certificate(\u0026#34;AzureRunAsCertificate\u0026#34;) pks12_cert = crypto.load_pkcs12(cert) pem_pkey = crypto.dump_privatekey(crypto.FILETYPE_PEM, pks12_cert.get_privatekey()) # Get run as connection information for the Azure Automation service principal application_id = runas_connection[\u0026#34;ApplicationId\u0026#34;] thumbprint = runas_connection[\u0026#34;CertificateThumbprint\u0026#34;] tenant_id = runas_connection[\u0026#34;TenantId\u0026#34;] # Authenticate with service principal certificate authority_full_url = (authority_url + \u0026#39;/\u0026#39; + tenant_id) context = adal.AuthenticationContext(authority_full_url) return azure_active_directory.AdalAuthentication( lambda: context.acquire_token_with_client_certificate( resource_url, application_id, pem_pkey, thumbprint) ) def start_vm(vm, resource_group, compute_client): try: print(\u0026#34;Try to start vm [%s]\u0026#34; % vm) async_vm_start = compute_client.virtual_machines.start(resource_group, vm) async_vm_start.wait() print(\u0026#34;vm [%s] was successfully started\u0026#34; % vm) return True except CloudError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): print(\u0026#34;Start command throttled, automatically retrying...\u0026#34;) start_vm(vm, resource_group, compute_client) else: print(\u0026#34;Start command failed!\\n%s\u0026#34; % str(err)) return False except: raise def stop_vm(vm, resource_group, compute_client): try: print(\u0026#34;Try to stop vm [%s]\u0026#34; % vm) async_vm_stop = compute_client.virtual_machines.power_off(resource_group, vm) async_vm_stop.wait() print(\u0026#34;vm [%s] was successfully stopped\u0026#34; % vm) return True except CloudError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): print(\u0026#34;Stop command throttled, automatically retrying...\u0026#34;) stop_vm(vm, resource_group, compute_client) else: print(\u0026#34;Stop command failed!\\n%s\u0026#34; % str(err)) return False except: raise def alert_on_slack(vm, resource_group): try: hook_url = \u0026#34;https://hooks.slack.com/services/XXXX/YYYY/ZZZZZ\u0026#34; payload={\u0026#34;text\u0026#34;: \u0026#34;Something wrong when stop/start vm [%s] [%s].\\nPlease go to Azure portal to take action.\u0026#34; % (vm, resource_group)} response = requests.post( hook_url, data = json.dumps(payload), headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} ) print(\u0026#34;Alert!\u0026#34;) if response.status_code != 200: raise ValueError( \u0026#39;Request to slack returned an error %s, the response is:\\n%s\u0026#39; % (response.status_code, response.text) ) return response except: raise def main(): if len(sys.argv) \u0026gt;= 3: action = str(sys.argv[1]) vm_list = str(sys.argv[2]) resource_group = str(sys.argv[3]) # need some regex to validate argument variables else: error_msg = \u0026#34;Positional parameters action, vm_list and resource_group are required...\u0026#34; print(error_msg) raise Exception(error_msg) vm_list = vm_list.split(\u0026#34;,\u0026#34;) # authenticate to Azure Cloud runas_connection = automationassets.get_automation_connection(\u0026#34;AzureRunAsConnection\u0026#34;) resource_url = AZURE_PUBLIC_CLOUD.endpoints.active_directory_resource_id authority_url = AZURE_PUBLIC_CLOUD.endpoints.active_directory resourceManager_url = AZURE_PUBLIC_CLOUD.endpoints.resource_manager azure_credential = get_automation_runas_credential(runas_connection, resource_url, authority_url) compute_client = ComputeManagementClient( azure_credential, str(runas_connection[\u0026#34;SubscriptionId\u0026#34;]) ) # stop/start VM base on action if action.lower() == \u0026#34;start\u0026#34;: for vm in vm_list: response = start_vm(vm, resource_group, compute_client) if response != True: alert_on_slack(vm, resource_group) elif action.lower() == \u0026#34;stop\u0026#34;: for vm in vm_list: response = stop_vm(vm, resource_group, compute_client) if response != True: alert_on_slack(vm, resource_group) if __name__ == \u0026#39;__main__\u0026#39;: main() ","href":"/posts/encrypt-azure-setup-auto-stop-start-virtual-machines/","title":"Azure: Setup Auto Stop/Start on schedule for Virtual Machines"},{"content":"Gi·ªõi thi·ªáu T∆∞·ªüng t∆∞·ª£ng r·∫±ng b·∫°n ƒë√£ c√≥:\n GKE Cluster, d·ª±ng Gitlab Runner, Gitlab self-hosted (ho·∫∑c d√πng gitlab.com). 1 java project (d√πng Maven) tr√™n Gitlab. B·∫°n mu·ªën s·ª≠ d·ª•ng Gitlab CI ƒë·ªÉ t·∫°o pipeline.  B√†i n√†y h∆∞·ªõng d·∫´n c√°ch setup 1 server Sonarqube ƒë·ªÉ scan quality code cho 1 Java (Spring) project.\nK·∫øt th√∫c b√†i n√†y s·∫Ω c√≥ 1 flow ki·ªÉu nh∆∞ sau, project Java c·ªßa b·∫°n m·ªói khi c√≥ 1 commit tr√™n Gitlab s·∫Ω:\n trigger Gitlab Runner th·ª±c hi·ªán scan quality code b·∫±ng Sonarqube, sau ƒë√≥ build artifacts (t·∫°o file .jar, .war), r·ªìi build Docker images, v√† publish Docker images ƒë√≥ l√™n Registry c·ªßa b·∫°n (c√≥ th·ªÉ l√† Dockerhub ho·∫∑c GCR, etc\u0026hellip;)  Y√™u c·∫ßu ƒê√£ ho√†n th√†nh b√†i K8S 9 n√†y ƒë·ªÉ setup domain v·ªõi nginx, external-dns, cert-manager:\nK8S 9\nC√°ch l√†m Workspace ch√∫ng ta v·∫´n s·∫Ω l√† GCP CloudShell\n1. Chu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng, namespace v√† secrets export PROJECT_ID=\u0026#34;your-project-id\u0026#34; export DOMAIN=\u0026#34;your-domain.net\u0026#34; export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; export SONAR_SUBDOMAIN=\u0026#34;sonarqube.your-subdomain.your-domain.net\u0026#34; export SPRINGAPP_SUBDOMAIN=\u0026#34;springapp.your-subdomain.your-domain.net\u0026#34; export CLUSTER_NAME=\u0026#34;your-cluster-name\u0026#34; gcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a # ·ªû ƒë√¢y ch√∫ng ta s·ª≠ d·ª•ng l·∫°i TLS secret m√† ƒë√£ dc issue th√†nh c√¥ng ·ªü b√†i `K8S 9` export TLS_SECRET_NAME=\u0026#34;echo-tls-secret-prod\u0026#34; 2. Install kubed ƒë·ªÉ sync secret sang c√°c namespace c·∫ßn thi·∫øt (N·∫øu b·∫°n ƒë√£ l√†m b∆∞·ªõc n√†y th√¨ c√≥ th·ªÉ b·ªè qua):\nhelm repo add appscode https://charts.appscode.com/stable/ helm repo update helm install -n kubed appscode/kubed --version 0.12.0 --namespace kube-system --wait --set config.clusterName=${CLUSTER_NAME} 2. Setup Sonarqube Helm chart T·∫°o namespace d√†nh cho sonarqube:\nkubectl create namespace sonarqube ƒê√°nh label app=kubed cho namespace sonarqube:\nkubectl label namespace sonarqube app=kubed Annotate ƒë·ªÉ secret m√¨nh mu·ªën s·∫Ω sync sang c√°c namespace c√≥ label app=kubed:\nkubectl annotate secret ${TLS_SECRET_NAME} -n default kubed.appscode.com/sync=\u0026#34;app=kubed\u0026#34; Annotate domain s·∫Ω d√πng cho sonarqube v√†o Ingress nginx ƒë√£ t·∫°o:\nkubectl annotate service nginx-ingress-controller \u0026#34;external-dns.alpha.kubernetes.io/hostname=${SONAR_SUBDOMAIN}.,${SUBDOMAIN}.\u0026#34; -n nginx-ingress --overwrite Sau khi annotate th√†nh c√¥ng b·∫°n v√†o CloudDNS s·∫Ω th·∫•y domain c·ªßa sonarqube ƒë√£ ƒë∆∞·ª£c t·∫°o ra, n√≥ s·∫Ω c√≥ chung IP v4 address v·ªõi your-subdomain.net nh∆∞ ·∫£nh sau:\nC√≤n khi v√†o cluster get th√¥ng tin c√°c service ra b·∫°n s·∫Ω th·∫•y IP v4 address ƒë√≥ ch√≠nh l√† IP c·ªßa Nginx-ingress-controller type LoadBalancer:\nk get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cert-manager cert-manager ClusterIP 10.68.114.39 \u0026lt;none\u0026gt; 9402/TCP 12h cert-manager cert-manager-webhook ClusterIP 10.68.119.213 \u0026lt;none\u0026gt; 443/TCP 12h default echo ClusterIP 10.68.181.83 \u0026lt;none\u0026gt; 80/TCP 10h default kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 12h kube-system default-http-backend NodePort 10.68.107.169 \u0026lt;none\u0026gt; 80:31554/TCP 12h kube-system kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 12h kube-system kubed ClusterIP 10.68.111.17 \u0026lt;none\u0026gt; 443/TCP 12h kube-system metrics-server ClusterIP 10.68.47.251 \u0026lt;none\u0026gt; 443/TCP 12h kube-system tiller-deploy ClusterIP 10.68.181.222 \u0026lt;none\u0026gt; 44134/TCP 12h nginx-ingress external-dns ClusterIP 10.68.239.9 \u0026lt;none\u0026gt; 7979/TCP 12h nginx-ingress nginx-ingress-controller LoadBalancer 10.68.8.115 34.11.12.50 80:31291/TCP,443:31035/TCP 12h nginx-ingress nginx-ingress-default-backend ClusterIP 10.68.72.132 \u0026lt;none\u0026gt; 80/TCP 12h sonarqube sonar-postgresql ClusterIP 10.68.24.201 \u0026lt;none\u0026gt; 5432/TCP 48m sonarqube sonar-postgresql-headless ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 48m sonarqube sonar-sonarqube ClusterIP 10.68.78.189 \u0026lt;none\u0026gt; 9000/TCP 48m M√¨nh m√¥ t·∫£ k·ªπ nh∆∞ v·∫ßy ƒë·ªÉ b·∫°n hi·ªÉu r·∫±ng n·∫øu b·∫°n th·∫•y domain cho sonarqube c·ªßa b·∫°n m√† ko d√πng chung 1 IP v·ªõi nginx v√† subdomain th√¨ c√≥ nghƒ©a b·∫°n ƒëang setting sai nha\nT·∫°o file sonarqube-values.yaml:\nc√¢u l·ªánh sau s·∫Ω s·ª≠ d·ª•ng 2 bi·∫øn m√¥i tr∆∞·ªùng ƒë√£ setup b√™n tr√™n l√† ${TLS_SECRET_NAME} v√† ${SONAR_SUBDOMAIN}:\ncat \u0026gt; ./sonarqube-values.yaml \u0026lt;\u0026lt;EOF # Default values for sonarqube. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 # This will use the default deployment strategy unless it is overriden deploymentStrategy: {} # Uncomment this to scheduler pods on priority # priorityClassName: \u0026#34;high-priority\u0026#34; image: repository: sonarqube tag: 8.2-community # If using a private repository, the name of the imagePullSecret to use # pullSecret: my-repo-secret # Set security context for sonarqube pod securityContext: fsGroup: 999 # Settings to configure elasticsearch host requirements elasticsearch: configureNode: true bootstrapChecks: true service: type: ClusterIP externalPort: 9000 internalPort: 9000 labels: annotations: {} # May be used in example for internal load balancing in GCP: # cloud.google.com/load-balancer-type: Internal # loadBalancerSourceRanges: # - 0.0.0.0/0 # loadBalancerIP: 1.2.3.4 ingress: enabled: true # Used to create an Ingress record. hosts: - name: ${SONAR_SUBDOMAIN} # Different clouds or configurations might need /* as the default path path: / # For additional control over serviceName and servicePort serviceName: sonar-sonarqube servicePort: 9000 annotations: kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026#34;true\u0026#34; # This property allows for reports up to a certain size to be uploaded to SonarQube # nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;8m\u0026#34; # Additional labels for Ingress manifest file # labels: # traffic-type: external # traffic-type: internal tls: # Secrets must be manually created in the namespace. - secretName: ${TLS_SECRET_NAME} hosts: - ${SONAR_SUBDOMAIN} # Affinity for pod assignment # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: {} # Tolerations for pod assignment # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] # Node labels for pod assignment # Ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: {} # hostAliases allows the modification of the hosts file inside a container hostAliases: [] # - ip: \u0026#34;192.168.1.10\u0026#34; # hostnames: # - \u0026#34;example.com\u0026#34; # - \u0026#34;www.example.com\u0026#34; readinessProbe: initialDelaySeconds: 60 periodSeconds: 30 failureThreshold: 6 # If an ingress *path* other than the root (/) is defined, it should be reflected here # A trailing \u0026#34;/\u0026#34; must be included sonarWebContext: / # sonarWebContext: /sonarqube/ livenessProbe: initialDelaySeconds: 60 periodSeconds: 30 # If an ingress *path* other than the root (/) is defined, it should be reflected here # A trailing \u0026#34;/\u0026#34; must be included sonarWebContext: / # sonarWebContext: /sonarqube/ # If an ingress *path* is defined, it should be reflected here # sonar.web.context: /sonarqube ## Provide a secret containing one or more certificate files in the keys that will be added to cacerts ## The cacerts file will be set via SONARQUBE_WEB_JVM_OPTS ## # caCerts: # secret: my-secret ## Values to add to SONARQUBE_WEB_JVM_OPTS ## # jvmOpts: \u0026#34;-Djava.net.preferIPv4Stack=true\u0026#34; jvmOpts: \u0026#34;\u0026#34; ## Environment variables to attach to the pods ## # env: # - name: VARIABLE # value: my-value # Set annotations for pods annotations: {} resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after \u0026#39;resources:\u0026#39;. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi persistence: enabled: false ## Set annotations on pvc annotations: {} ## Specify an existing volume claim instead of creating a new one. ## When using this option all following options like storageClass, accessMode and size are ignored. # existingClaim: ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026#34;-\u0026#34;, storageClassName: \u0026#34;\u0026#34;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: accessMode: ReadWriteOnce size: 10Gi ## Specify extra volumes. Refer to \u0026#34;.spec.volumes\u0026#34; specification : https://kubernetes.io/fr/docs/concepts/storage/volumes/ volumes: [] ## Specify extra mounts. Refer to \u0026#34;.spec.containers.volumeMounts\u0026#34; specification : https://kubernetes.io/fr/docs/concepts/storage/volumes/ mounts: [] # In case you want to specify different resources for emptyDir than {} emptyDir: {} # Example of resouces that might be used: # medium: Memory # sizeLimit: 16Mi # List of plugins to install. # For example: # plugins: # install: # - \u0026#34;https://github.com/AmadeusITGroup/sonar-stash/releases/download/1.3.0/sonar-stash-plugin-1.3.0.jar\u0026#34; # - \u0026#34;https://github.com/SonarSource/sonar-ldap/releases/download/2.2-RC3/sonar-ldap-plugin-2.2.0.601.jar\u0026#34; plugins: install: [] lib: [] # For use behind a corporate proxy when downloading plugins # httpProxy: \u0026#34;\u0026#34; # httpsProxy: \u0026#34;\u0026#34; # initContainerImage: rjkernick/alpine-wget:latest # initSysctlContainerImage: busybox:1.31 # initCertsContainerImage: adoptopenjdk/openjdk11:alpine # initTestContainerImage: dduportal/bats:0.4.0 # deleteDefaultPlugins: true resources: {} # We allow the plugins init container to have a separate resources declaration because # the initContainer does not take as much resources. # A custom sonar.properties file can be provided via dictionary. # For example: # sonarProperties: # sonar.forceAuthentication: true # sonar.security.realm: LDAP # ldap.url: ldaps://organization.com # Additional sonar properties to load from a secret with a key \u0026#34;secret.properties\u0026#34; (must be a string) # sonarSecretProperties: # Kubernetes secret that contains the encryption key for the sonarqube instance. # The secret must contain the key \u0026#39;sonar-secret.txt\u0026#39;. # The \u0026#39;sonar.secretKeyPath\u0026#39; property will be set automatically. # sonarSecretKey: \u0026#34;settings-encryption-secret\u0026#34; ## JDBC Database Type; by default postgresql. To use a different Database type, adjust jdbcDatabaseType: postgresql ## Override JDBC URL # jdbcUrlOverride: \u0026#34;jdbc:postgresql://myPostgress/myDatabase;socketTimeout=1500\u0026#34; ## Configuration values for postgresql dependency ## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md postgresql: # Enable to deploy the PostgreSQL chart enabled: true # To use an external PostgreSQL instance, set enabled to false and uncomment # the line below: # postgresqlServer: \u0026#34;\u0026#34; # To use an external secret for the password for an external PostgreSQL # instance, set enabled to false and provide the name of the secret on the # line below: # existingSecret: \u0026#34;\u0026#34; postgresqlUsername: \u0026#34;sonarUser\u0026#34; postgresqlPassword: \u0026#34;sonarPass\u0026#34; postgresqlDatabase: \u0026#34;sonarDB\u0026#34; # Specify the TCP port that PostgreSQL should use service: port: 5432 # Additional labels to add to the pods: # podLabels: # key: value podLabels: {} # For compatibility with 8.0 replace by \u0026#34;/opt/sq\u0026#34; # For compatibility with 8.2, leave the default. They changed it back to /opt/sonarqube sonarqubeFolder: /opt/sonarqube enableTests: true serviceAccount: create: false # name: ## Annotations for the Service Account annotations: {} # extraConfig is used to load Environment Variables from secrets and configmaps # which may have been written by other tools, such as external orchestrators. # # These Secrets/ ConfigMaps are expecetd to contain Key/ Value pairs, such as: # # apiVersion: v1 # kind: ConfigMap # metadata: # name: external-sonarqube-opts # data: # SONARQUBE_JDBC_USERNAME: foo # SONARQUBE_JDBC_URL: jdbc:postgresql://db.example.com:5432/sonar # # These vars can then be injected into the environment by uncommenting the following: # # extraConfig: # configmaps: # - external-sonarqube-opts extraConfig: secrets: [] configmaps: [] EOF install helm chart, m√¨nh s·∫Ω ƒë·∫∑t t√™n chart l√† sonar, c√†i v√†o namespace sonarqube:\nhelm repo add oteemocharts https://oteemo.github.io/charts helm install --name sonar oteemocharts/sonarqube --namespace sonarqube --values sonarqube-values.yaml Gi·∫£ s·ª≠ n·∫øu b·∫°n mu·ªën upgrade helm chart tr√™n:\nhelm upgrade sonar oteemocharts/sonarqube --namespace sonarqube -f sonarqube-values.yaml N·∫øu mu·ªën delete chart tr√™n:\nhelm delete --purge sonar logs:\nk get pods,svc,ing -n sonarqube NAME READY STATUS RESTARTS AGE pod/sonar-postgresql-0 1/1 Running 0 6m36s pod/sonar-sonarqube-6cf557759c-6zlqf 1/1 Running 0 47h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sonar-postgresql ClusterIP 10.68.81.62 \u0026lt;none\u0026gt; 5432/TCP 2d18h service/sonar-postgresql-headless ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 2d18h service/sonar-sonarqube ClusterIP 10.68.76.133 \u0026lt;none\u0026gt; 9000/TCP 2d18h NAME HOSTS ADDRESS PORTS AGE ingress.extensions/sonar-sonarqube sonarqube.your-subdomain.your-domain.net 10.0.16.6 80, 443 2d18h V√†o link sonarqube.your-subdomain.your-domain.net s·∫Ω th·∫•y c√≥ HTTPS do m√¨nh s·ª≠ d·ª•ng TLS ƒë√£ ƒëc issue th√†nh c√¥ng t·ª´ tr∆∞·ªõc:\nƒêƒÉng nh·∫≠p v√†o Sonarqube v·ªõi account default l√† admin/admin r·ªìi ƒë·ªïi pass cho Admin nh√©\n3. Setup Maven project using Spring Boots v√†o cluster Gi·ªù m√¨nh s·∫Ω t·∫°o 1 app microservices g·ªìm 2 services:\n spring app, postgres  B·∫Øt ƒë·∫ßu n√†o!\nT·∫°o namespace cho app:\nkubectl create ns spring-app ƒê√°nh label app=kubed cho namespace spring-app:\nkubectl label namespace spring-app app=kubed Annotate ƒë·ªÉ secret m√¨nh mu·ªën s·∫Ω sync sang c√°c namespace c√≥ label app=kubed:\nkubectl annotate secret ${TLS_SECRET_NAME} -n default kubed.appscode.com/sync=\u0026#34;app=kubed\u0026#34; Annotate domain s·∫Ω d√πng cho SpringApp v√†o Ingress nginx ƒë√£ t·∫°o:\nkubectl annotate service nginx-ingress-controller \u0026#34;external-dns.alpha.kubernetes.io/hostname=${SPRINGAPP_SUBDOMAIN}.,${SONAR_SUBDOMAIN}.,${SUBDOMAIN}.\u0026#34; -n nginx-ingress --overwrite Tr√™n Cloud DNS s·∫Ω th·∫•y hi·ªán ra A record m·ªõi cho springapp.your-subdomain.your-domain.net,\nconfirm r·∫±ng n√≥ d√πng chung 1 IP v·ªõi $SPRINGAPP_SUBDOMAIN v√† $SUBDOMAIN l√† ok!\nclone project sau v·ªÅ t·ª´ Github:\ngit clone https://github.com/hoangmnsd/spring-maven-postgres-docker cd spring-maven-postgres-docker # t·∫°o folder resource-manifest ch·ª©a k8s template mkdir -p resource-manifest \u0026amp;\u0026amp; cd resource-manifest ###1 t·∫°o file docker_postgres-deployment.yaml:\nCh√∫ √Ω file n√†y m√¨nh s·ª≠ d·ª•ng image image: hoangmnsd/docker_postgres l√† image tr√™n DockerHub c·ªßa m√¨nh, c√°c b·∫°n n√™n tham kh·∫£o b√†i K8S 4 n√†y ƒë·ªÉ t·ª± t·∫°o image ri√™ng c·ªßa c√°c b·∫°n:\nK8S 4\ncat \u0026gt; ./docker_postgres-deployment.yaml \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: docker-postgres labels: app: docker-postgres spec: selector: matchLabels: app: docker-postgres replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-postgres spec: containers: - image: hoangmnsd/docker_postgres env: - name: POSTGRES_USER value: dbuser - name: POSTGRES_PASSWORD value: password - name: POSTGRES_DB value: store imagePullPolicy: Always name: docker-postgres ports: - containerPort: 5432 EOF ###2 t·∫°o file docker_postgres-service.yaml:\ncat \u0026gt; ./docker_postgres-service.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: docker-postgres spec: ports: - port: 5432 protocol: TCP targetPort: 5432 selector: app: docker-postgres EOF ###3 t·∫°o file docker_spring-boot-containers-deployment.yaml:\nCh√∫ √Ω file n√†y m√¨nh s·ª≠ d·ª•ng image image: hoangmnsd/docker_spring-boot-containers l√† image tr√™n DockerHub c·ªßa m√¨nh, c√°c b·∫°n n√™n tham kh·∫£o b√†i K8S 4 n√†y ƒë·ªÉ t·ª± t·∫°o image ri√™ng c·ªßa c√°c b·∫°n:\nK8S 4\ncat \u0026gt; ./docker_spring-boot-containers-deployment.yaml \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: docker-spring-boot-containers labels: app: docker-spring-boot-containers spec: selector: matchLabels: app: docker-spring-boot-containers replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-spring-boot-containers spec: containers: - image: hoangmnsd/docker_spring-boot-containers imagePullPolicy: Always name: docker-spring-boot-containers ports: - containerPort: 12345 EOF ###4 t·∫°o file docker_spring-boot-containers-service.yaml:\ncat \u0026gt; ./docker_spring-boot-containers-service.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: docker-spring-boot-containers spec: ports: - port: 80 protocol: TCP targetPort: 12345 selector: app: docker-spring-boot-containers EOF ###5 t·∫°o file docker_spring-boot-containers-ingress.yaml:\nFile n√†y s·∫Ω s·ª≠ d·ª•ng ${SPRINGAPP_SUBDOMAIN} v√† ${TLS_SECRET_NAME}\ncat \u0026gt; ./docker_spring-boot-containers-ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: spring-app-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: tls: - hosts: - ${SPRINGAPP_SUBDOMAIN} secretName: ${TLS_SECRET_NAME} rules: - host: ${SPRINGAPP_SUBDOMAIN} http: paths: - path: / backend: serviceName: docker-spring-boot-containers servicePort: 12345 EOF V·ª´a r·ªìi ch√∫ng ta ƒë√£ t·∫°o xong 5 K8S template c·∫ßn thi·∫øt,\nGi·ªù s·∫Ω apply ƒë·ªÉ t·∫°o c√°c k8s resource c·ªßa SpringApp tr√™n:\nkubectl apply -n spring-app -f docker_postgres-deployment.yaml kubectl apply -n spring-app -f docker_postgres-service.yaml kubectl apply -n spring-app -f docker_spring-boot-containers-deployment.yaml kubectl apply -n spring-app -f docker_spring-boot-containers-service.yaml kubectl apply -n spring-app -f docker_spring-boot-containers-ingress.yaml B·∫°n s·∫Ω th·∫•y trong namespace spring-app ƒë√£ t·∫°o ra c√°c resource sau:\nk get pods,svc,ing -n spring-app NAME READY STATUS RESTARTS AGE pod/docker-postgres-cddbcc96f-84smg 1/1 Running 0 2d pod/docker-spring-boot-containers-6fbcdb989c-cjj5b 1/1 Running 0 2d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/docker-postgres ClusterIP 10.68.68.157 \u0026lt;none\u0026gt; 5432/TCP 3d4h service/docker-spring-boot-containers ClusterIP 10.68.242.118 \u0026lt;none\u0026gt; 80/TCP 3d6h NAME HOSTS ADDRESS PORTS AGE ingress.extensions/spring-app-ingress springapp.your-subdomain.your-domain.net 10.0.16.6 80, 443 3d4h Test SpringApp c√≥ ho·∫°t ƒë·ªông ƒë√∫ng ko? S·ª≠ d·ª•ng Postman, g·ª≠i POST request ƒë·∫øn https://springapp.your-subdomain.your-domain.net/v1/product v·ªõi body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} Ch√∫ √Ω l√† body c·ªßa request c·∫ßn set l√† raw v√† JSON: (·∫£nh)\nif success, Postman s·∫Ω tr·∫£ v·ªÅ response d·∫°ng n√†y:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link https://springapp.your-subdomain.your-domain.net/v1/product/product001\nif success browser s·∫Ω tr·∫£ v·ªÅ chu·ªói n√†y:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n(·∫£nh)\n4. Setup Gitlab Runner L√†m theo b√†i n√†y, ƒë·ªÉ t·∫°o ra ƒë∆∞·ª£c Gitlab Runner th√¥i nh√©:\nK8S 13\nCh√∫ √Ω l√† GITLAB_URL th√¨ ƒë·ªÉ l√† gitlab.com, cho ƒë·ª° t·ªën ti·ªÅn, v√¨ d·ª±ng c·∫£ Gitlab self-hosted s·∫Ω t·ªën nhi·ªÅu k8s resource\n5. Generate a token of Sonarqube Login v√†o Sonarqube, t·∫°o token c·ªßa User nh∆∞ ·∫£nh sau l√† ok:\ngi·∫£ s·ª≠ b·∫°n ƒë√£ copy ƒë∆∞·ª£c sonar-token nh∆∞ sau: fd059f3efd39debc0b205d4cae22c57d00be42d5\nexport SONAR_TOKEN=\u0026#34;fd059f3efd39debc0b205d4cae22c57d00be42d5\u0026#34; 6. Create empty project on Gitlab and push source from local Login v√†o gitlab.com, t·∫°o 1 project r·ªóng t√™n l√† spring-maven-postgres-docker-k8s:\nC√≤n nh·ªõ ·ªü b∆∞·ªõc 3, ch√∫ng ta ƒë√£ clone 1 project spring-maven-postgres-docker t·ª´ Github v·ªÅ CloudShell,\ngi·ªù s·∫Ω push n·ªôi dung trong project ƒë√≥ l√™n project c·ªßa Gitlab nh√©:\ncd spring-maven-postgres-docker # thay \u0026lt;GITLAB-USER\u0026gt; b·∫±ng user c·ªßa b·∫°n git remote origin set-url https://gitlab.com/\u0026lt;GITLAB-USER\u0026gt;/spring-maven-postgres-docker-k8s.git git add . git commit -m \u0026#34;add files\u0026#34; git push origin master T·∫°o file gitlab-ci.yml:\ncd spring-maven-postgres-docker cat \u0026gt; ./gitlab-ci.yml \u0026lt;\u0026lt;EOF image: docker:latest services: - docker:18.09.7-dind variables: # DOCKER_HOST: tcp://127.0.0.1:2375 # When using Gitlab self-hosted on K8s DOCKER_HOST: tcp://docker:2375/ # When using Gitlab.com DOCKER_DRIVER: overlay DOCKER_API_VERSION: \u0026#34;1.39\u0026#34; SONAR_URL: \u0026#34;${SONAR_SUBDOMAIN}\u0026#34; SONAR_TOKEN: \u0026#34;${SONAR_TOKEN}\u0026#34; stages: - test - build - publish sonarqube_master_job: stage: test only: - master image: maven:3.3.9-jdk-8-alpine script: # - mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_TOKEN - mvn clean package sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_TOKEN build_artifacts: stage: build only: - master image: maven:3.3.9-jdk-8-alpine script: - mvn clean package artifacts: paths: - target/*.jar publish_docker_images: stage: publish only: - master # before_script: # # Login to Google Cloud Registry # - echo $GCLOUD_SERVICE_KEY | docker login -u _json_key --password-stdin https://gcr.io script: # # Build docker images from `Dockerfile` and artifact of above steps `build-artifacts` - docker build -t springapp . # - docker tag $IMAGE_NAME \u0026#34;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME\u0026#34; # - docker push \u0026#34;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:latest\u0026#34; EOF push thay ƒë·ªïi l√™n Gitlab v√† check xem pipeline c·ªßa Gitlab CI c√≥ ho·∫°t ƒë·ªông t·ªët kh√¥ng n√†o.\ngit add . git commit -m \u0026#34;update gitlab-ci\u0026#34; git push origin master Nh∆∞ h√¨nh sau l√† OK, c·∫£ 3 stage ƒë·ªÅu passed c√≥ th·ªÉ ·∫•n v√†o xem log t·ª´ng job:\njob sonarqube ch·∫°y ok v√† log ra nh∆∞ n√†y: V·∫≠y l√† xong, c√≥ th·ªÉ c√≤n nhi·ªÅu c√°ch s√°ng t·∫°o v√† k·∫øt h·ª£p kh√°c nhau, nh∆∞ng t√≥m l·∫°i c√°ch setup v√† s·ª≠ d·ª•ng Sonarqube cho project c·ªßa b·∫°n c≈©ng ko kh√≥ khƒÉn l·∫Øm ƒë√¢u, ch√∫c b·∫°n th√†nh c√¥ng ^^\nREFERENCES https://medium.com/@speedforcerun/sonarqube-with-gitlab-ci-setup-stepbystep-java-maven-version-7e131dce0bb1\nhttps://stackoverflow.com/questions/48323223/sonarqube-autorun-with-gitlab\nhttps://github.com/Oteemo/charts/tree/master/charts/sonarqube\n","href":"/bk/k8s-xiv-on-gke-cluster-setup-sonarqube-for-maven-project-with-gitlabci/","title":"K8S 14: (on GKE Cluster) Setup Sonarqube for Maven Project With GitlabCI"},{"content":"Gi·ªõi thi·ªáu T∆∞·ªüng t∆∞·ª£ng r·∫±ng b·∫°n ƒë√£ c√≥:\n GKE Cluster, d·ª±ng Gitlab Runner, Gitlab self-hosted (ho·∫∑c d√πng gitlab.com). 1 java project (d√πng Maven) tr√™n Gitlab. B·∫°n mu·ªën s·ª≠ d·ª•ng Gitlab CI ƒë·ªÉ t·∫°o pipeline.  B√†i n√†y h∆∞·ªõng d·∫´n c√°ch setup 1 server Sonarqube ƒë·ªÉ scan quality code cho 1 Java (Spring) project.\nK·∫øt th√∫c b√†i n√†y s·∫Ω c√≥ 1 flow ki·ªÉu nh∆∞ sau, project Java c·ªßa b·∫°n m·ªói khi c√≥ 1 commit tr√™n Gitlab s·∫Ω:\n trigger Gitlab Runner th·ª±c hi·ªán scan quality code b·∫±ng Sonarqube, sau ƒë√≥ build artifacts (t·∫°o file .jar, .war), r·ªìi build Docker images, v√† publish Docker images ƒë√≥ l√™n Registry c·ªßa b·∫°n (c√≥ th·ªÉ l√† Dockerhub ho·∫∑c GCR, etc\u0026hellip;)  Y√™u c·∫ßu ƒê√£ ho√†n th√†nh b√†i K8S 9 n√†y ƒë·ªÉ setup domain v·ªõi nginx, external-dns, cert-manager:\nK8S 9\nC√°ch l√†m Workspace ch√∫ng ta v·∫´n s·∫Ω l√† GCP CloudShell\n1. Chu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng, namespace v√† secrets export PROJECT_ID=\u0026#34;your-project-id\u0026#34; export DOMAIN=\u0026#34;your-domain.net\u0026#34; export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; export SONAR_SUBDOMAIN=\u0026#34;sonarqube.your-subdomain.your-domain.net\u0026#34; export SPRINGAPP_SUBDOMAIN=\u0026#34;springapp.your-subdomain.your-domain.net\u0026#34; export CLUSTER_NAME=\u0026#34;your-cluster-name\u0026#34; gcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a # ·ªû ƒë√¢y ch√∫ng ta s·ª≠ d·ª•ng l·∫°i TLS secret m√† ƒë√£ dc issue th√†nh c√¥ng ·ªü b√†i `K8S 9` export TLS_SECRET_NAME=\u0026#34;echo-tls-secret-prod\u0026#34; 2. Install kubed ƒë·ªÉ sync secret sang c√°c namespace c·∫ßn thi·∫øt (N·∫øu b·∫°n ƒë√£ l√†m b∆∞·ªõc n√†y th√¨ c√≥ th·ªÉ b·ªè qua):\nhelm repo add appscode https://charts.appscode.com/stable/ helm repo update helm install -n kubed appscode/kubed --version 0.12.0 --namespace kube-system --wait --set config.clusterName=${CLUSTER_NAME} 2. Setup Sonarqube Helm chart T·∫°o namespace d√†nh cho sonarqube:\nkubectl create namespace sonarqube ƒê√°nh label app=kubed cho namespace sonarqube:\nkubectl label namespace sonarqube app=kubed Annotate ƒë·ªÉ secret m√¨nh mu·ªën s·∫Ω sync sang c√°c namespace c√≥ label app=kubed:\nkubectl annotate secret ${TLS_SECRET_NAME} -n default kubed.appscode.com/sync=\u0026#34;app=kubed\u0026#34; Annotate domain s·∫Ω d√πng cho sonarqube v√†o Ingress nginx ƒë√£ t·∫°o:\nkubectl annotate service nginx-ingress-controller \u0026#34;external-dns.alpha.kubernetes.io/hostname=${SONAR_SUBDOMAIN}.,${SUBDOMAIN}.\u0026#34; -n nginx-ingress --overwrite Sau khi annotate th√†nh c√¥ng b·∫°n v√†o CloudDNS s·∫Ω th·∫•y domain c·ªßa sonarqube ƒë√£ ƒë∆∞·ª£c t·∫°o ra, n√≥ s·∫Ω c√≥ chung IP v4 address v·ªõi your-subdomain.net nh∆∞ ·∫£nh sau:\nC√≤n khi v√†o cluster get th√¥ng tin c√°c service ra b·∫°n s·∫Ω th·∫•y IP v4 address ƒë√≥ ch√≠nh l√† IP c·ªßa Nginx-ingress-controller type LoadBalancer:\nk get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cert-manager cert-manager ClusterIP 10.68.114.39 \u0026lt;none\u0026gt; 9402/TCP 12h cert-manager cert-manager-webhook ClusterIP 10.68.119.213 \u0026lt;none\u0026gt; 443/TCP 12h default echo ClusterIP 10.68.181.83 \u0026lt;none\u0026gt; 80/TCP 10h default kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 12h kube-system default-http-backend NodePort 10.68.107.169 \u0026lt;none\u0026gt; 80:31554/TCP 12h kube-system kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 12h kube-system kubed ClusterIP 10.68.111.17 \u0026lt;none\u0026gt; 443/TCP 12h kube-system metrics-server ClusterIP 10.68.47.251 \u0026lt;none\u0026gt; 443/TCP 12h kube-system tiller-deploy ClusterIP 10.68.181.222 \u0026lt;none\u0026gt; 44134/TCP 12h nginx-ingress external-dns ClusterIP 10.68.239.9 \u0026lt;none\u0026gt; 7979/TCP 12h nginx-ingress nginx-ingress-controller LoadBalancer 10.68.8.115 34.11.12.50 80:31291/TCP,443:31035/TCP 12h nginx-ingress nginx-ingress-default-backend ClusterIP 10.68.72.132 \u0026lt;none\u0026gt; 80/TCP 12h sonarqube sonar-postgresql ClusterIP 10.68.24.201 \u0026lt;none\u0026gt; 5432/TCP 48m sonarqube sonar-postgresql-headless ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 48m sonarqube sonar-sonarqube ClusterIP 10.68.78.189 \u0026lt;none\u0026gt; 9000/TCP 48m M√¨nh m√¥ t·∫£ k·ªπ nh∆∞ v·∫ßy ƒë·ªÉ b·∫°n hi·ªÉu r·∫±ng n·∫øu b·∫°n th·∫•y domain cho sonarqube c·ªßa b·∫°n m√† ko d√πng chung 1 IP v·ªõi nginx v√† subdomain th√¨ c√≥ nghƒ©a b·∫°n ƒëang setting sai nha\nT·∫°o file sonarqube-values.yaml:\nc√¢u l·ªánh sau s·∫Ω s·ª≠ d·ª•ng 2 bi·∫øn m√¥i tr∆∞·ªùng ƒë√£ setup b√™n tr√™n l√† ${TLS_SECRET_NAME} v√† ${SONAR_SUBDOMAIN}:\ncat \u0026gt; ./sonarqube-values.yaml \u0026lt;\u0026lt;EOF # Default values for sonarqube. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 # This will use the default deployment strategy unless it is overriden deploymentStrategy: {} # Uncomment this to scheduler pods on priority # priorityClassName: \u0026#34;high-priority\u0026#34; image: repository: sonarqube tag: 8.2-community # If using a private repository, the name of the imagePullSecret to use # pullSecret: my-repo-secret # Set security context for sonarqube pod securityContext: fsGroup: 999 # Settings to configure elasticsearch host requirements elasticsearch: configureNode: true bootstrapChecks: true service: type: ClusterIP externalPort: 9000 internalPort: 9000 labels: annotations: {} # May be used in example for internal load balancing in GCP: # cloud.google.com/load-balancer-type: Internal # loadBalancerSourceRanges: # - 0.0.0.0/0 # loadBalancerIP: 1.2.3.4 ingress: enabled: true # Used to create an Ingress record. hosts: - name: ${SONAR_SUBDOMAIN} # Different clouds or configurations might need /* as the default path path: / # For additional control over serviceName and servicePort serviceName: sonar-sonarqube servicePort: 9000 annotations: kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026#34;true\u0026#34; # This property allows for reports up to a certain size to be uploaded to SonarQube # nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;8m\u0026#34; # Additional labels for Ingress manifest file # labels: # traffic-type: external # traffic-type: internal tls: # Secrets must be manually created in the namespace. - secretName: ${TLS_SECRET_NAME} hosts: - ${SONAR_SUBDOMAIN} # Affinity for pod assignment # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: {} # Tolerations for pod assignment # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] # Node labels for pod assignment # Ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: {} # hostAliases allows the modification of the hosts file inside a container hostAliases: [] # - ip: \u0026#34;192.168.1.10\u0026#34; # hostnames: # - \u0026#34;example.com\u0026#34; # - \u0026#34;www.example.com\u0026#34; readinessProbe: initialDelaySeconds: 60 periodSeconds: 30 failureThreshold: 6 # If an ingress *path* other than the root (/) is defined, it should be reflected here # A trailing \u0026#34;/\u0026#34; must be included sonarWebContext: / # sonarWebContext: /sonarqube/ livenessProbe: initialDelaySeconds: 60 periodSeconds: 30 # If an ingress *path* other than the root (/) is defined, it should be reflected here # A trailing \u0026#34;/\u0026#34; must be included sonarWebContext: / # sonarWebContext: /sonarqube/ # If an ingress *path* is defined, it should be reflected here # sonar.web.context: /sonarqube ## Provide a secret containing one or more certificate files in the keys that will be added to cacerts ## The cacerts file will be set via SONARQUBE_WEB_JVM_OPTS ## # caCerts: # secret: my-secret ## Values to add to SONARQUBE_WEB_JVM_OPTS ## # jvmOpts: \u0026#34;-Djava.net.preferIPv4Stack=true\u0026#34; jvmOpts: \u0026#34;\u0026#34; ## Environment variables to attach to the pods ## # env: # - name: VARIABLE # value: my-value # Set annotations for pods annotations: {} resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after \u0026#39;resources:\u0026#39;. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi persistence: enabled: false ## Set annotations on pvc annotations: {} ## Specify an existing volume claim instead of creating a new one. ## When using this option all following options like storageClass, accessMode and size are ignored. # existingClaim: ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026#34;-\u0026#34;, storageClassName: \u0026#34;\u0026#34;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: accessMode: ReadWriteOnce size: 10Gi ## Specify extra volumes. Refer to \u0026#34;.spec.volumes\u0026#34; specification : https://kubernetes.io/fr/docs/concepts/storage/volumes/ volumes: [] ## Specify extra mounts. Refer to \u0026#34;.spec.containers.volumeMounts\u0026#34; specification : https://kubernetes.io/fr/docs/concepts/storage/volumes/ mounts: [] # In case you want to specify different resources for emptyDir than {} emptyDir: {} # Example of resouces that might be used: # medium: Memory # sizeLimit: 16Mi # List of plugins to install. # For example: # plugins: # install: # - \u0026#34;https://github.com/AmadeusITGroup/sonar-stash/releases/download/1.3.0/sonar-stash-plugin-1.3.0.jar\u0026#34; # - \u0026#34;https://github.com/SonarSource/sonar-ldap/releases/download/2.2-RC3/sonar-ldap-plugin-2.2.0.601.jar\u0026#34; plugins: install: [] lib: [] # For use behind a corporate proxy when downloading plugins # httpProxy: \u0026#34;\u0026#34; # httpsProxy: \u0026#34;\u0026#34; # initContainerImage: rjkernick/alpine-wget:latest # initSysctlContainerImage: busybox:1.31 # initCertsContainerImage: adoptopenjdk/openjdk11:alpine # initTestContainerImage: dduportal/bats:0.4.0 # deleteDefaultPlugins: true resources: {} # We allow the plugins init container to have a separate resources declaration because # the initContainer does not take as much resources. # A custom sonar.properties file can be provided via dictionary. # For example: # sonarProperties: # sonar.forceAuthentication: true # sonar.security.realm: LDAP # ldap.url: ldaps://organization.com # Additional sonar properties to load from a secret with a key \u0026#34;secret.properties\u0026#34; (must be a string) # sonarSecretProperties: # Kubernetes secret that contains the encryption key for the sonarqube instance. # The secret must contain the key \u0026#39;sonar-secret.txt\u0026#39;. # The \u0026#39;sonar.secretKeyPath\u0026#39; property will be set automatically. # sonarSecretKey: \u0026#34;settings-encryption-secret\u0026#34; ## JDBC Database Type; by default postgresql. To use a different Database type, adjust jdbcDatabaseType: postgresql ## Override JDBC URL # jdbcUrlOverride: \u0026#34;jdbc:postgresql://myPostgress/myDatabase;socketTimeout=1500\u0026#34; ## Configuration values for postgresql dependency ## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md postgresql: # Enable to deploy the PostgreSQL chart enabled: true # To use an external PostgreSQL instance, set enabled to false and uncomment # the line below: # postgresqlServer: \u0026#34;\u0026#34; # To use an external secret for the password for an external PostgreSQL # instance, set enabled to false and provide the name of the secret on the # line below: # existingSecret: \u0026#34;\u0026#34; postgresqlUsername: \u0026#34;sonarUser\u0026#34; postgresqlPassword: \u0026#34;sonarPass\u0026#34; postgresqlDatabase: \u0026#34;sonarDB\u0026#34; # Specify the TCP port that PostgreSQL should use service: port: 5432 # Additional labels to add to the pods: # podLabels: # key: value podLabels: {} # For compatibility with 8.0 replace by \u0026#34;/opt/sq\u0026#34; # For compatibility with 8.2, leave the default. They changed it back to /opt/sonarqube sonarqubeFolder: /opt/sonarqube enableTests: true serviceAccount: create: false # name: ## Annotations for the Service Account annotations: {} # extraConfig is used to load Environment Variables from secrets and configmaps # which may have been written by other tools, such as external orchestrators. # # These Secrets/ ConfigMaps are expecetd to contain Key/ Value pairs, such as: # # apiVersion: v1 # kind: ConfigMap # metadata: # name: external-sonarqube-opts # data: # SONARQUBE_JDBC_USERNAME: foo # SONARQUBE_JDBC_URL: jdbc:postgresql://db.example.com:5432/sonar # # These vars can then be injected into the environment by uncommenting the following: # # extraConfig: # configmaps: # - external-sonarqube-opts extraConfig: secrets: [] configmaps: [] EOF install helm chart, m√¨nh s·∫Ω ƒë·∫∑t t√™n chart l√† sonar, c√†i v√†o namespace sonarqube:\nhelm repo add oteemocharts https://oteemo.github.io/charts helm install --name sonar oteemocharts/sonarqube --namespace sonarqube --values sonarqube-values.yaml Gi·∫£ s·ª≠ n·∫øu b·∫°n mu·ªën upgrade helm chart tr√™n:\nhelm upgrade sonar oteemocharts/sonarqube --namespace sonarqube -f sonarqube-values.yaml N·∫øu mu·ªën delete chart tr√™n:\nhelm delete --purge sonar logs:\nk get pods,svc,ing -n sonarqube NAME READY STATUS RESTARTS AGE pod/sonar-postgresql-0 1/1 Running 0 6m36s pod/sonar-sonarqube-6cf557759c-6zlqf 1/1 Running 0 47h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sonar-postgresql ClusterIP 10.68.81.62 \u0026lt;none\u0026gt; 5432/TCP 2d18h service/sonar-postgresql-headless ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 2d18h service/sonar-sonarqube ClusterIP 10.68.76.133 \u0026lt;none\u0026gt; 9000/TCP 2d18h NAME HOSTS ADDRESS PORTS AGE ingress.extensions/sonar-sonarqube sonarqube.your-subdomain.your-domain.net 10.0.16.6 80, 443 2d18h V√†o link sonarqube.your-subdomain.your-domain.net s·∫Ω th·∫•y c√≥ HTTPS do m√¨nh s·ª≠ d·ª•ng TLS ƒë√£ ƒëc issue th√†nh c√¥ng t·ª´ tr∆∞·ªõc:\nƒêƒÉng nh·∫≠p v√†o Sonarqube v·ªõi account default l√† admin/admin r·ªìi ƒë·ªïi pass cho Admin nh√©\n3. Setup Maven project using Spring Boots v√†o cluster Gi·ªù m√¨nh s·∫Ω t·∫°o 1 app microservices g·ªìm 2 services:\n spring app, postgres  B·∫Øt ƒë·∫ßu n√†o!\nT·∫°o namespace cho app:\nkubectl create ns spring-app ƒê√°nh label app=kubed cho namespace spring-app:\nkubectl label namespace spring-app app=kubed Annotate ƒë·ªÉ secret m√¨nh mu·ªën s·∫Ω sync sang c√°c namespace c√≥ label app=kubed:\nkubectl annotate secret ${TLS_SECRET_NAME} -n default kubed.appscode.com/sync=\u0026#34;app=kubed\u0026#34; Annotate domain s·∫Ω d√πng cho SpringApp v√†o Ingress nginx ƒë√£ t·∫°o:\nkubectl annotate service nginx-ingress-controller \u0026#34;external-dns.alpha.kubernetes.io/hostname=${SPRINGAPP_SUBDOMAIN}.,${SONAR_SUBDOMAIN}.,${SUBDOMAIN}.\u0026#34; -n nginx-ingress --overwrite Tr√™n Cloud DNS s·∫Ω th·∫•y hi·ªán ra A record m·ªõi cho springapp.your-subdomain.your-domain.net,\nconfirm r·∫±ng n√≥ d√πng chung 1 IP v·ªõi $SPRINGAPP_SUBDOMAIN v√† $SUBDOMAIN l√† ok!\nclone project sau v·ªÅ t·ª´ Github:\ngit clone https://github.com/hoangmnsd/spring-maven-postgres-docker cd spring-maven-postgres-docker # t·∫°o folder resource-manifest ch·ª©a k8s template mkdir -p resource-manifest \u0026amp;\u0026amp; cd resource-manifest ###1 t·∫°o file docker_postgres-deployment.yaml:\nCh√∫ √Ω file n√†y m√¨nh s·ª≠ d·ª•ng image image: hoangmnsd/docker_postgres l√† image tr√™n DockerHub c·ªßa m√¨nh, c√°c b·∫°n n√™n tham kh·∫£o b√†i K8S 4 n√†y ƒë·ªÉ t·ª± t·∫°o image ri√™ng c·ªßa c√°c b·∫°n:\nK8S 4\ncat \u0026gt; ./docker_postgres-deployment.yaml \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: docker-postgres labels: app: docker-postgres spec: selector: matchLabels: app: docker-postgres replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-postgres spec: containers: - image: hoangmnsd/docker_postgres env: - name: POSTGRES_USER value: dbuser - name: POSTGRES_PASSWORD value: password - name: POSTGRES_DB value: store imagePullPolicy: Always name: docker-postgres ports: - containerPort: 5432 EOF ###2 t·∫°o file docker_postgres-service.yaml:\ncat \u0026gt; ./docker_postgres-service.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: docker-postgres spec: ports: - port: 5432 protocol: TCP targetPort: 5432 selector: app: docker-postgres EOF ###3 t·∫°o file docker_spring-boot-containers-deployment.yaml:\nCh√∫ √Ω file n√†y m√¨nh s·ª≠ d·ª•ng image image: hoangmnsd/docker_spring-boot-containers l√† image tr√™n DockerHub c·ªßa m√¨nh, c√°c b·∫°n n√™n tham kh·∫£o b√†i K8S 4 n√†y ƒë·ªÉ t·ª± t·∫°o image ri√™ng c·ªßa c√°c b·∫°n:\nK8S 4\ncat \u0026gt; ./docker_spring-boot-containers-deployment.yaml \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: docker-spring-boot-containers labels: app: docker-spring-boot-containers spec: selector: matchLabels: app: docker-spring-boot-containers replicas: 1 minReadySeconds: 15 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: docker-spring-boot-containers spec: containers: - image: hoangmnsd/docker_spring-boot-containers imagePullPolicy: Always name: docker-spring-boot-containers ports: - containerPort: 12345 EOF ###4 t·∫°o file docker_spring-boot-containers-service.yaml:\ncat \u0026gt; ./docker_spring-boot-containers-service.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: docker-spring-boot-containers spec: ports: - port: 80 protocol: TCP targetPort: 12345 selector: app: docker-spring-boot-containers EOF ###5 t·∫°o file docker_spring-boot-containers-ingress.yaml:\nFile n√†y s·∫Ω s·ª≠ d·ª•ng ${SPRINGAPP_SUBDOMAIN} v√† ${TLS_SECRET_NAME}\ncat \u0026gt; ./docker_spring-boot-containers-ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: spring-app-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: tls: - hosts: - ${SPRINGAPP_SUBDOMAIN} secretName: ${TLS_SECRET_NAME} rules: - host: ${SPRINGAPP_SUBDOMAIN} http: paths: - path: / backend: serviceName: docker-spring-boot-containers servicePort: 12345 EOF V·ª´a r·ªìi ch√∫ng ta ƒë√£ t·∫°o xong 5 K8S template c·∫ßn thi·∫øt,\nGi·ªù s·∫Ω apply ƒë·ªÉ t·∫°o c√°c k8s resource c·ªßa SpringApp tr√™n:\nkubectl apply -n spring-app -f docker_postgres-deployment.yaml kubectl apply -n spring-app -f docker_postgres-service.yaml kubectl apply -n spring-app -f docker_spring-boot-containers-deployment.yaml kubectl apply -n spring-app -f docker_spring-boot-containers-service.yaml kubectl apply -n spring-app -f docker_spring-boot-containers-ingress.yaml B·∫°n s·∫Ω th·∫•y trong namespace spring-app ƒë√£ t·∫°o ra c√°c resource sau:\nk get pods,svc,ing -n spring-app NAME READY STATUS RESTARTS AGE pod/docker-postgres-cddbcc96f-84smg 1/1 Running 0 2d pod/docker-spring-boot-containers-6fbcdb989c-cjj5b 1/1 Running 0 2d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/docker-postgres ClusterIP 10.68.68.157 \u0026lt;none\u0026gt; 5432/TCP 3d4h service/docker-spring-boot-containers ClusterIP 10.68.242.118 \u0026lt;none\u0026gt; 80/TCP 3d6h NAME HOSTS ADDRESS PORTS AGE ingress.extensions/spring-app-ingress springapp.your-subdomain.your-domain.net 10.0.16.6 80, 443 3d4h Test SpringApp c√≥ ho·∫°t ƒë·ªông ƒë√∫ng ko? S·ª≠ d·ª•ng Postman, g·ª≠i POST request ƒë·∫øn https://springapp.your-subdomain.your-domain.net/v1/product v·ªõi body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} Ch√∫ √Ω l√† body c·ªßa request c·∫ßn set l√† raw v√† JSON: (·∫£nh)\nif success, Postman s·∫Ω tr·∫£ v·ªÅ response d·∫°ng n√†y:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link https://springapp.your-subdomain.your-domain.net/v1/product/product001\nif success browser s·∫Ω tr·∫£ v·ªÅ chu·ªói n√†y:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n(·∫£nh)\n4. Setup Gitlab Runner L√†m theo b√†i n√†y, ƒë·ªÉ t·∫°o ra ƒë∆∞·ª£c Gitlab Runner th√¥i nh√©:\nK8S 13\nCh√∫ √Ω l√† GITLAB_URL th√¨ ƒë·ªÉ l√† gitlab.com, cho ƒë·ª° t·ªën ti·ªÅn, v√¨ d·ª±ng c·∫£ Gitlab self-hosted s·∫Ω t·ªën nhi·ªÅu k8s resource\n5. Generate a token of Sonarqube Login v√†o Sonarqube, t·∫°o token c·ªßa User nh∆∞ ·∫£nh sau l√† ok:\ngi·∫£ s·ª≠ b·∫°n ƒë√£ copy ƒë∆∞·ª£c sonar-token nh∆∞ sau: fd059f3efd39debc0b205d4cae22c57d00be42d5\nexport SONAR_TOKEN=\u0026#34;fd059f3efd39debc0b205d4cae22c57d00be42d5\u0026#34; 6. Create empty project on Gitlab and push source from local Login v√†o gitlab.com, t·∫°o 1 project r·ªóng t√™n l√† spring-maven-postgres-docker-k8s:\nC√≤n nh·ªõ ·ªü b∆∞·ªõc 3, ch√∫ng ta ƒë√£ clone 1 project spring-maven-postgres-docker t·ª´ Github v·ªÅ CloudShell,\ngi·ªù s·∫Ω push n·ªôi dung trong project ƒë√≥ l√™n project c·ªßa Gitlab nh√©:\ncd spring-maven-postgres-docker # thay \u0026lt;GITLAB-USER\u0026gt; b·∫±ng user c·ªßa b·∫°n git remote origin set-url https://gitlab.com/\u0026lt;GITLAB-USER\u0026gt;/spring-maven-postgres-docker-k8s.git git add . git commit -m \u0026#34;add files\u0026#34; git push origin master T·∫°o file gitlab-ci.yml:\ncd spring-maven-postgres-docker cat \u0026gt; ./gitlab-ci.yml \u0026lt;\u0026lt;EOF image: docker:latest services: - docker:18.09.7-dind variables: # DOCKER_HOST: tcp://127.0.0.1:2375 # When using Gitlab self-hosted on K8s DOCKER_HOST: tcp://docker:2375/ # When using Gitlab.com DOCKER_DRIVER: overlay DOCKER_API_VERSION: \u0026#34;1.39\u0026#34; SONAR_URL: \u0026#34;${SONAR_SUBDOMAIN}\u0026#34; SONAR_TOKEN: \u0026#34;${SONAR_TOKEN}\u0026#34; stages: - test - build - publish sonarqube_master_job: stage: test only: - master image: maven:3.3.9-jdk-8-alpine script: # - mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_TOKEN - mvn clean package sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_TOKEN build_artifacts: stage: build only: - master image: maven:3.3.9-jdk-8-alpine script: - mvn clean package artifacts: paths: - target/*.jar publish_docker_images: stage: publish only: - master # before_script: # # Login to Google Cloud Registry # - echo $GCLOUD_SERVICE_KEY | docker login -u _json_key --password-stdin https://gcr.io script: # # Build docker images from `Dockerfile` and artifact of above steps `build-artifacts` - docker build -t springapp . # - docker tag $IMAGE_NAME \u0026#34;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME\u0026#34; # - docker push \u0026#34;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:latest\u0026#34; EOF push thay ƒë·ªïi l√™n Gitlab v√† check xem pipeline c·ªßa Gitlab CI c√≥ ho·∫°t ƒë·ªông t·ªët kh√¥ng n√†o.\ngit add . git commit -m \u0026#34;update gitlab-ci\u0026#34; git push origin master Nh∆∞ h√¨nh sau l√† OK, c·∫£ 3 stage ƒë·ªÅu passed c√≥ th·ªÉ ·∫•n v√†o xem log t·ª´ng job:\njob sonarqube ch·∫°y ok v√† log ra nh∆∞ n√†y: V·∫≠y l√† xong, c√≥ th·ªÉ c√≤n nhi·ªÅu c√°ch s√°ng t·∫°o v√† k·∫øt h·ª£p kh√°c nhau, nh∆∞ng t√≥m l·∫°i c√°ch setup v√† s·ª≠ d·ª•ng Sonarqube cho project c·ªßa b·∫°n c≈©ng ko kh√≥ khƒÉn l·∫Øm ƒë√¢u, ch√∫c b·∫°n th√†nh c√¥ng ^^\nREFERENCES https://medium.com/@speedforcerun/sonarqube-with-gitlab-ci-setup-stepbystep-java-maven-version-7e131dce0bb1\nhttps://stackoverflow.com/questions/48323223/sonarqube-autorun-with-gitlab\nhttps://github.com/Oteemo/charts/tree/master/charts/sonarqube\n","href":"/posts/k8s-xiv-on-gke-cluster-setup-sonarqube-for-maven-project-with-gitlabci/","title":"K8S 14: (on GKE Cluster) Setup Sonarqube for Maven Project With GitlabCI"},{"content":"","href":"/tags/sonarqube/","title":"Sonarqube"},{"content":"Gi·ªõi thi·ªáu C√°c b√†i tr∆∞·ªõc th√¨ m√¨nh ƒë√£ n√≥i ƒë·∫øn CD (Continuous Deployment) r·ªìi, gi·ªù chuy·ªÉn sang CI (Continuous Integration)\nNh·∫Øc ƒë·∫øn tool v·ªÅ CI th√¨ c√≥ nhi·ªÅu, v√≠ d·ª• nh∆∞ Jenkins, Travis CI, Circle CI, Gitlab CI,\u0026hellip; etc.\nH√¥m nay m√¨nh s·∫Ω gi·ªõi thi·ªáu v·ªÅ Gitlab CI\nM·ª•c ƒë√≠ch c·ªßa tutorial n√†y l√†:\nSetup 1 project sample ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng Gitlab CI, push Docker image l√™n Google Container Registry (GCR).\nTrong ƒë√≥ th√¨ Gitlab server c√≥ th·ªÉ l√†:\n m·ªôt l√† Gitlab t·ª± d·ª±ng (Gitlab self-hosted) tr√™n GKE (kubernetes) ho·∫∑c l√† b·∫°n d√πng lu√¥n gitlab.com cho ƒë·ª° t·ªën c√¥ng t·∫°o Gitlab self-hosted  Y√™u c·∫ßu  N·∫øu mu·ªën s·ª≠ d·ª•ng Gitlab t·ª± d·ª±ng (Gitlab self-hosted), b·∫°n c√≥ th·ªÉ l√†m theo b√†i sau (N·∫øu b·∫°n ƒë√£ c√≥ Gitlab server ri√™ng (Gitlab self-hosted) th√¨ c√≥ th·ªÉ b·ªè qua):\nK8S 10  Sau khi l√†m theo link tr√™n th√¨ ch√∫ng ta c√≥ 1 Gitlab server ri√™ng ·ªü domain nh∆∞ sau:\nhttps://gitlab.your-subdomain.your-domain.net/\n N·∫øu mu·ªën s·ª≠ d·ª•ng gitlab.com th√¨ ko c·∫ßn l√†m theo link tr√™n, ch·ªâ c·∫ßn b·∫°n ƒë√£ t·∫°o account tr√™n gitlab.com l√† ƒë∆∞·ª£c\n  ƒê√£ quen v·ªõi vi·ªác s·ª≠ d·ª•ng GCP Service, CloudShell, GKE\u0026hellip;\n  C√°ch l√†m ƒê·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng Gitlab CI th√¨ b·∫°n c·∫ßn setup Gitlab Runner (ƒê√¢y l√† th·ª© d√πng ƒë·ªÉ ch·∫°y test/build/deploy source code cho b·∫°n)\nV√† b·∫°n n√™n setup n√≥ ri√™ng bi·ªát v·ªõi Gitlab Server nh√©. (ki·ªÉu nh∆∞ Gitlab Runner s·∫Ω ch·∫°y trong 1 namespace ri√™ng, hay cluster ri√™ng ch·∫≥ng h·∫°n), b√†i n√†y m√¨nh d·ª±ng Gitlab Runner trong 1 namespace ri√™ng l√† gitlab-runner.\n1. Chu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng trong CloudShell # N·∫øu d√πng Gitlab t·ª± d·ª±ng (Gitlab self-hosted) th√¨ s·ª≠ d·ª•ng command: export GITLAB_URL=\u0026#34;https://gitlab.your-subdomain.your-domain.net/\u0026#34; # N·∫øu d√πng gitlab.com th√¨ s·ª≠ d·ª•ng command: export GITLAB_URL=\u0026#34;https://gitlab.com/\u0026#34; gcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a 2. T·∫°o project tr√™n Gitlab ƒë·ªÉ test ·ªû ƒë√¢y m√¨nh s·∫Ω t·∫°o 1 project r·ªóng t√™n l√† gitlabci-demo:\n3. T·∫°o service account Ti·∫øp theo v√†o Google Cloud Console, t·∫°o service account gitlab-ci, l·∫•y file json key, save l·∫°i ƒë√¢u ƒë√≥ b√≠ m·∫≠t nh√©:\nfile json key c·ªßa b·∫°n nh·∫≠n ƒë∆∞·ª£c c√≥ d·∫°ng nh∆∞ n√†y:\n4. Add variable v√†o Gitlab project Quay l·∫°i Gitlab project gitlabci-demo, add th√™m variable GCLOUD_SERVICE_KEY, value s·∫Ω l√† n·ªôi dung c·ªßa file json key ƒë√£ l·∫•y dc ·ªü step tr√™n, nh∆∞ h√¨nh sau:\nT∆∞∆°ng t·ª± nh∆∞ v·∫≠y, ch√∫ng ta ti·∫øp t·ª•c t·∫°o th√™m 2 variable n·ªØa l√† :\nGCLOUD_PROJECT_ID (l√† Google Cloud Project ID c·ªßa b·∫°n) v√†\nIMAGE_NAME (l√† t√™n c·ªßa images m√† b·∫°n s·∫Ω mu·ªën t·∫°o, c√°i n√†y v√≠ d·ª• m√¨nh s·∫Ω ƒë·ªÉ value l√† helloapp).\nCh√∫ng ta c√≥ k·∫øt qu·∫£ sau khi add 3 variable nh∆∞ sau:\n5. Get runner registration token Ti·∫øp theo l·∫•y runner registration token ƒë·ªÉ sau n√†y install Gitlab Runner, v√†o ph·∫ßn n√†y ƒë·ªÉ l·∫•y: trong CloudShell h√£y export token ƒë√≥ v√†o bi·∫øn RUNNER_REG_TOKEN nh∆∞ sau:\nexport RUNNER_REG_TOKEN=\u0026#34;_MBMCAYPWyeao5hf_2iH\u0026#34; 6. Install Gitlab Runner Helm Chart Trong Cloudshell, T·∫°o file gitlab-runner-values.yaml:\ncat \u0026gt; ./gitlab-runner-values.yaml \u0026lt;\u0026lt;EOF ## The GitLab Server URL (with protocol) that want to register the runner against ## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-register ## gitlabUrl: ${GITLAB_URL} ## The registration token for adding new Runners to the GitLab server. This must ## be retrieved from your GitLab instance. ## ref: https://docs.gitlab.com/ee/ci/runners/ ## runnerRegistrationToken: \u0026#34;${RUNNER_REG_TOKEN}\u0026#34; ## Set the certsSecretName in order to pass custom certificates for GitLab Runner to use ## Provide resource name for a Kubernetes Secret Object in the same namespace, ## this is used to populate the /etc/gitlab-runner/certs directory ## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates ## #certsSecretName: ## Configure the maximum number of concurrent jobs ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## concurrent: 10 ## Defines in seconds how often to check GitLab for a new builds ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## checkInterval: 30 ## For RBAC support: rbac: create: true ## Define specific rbac permissions. resources: [\u0026#34;pods\u0026#34;, \u0026#34;pods/exec\u0026#34;, \u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] ## Run the gitlab-bastion container with the ability to deploy/manage containers of jobs ## cluster-wide or only within namespace clusterWideAccess: false ## If RBAC is disabled in this Helm chart, use the following Kubernetes Service Account name. ## # serviceAccountName: default ## Configuration for the Pods that the runner launches for each new job ## runners: ## Default container image to use for builds when none is specified ## image: ubuntu:18.04 ## Run all containers with the privileged flag enabled ## This will allow the docker:stable-dind image to run if you need to run Docker ## commands. Please read the docs before turning this on: ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## privileged: true ## Namespace to run Kubernetes jobs in (defaults to \u0026#39;default\u0026#39;) ## namespace: gitlab-runner ## Build Container specific configuration ## builds: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Service Container specific configuration ## services: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Helper Container specific configuration ## helpers: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi EOF Sau ƒë√≥ th√¨ install Gitlab Runner Helm chart b·∫±ng file gitlab-runner-values.yaml v·ª´a t·∫°o:\nhelm repo add gitlab https://charts.gitlab.io helm init helm install --namespace gitlab-runner --name gitlab-runner gitlab/gitlab-runner -f gitlab-runner-values.yaml Gi·∫£ s·ª≠ t∆∞∆°ng lai b·∫°n c√≥ thay ƒë·ªïi g√¨ file values v√† mu·ªën update l·∫°i c√°i helm chart Gitlab Runner th√¨ d√πng command sau:\nhelm upgrade --namespace gitlab-runner --name gitlab-runner gitlab/gitlab-runner -f gitlab-runner-values.yaml N·∫øu ch·∫°y th√†nh c√¥ng, 1 pod s·∫Ω ƒë∆∞·ª£c t·∫°o trong namespace gitlab-runner nh∆∞ n√†y:\nk get pods,svc,ing -n gitlab-runner NAME READY STATUS RESTARTS AGE pod/gitlab-runner-gitlab-runner-5876c44c67-q5bs5 1/1 Running 0 66s 7. t·∫°o sample app helloapp v√† config .gitlab-ci.yml ƒê·∫ßu ti√™n clone project r·ªóng gitlabci-demo m√† ch√∫ng ta ƒë√£ t·∫°o ban ƒë·∫ßu v·ªÅ CloudShell:\ngit clone https://gitlab.your-subdomain.your-domain.net/root/gitlabci-demo.git cd gitlabci-demo t·∫°o file main.go l√† 1 app ƒë∆°n gi·∫£n, n·ªôi dung nh∆∞ sau:\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) var version = 1 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;Hello ‰∏ñÁïå... from v %s\u0026quot;, version) } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, handler) log.Fatal(http.ListenAndServe(\u0026quot;:8888\u0026quot;, nil)) } t·∫°o Dockerfile n·ªôi dung nh∆∞ sau:\nFROM golang:1.11-alpine AS build WORKDIR / COPY main.go go.* / RUN CGO_ENABLED=0 go build -o /bin/demo FROM scratch COPY --from=build /bin/demo /bin/demo ENTRYPOINT [\u0026quot;/bin/demo\u0026quot;] t·∫°o file .gitlab-ci.yml n·ªôi dung nh∆∞ sau:\nimage: docker:latest services: - docker:18.09.7-dind variables: DOCKER_HOST: tcp://127.0.0.1:2375 # When using Gitlab self-hosted on K8s # DOCKER_HOST: tcp://docker:2375/ # When using Gitlab.com DOCKER_DRIVER: overlay DOCKER_API_VERSION: \u0026quot;1.39\u0026quot; stages: - publish publish: stage: publish before_script: # Login to Google Cloud Registry - echo $GCLOUD_SERVICE_KEY | docker login -u _json_key --password-stdin https://gcr.io script: - docker build -t $IMAGE_NAME . - docker tag $IMAGE_NAME \u0026quot;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME\u0026quot; - docker push \u0026quot;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:latest\u0026quot; only: - master gi·∫£i th√≠ch: ·ªü ƒë√¢y m√¨nh ch·ªâ t·∫°o 1 step publish, step bao g·ªìm build Docker image, tagging v√† push image ƒë√≥ l√™n GCR\n8. Push local project l√™n Gitlab Server Gi·ªù push t·∫•t c·∫£ nh·ªØng file ƒë√£ t·∫°o th√™m l√™n Gitlab Server:\ngit add . git commit -m \u0026#34;push app\u0026#34; git push origin master 9. K·∫øt qu·∫£ k·∫øt qu·∫£ nh∆∞ sau, Gitlab s·∫Ω detect ƒë∆∞·ª£c project c·ªßa b·∫°n c√≥ .gitlab-ci.yml n√™n n√≥ s·∫Ω trigger Gitlab Runner th·ª±c hi·ªán c√°c step trong file .gitlab-ci.yml ƒë√≥.\nB·∫°n c√≥ th·ªÉ check k·∫øt qu·∫£ khi v√†o c√°c m√†n h√¨nh sau:\nV√†o Google Container Registry Service s·∫Ω th·∫•y image helloapp ƒë√£ ƒë∆∞·ª£c push l√™n nh∆∞ sau:\nDONE! üéâüéâ\nK·∫øt h·ª£p v·ªõi c√°c b√†i v·ªÅ CD tr∆∞·ªõc th√¨ b·∫°n ƒë√£ c√≥ 1 flow CI/CD ho√†n ch·ªânh r·ªìi üòç\n10. (Bonus) T√πy bi·∫øn th√™m v·ªõi file .gitlab-ci.yml image: docker:latest services: - docker:18.09.7-dind variables: DOCKER_HOST: tcp://127.0.0.1:2375 # When using Gitlab self-hosted on K8s # DOCKER_HOST: tcp://docker:2375/ # When using Gitlab.com DOCKER_DRIVER: overlay DOCKER_API_VERSION: \u0026quot;1.39\u0026quot; TEST_IMAGE: gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA RELEASE_IMAGE: gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:latest-$CI_COMMIT_SHORT_SHA stages: - build # - test - release before_script: # Login to Google Cloud Registry - echo $GCLOUD_SERVICE_KEY | docker login -u _json_key --password-stdin https://gcr.io build: stage: build script: - docker build -t $TEST_IMAGE . - docker push $TEST_IMAGE # test: # stage: test # script: # - docker pull $TEST_IMAGE # - docker run $TEST_IMAGE npm test release: stage: release script: - docker pull $TEST_IMAGE - docker tag $TEST_IMAGE $RELEASE_IMAGE - docker push $RELEASE_IMAGE only: - master REFERENCES https://docs.gitlab.com/runner/install/kubernetes.html\nhttps://docs.gitlab.com/ee/ci/runners/\nhttp://www.idevops.site/gitlabci/chapter04/01/5/\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/issues/25803\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/issues/2623\nhttps://gitlab.com/gitlab-org/charts/gitlab/-/issues/478\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/blob/master/docs/executors/kubernetes.md#using-dockerdind\nhttps://gitlab.com/gitlab-org/gitlab-foss/-/issues/34419\nhttps://stackoverflow.com/questions/36408339/best-cd-strategy-for-kubernetes-deployments/48268573#48268573\nhttps://stackoverflow.com/questions/47888027/how-to-deploy-staging-in-google-cloud-platform-with-kubernetes-and-gitlab-ci-cd\nhttps://www.digitalocean.com/community/tutorials/how-to-build-docker-images-and-host-a-docker-image-repository-with-gitlab\nhttps://github.com/JanMikes/gitlab-ci-push-to-gcr/blob/master/.gitlab-ci.yml\nhttps://rhazn.com/posts/build-a-docker-image-on-gitlab-ci-and-publish-it-to-google-container-registry/\nhttps://gist.github.com/foklepoint/2f9087375830068ec032ef326d93f423\nhttps://medium.com/@gaforres/publishing-google-cloud-container-registry-images-from-gitlab-ci-23c45356ff0e\nhttps://qiita.com/proudust/items/d94c60ec69dead927954\nhttps://docs.gitlab.com/ee/ci/variables/\n","href":"/bk/k8s-xiii-using-gitlab-ci-push-docker-image-to-gcr-on-gke/","title":"K8S 13: Using Gitlab CI on GKE Cluster - Push Docker Image to GCR for Continuous Integration (CI)"},{"content":"Gi·ªõi thi·ªáu C√°c b√†i tr∆∞·ªõc th√¨ m√¨nh ƒë√£ n√≥i ƒë·∫øn CD (Continuous Deployment) r·ªìi, gi·ªù chuy·ªÉn sang CI (Continuous Integration)\nNh·∫Øc ƒë·∫øn tool v·ªÅ CI th√¨ c√≥ nhi·ªÅu, v√≠ d·ª• nh∆∞ Jenkins, Travis CI, Circle CI, Gitlab CI,\u0026hellip; etc.\nH√¥m nay m√¨nh s·∫Ω gi·ªõi thi·ªáu v·ªÅ Gitlab CI\nM·ª•c ƒë√≠ch c·ªßa tutorial n√†y l√†:\nSetup 1 project sample ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng Gitlab CI, push Docker image l√™n Google Container Registry (GCR).\nTrong ƒë√≥ th√¨ Gitlab server c√≥ th·ªÉ l√†:\n m·ªôt l√† Gitlab t·ª± d·ª±ng (Gitlab self-hosted) tr√™n GKE (kubernetes) ho·∫∑c l√† b·∫°n d√πng lu√¥n gitlab.com cho ƒë·ª° t·ªën c√¥ng t·∫°o Gitlab self-hosted  Y√™u c·∫ßu  N·∫øu mu·ªën s·ª≠ d·ª•ng Gitlab t·ª± d·ª±ng (Gitlab self-hosted), b·∫°n c√≥ th·ªÉ l√†m theo b√†i sau (N·∫øu b·∫°n ƒë√£ c√≥ Gitlab server ri√™ng (Gitlab self-hosted) th√¨ c√≥ th·ªÉ b·ªè qua):\nK8S 10  Sau khi l√†m theo link tr√™n th√¨ ch√∫ng ta c√≥ 1 Gitlab server ri√™ng ·ªü domain nh∆∞ sau:\nhttps://gitlab.your-subdomain.your-domain.net/\n N·∫øu mu·ªën s·ª≠ d·ª•ng gitlab.com th√¨ ko c·∫ßn l√†m theo link tr√™n, ch·ªâ c·∫ßn b·∫°n ƒë√£ t·∫°o account tr√™n gitlab.com l√† ƒë∆∞·ª£c\n  ƒê√£ quen v·ªõi vi·ªác s·ª≠ d·ª•ng GCP Service, CloudShell, GKE\u0026hellip;\n  C√°ch l√†m ƒê·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng Gitlab CI th√¨ b·∫°n c·∫ßn setup Gitlab Runner (ƒê√¢y l√† th·ª© d√πng ƒë·ªÉ ch·∫°y test/build/deploy source code cho b·∫°n)\nV√† b·∫°n n√™n setup n√≥ ri√™ng bi·ªát v·ªõi Gitlab Server nh√©. (ki·ªÉu nh∆∞ Gitlab Runner s·∫Ω ch·∫°y trong 1 namespace ri√™ng, hay cluster ri√™ng ch·∫≥ng h·∫°n), b√†i n√†y m√¨nh d·ª±ng Gitlab Runner trong 1 namespace ri√™ng l√† gitlab-runner.\n1. Chu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng trong CloudShell # N·∫øu d√πng Gitlab t·ª± d·ª±ng (Gitlab self-hosted) th√¨ s·ª≠ d·ª•ng command: export GITLAB_URL=\u0026#34;https://gitlab.your-subdomain.your-domain.net/\u0026#34; # N·∫øu d√πng gitlab.com th√¨ s·ª≠ d·ª•ng command: export GITLAB_URL=\u0026#34;https://gitlab.com/\u0026#34; gcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a 2. T·∫°o project tr√™n Gitlab ƒë·ªÉ test ·ªû ƒë√¢y m√¨nh s·∫Ω t·∫°o 1 project r·ªóng t√™n l√† gitlabci-demo:\n3. T·∫°o service account Ti·∫øp theo v√†o Google Cloud Console, t·∫°o service account gitlab-ci, l·∫•y file json key, save l·∫°i ƒë√¢u ƒë√≥ b√≠ m·∫≠t nh√©:\nfile json key c·ªßa b·∫°n nh·∫≠n ƒë∆∞·ª£c c√≥ d·∫°ng nh∆∞ n√†y:\n4. Add variable v√†o Gitlab project Quay l·∫°i Gitlab project gitlabci-demo, add th√™m variable GCLOUD_SERVICE_KEY, value s·∫Ω l√† n·ªôi dung c·ªßa file json key ƒë√£ l·∫•y dc ·ªü step tr√™n, nh∆∞ h√¨nh sau:\nT∆∞∆°ng t·ª± nh∆∞ v·∫≠y, ch√∫ng ta ti·∫øp t·ª•c t·∫°o th√™m 2 variable n·ªØa l√† :\nGCLOUD_PROJECT_ID (l√† Google Cloud Project ID c·ªßa b·∫°n) v√†\nIMAGE_NAME (l√† t√™n c·ªßa images m√† b·∫°n s·∫Ω mu·ªën t·∫°o, c√°i n√†y v√≠ d·ª• m√¨nh s·∫Ω ƒë·ªÉ value l√† helloapp).\nCh√∫ng ta c√≥ k·∫øt qu·∫£ sau khi add 3 variable nh∆∞ sau:\n5. Get runner registration token Ti·∫øp theo l·∫•y runner registration token ƒë·ªÉ sau n√†y install Gitlab Runner, v√†o ph·∫ßn n√†y ƒë·ªÉ l·∫•y: trong CloudShell h√£y export token ƒë√≥ v√†o bi·∫øn RUNNER_REG_TOKEN nh∆∞ sau:\nexport RUNNER_REG_TOKEN=\u0026#34;_MBMCAYPWyeao5hf_2iH\u0026#34; 6. Install Gitlab Runner Helm Chart Trong Cloudshell, T·∫°o file gitlab-runner-values.yaml:\ncat \u0026gt; ./gitlab-runner-values.yaml \u0026lt;\u0026lt;EOF ## The GitLab Server URL (with protocol) that want to register the runner against ## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-register ## gitlabUrl: ${GITLAB_URL} ## The registration token for adding new Runners to the GitLab server. This must ## be retrieved from your GitLab instance. ## ref: https://docs.gitlab.com/ee/ci/runners/ ## runnerRegistrationToken: \u0026#34;${RUNNER_REG_TOKEN}\u0026#34; ## Set the certsSecretName in order to pass custom certificates for GitLab Runner to use ## Provide resource name for a Kubernetes Secret Object in the same namespace, ## this is used to populate the /etc/gitlab-runner/certs directory ## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates ## #certsSecretName: ## Configure the maximum number of concurrent jobs ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## concurrent: 10 ## Defines in seconds how often to check GitLab for a new builds ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## checkInterval: 30 ## For RBAC support: rbac: create: true ## Define specific rbac permissions. resources: [\u0026#34;pods\u0026#34;, \u0026#34;pods/exec\u0026#34;, \u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] ## Run the gitlab-bastion container with the ability to deploy/manage containers of jobs ## cluster-wide or only within namespace clusterWideAccess: false ## If RBAC is disabled in this Helm chart, use the following Kubernetes Service Account name. ## # serviceAccountName: default ## Configuration for the Pods that the runner launches for each new job ## runners: ## Default container image to use for builds when none is specified ## image: ubuntu:18.04 ## Run all containers with the privileged flag enabled ## This will allow the docker:stable-dind image to run if you need to run Docker ## commands. Please read the docs before turning this on: ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## privileged: true ## Namespace to run Kubernetes jobs in (defaults to \u0026#39;default\u0026#39;) ## namespace: gitlab-runner ## Build Container specific configuration ## builds: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Service Container specific configuration ## services: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Helper Container specific configuration ## helpers: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi EOF Sau ƒë√≥ th√¨ install Gitlab Runner Helm chart b·∫±ng file gitlab-runner-values.yaml v·ª´a t·∫°o:\nhelm repo add gitlab https://charts.gitlab.io helm init helm install --namespace gitlab-runner --name gitlab-runner gitlab/gitlab-runner -f gitlab-runner-values.yaml Gi·∫£ s·ª≠ t∆∞∆°ng lai b·∫°n c√≥ thay ƒë·ªïi g√¨ file values v√† mu·ªën update l·∫°i c√°i helm chart Gitlab Runner th√¨ d√πng command sau:\nhelm upgrade --namespace gitlab-runner --name gitlab-runner gitlab/gitlab-runner -f gitlab-runner-values.yaml N·∫øu ch·∫°y th√†nh c√¥ng, 1 pod s·∫Ω ƒë∆∞·ª£c t·∫°o trong namespace gitlab-runner nh∆∞ n√†y:\nk get pods,svc,ing -n gitlab-runner NAME READY STATUS RESTARTS AGE pod/gitlab-runner-gitlab-runner-5876c44c67-q5bs5 1/1 Running 0 66s 7. t·∫°o sample app helloapp v√† config .gitlab-ci.yml ƒê·∫ßu ti√™n clone project r·ªóng gitlabci-demo m√† ch√∫ng ta ƒë√£ t·∫°o ban ƒë·∫ßu v·ªÅ CloudShell:\ngit clone https://gitlab.your-subdomain.your-domain.net/root/gitlabci-demo.git cd gitlabci-demo t·∫°o file main.go l√† 1 app ƒë∆°n gi·∫£n, n·ªôi dung nh∆∞ sau:\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) var version = 1 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;Hello ‰∏ñÁïå... from v %s\u0026quot;, version) } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, handler) log.Fatal(http.ListenAndServe(\u0026quot;:8888\u0026quot;, nil)) } t·∫°o Dockerfile n·ªôi dung nh∆∞ sau:\nFROM golang:1.11-alpine AS build WORKDIR / COPY main.go go.* / RUN CGO_ENABLED=0 go build -o /bin/demo FROM scratch COPY --from=build /bin/demo /bin/demo ENTRYPOINT [\u0026quot;/bin/demo\u0026quot;] t·∫°o file .gitlab-ci.yml n·ªôi dung nh∆∞ sau:\nimage: docker:latest services: - docker:18.09.7-dind variables: DOCKER_HOST: tcp://127.0.0.1:2375 # When using Gitlab self-hosted on K8s # DOCKER_HOST: tcp://docker:2375/ # When using Gitlab.com DOCKER_DRIVER: overlay DOCKER_API_VERSION: \u0026quot;1.39\u0026quot; stages: - publish publish: stage: publish before_script: # Login to Google Cloud Registry - echo $GCLOUD_SERVICE_KEY | docker login -u _json_key --password-stdin https://gcr.io script: - docker build -t $IMAGE_NAME . - docker tag $IMAGE_NAME \u0026quot;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME\u0026quot; - docker push \u0026quot;gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:latest\u0026quot; only: - master gi·∫£i th√≠ch: ·ªü ƒë√¢y m√¨nh ch·ªâ t·∫°o 1 step publish, step bao g·ªìm build Docker image, tagging v√† push image ƒë√≥ l√™n GCR\n8. Push local project l√™n Gitlab Server Gi·ªù push t·∫•t c·∫£ nh·ªØng file ƒë√£ t·∫°o th√™m l√™n Gitlab Server:\ngit add . git commit -m \u0026#34;push app\u0026#34; git push origin master 9. K·∫øt qu·∫£ k·∫øt qu·∫£ nh∆∞ sau, Gitlab s·∫Ω detect ƒë∆∞·ª£c project c·ªßa b·∫°n c√≥ .gitlab-ci.yml n√™n n√≥ s·∫Ω trigger Gitlab Runner th·ª±c hi·ªán c√°c step trong file .gitlab-ci.yml ƒë√≥.\nB·∫°n c√≥ th·ªÉ check k·∫øt qu·∫£ khi v√†o c√°c m√†n h√¨nh sau:\nV√†o Google Container Registry Service s·∫Ω th·∫•y image helloapp ƒë√£ ƒë∆∞·ª£c push l√™n nh∆∞ sau:\nDONE! üéâüéâ\nK·∫øt h·ª£p v·ªõi c√°c b√†i v·ªÅ CD tr∆∞·ªõc th√¨ b·∫°n ƒë√£ c√≥ 1 flow CI/CD ho√†n ch·ªânh r·ªìi üòç\n10. (Bonus) T√πy bi·∫øn th√™m v·ªõi file .gitlab-ci.yml image: docker:latest services: - docker:18.09.7-dind variables: DOCKER_HOST: tcp://127.0.0.1:2375 # When using Gitlab self-hosted on K8s # DOCKER_HOST: tcp://docker:2375/ # When using Gitlab.com DOCKER_DRIVER: overlay DOCKER_API_VERSION: \u0026quot;1.39\u0026quot; TEST_IMAGE: gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA RELEASE_IMAGE: gcr.io/$GCLOUD_PROJECT_ID/$IMAGE_NAME:latest-$CI_COMMIT_SHORT_SHA stages: - build # - test - release before_script: # Login to Google Cloud Registry - echo $GCLOUD_SERVICE_KEY | docker login -u _json_key --password-stdin https://gcr.io build: stage: build script: - docker build -t $TEST_IMAGE . - docker push $TEST_IMAGE # test: # stage: test # script: # - docker pull $TEST_IMAGE # - docker run $TEST_IMAGE npm test release: stage: release script: - docker pull $TEST_IMAGE - docker tag $TEST_IMAGE $RELEASE_IMAGE - docker push $RELEASE_IMAGE only: - master REFERENCES https://docs.gitlab.com/runner/install/kubernetes.html\nhttps://docs.gitlab.com/ee/ci/runners/\nhttp://www.idevops.site/gitlabci/chapter04/01/5/\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/issues/25803\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/issues/2623\nhttps://gitlab.com/gitlab-org/charts/gitlab/-/issues/478\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/blob/master/docs/executors/kubernetes.md#using-dockerdind\nhttps://gitlab.com/gitlab-org/gitlab-foss/-/issues/34419\nhttps://stackoverflow.com/questions/36408339/best-cd-strategy-for-kubernetes-deployments/48268573#48268573\nhttps://stackoverflow.com/questions/47888027/how-to-deploy-staging-in-google-cloud-platform-with-kubernetes-and-gitlab-ci-cd\nhttps://www.digitalocean.com/community/tutorials/how-to-build-docker-images-and-host-a-docker-image-repository-with-gitlab\nhttps://github.com/JanMikes/gitlab-ci-push-to-gcr/blob/master/.gitlab-ci.yml\nhttps://rhazn.com/posts/build-a-docker-image-on-gitlab-ci-and-publish-it-to-google-container-registry/\nhttps://gist.github.com/foklepoint/2f9087375830068ec032ef326d93f423\nhttps://medium.com/@gaforres/publishing-google-cloud-container-registry-images-from-gitlab-ci-23c45356ff0e\nhttps://qiita.com/proudust/items/d94c60ec69dead927954\nhttps://docs.gitlab.com/ee/ci/variables/\n","href":"/posts/k8s-xiii-using-gitlab-ci-push-docker-image-to-gcr-on-gke/","title":"K8S 13: Using Gitlab CI on GKE Cluster - Push Docker Image to GCR for Continuous Integration (CI)"},{"content":"","href":"/tags/flux/","title":"Flux"},{"content":"Gi·ªõi thi·ªáu L·∫ßn m√≤ v·ªÅ GitOps th√¨ th·∫•y ƒë√¢y l√† 1 kh√°i ni·ªám r·∫•t hay v√† th√∫ v·ªã\nNh∆∞ c√°c b·∫°n th·∫•y tr√™n h√¨nh, Flux ho·∫°t ƒë·ªông nh∆∞ 1 operator s·∫Ω ƒëi scan 1 Git repository v√† 1 (ho·∫∑c nhi·ªÅu) Docker registries.\nM·ªói khi b·∫°n commit 1 file yaml l√™n Git repository ƒë√≥, Flux s·∫Ω t·ª± ƒë·ªông deploy file ƒë√≥ v√†o k8s (ƒê√¢y ch√≠nh l√† GitOps). N·∫øu b·∫°n thay ƒë·ªïi n·ªôi dung file yaml ƒë√≥ v√† commit ti·∫øp, Flux c≈©ng s·∫Ω update resources t∆∞∆°ng ·ª©ng. Ho·∫∑c khi b·∫°n push 1 images m·ªõi l√™n Container Registry, Flux c≈©ng scan Registry v√† s·∫Ω update app c·ªßa b·∫°n s·ª≠ d·ª•ng images m·ªõi.\nNh∆∞ v·∫≠y, vi·ªác centralize m·ªçi th·ª© trong 1 Git repository gi√∫p b·∫°n d·ªÖ d√†ng audit nh·ªØng thay ƒë·ªïi th√¥ng qua c√°c commit c·ªßa Git. C√°c kubernetes resources c≈©ng d·ªÖ d√†ng ƒë·ªçc hi·ªÉu v√† ch·ªânh s·ª≠a.\nB√†i n√†y m√¨nh s·∫Ω l√†m 1 demo v·ªÅ vi·ªác s·ª≠ d·ª•ng Weave Flux ƒë·ªÉ setup CD (Continuous Deployment) v·ªõi Gitlab Server ri√™ng v√† Private Registry ri√™ng c·ªßa Google (GCR). M·ªçi th·ª© l√†m tr√™n GCP, GKE (c≈©ng c·ªßa Google th√¥i)\nC√°c c√¢u l·ªánh ·ªü ƒë√¢y ƒë·ªÅu l√†m tr√™n CloudShell c·ªßa GCP nh√©\nY√™u c·∫ßu ƒê√£ l√†m theo b√†i sau (N·∫øu b·∫°n ƒë√£ c√≥ Gitlab server ri√™ng th√¨ c√≥ th·ªÉ b·ªè qua):\nK8S 10\nSau khi l√†m theo link tr√™n th√¨ ch√∫ng ta c√≥ 1 Gitlab server ri√™ng ·ªü domain nh∆∞ sau:\nhttps://gitlab.your-subdomain.your-domain.net/\nChu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng, nh·ªØng bi·∫øn n√†y s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o 1 s·ªë file ·ªü c√°c ph·∫ßn sau:\nexport PROJECT_ID=\u0026#34;your-project-id\u0026#34; # ·ªû ƒë√¢y s·∫Ω l√† subdomain ƒë·ªÉ route traffic c·ªßa app Hello World v√†o nh√©,  # n·∫øu ko c√≥ c√°i n√†y c≈©ng ko sao, khi test ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ch·ª©c nƒÉng Web Preview c·ªßa CloudShell c≈©ng ƒë∆∞·ª£c export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; C√°ch l√†m 1. Setting GitOps Repository Gi·ªù s·∫Ω setting 1 ch√∫t tr√™n Gitlab server c·ªßa b·∫°n:\nT·∫°o user flux v·ªõi role Admin\nLogin v√†o user flux T·∫°o personal access token c·ªßa user flux (https://gitlab.your-subdomain.your-domain.net/profile/personal_access_tokens) Token c√≥ d·∫°ng 3KzPQxrkoc6amdQKxAhw, h√£y gi·ªØ b√≠ m·∫≠t token n√†y ƒë·ªÉ l√°t s·ª≠ d·ª•ng sau\nTr√™n CloudShell t·∫°o bi·∫øn m√¥i tr∆∞·ªùng l∆∞u token v√†o:\nexport PERSONAL_ACCESS_TOKEN=\u0026#34;3KzPQxrkoc6amdQKxAhw\u0026#34; Tr√™n Gitlab t·∫°o project ƒë·ªÉ user flux s·ª≠ d·ª•ng l√†m GitOps repository\nT√™n project th√¨ t√πy √Ω: ƒë√¢y m√¨nh s·∫Ω ƒë·∫∑t flux-get-started\nLink project s·∫Ω ki·ªÉu ki·ªÉu nh∆∞ n√†y https://gitlab.your-subdomain.your-domain.net/flux/flux-get-started\nT·ª´ CloudShell clone project v·ª´a t·∫°o v·ªÅ:\ncd ~ git clone https://gitlab.your-subdomain.your-domain.net/flux/flux-get-started \u0026amp;\u0026amp; cd flux-get-started 2. T·∫°o 1 App sample Hello World App ƒë·ªÉ test Vi·∫øt 1 app ƒë∆°n gi·∫£n ch·∫°y b·∫±ng Go (command sau s·∫Ω t·∫°o ra file main.go)\ncat \u0026gt; ./main.go \u0026lt;\u0026lt;EOF package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var version = 1 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello ‰∏ñÁïå... from v %s\u0026#34;, version) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Fatal(http.ListenAndServe(\u0026#34;:8888\u0026#34;, nil)) } EOF T·∫°o Dockerfile v√† build images ri√™ng c·ªßa b·∫°n\ncat \u0026gt; ./Dockerfile \u0026lt;\u0026lt;EOF FROM golang:1.11-alpine AS build WORKDIR / COPY main.go go.* / RUN CGO_ENABLED=0 go build -o /bin/demo FROM scratch COPY --from=build /bin/demo /bin/demo ENTRYPOINT [\u0026#34;/bin/demo\u0026#34;] EOF Build image t·ª´ Dockerfile tr√™n\ndocker image build -t helloapp:latest . Confirm r·∫±ng images ƒë√£ ƒë∆∞·ª£c t·∫°o ra\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE helloapp latest c06c85757d4e About a minute ago 6.51MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9c7ddef38ecb About a minute ago 325MB golang 1.11-alpine e116d2efa2ab 9 months ago 312MB ƒê√°nh tag cho images ƒë√≥\ndocker tag helloapp:latest gcr.io/${PROJECT_ID}/helloapp:latest Push images c√≥ tag latest ƒë√≥ l√™n GCR c·ªßa b·∫°n\ndocker push gcr.io/${PROJECT_ID}/helloapp:latest file hello-app.yaml bao g·ªìm k8s Service v√† Deployment resources\ncat \u0026gt; ./hello-app.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: hello spec: ports: - port: 80 targetPort: 8888 selector: app: hello --- apiVersion: apps/v1 kind: Deployment metadata: name: hello spec: selector: matchLabels: app: hello replicas: 1 template: metadata: labels: app: hello spec: containers: - name: hello image: gcr.io/${PROJECT_ID}/helloapp:latest ports: - containerPort: 8888 EOF file hello-ingress.yaml ch·ª©a Ingress gi√∫p route traffic t·ª´ 1 subdomain c·ª• th·ªÉ v√†o service c·ªßa Hello App\nN·∫øu c√°c b·∫°n ko mu·ªën t·∫°o file n√†y c≈©ng ko sao, khi test app ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ch·ª©c nƒÉng Web Preview c·ªßa CloudShell\ncat \u0026gt; ./hello-ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: hello-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: ${SUBDOMAIN} http: paths: - path: / backend: serviceName: hello servicePort: 80 EOF T·ª´ CloudShell b·∫°n h√£y push 2 file hello-app.yaml v√† hello-ingress.yaml l√™n project flux-get-started\ncd ~/flux-get-started git add hello-app.yaml hello-ingress.yaml git commit \u0026#34;Commit yaml files\u0026#34; git push origin master 2. Install Flux # Add the Flux repo: helm repo add fluxcd https://charts.fluxcd.io # Trong Cluster t·∫°o namespace `flux`:  kubectl create namespace flux # T·∫°o Secret trong k8s Cluster b·∫±ng personal access token ƒë√£ t·∫°o ·ªü step 1:  kubectl create secret generic flux-git-auth --namespace flux --from-literal=GIT_AUTHUSER=flux --from-literal=GIT_AUTHKEY=${PERSONAL_ACCESS_TOKEN} Install flux, ch√∫ √Ω s·ª≠a link ƒë·∫øn Gitlab Server c·ªßa b·∫°n:\nhelm upgrade -i flux fluxcd/flux \\ --set git.url=\u0026#39;https://$(GIT_AUTHUSER):$(GIT_AUTHKEY)@gitlab.your-subdomain.your-domain.net/flux/flux-get-started.git\u0026#39; \\ --set env.secretName=flux-git-auth \\ --namespace flux 3. Test Sau khi install flux th√†nh c√¥ng, b·∫°n s·∫Ω nh·∫≠n ra l√† c√≥ 1 pod v√† 1 svc v√† 1 ingress ƒë∆∞·ª£c t·∫°o ra, ƒë√≥ ch√≠nh l√† c√°c resource ƒë·ªÉ tr√™n GitOps repository c·ªßa b·∫°n\nk get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default hello-54d49c4bb5-hm9z6 1/1 Running 0 36s flux flux-7b88d998fd-zclth 1/1 Running 0 60s flux flux-memcached-64f7865494-xq46b 1/1 Running 0 60s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 140m default hello ClusterIP 10.68.71.230 \u0026lt;none\u0026gt; 80/TCP 79s default kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 151m flux flux ClusterIP 10.68.188.216 \u0026lt;none\u0026gt; 3030/TCP 103s flux flux-memcached ClusterIP 10.68.74.32 \u0026lt;none\u0026gt; 11211/TCP 103s Gi·ªù truy c·∫≠p v√†o website your-subdomain.your-domain.net s·∫Ω th·∫•y app c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c apply t·ª± ƒë·ªông\nHo·∫∑c c≈©ng c√≥ th·ªÉ b·∫°n d√πng ch·ª©c nƒÉng Web Preview c·ªßa ClouShell ƒë·ªÉ view web app:\nkubectl port-forward pod/hello-54d49c4bb5-hm9z6 8888:8888 3.1. Flux t·ª± ƒë·ªông scan GitOps Repository ƒë·ªÉ update application Gi·ªù h√£y th·ª≠ thay ƒë·ªïi file yaml ƒë·ªÉ xem Flux c√≥ t·ª± ƒë·ªông ph·∫£n √°nh nh·ªØng thay ƒë·ªïi ƒë√≥ v√†o Kubernetes cho b·∫°n ko\nTr∆∞·ªõc ti√™n h√£y s·ª≠a version = 2 trong file main.go, r·ªìi build l·∫°i 1 Docker images v·ªõi tag :0.2.0 v√† push l√™n GCR\nnano main.go # change `version = 2` # Build l·∫°i Docker images v√† ƒë√°nh tag 0.2.0 docker image build -t helloapp:0.2.0 . # tag cho images tr√™n GCR l√† 0.2.0 docker tag helloapp:0.2.0 gcr.io/${PROJECT_ID}/helloapp:0.2.0 # Push images m·ªõi v·ªõi tag :0.2.0 l√™n GCR docker push gcr.io/${PROJECT_ID}/helloapp:0.2.0 Sau ƒë√≥ b·∫°n s·∫Ω s·ª≠a image trong file hello-app.yaml th√†nh d√πng image tag :0.2.0:\nOLD ~~~ image: gcr.io/YOUR_PROJECT_ID/helloapp:latest ~~~ NEW ~~~ image: gcr.io/YOUR_PROJECT_ID/helloapp:0.2.0 ~~~ r·ªìi commit v√† push thay ƒë·ªïi tr√™n l√™n GitOps repository c·ªßa b·∫°n:\ngit add hello-app.yaml git commit -m \u0026#34;change to using image :0.2.0\u0026#34; git push origin master V√†i ph√∫t sau b·∫°n s·∫Ω th·∫•y 1 pod m·ªõi c·ªßa hello app ƒë∆∞·ª£c t·∫°o l√™n, pod c≈© b·ªã x√≥a ƒëi\nk get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default hello-5856bc89cd-rdrdd 1/1 Running 0 37s V√†o check app tr√™n giao di·ªán:\n‚áí Nh∆∞ v·∫≠y nghƒ©a l√† Flux ƒë√£ t·ª± ƒë·ªông apply file yaml m·ªõi cho ch√∫ng ta r·ªìi!\n3.2. Flux t·ª± ƒë·ªông scan Registry ƒë·ªÉ update application Tr∆∞·ªõc h·∫øt ƒë·ªÉ l√†m ƒëi·ªÅu n√†y th√¨ c·∫ßn ph·∫£i add annotation v√†o Deployment ƒë√£\nS·ª≠a file hello-app.yaml add th√™m d√≤ng sau v√†o\n annotations: flux.weave.works/automated: \u0026quot;true\u0026quot; flux.weave.works/tag.chart-image: semver:~0.2 cat \u0026gt; ./hello-app.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: hello spec: ports: - port: 80 targetPort: 8888 selector: app: hello --- apiVersion: apps/v1 kind: Deployment metadata: name: hello annotations: flux.weave.works/automated: \u0026#34;true\u0026#34; flux.weave.works/tag.chart-image: semver:~0.2 spec: selector: matchLabels: app: hello replicas: 1 template: metadata: labels: app: hello spec: containers: - name: hello image: gcr.io/${PROJECT_ID}/helloapp:0.2.0 ports: - containerPort: 8888 EOF r·ªìi commit v√† push thay ƒë·ªïi tr√™n l√™n GitOps repository c·ªßa b·∫°n:\ngit add hello-app.yaml git commit -m \u0026#34;add annotation using semver\u0026#34; git push origin master Gi·ªù b·∫°n th·∫•y app c·ªßa b·∫°n ch∆∞a c√≥ g√¨ thay ƒë·ªïi, v·∫´n l√† app version c≈© nh√© Gi·∫£ s·ª≠ v√¨ 1 l√Ω do g√¨ ƒë√≥, b·∫°n c·∫ßn upgrade version c·ªßa app l√™n 0.2.2,\ns·ª≠a version = 2.2 trong file main.go, r·ªìi build l·∫°i 1 Docker images v·ªõi tag :0.2.2 v√† push l√™n GCR\nnano main.go # change version to `2.2` # Build l·∫°i Docker images v√† ƒë√°nh tag 0.2.2 docker image build -t helloapp:0.2.2 . # tag cho images tr√™n GCR l√† 0.2.2 docker tag helloapp:0.2.2 gcr.io/${PROJECT_ID}/helloapp:0.2.2 # Push images m·ªõi v·ªõi tag :0.2.2 l√™n GCR docker push gcr.io/${PROJECT_ID}/helloapp:0.2.2 G·∫ßn nh∆∞ ngay l·∫≠p t·ª©c, b·∫°n s·∫Ω th·∫•y 1 pod m·ªõi c·ªßa Hello App ƒë∆∞·ª£c t·∫°o l√™n, pod c≈© b·ªã x√≥a ƒëi, webapp c≈©ng thay ƒë·ªïi giao di·ªán t∆∞∆°ng ·ª©ng\nƒê·∫•y ch√≠nh l√† nh·ªù c√°c annotation trong file hello-app.yaml, n√≥ gi√∫p Flux t·ª± ƒë·ªông apply thay ƒë·ªïi.\nM·ªói khi c√≥ image m·ªõi v·ªõi tag b·∫Øt ƒë·∫ßu b·∫±ng 0.2.* th√¨ app s·∫Ω ƒë∆∞·ª£c update t·ª± ƒë·ªông\nR·∫•t th√∫ v·ªã v√† d·ªÖ hi·ªÉu ph·∫£i ko, h√£y th·ª≠ √°p d·ª•ng Flux xem sao nh√©\nDone! üéâüéâ\nREFERENCES https://medium.com/@m.k.joerg/gitops-weave-flux-in-detail-77ce36945646\nhttps://docs.fluxcd.io/en/1.19.0/tutorials/get-started-helm/\nhttps://github.com/fluxcd/flux/blob/master/chart/flux/values.yaml\nhttps://www.weave.works/blog/managing-helm-releases-the-gitops-way\nhttps://github.com/fluxcd/flux/blob/master/docs/guides/use-private-git-host.md\nhttps://github.com/fluxcd/flux/issues/1358\n","href":"/bk/k8s-xii-gitops-using-flux-with-private-gitlab-server-with-gcr-for-cd/","title":"K8S 12: GitOps - Using Flux with private Gitlab server and GCR for Continuous Deployment (CD) on GKE Cluster"},{"content":"Gi·ªõi thi·ªáu L·∫ßn m√≤ v·ªÅ GitOps th√¨ th·∫•y ƒë√¢y l√† 1 kh√°i ni·ªám r·∫•t hay v√† th√∫ v·ªã\nNh∆∞ c√°c b·∫°n th·∫•y tr√™n h√¨nh, Flux ho·∫°t ƒë·ªông nh∆∞ 1 operator s·∫Ω ƒëi scan 1 Git repository v√† 1 (ho·∫∑c nhi·ªÅu) Docker registries.\nM·ªói khi b·∫°n commit 1 file yaml l√™n Git repository ƒë√≥, Flux s·∫Ω t·ª± ƒë·ªông deploy file ƒë√≥ v√†o k8s (ƒê√¢y ch√≠nh l√† GitOps). N·∫øu b·∫°n thay ƒë·ªïi n·ªôi dung file yaml ƒë√≥ v√† commit ti·∫øp, Flux c≈©ng s·∫Ω update resources t∆∞∆°ng ·ª©ng. Ho·∫∑c khi b·∫°n push 1 images m·ªõi l√™n Container Registry, Flux c≈©ng scan Registry v√† s·∫Ω update app c·ªßa b·∫°n s·ª≠ d·ª•ng images m·ªõi.\nNh∆∞ v·∫≠y, vi·ªác centralize m·ªçi th·ª© trong 1 Git repository gi√∫p b·∫°n d·ªÖ d√†ng audit nh·ªØng thay ƒë·ªïi th√¥ng qua c√°c commit c·ªßa Git. C√°c kubernetes resources c≈©ng d·ªÖ d√†ng ƒë·ªçc hi·ªÉu v√† ch·ªânh s·ª≠a.\nB√†i n√†y m√¨nh s·∫Ω l√†m 1 demo v·ªÅ vi·ªác s·ª≠ d·ª•ng Weave Flux ƒë·ªÉ setup CD (Continuous Deployment) v·ªõi Gitlab Server ri√™ng v√† Private Registry ri√™ng c·ªßa Google (GCR). M·ªçi th·ª© l√†m tr√™n GCP, GKE (c≈©ng c·ªßa Google th√¥i)\nC√°c c√¢u l·ªánh ·ªü ƒë√¢y ƒë·ªÅu l√†m tr√™n CloudShell c·ªßa GCP nh√©\nY√™u c·∫ßu ƒê√£ l√†m theo b√†i sau (N·∫øu b·∫°n ƒë√£ c√≥ Gitlab server ri√™ng th√¨ c√≥ th·ªÉ b·ªè qua):\nK8S 10\nSau khi l√†m theo link tr√™n th√¨ ch√∫ng ta c√≥ 1 Gitlab server ri√™ng ·ªü domain nh∆∞ sau:\nhttps://gitlab.your-subdomain.your-domain.net/\nChu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng, nh·ªØng bi·∫øn n√†y s·∫Ω s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o 1 s·ªë file ·ªü c√°c ph·∫ßn sau:\nexport PROJECT_ID=\u0026#34;your-project-id\u0026#34; # ·ªû ƒë√¢y s·∫Ω l√† subdomain ƒë·ªÉ route traffic c·ªßa app Hello World v√†o nh√©,  # n·∫øu ko c√≥ c√°i n√†y c≈©ng ko sao, khi test ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ch·ª©c nƒÉng Web Preview c·ªßa CloudShell c≈©ng ƒë∆∞·ª£c export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; C√°ch l√†m 1. Setting GitOps Repository Gi·ªù s·∫Ω setting 1 ch√∫t tr√™n Gitlab server c·ªßa b·∫°n:\nT·∫°o user flux v·ªõi role Admin\nLogin v√†o user flux T·∫°o personal access token c·ªßa user flux (https://gitlab.your-subdomain.your-domain.net/profile/personal_access_tokens) Token c√≥ d·∫°ng 3KzPQxrkoc6amdQKxAhw, h√£y gi·ªØ b√≠ m·∫≠t token n√†y ƒë·ªÉ l√°t s·ª≠ d·ª•ng sau\nTr√™n CloudShell t·∫°o bi·∫øn m√¥i tr∆∞·ªùng l∆∞u token v√†o:\nexport PERSONAL_ACCESS_TOKEN=\u0026#34;3KzPQxrkoc6amdQKxAhw\u0026#34; Tr√™n Gitlab t·∫°o project ƒë·ªÉ user flux s·ª≠ d·ª•ng l√†m GitOps repository\nT√™n project th√¨ t√πy √Ω: ƒë√¢y m√¨nh s·∫Ω ƒë·∫∑t flux-get-started\nLink project s·∫Ω ki·ªÉu ki·ªÉu nh∆∞ n√†y https://gitlab.your-subdomain.your-domain.net/flux/flux-get-started\nT·ª´ CloudShell clone project v·ª´a t·∫°o v·ªÅ:\ncd ~ git clone https://gitlab.your-subdomain.your-domain.net/flux/flux-get-started \u0026amp;\u0026amp; cd flux-get-started 2. T·∫°o 1 App sample Hello World App ƒë·ªÉ test Vi·∫øt 1 app ƒë∆°n gi·∫£n ch·∫°y b·∫±ng Go (command sau s·∫Ω t·∫°o ra file main.go)\ncat \u0026gt; ./main.go \u0026lt;\u0026lt;EOF package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var version = 1 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello ‰∏ñÁïå... from v %s\u0026#34;, version) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Fatal(http.ListenAndServe(\u0026#34;:8888\u0026#34;, nil)) } EOF T·∫°o Dockerfile v√† build images ri√™ng c·ªßa b·∫°n\ncat \u0026gt; ./Dockerfile \u0026lt;\u0026lt;EOF FROM golang:1.11-alpine AS build WORKDIR / COPY main.go go.* / RUN CGO_ENABLED=0 go build -o /bin/demo FROM scratch COPY --from=build /bin/demo /bin/demo ENTRYPOINT [\u0026#34;/bin/demo\u0026#34;] EOF Build image t·ª´ Dockerfile tr√™n\ndocker image build -t helloapp:latest . Confirm r·∫±ng images ƒë√£ ƒë∆∞·ª£c t·∫°o ra\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE helloapp latest c06c85757d4e About a minute ago 6.51MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9c7ddef38ecb About a minute ago 325MB golang 1.11-alpine e116d2efa2ab 9 months ago 312MB ƒê√°nh tag cho images ƒë√≥\ndocker tag helloapp:latest gcr.io/${PROJECT_ID}/helloapp:latest Push images c√≥ tag latest ƒë√≥ l√™n GCR c·ªßa b·∫°n\ndocker push gcr.io/${PROJECT_ID}/helloapp:latest file hello-app.yaml bao g·ªìm k8s Service v√† Deployment resources\ncat \u0026gt; ./hello-app.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: hello spec: ports: - port: 80 targetPort: 8888 selector: app: hello --- apiVersion: apps/v1 kind: Deployment metadata: name: hello spec: selector: matchLabels: app: hello replicas: 1 template: metadata: labels: app: hello spec: containers: - name: hello image: gcr.io/${PROJECT_ID}/helloapp:latest ports: - containerPort: 8888 EOF file hello-ingress.yaml ch·ª©a Ingress gi√∫p route traffic t·ª´ 1 subdomain c·ª• th·ªÉ v√†o service c·ªßa Hello App\nN·∫øu c√°c b·∫°n ko mu·ªën t·∫°o file n√†y c≈©ng ko sao, khi test app ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ch·ª©c nƒÉng Web Preview c·ªßa CloudShell\ncat \u0026gt; ./hello-ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: hello-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: ${SUBDOMAIN} http: paths: - path: / backend: serviceName: hello servicePort: 80 EOF T·ª´ CloudShell b·∫°n h√£y push 2 file hello-app.yaml v√† hello-ingress.yaml l√™n project flux-get-started\ncd ~/flux-get-started git add hello-app.yaml hello-ingress.yaml git commit \u0026#34;Commit yaml files\u0026#34; git push origin master 2. Install Flux # Add the Flux repo: helm repo add fluxcd https://charts.fluxcd.io # Trong Cluster t·∫°o namespace `flux`:  kubectl create namespace flux # T·∫°o Secret trong k8s Cluster b·∫±ng personal access token ƒë√£ t·∫°o ·ªü step 1:  kubectl create secret generic flux-git-auth --namespace flux --from-literal=GIT_AUTHUSER=flux --from-literal=GIT_AUTHKEY=${PERSONAL_ACCESS_TOKEN} Install flux, ch√∫ √Ω s·ª≠a link ƒë·∫øn Gitlab Server c·ªßa b·∫°n:\nhelm upgrade -i flux fluxcd/flux \\ --set git.url=\u0026#39;https://$(GIT_AUTHUSER):$(GIT_AUTHKEY)@gitlab.your-subdomain.your-domain.net/flux/flux-get-started.git\u0026#39; \\ --set env.secretName=flux-git-auth \\ --namespace flux 3. Test Sau khi install flux th√†nh c√¥ng, b·∫°n s·∫Ω nh·∫≠n ra l√† c√≥ 1 pod v√† 1 svc v√† 1 ingress ƒë∆∞·ª£c t·∫°o ra, ƒë√≥ ch√≠nh l√† c√°c resource ƒë·ªÉ tr√™n GitOps repository c·ªßa b·∫°n\nk get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default hello-54d49c4bb5-hm9z6 1/1 Running 0 36s flux flux-7b88d998fd-zclth 1/1 Running 0 60s flux flux-memcached-64f7865494-xq46b 1/1 Running 0 60s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 140m default hello ClusterIP 10.68.71.230 \u0026lt;none\u0026gt; 80/TCP 79s default kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 151m flux flux ClusterIP 10.68.188.216 \u0026lt;none\u0026gt; 3030/TCP 103s flux flux-memcached ClusterIP 10.68.74.32 \u0026lt;none\u0026gt; 11211/TCP 103s Gi·ªù truy c·∫≠p v√†o website your-subdomain.your-domain.net s·∫Ω th·∫•y app c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c apply t·ª± ƒë·ªông\nHo·∫∑c c≈©ng c√≥ th·ªÉ b·∫°n d√πng ch·ª©c nƒÉng Web Preview c·ªßa ClouShell ƒë·ªÉ view web app:\nkubectl port-forward pod/hello-54d49c4bb5-hm9z6 8888:8888 3.1. Flux t·ª± ƒë·ªông scan GitOps Repository ƒë·ªÉ update application Gi·ªù h√£y th·ª≠ thay ƒë·ªïi file yaml ƒë·ªÉ xem Flux c√≥ t·ª± ƒë·ªông ph·∫£n √°nh nh·ªØng thay ƒë·ªïi ƒë√≥ v√†o Kubernetes cho b·∫°n ko\nTr∆∞·ªõc ti√™n h√£y s·ª≠a version = 2 trong file main.go, r·ªìi build l·∫°i 1 Docker images v·ªõi tag :0.2.0 v√† push l√™n GCR\nnano main.go # change `version = 2` # Build l·∫°i Docker images v√† ƒë√°nh tag 0.2.0 docker image build -t helloapp:0.2.0 . # tag cho images tr√™n GCR l√† 0.2.0 docker tag helloapp:0.2.0 gcr.io/${PROJECT_ID}/helloapp:0.2.0 # Push images m·ªõi v·ªõi tag :0.2.0 l√™n GCR docker push gcr.io/${PROJECT_ID}/helloapp:0.2.0 Sau ƒë√≥ b·∫°n s·∫Ω s·ª≠a image trong file hello-app.yaml th√†nh d√πng image tag :0.2.0:\nOLD ~~~ image: gcr.io/YOUR_PROJECT_ID/helloapp:latest ~~~ NEW ~~~ image: gcr.io/YOUR_PROJECT_ID/helloapp:0.2.0 ~~~ r·ªìi commit v√† push thay ƒë·ªïi tr√™n l√™n GitOps repository c·ªßa b·∫°n:\ngit add hello-app.yaml git commit -m \u0026#34;change to using image :0.2.0\u0026#34; git push origin master V√†i ph√∫t sau b·∫°n s·∫Ω th·∫•y 1 pod m·ªõi c·ªßa hello app ƒë∆∞·ª£c t·∫°o l√™n, pod c≈© b·ªã x√≥a ƒëi\nk get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default hello-5856bc89cd-rdrdd 1/1 Running 0 37s V√†o check app tr√™n giao di·ªán:\n‚áí Nh∆∞ v·∫≠y nghƒ©a l√† Flux ƒë√£ t·ª± ƒë·ªông apply file yaml m·ªõi cho ch√∫ng ta r·ªìi!\n3.2. Flux t·ª± ƒë·ªông scan Registry ƒë·ªÉ update application Tr∆∞·ªõc h·∫øt ƒë·ªÉ l√†m ƒëi·ªÅu n√†y th√¨ c·∫ßn ph·∫£i add annotation v√†o Deployment ƒë√£\nS·ª≠a file hello-app.yaml add th√™m d√≤ng sau v√†o\n annotations: flux.weave.works/automated: \u0026quot;true\u0026quot; flux.weave.works/tag.chart-image: semver:~0.2 cat \u0026gt; ./hello-app.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: hello spec: ports: - port: 80 targetPort: 8888 selector: app: hello --- apiVersion: apps/v1 kind: Deployment metadata: name: hello annotations: flux.weave.works/automated: \u0026#34;true\u0026#34; flux.weave.works/tag.chart-image: semver:~0.2 spec: selector: matchLabels: app: hello replicas: 1 template: metadata: labels: app: hello spec: containers: - name: hello image: gcr.io/${PROJECT_ID}/helloapp:0.2.0 ports: - containerPort: 8888 EOF r·ªìi commit v√† push thay ƒë·ªïi tr√™n l√™n GitOps repository c·ªßa b·∫°n:\ngit add hello-app.yaml git commit -m \u0026#34;add annotation using semver\u0026#34; git push origin master Gi·ªù b·∫°n th·∫•y app c·ªßa b·∫°n ch∆∞a c√≥ g√¨ thay ƒë·ªïi, v·∫´n l√† app version c≈© nh√© Gi·∫£ s·ª≠ v√¨ 1 l√Ω do g√¨ ƒë√≥, b·∫°n c·∫ßn upgrade version c·ªßa app l√™n 0.2.2,\ns·ª≠a version = 2.2 trong file main.go, r·ªìi build l·∫°i 1 Docker images v·ªõi tag :0.2.2 v√† push l√™n GCR\nnano main.go # change version to `2.2` # Build l·∫°i Docker images v√† ƒë√°nh tag 0.2.2 docker image build -t helloapp:0.2.2 . # tag cho images tr√™n GCR l√† 0.2.2 docker tag helloapp:0.2.2 gcr.io/${PROJECT_ID}/helloapp:0.2.2 # Push images m·ªõi v·ªõi tag :0.2.2 l√™n GCR docker push gcr.io/${PROJECT_ID}/helloapp:0.2.2 G·∫ßn nh∆∞ ngay l·∫≠p t·ª©c, b·∫°n s·∫Ω th·∫•y 1 pod m·ªõi c·ªßa Hello App ƒë∆∞·ª£c t·∫°o l√™n, pod c≈© b·ªã x√≥a ƒëi, webapp c≈©ng thay ƒë·ªïi giao di·ªán t∆∞∆°ng ·ª©ng\nƒê·∫•y ch√≠nh l√† nh·ªù c√°c annotation trong file hello-app.yaml, n√≥ gi√∫p Flux t·ª± ƒë·ªông apply thay ƒë·ªïi.\nM·ªói khi c√≥ image m·ªõi v·ªõi tag b·∫Øt ƒë·∫ßu b·∫±ng 0.2.* th√¨ app s·∫Ω ƒë∆∞·ª£c update t·ª± ƒë·ªông\nR·∫•t th√∫ v·ªã v√† d·ªÖ hi·ªÉu ph·∫£i ko, h√£y th·ª≠ √°p d·ª•ng Flux xem sao nh√©\nDone! üéâüéâ\nREFERENCES https://medium.com/@m.k.joerg/gitops-weave-flux-in-detail-77ce36945646\nhttps://docs.fluxcd.io/en/1.19.0/tutorials/get-started-helm/\nhttps://github.com/fluxcd/flux/blob/master/chart/flux/values.yaml\nhttps://www.weave.works/blog/managing-helm-releases-the-gitops-way\nhttps://github.com/fluxcd/flux/blob/master/docs/guides/use-private-git-host.md\nhttps://github.com/fluxcd/flux/issues/1358\n","href":"/posts/k8s-xii-gitops-using-flux-with-private-gitlab-server-with-gcr-for-cd/","title":"K8S 12: GitOps - Using Flux with private Gitlab server and GCR for Continuous Deployment (CD) on GKE Cluster"},{"content":"","href":"/tags/gke/","title":"GKE"},{"content":"Gi·ªõi thi·ªáu  Keel is a tool for automating Kubernetes deployment updates. Keel is stateless, robust and lightweight.\n Keel ƒë∆∞·ª£c ƒë√°nh gi√° l√† 1 tool d·ªÖ c√†i ƒë·∫∑t, d·ªÖ s·ª≠ d·ª•ng, v√† r·∫•t nh·∫π.\nB√†i n√†y ch·ªâ ƒë∆°n gi·∫£n l√† m√¨nh mu·ªën demo v·ªÅ c√°ch s·ª≠ d·ª•ng 1 tool lightweight ƒë∆∞·ª£c nhi·ªÅu ng∆∞·ªùi gi·ªõi thi·ªáu (Keel) th√¥i. Ngo√†i ra c√≤n 1 s·ªë tool kh√°c c≈©ng ƒë∆∞·ª£c suggest nhi·ªÅu, ƒë√≥ l√† Weave Flux. Tool n√†y nhi·ªÅu ch·ª©c nƒÉng h∆°n Keel, m√¨nh c≈©ng mu·ªën th·ª≠ d√πng trong t∆∞∆°ng lai.\nquay l·∫°i v·ªõi Keel:\nY√™u c·∫ßu Kh√¥ng g√¨ kh√°c ngo√†i 1 Cluster tr√™n GKE.\nNgo√†i ra b√†i n√†y m√¨nh s·∫Ω d√πng Docker Hub ƒë·ªÉ l√†m Registry (ch·ªó l∆∞u Docker images) n√™n b·∫°n c≈©ng c·∫ßn c√≥ account Docker Hub.\nC√°ch l√†m 1- Trong Cloudshell c·ªßa GCP Console, t·∫°o folder ƒë·ªÉ l√†m workspace c·ªßa b·∫°n\nmkdir keel-demo-golang-app \u0026amp;\u0026amp; cd keel-demo-golang-app 2- Chu·∫©n b·ªã c√°c bi·∫øn m√¥i tr∆∞·ªùng, v√† login v√† Docker account c·ªßa b·∫°n\nexport DOCKER_USERNAME=AAAAAA export DOCKER_PASSWORD=BBBBBB docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; 3- Vi·∫øt 1 app ƒë∆°n gi·∫£n ch·∫°y b·∫±ng Go (command sau s·∫Ω t·∫°o ra file main.go)\ncat \u0026gt; ./main.go \u0026lt;\u0026lt;EOF package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var version = 1 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello ‰∏ñÁïå... from v %s\u0026#34;, version) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Fatal(http.ListenAndServe(\u0026#34;:8888\u0026#34;, nil)) } EOF 4- T·∫°o Dockerfile v√† build images ri√™ng c·ªßa b·∫°n\ncat \u0026gt; ./Dockerfile \u0026lt;\u0026lt;EOF FROM golang:1.11-alpine AS build WORKDIR / COPY main.go go.* / RUN CGO_ENABLED=0 go build -o /bin/demo FROM scratch COPY --from=build /bin/demo /bin/demo ENTRYPOINT [\u0026#34;/bin/demo\u0026#34;] EOF 5- Build image t·ª´ Dockerfile tr√™n\ndocker image build -t keeldemo:latest . Confirm r·∫±ng images ƒë√£ ƒë∆∞·ª£c t·∫°o ra\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE keeldemo latest 83dc90c47721 18 seconds ago 6.51MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 37109af028d4 19 seconds ago 325MB golang 1.11-alpine e116d2efa2ab 8 months ago 312MB ƒê√°nh tag cho images ƒë√≥\ndocker image tag keeldemo:latest $DOCKER_USERNAME/keeldemo:latest Push images c√≥ tag latest ƒë√≥ l√™n Docker Hub c·ªßa b·∫°n\ndocker push $DOCKER_USERNAME/keeldemo:latest 6- Login to your GKE cluster\ngcloud container clusters get-credentials \u0026lt;YOUR_CLUSTER\u0026gt; --zone asia-northeast1-a --project \u0026lt;YOUR_PROJECT\u0026gt; Fetching cluster endpoint and auth data. kubeconfig entry generated for \u0026lt;YOUR_CLUSTER\u0026gt;. 7- Install Keel to your cluster, this will create new keel namespace for Keel\nkubectl apply -f https://sunstone.dev/keel?namespace=keel\u0026amp;username=admin\u0026amp;password=admin\u0026amp;tag=latest Check your Keel pods and service\nkubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE keel pod/keel-7c75bc645c-tpbzw 1/1 Running 0 99s kube-system pod/kube-dns-5f7d7d8796-5kz4s 3/3 Running 0 14m kube-system pod/kube-dns-autoscaler-6b7f784798-xtm6s 1/1 Running 0 14m kube-system pod/kube-proxy-gke-hoang1105-default-pool-4cc65383-5zqf 1/1 Running 0 14m kube-system pod/l7-default-backend-84c9fcfbb-rv8fs 1/1 Running 0 14m kube-system pod/metrics-server-v0.3.3-7599dd85cd-vmzqb 2/2 Running 0 14m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 14m keel service/keel LoadBalancer 10.68.170.35 35.221.94.65 9300:32712/TCP 101s kube-system service/default-http-backend NodePort 10.68.178.170 \u0026lt;none\u0026gt; 80:32137/TCP 14m kube-system service/kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 14m kube-system service/metrics-server ClusterIP 10.68.177.176 \u0026lt;none\u0026gt; 443/TCP 14m 8- T·∫°o file keel-demo-deployment.yaml ƒë·ªÉ deploy simple app\ncat \u0026gt; ./keel-demo-deployment.yaml \u0026lt;\u0026lt;EOF --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: wd namespace: default labels: name: \u0026#34;wd\u0026#34; annotations: keel.sh/policy: major keel.sh/trigger: poll keel.sh/pollSchedule: \u0026#34;@every 30s\u0026#34; spec: replicas: 1 template: metadata: name: wd labels: app: wd spec: containers: - image: $DOCKER_USERNAME/keeldemo:latest imagePullPolicy: Always name: wd ports: - containerPort: 8888 EOF Gi·∫£i th√≠ch file tr√™n:\nGi·ªëng nh∆∞ nh·ªØng file Deployment b√¨nh th∆∞·ªùng,\nnh∆∞ng ƒë∆∞·ª£c add th√™m annotations ƒë·ªÉ d√πng keel,\nc·ª© 30s n√≥ s·∫Ω poll t·ª´ Docker Hub c·ªßa b·∫°n v·ªÅ xem c√≥ images m·ªõi ko,\nn·∫øu c√≥ th√¨ n√≥ s·∫Ω t·∫°o pod m·ªõi t·ª´ images m·ªõi 1 c√°ch t·ª± ƒë·ªông (ƒê√¢y ch√≠nh l√† CD (Continuos Deployment))\nDeploy your simple application\nkubectl apply -f keel-demo-deployment.yaml Check your simple app pod:\nkubectl get pods,svc -A k get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/wd-75c65cbf5b-fg2r2 1/1 Running 0 8s keel pod/keel-7c75bc645c-tpbzw 1/1 Running 0 3m33s kube-system pod/kube-dns-5f7d7d8796-5kz4s 3/3 Running 0 16m kube-system pod/kube-dns-autoscaler-6b7f784798-xtm6s 1/1 Running 0 16m kube-system pod/kube-proxy-gke-hoang1105-default-pool-4cc65383-5zqf 1/1 Running 0 16m kube-system pod/l7-default-backend-84c9fcfbb-rv8fs 1/1 Running 0 16m kube-system pod/metrics-server-v0.3.3-7599dd85cd-vmzqb 2/2 Running 0 16m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 16m keel service/keel LoadBalancer 10.68.170.35 35.221.94.65 9300:32712/TCP 3m34s kube-system service/default-http-backend NodePort 10.68.178.170 \u0026lt;none\u0026gt; 80:32137/TCP 16m kube-system service/kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 16m kube-system service/metrics-server ClusterIP 10.68.177.176 \u0026lt;none\u0026gt; 443/TCP 16m 9- View app c·ªßa b·∫°n b·∫±ng ch·ª©c nƒÉng Web Preview c·ªßa Cloudshell\nkubectl port-forward pod/wd-75c65cbf5b-fg2r2 8888:8888 (·∫£nh) Test Gi·ªù b·∫°n mu·ªën deploy 1 version m·ªõi c·ªßa app th√¨ ch·ªâ c·∫ßn t·∫°o dc images m·ªõi th√¥i. C√¥ng vi·ªác c√≤n l·∫°i do keel x·ª≠ l√Ω\n1- S·ª≠a app th√†nh version 2\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) var version = 2 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;Hello ‰∏ñÁïå... from v\u0026quot;, version) } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, handler) log.Fatal(http.ListenAndServe(\u0026quot;:8888\u0026quot;, nil)) } 2- Build, tag, v√† push l·∫°i image m·ªõi l√™n Docker Hub\ndocker image build -t keeldemo:latest . docker image tag keeldemo:latest $DOCKER_USERNAME/keeldemo:latest docker push $DOCKER_USERNAME/keeldemo:latest Ch·ªù 30s v√† check pod m·ªõi c·ªßa app ƒë√£ ƒë∆∞·ª£c t·∫°o ra\nkubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/wd-69c5c4fd89-xwxhn 1/1 Running 0 12s keel pod/keel-7c75bc645c-tpbzw 1/1 Running 0 19m kube-system pod/kube-dns-5f7d7d8796-5kz4s 3/3 Running 0 32m kube-system pod/kube-dns-autoscaler-6b7f784798-xtm6s 1/1 Running 0 32m kube-system pod/kube-proxy-gke-hoang1105-default-pool-4cc65383-5zqf 1/1 Running 0 32m kube-system pod/l7-default-backend-84c9fcfbb-rv8fs 1/1 Running 0 32m kube-system pod/metrics-server-v0.3.3-7599dd85cd-vmzqb 2/2 Running 0 32m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 32m keel service/keel LoadBalancer 10.68.170.35 35.221.94.65 9300:32712/TCP 19m kube-system service/default-http-backend NodePort 10.68.178.170 \u0026lt;none\u0026gt; 80:32137/TCP 32m kube-system service/kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 32m kube-system service/metrics-server ClusterIP 10.68.177.176 \u0026lt;none\u0026gt; 443/TCP 32m 3- View app c·ªßa b·∫°n b·∫±ng ch·ª©c nƒÉng Web Preview c·ªßa Cloudshell\nkubectl port-forward pods/wd-69c5c4fd89-xwxhn 8888:8888 (·∫£nh)\nVersion ƒë√£ ƒë∆∞·ª£c thay ƒë·ªïi\nDone! Nh∆∞ v·∫≠y l√† v·ªÅ c∆° b·∫£n ta ƒë√£ hi·ªÉu ƒë∆∞·ª£c c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa keel, ngo√†i ra keel c√≤n 1 v√†i t√≠nh nƒÉng hay ho n·ªØa (notify qua Slack, UI Dashboard, integrate v·ªõi AWS ECR, GCR, Gitlab\u0026hellip;) m√† b·∫°n c√≥ th·ªÉ t√¨m hi·ªÉu tr√™n trang ch·ªß c·ªßa keel t·∫°i ƒë√¢y (https://keel.sh)\nREFERENCES\nhttps://github.com/keel-hq/keel\nhttps://keel.sh\nhttps://github.com/Chams91/Golang_keelDemo\n","href":"/bk/k8s-xi-using-keel-on-gke-cluster-for-continuous-deployment/","title":"K8S 11: Using Keel on GKE Cluster for Continuous Deployment (CD)"},{"content":"Gi·ªõi thi·ªáu  Keel is a tool for automating Kubernetes deployment updates. Keel is stateless, robust and lightweight.\n Keel ƒë∆∞·ª£c ƒë√°nh gi√° l√† 1 tool d·ªÖ c√†i ƒë·∫∑t, d·ªÖ s·ª≠ d·ª•ng, v√† r·∫•t nh·∫π.\nB√†i n√†y ch·ªâ ƒë∆°n gi·∫£n l√† m√¨nh mu·ªën demo v·ªÅ c√°ch s·ª≠ d·ª•ng 1 tool lightweight ƒë∆∞·ª£c nhi·ªÅu ng∆∞·ªùi gi·ªõi thi·ªáu (Keel) th√¥i. Ngo√†i ra c√≤n 1 s·ªë tool kh√°c c≈©ng ƒë∆∞·ª£c suggest nhi·ªÅu, ƒë√≥ l√† Weave Flux. Tool n√†y nhi·ªÅu ch·ª©c nƒÉng h∆°n Keel, m√¨nh c≈©ng mu·ªën th·ª≠ d√πng trong t∆∞∆°ng lai.\nquay l·∫°i v·ªõi Keel:\nY√™u c·∫ßu Kh√¥ng g√¨ kh√°c ngo√†i 1 Cluster tr√™n GKE.\nNgo√†i ra b√†i n√†y m√¨nh s·∫Ω d√πng Docker Hub ƒë·ªÉ l√†m Registry (ch·ªó l∆∞u Docker images) n√™n b·∫°n c≈©ng c·∫ßn c√≥ account Docker Hub.\nC√°ch l√†m 1- Trong Cloudshell c·ªßa GCP Console, t·∫°o folder ƒë·ªÉ l√†m workspace c·ªßa b·∫°n\nmkdir keel-demo-golang-app \u0026amp;\u0026amp; cd keel-demo-golang-app 2- Chu·∫©n b·ªã c√°c bi·∫øn m√¥i tr∆∞·ªùng, v√† login v√† Docker account c·ªßa b·∫°n\nexport DOCKER_USERNAME=AAAAAA export DOCKER_PASSWORD=BBBBBB docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; 3- Vi·∫øt 1 app ƒë∆°n gi·∫£n ch·∫°y b·∫±ng Go (command sau s·∫Ω t·∫°o ra file main.go)\ncat \u0026gt; ./main.go \u0026lt;\u0026lt;EOF package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var version = 1 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello ‰∏ñÁïå... from v %s\u0026#34;, version) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Fatal(http.ListenAndServe(\u0026#34;:8888\u0026#34;, nil)) } EOF 4- T·∫°o Dockerfile v√† build images ri√™ng c·ªßa b·∫°n\ncat \u0026gt; ./Dockerfile \u0026lt;\u0026lt;EOF FROM golang:1.11-alpine AS build WORKDIR / COPY main.go go.* / RUN CGO_ENABLED=0 go build -o /bin/demo FROM scratch COPY --from=build /bin/demo /bin/demo ENTRYPOINT [\u0026#34;/bin/demo\u0026#34;] EOF 5- Build image t·ª´ Dockerfile tr√™n\ndocker image build -t keeldemo:latest . Confirm r·∫±ng images ƒë√£ ƒë∆∞·ª£c t·∫°o ra\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE keeldemo latest 83dc90c47721 18 seconds ago 6.51MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 37109af028d4 19 seconds ago 325MB golang 1.11-alpine e116d2efa2ab 8 months ago 312MB ƒê√°nh tag cho images ƒë√≥\ndocker image tag keeldemo:latest $DOCKER_USERNAME/keeldemo:latest Push images c√≥ tag latest ƒë√≥ l√™n Docker Hub c·ªßa b·∫°n\ndocker push $DOCKER_USERNAME/keeldemo:latest 6- Login to your GKE cluster\ngcloud container clusters get-credentials \u0026lt;YOUR_CLUSTER\u0026gt; --zone asia-northeast1-a --project \u0026lt;YOUR_PROJECT\u0026gt; Fetching cluster endpoint and auth data. kubeconfig entry generated for \u0026lt;YOUR_CLUSTER\u0026gt;. 7- Install Keel to your cluster, this will create new keel namespace for Keel\nkubectl apply -f https://sunstone.dev/keel?namespace=keel\u0026amp;username=admin\u0026amp;password=admin\u0026amp;tag=latest Check your Keel pods and service\nkubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE keel pod/keel-7c75bc645c-tpbzw 1/1 Running 0 99s kube-system pod/kube-dns-5f7d7d8796-5kz4s 3/3 Running 0 14m kube-system pod/kube-dns-autoscaler-6b7f784798-xtm6s 1/1 Running 0 14m kube-system pod/kube-proxy-gke-hoang1105-default-pool-4cc65383-5zqf 1/1 Running 0 14m kube-system pod/l7-default-backend-84c9fcfbb-rv8fs 1/1 Running 0 14m kube-system pod/metrics-server-v0.3.3-7599dd85cd-vmzqb 2/2 Running 0 14m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 14m keel service/keel LoadBalancer 10.68.170.35 35.221.94.65 9300:32712/TCP 101s kube-system service/default-http-backend NodePort 10.68.178.170 \u0026lt;none\u0026gt; 80:32137/TCP 14m kube-system service/kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 14m kube-system service/metrics-server ClusterIP 10.68.177.176 \u0026lt;none\u0026gt; 443/TCP 14m 8- T·∫°o file keel-demo-deployment.yaml ƒë·ªÉ deploy simple app\ncat \u0026gt; ./keel-demo-deployment.yaml \u0026lt;\u0026lt;EOF --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: wd namespace: default labels: name: \u0026#34;wd\u0026#34; annotations: keel.sh/policy: major keel.sh/trigger: poll keel.sh/pollSchedule: \u0026#34;@every 30s\u0026#34; spec: replicas: 1 template: metadata: name: wd labels: app: wd spec: containers: - image: $DOCKER_USERNAME/keeldemo:latest imagePullPolicy: Always name: wd ports: - containerPort: 8888 EOF Gi·∫£i th√≠ch file tr√™n:\nGi·ªëng nh∆∞ nh·ªØng file Deployment b√¨nh th∆∞·ªùng,\nnh∆∞ng ƒë∆∞·ª£c add th√™m annotations ƒë·ªÉ d√πng keel,\nc·ª© 30s n√≥ s·∫Ω poll t·ª´ Docker Hub c·ªßa b·∫°n v·ªÅ xem c√≥ images m·ªõi ko,\nn·∫øu c√≥ th√¨ n√≥ s·∫Ω t·∫°o pod m·ªõi t·ª´ images m·ªõi 1 c√°ch t·ª± ƒë·ªông (ƒê√¢y ch√≠nh l√† CD (Continuos Deployment))\nDeploy your simple application\nkubectl apply -f keel-demo-deployment.yaml Check your simple app pod:\nkubectl get pods,svc -A k get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/wd-75c65cbf5b-fg2r2 1/1 Running 0 8s keel pod/keel-7c75bc645c-tpbzw 1/1 Running 0 3m33s kube-system pod/kube-dns-5f7d7d8796-5kz4s 3/3 Running 0 16m kube-system pod/kube-dns-autoscaler-6b7f784798-xtm6s 1/1 Running 0 16m kube-system pod/kube-proxy-gke-hoang1105-default-pool-4cc65383-5zqf 1/1 Running 0 16m kube-system pod/l7-default-backend-84c9fcfbb-rv8fs 1/1 Running 0 16m kube-system pod/metrics-server-v0.3.3-7599dd85cd-vmzqb 2/2 Running 0 16m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 16m keel service/keel LoadBalancer 10.68.170.35 35.221.94.65 9300:32712/TCP 3m34s kube-system service/default-http-backend NodePort 10.68.178.170 \u0026lt;none\u0026gt; 80:32137/TCP 16m kube-system service/kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 16m kube-system service/metrics-server ClusterIP 10.68.177.176 \u0026lt;none\u0026gt; 443/TCP 16m 9- View app c·ªßa b·∫°n b·∫±ng ch·ª©c nƒÉng Web Preview c·ªßa Cloudshell\nkubectl port-forward pod/wd-75c65cbf5b-fg2r2 8888:8888 (·∫£nh) Test Gi·ªù b·∫°n mu·ªën deploy 1 version m·ªõi c·ªßa app th√¨ ch·ªâ c·∫ßn t·∫°o dc images m·ªõi th√¥i. C√¥ng vi·ªác c√≤n l·∫°i do keel x·ª≠ l√Ω\n1- S·ª≠a app th√†nh version 2\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) var version = 2 func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;Hello ‰∏ñÁïå... from v\u0026quot;, version) } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, handler) log.Fatal(http.ListenAndServe(\u0026quot;:8888\u0026quot;, nil)) } 2- Build, tag, v√† push l·∫°i image m·ªõi l√™n Docker Hub\ndocker image build -t keeldemo:latest . docker image tag keeldemo:latest $DOCKER_USERNAME/keeldemo:latest docker push $DOCKER_USERNAME/keeldemo:latest Ch·ªù 30s v√† check pod m·ªõi c·ªßa app ƒë√£ ƒë∆∞·ª£c t·∫°o ra\nkubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/wd-69c5c4fd89-xwxhn 1/1 Running 0 12s keel pod/keel-7c75bc645c-tpbzw 1/1 Running 0 19m kube-system pod/kube-dns-5f7d7d8796-5kz4s 3/3 Running 0 32m kube-system pod/kube-dns-autoscaler-6b7f784798-xtm6s 1/1 Running 0 32m kube-system pod/kube-proxy-gke-hoang1105-default-pool-4cc65383-5zqf 1/1 Running 0 32m kube-system pod/l7-default-backend-84c9fcfbb-rv8fs 1/1 Running 0 32m kube-system pod/metrics-server-v0.3.3-7599dd85cd-vmzqb 2/2 Running 0 32m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 32m keel service/keel LoadBalancer 10.68.170.35 35.221.94.65 9300:32712/TCP 19m kube-system service/default-http-backend NodePort 10.68.178.170 \u0026lt;none\u0026gt; 80:32137/TCP 32m kube-system service/kube-dns ClusterIP 10.68.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 32m kube-system service/metrics-server ClusterIP 10.68.177.176 \u0026lt;none\u0026gt; 443/TCP 32m 3- View app c·ªßa b·∫°n b·∫±ng ch·ª©c nƒÉng Web Preview c·ªßa Cloudshell\nkubectl port-forward pods/wd-69c5c4fd89-xwxhn 8888:8888 (·∫£nh)\nVersion ƒë√£ ƒë∆∞·ª£c thay ƒë·ªïi\nDone! Nh∆∞ v·∫≠y l√† v·ªÅ c∆° b·∫£n ta ƒë√£ hi·ªÉu ƒë∆∞·ª£c c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa keel, ngo√†i ra keel c√≤n 1 v√†i t√≠nh nƒÉng hay ho n·ªØa (notify qua Slack, UI Dashboard, integrate v·ªõi AWS ECR, GCR, Gitlab\u0026hellip;) m√† b·∫°n c√≥ th·ªÉ t√¨m hi·ªÉu tr√™n trang ch·ªß c·ªßa keel t·∫°i ƒë√¢y (https://keel.sh)\nREFERENCES\nhttps://github.com/keel-hq/keel\nhttps://keel.sh\nhttps://github.com/Chams91/Golang_keelDemo\n","href":"/posts/k8s-xi-using-keel-on-gke-cluster-for-continuous-deployment/","title":"K8S 11: Using Keel on GKE Cluster for Continuous Deployment (CD)"},{"content":"","href":"/tags/keel/","title":"Keel"},{"content":"Y√™u C·∫ßu B·∫°n ƒë√£ c√≥ 1 Github Pages Blog d·ª±ng b·∫±ng Hugo, s·ª≠ d·ª•ng theme Minimo ho·∫∑c nh·ªØng theme t∆∞∆°ng t·ª± nh∆∞ Minimo\nGi·ªõi thi·ªáu N·∫øu b·∫°n ƒëang host 1 Hugo blog tr√™n Github, h·∫≥n b·∫°n s·∫Ω mu·ªën ng∆∞·ªùi kh√°c c√≥ th·ªÉ comment tr√™n blog c·ªßa m√¨nh\nNh∆∞ng v√¨ blog c·ªßa b·∫°n l√† static site. N·∫øu ng∆∞·ªùi kh√°c c√≥ th·ªÉ comment dc th√¨ ch·∫≥ng ph·∫£i l√† dynamic site sao :v\nƒê·ªÉ l√†m ƒëi·ªÅu n√†y b·∫°n c·∫ßn 1 b√™n th·ª© 3 ƒë·ªÉ host c√°i ch·ª©c nƒÉng n√†y, ·ªü ƒë√¢y m√¨nh ch·ªâ ra 2 c√°ch:\n-C√°ch 1: S·ª≠ d·ª•ng staticmanlab c·ªßa @VincentTam\n-C√°ch 2: T·ª± setup 1 h·ªá th·ªëng Staticman l√†m comment system ri√™ng tr√™n Heroku free plan\n-C√°ch 3 (Recommend): T·ª± setup 1 h·ªá th·ªëng Staticman l√†m comment system ri√™ng tr√™n 1 VM\nC√°ch 1 th√¨ do ch√∫ng ta s·ª≠ d·ª•ng API c·ªßa ƒë·ªìng ch√≠ @VincentTam t·∫°o ra tr√™n Heroku n√™n n√≥ ph·ª• thu·ªôc v√†o ƒë·ªìng ch√≠ ·∫•y, n·∫øu v√¨ 1 l√Ω do n√†o ƒë√≥ anh ta x√≥a app th√¨ ch·ª©c nƒÉng comment s·∫Ω b·ªã disable. Th·∫ø n√™n c√°ch n√†y hi·ªán t·∫°i d√πng ƒë∆∞·ª£c, nh∆∞ng ko ho√†n to√†n ƒë·∫£m b·∫£o trong t∆∞∆°ng lai.\nC√°ch 2 th√¨ b·∫°n s·∫Ω t·ª± host app ƒë√≥ tr√™n Heroku server c·ªßa b·∫°n n√™n bao gi·ªù b·∫°n ko d√πng Heroku n·ªØa th√¨ m·ªõi th√¥i (nhi·ªÅu kh·∫£ nƒÉng v√¨ Heroku ko cho x√†i free n√™n b·∫°n ko d√πng n·ªØa ü§£)\nC√°ch 3 th√¨ b·∫°n s·∫Ω t·ª± host app ƒë√≥ tr√™n server ri√™ng c·ªßa b·∫°n n√™n bao gi·ªù b·∫°n stop server th√¨ m·ªõi th√¥i.\nGi·ªù m√¨nh s·∫Ω gi·ªõi thi·ªáu C√°ch 1:\n1. C√°ch 1: S·ª≠ d·ª•ng staticmanlab c·ªßa @VincentTam 1- Add @staticmanlab v√†o collaborator github setting c·ªßa github repo Hugo blog:\n2- Access v√†o link sau ƒë·ªÉ active c√°i @staticmanlab:\nhttps://staticman3.herokuapp.com/v3/connect/github/\u0026lt;GITHUB_USER\u0026gt;/\u0026lt;REPO_NAME\u0026gt;/\nnh·ªõ s·ª≠a \u0026lt;GITHUB_USER\u0026gt;,\u0026lt;REPO_NAME\u0026gt; l√† c·ªßa b·∫°n M√†n h√¨nh tr·∫£ v·ªÅ n·∫øu th·∫•y ch·ªØ OK! th√¨ m·ªõi l√† th√†nh c√¥ng nh√©\n3- Add file staticman.yml v√†o root folder (c√πng v·ªã tr√≠ v·ªõi file config.toml), n·ªôi dung file staticman.yml nh∆∞ sau, nh·ªõ s·ª≠a l·∫°i d√≤ng name: '\u0026lt;REPO_NAME\u0026gt;' t√πy theo repository name c·ªßa b·∫°n:\ncomments: allowedFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;parent_id\u0026#39;, \u0026#39;permalink\u0026#39;, \u0026#39;site\u0026#39;] branch: \u0026#39;master\u0026#39; commitMessage: \u0026#34;add [comment]: by {fields.author} \u0026lt;Staticman\u0026gt;\\n\\n{fields.permalink}#comment-{@id}\u0026#34; filename: \u0026#39;{@id}\u0026#39; format: \u0026#39;yaml\u0026#39; generatedFields: date: type: date options: format: \u0026#39;timestamp\u0026#39; moderation: false name: \u0026#39;\u0026lt;REPO_NAME\u0026gt;\u0026#39; path: \u0026#39;data/comments/{options.postId}\u0026#39; requiredFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;] reCaptcha: enabled: false # siteKey: \u0026#39;\u0026#39; # secret: \u0026#39;\u0026#39; transforms: email: md5 4- Update file config.toml nh∆∞ ƒëo·∫°n n√†y, nh·ªõ s·ª≠a \u0026lt;GITHUB_USER\u0026gt; v√† \u0026lt;REPO_NAME\u0026gt; ph√π h·ª£p v·ªõi th√¥ng tin c·ªßa b·∫°n:\n# Staticman [params.comments.staticman] enable = true apiEndpoint = \u0026#34;https://staticman3.herokuapp.com/v3/entry/github\u0026#34; maxDepth = 2 [params.comments.staticman.github] username = \u0026#34;\u0026lt;GITHUB_USER\u0026gt;\u0026#34; repository = \u0026#34;\u0026lt;REPO_NAME\u0026gt;\u0026#34; branch = \u0026#34;master\u0026#34; V·∫≠y l√† xong, gi·ªù push nh·ªØng thay ƒë·ªïi v·ª´a xong l√™n nh√°nh master th√¥i.\nGi·∫£i th√≠ch:\nM·ªói khi c√≥ ng∆∞·ªùi comment, ƒë·ªìng nghƒ©a v·ªõi vi·ªác h·ªç s·∫Ω g·ª≠i 1 POST request ƒë·∫øn Staticman API c·ªßa ƒë·ªìng ch√≠ @VincentTam ƒë√£ t·∫°o ra tr√™n Heroku.\nSau khi API ƒë√≥ nh·∫≠n ƒë∆∞·ª£c n·ªôi dung comment, n√≥ s·∫Ω commit n·ªôi dung comment ƒë√≥ v√†o branch master c·ªßa Blog ch√∫ng ta (C·ª• th·ªÉ th√¨ n·ªôi dung comment ƒë√≥ s·∫Ω dc push v√†o folder /data/comments).\nKhi nh√°nh master ph√°t hi·ªán commit m·ªõi ƒë√≥, n√≥ s·∫Ω build l·∫°i page v√† hi·ªÉn th·ªã l·∫°i n·ªôi dung comment tr√™n page. ƒê∆°n gi·∫£n th·∫ø th√¥i. üòÜ\n2. C√°ch 2: T·ª± setup 1 h·ªá th·ªëng Staticman v2 (d√πng Heroku free plan) 2.1. Y√™u c·∫ßu B·∫°n ƒëang d√πng Windows, ƒë√£ c√†i Gitbash, Visual Studio Code, ƒë√£ ƒëƒÉng k√Ω t√†i kho·∫£n Heroku (r·∫•t ƒë∆°n gi·∫£n, ƒëƒÉng k√Ω c≈©ng free), ƒë√£ c√†i Heroku CLI v√†o m√°y\n2.2. C√°ch l√†m 2.2.1. T·∫°o RSA Private key M·ªü GitBash l√™n, ch·∫°y command sau ƒë·ªÉ generate ra RSA Private key\nopenssl genrsa -out key.pem N√≥ s·∫Ω t·∫°o ra 1 file key.pem\n2.2.2. T·∫°o 1 account Github ri√™ng (g·ªçi l√† bot account) ph·ª•c v·ª• m·ª•c ƒë√≠ch d√πng Staticman Login v√†o bot account, d√πng link n√†y ƒë·ªÉ t·∫°o Personal Access Token:\nhttps://github.com/settings/tokens\nH√£y ch·ªçn nh·ªØng quy·ªÅn nh∆∞ read:packages, repo, write:packages\nNh·ªõ copy c√°i chu·ªói token ƒë√≥ ra ƒë√¢u ƒë√≥ l√°t n·ªØa d√πng nh√©\n2.2.3. Clone source staticman v·ªÅ v√† cd v√†o n√≥ git clone https://github.com/eduardoboucas/staticman cd staticman M·ªü folder staticman ƒë√≥ trong Visual Studio Code (VSC) ƒë·ªÉ d√πng Powershell terminal c·ªßa VSC (t·ª´ b∆∞·ªõc n√†y ko d√πng GitBash n·ªØa v√¨ GitBash ko ƒëƒÉng nh·∫≠p Heroku ƒë∆∞·ª£c)\n2.2.4. T·∫°o 1 file t√™n l√† Procfile trong folder staticman n·ªôi dung file ch·ªâ c√≥ 1 d√≤ng l√†:\nweb: npm start 2.2.5. T·∫°o file config.production.json trong folder staticman n·ªôi dung file:\n{ \u0026quot;githubToken\u0026quot;: process.env.GITHUB_TOKEN, \u0026quot;rsaPrivateKey\u0026quot;: JSON.stringify(process.env.RSA_PRIVATE_KEY), \u0026quot;port\u0026quot;: 8080 } 2.2.6. S·ª≠a file .gitignore add th√™m d√≤ng sau:\n!config.production.json 2.2.7. Login v√†o Heroku heroku login T·∫°o app tr√™n Heroku\nheroku create \u0026lt;app_name\u0026gt; T·∫°o bi·∫øn m√¥i tr∆∞·ªùng c·ªßa app tr√™n Heroku\nheroku config:set GITHUB_TOKEN=\u0026#34;\u0026lt;Paste token l·∫•y ƒë∆∞·ª£c ·ªü step 2\u0026gt;\u0026#34; heroku config:set RSA_PRIVATE_KEY=\u0026#34;$(cat ../key.pem)\u0026#34; heroku config:set NODE_ENV=\u0026#34;production\u0026#34; check logs nh√¨n ƒë·∫πp ƒë·∫πp, ko c√≥ ERROR g√¨ l√† ƒë∆∞·ª£c:\nheroku logs --tail T·∫°o branch production:\ngit checkout -b production Commit thay ƒë·ªïi l√™n Heroku:\ngit add . git commit -m \u0026#34;Setup Staticman v2 to Heroku\u0026#34; git push heroku production:master check logs nh√¨n ƒë·∫πp ƒë·∫πp ki·ªÉu n√†y l√† ok:\nheroku logs --tail Truy c·∫≠p v√†o link n√†y ƒë·ªÉ check app ƒë√£ ho·∫°t ƒë·ªông ch∆∞a: https://\u0026lt;app_name\u0026gt;.herokuapp.com\nN·∫øu tr·∫£ v·ªÅ Hello from Staticman version 3.0.0! nghƒ©a l√† ƒë√£ deploy th√†nh c√¥ng\n2.2.8. Add bot account v√†o l√†m Collaborator Login v√†o Github ch√≠nh ch·ªß c·ªßa b·∫°n (ko ph·∫£i bot account nh√©)\nV√†o repository ch·ª©a blog c·ªßa b·∫°n, ch·ªçn m·ª•c Settings -\u0026gt; Manage Access -\u0026gt; Invite a Collaborator\nG√µ username c·ªßa bot account v√†o, v√† Invite n√≥\nƒê·∫øn ƒë√¢y th√¨ ƒê·ª™NG login v√†o con bot account ƒë·ªÉ accept l·ªùi m·ªùi b·∫±ng tay nh√©!\nB·∫°n c·∫ßn ph·∫£i truy c·∫≠p v√†o link n√†y ƒë·ªÉ accept m·ªõi ƒë∆∞·ª£c:\n(ch√∫ √Ω thay ƒë·ªïi \u0026lt;app_name\u0026gt;, \u0026lt;GITHUB_USER\u0026gt;, \u0026lt;REPO_NAME\u0026gt; cho ph√π h·ª£p) https://\u0026lt;app_name\u0026gt;.herokuapp.com/v2/connect/\u0026lt;GITHUB_USER\u0026gt;/\u0026lt;REPO_NAME\u0026gt;/ N·∫øu hi·ªán ch·ªØ OK! c√≥ nghƒ©a l√† ƒë√£ accept invitation th√†nh c√¥ng.\n2.2.9. Gi·ªù c·∫ßn s·ª≠a 1 ch√∫t c√°i Blog repository c·ªßa b·∫°n n·ªØa 9.1- Add file staticman.yml v√†o root folder (c√πng v·ªã tr√≠ v·ªõi file config.toml),\nn·ªôi dung file staticman.yml nh∆∞ sau, nh·ªõ s·ª≠a l·∫°i d√≤ng name: '\u0026lt;REPO_NAME\u0026gt;' t√πy theo repo name c·ªßa b·∫°n:\ncomments: allowedFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;parent_id\u0026#39;, \u0026#39;permalink\u0026#39;, \u0026#39;site\u0026#39;] branch: \u0026#39;master\u0026#39; commitMessage: \u0026#34;add [comment]: by {fields.author} \u0026lt;Staticman\u0026gt;\\n\\n{fields.permalink}#comment-{@id}\u0026#34; filename: \u0026#39;{@id}\u0026#39; format: \u0026#39;yaml\u0026#39; generatedFields: date: type: date options: format: \u0026#39;timestamp\u0026#39; moderation: false name: \u0026#39;\u0026lt;REPO_NAME\u0026gt;\u0026#39; path: \u0026#39;data/comments/{options.postId}\u0026#39; requiredFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;] reCaptcha: enabled: false # siteKey: \u0026#39;\u0026#39; # secret: \u0026#39;\u0026#39; transforms: email: md5 9.2- Update file config.toml nh∆∞ ƒëo·∫°n n√†y, nh·ªõ s·ª≠a \u0026lt;GITHUB_USER\u0026gt;, \u0026lt;app_name\u0026gt; v√† \u0026lt;REPO_NAME\u0026gt; ph√π h·ª£p v·ªõi th√¥ng tin c·ªßa b·∫°n:\n# Staticman [params.comments.staticman] enable = true apiEndpoint = \u0026#34;https://\u0026lt;app_name\u0026gt;.herokuapp.com/v2/entry\u0026#34; maxDepth = 2 [params.comments.staticman.github] username = \u0026#34;\u0026lt;GITHUB_USER\u0026gt;\u0026#34; repository = \u0026#34;\u0026lt;REPO_NAME\u0026gt;\u0026#34; branch = \u0026#34;master\u0026#34; Gi·ªù push nh·ªØng thay ƒë·ªïi v·ª´a xong l√™n nh√°nh master th√¥i.\nGi·ªù b·∫°n ƒë√£ c√≥ th·ªÉ comment v√†o blog ƒë·ªÉ xem Staticman c·ªßa ri√™ng b·∫°n ho·∫°t ƒë·ªông th·∫ø n√†o r·ªìi üòÜüòÜ\nTuy nhi√™n th√¨ Heroku c≈©ng c√≥ 1 s·ªë nh∆∞·ª£c ƒëi·ªÉm ng∆∞·ªùi ta li·ªát k√™ nh∆∞ sau:\n Limitations of Heroku apps Free Heroku apps come with some limitations. The most important limitation is that the number of free dyno hours is limited. A dyno is the isolated container in which your application is running. A free dyno begins to sleep after 30 mins of inactivity. Otherwise, it is always on as long as you still have remaining dyno hours (currently 550 free dyno hours per month).\n  When your free dyno hours are exhausted, all of your free dynos start sleeping. You will receive a notification from Heroku before this happens though. If the only app that you are running is Staticman, however, it is unlikely that you will use up you free dyno hours because this would mean that you are receiving comments for more than 18 hours per day, which would be quite a lot.\n  So, the main disadvantage of free dynos when is that posting comments will be slow (delay of a few seconds) when no one has posted anything within a while (30 minutes). In my opinion, this limitation is sufferable for a free service. If you want to get rid of this limitation or if you need more dyno hours, you can always upgrade to a paid dyno.\n Trong t∆∞∆°ng lai c√≥ th·ªÉ m√¨nh s·∫Ω d√πng th√™m 1 s·ªë feature n·ªØa, nh∆∞ Captcha, Mailgun\n2.3. B·∫≠t t√≠nh nƒÉng Captcha cho c√°c comment (cho C√°ch 2 d√πng Heroku free plan) G·∫ßn ƒë√¢y m√¨nh ph√°t hi·ªán c√≥ 1 s·ªë comment tr√™n blog c·ªßa m√¨nh c√≥ n·ªôi dung r·∫•t l·∫°, comment kh√¥ng li√™n quan b√†i vi·∫øt, ho·∫∑c comment ch·ª©a nh·ªØng ƒë∆∞·ªùng link m√£ ƒë·ªôc r·∫•t nguy hi·ªÉm. Thi tho·∫£ng m√¨nh ph·∫£i v√†o v√† x√≥a comment ƒë√≥ ƒëi.\nTh·∫ø n√™n m√¨nh quy·∫øt ƒë·ªãnh apply reCaptcha cho t√≠nh nƒÉng comment n√†y. √çt nh·∫•t th√¨ c≈©ng l√†m kh√≥ 1 ch√∫t cho b·ªçn bot.\n2.3.1. ƒêƒÉng k√Ω ReCaptcha ƒë·ªÉ l·∫•y key v√†o link sau ƒë·ªÉ ƒëƒÉng k√Ω nh√©:\nhttps://www.google.com/recaptcha/admin\nNh·∫≠p c√°c th√¥ng tin nh∆∞ n√†y v√† ·∫•n Submit:\nSau khi submit b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c SiteKey v√† Secret, h√£y gi·ªØ ch√∫ng c·∫©n th·∫≠n\n2.3.2. Encrypt Secret b·∫±ng c√°ch truy c·∫≠p link sau https://\u0026lt;app_name\u0026gt;.herokuapp.com/v3/encrypt/\u0026lt;Secret\u0026gt;\nV√¨ m√¨nh build Static tr√™n Heroku n√™n d∆∞·ªùng link c·ªßa m√¨nh n√≥ tr√¥ng nh∆∞ v·∫≠y.\nB·∫°n xem Staticman ApiEnpoint c·ªßa b·∫°n l√† g√¨ ƒë·ªÉ call ƒë·∫øn ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ·ª©ng nh√©.\nN√≥ s·∫Ω tr·∫£ v·ªÅ k·∫øt qu·∫£ l√† 1 chu·ªói k√≠ t·ª± ƒë√£ m√£ h√≥a h√£y copy chu·ªói ·∫•y.\n2.3.3. S·ª≠a file staticman.yml ~~~ reCaptcha: enabled: true siteKey: \u0026#39;65AAAAAABkgfodgkeMK_VmkgmkKkf3\u0026#39; secret: \u0026#39;PxQcp............\u0026#39; ~~~ 2.3.4. H√£y ch·∫Øc ch·∫Øn r·∫±ng theme m√† b·∫°n ƒëang d√πng c√≥ h·ªó tr·ª£ reCaptcha V√≠ d·ª• m√¨nh ƒëang d√πng theme Minimo. Trong ƒë∆∞·ªùng link themes/minimo/layouts/partials/comments/staticman/form.html, m√¨nh c√≥ th·ªÉ th·∫•y t·ª´ nƒÉm 2019 h·ªç m·ªõi add th√™m c√°c m√£ html ƒë·ªÉ h·ªó tr·ª£ reCaptcha (tr∆∞·ªõc ƒë√≥ l√† kh√¥ng c√≥, n·∫øu mu·ªën ph·∫£i t·ª± vi·∫øt)\nGi·ªù push t·∫•t c·∫£ l√™n nh√°nh master v√† t·∫≠n h∆∞·ªüng th√†nh qu·∫£ th√¥i. üòÜüòÜ 3. C√°ch 3: T·ª± setup 1 h·ªá th·ªëng Staticman v2 (d√πng Cloud VM) Ph·∫ßn n√†y ƒë∆∞·ª£c update v√†o 12/2022, khi m√† Heroku ko cho d√πng free n·ªØa. M√¨nh ph·∫£i chuy·ªÉn server Staticman qua 1 VM free kh√°c, l√† Oracle Cloud VM.\nC√°ch n√†y c√≥ 1 nh∆∞·ª£c ƒëi·ªÉm l√† do Server expose ra IP endpoint th√¥i n√™n ko c√≥ HTTPS, d·∫´n ƒë·∫øn khi comment s·∫Ω c√≥ warning nh∆∞ ·∫£nh ·ªü cu·ªëi b√†i.\nM√¨nh s·∫Ω d√πng Docker ƒë·ªÉ run app staticman.\nClone source v·ªÅ /opt/devops/\nCopy file key.pem v√†o trong folder source.\nT·∫°o file config.development.json:\n{ \u0026#34;githubToken\u0026#34;: process.env.GITHUB_TOKEN, \u0026#34;rsaPrivateKey\u0026#34;: JSON.stringify(process.env.RSA_PRIVATE_KEY), \u0026#34;port\u0026#34;: 8080 } S·ª≠a file docker-compose.development.yml, add th√™m c√°c bi·∫øn ENV, ch√∫ √Ω bi·∫øn RSA_PRIVATE_KEY h√£y paste n·ªôi d√πng file key.pem v√†o:\nversion: \u0026#39;2\u0026#39; services: staticman: container_name: staticman extends: file: docker-compose.yml service: staticman environment: PORT: 3000 NODE_ENV: development GITHUB_TOKEN: \u0026#34;cb9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx26\u0026#34; RSA_PRIVATE_KEY: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\n MAIDpcccccccccccccccccccccccccccccccccccccccccccccccccccccccct3W .... aYxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxp ivvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvwyYA==\\n -----END RSA PRIVATE KEY-----\u0026#34; volumes: - ./:/app Run:\ndocker-compose -f docker-compose.development.yml up -d docker ps Truy c·∫≠p v√†o Browser, VM public IP port 8080, example: http://111.222.333.444:8080/\nN·∫øu tr·∫£ v·ªÅ: Hello from Staticman version 3.0.0! nghƒ©a l√† ƒë√£ OK\nChuy·ªÉn sang l√†m tr√™n production, stop m√¥i tr∆∞·ªùng development:\ndocker stop staticman T·∫°o file config.production.json:\n{ \u0026#34;githubToken\u0026#34;: process.env.GITHUB_TOKEN, \u0026#34;rsaPrivateKey\u0026#34;: JSON.stringify(process.env.RSA_PRIVATE_KEY), \u0026#34;port\u0026#34;: 8080 } T·∫°o file docker-compose.production.yml:\nversion: \u0026#39;2\u0026#39; services: staticman: container_name: staticman extends: file: docker-compose.yml service: staticman environment: PORT: 3000 NODE_ENV: production GITHUB_TOKEN: \u0026#34;cbxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx26\u0026#34; RSA_PRIVATE_KEY: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\n MAIDpcccccccccccccccccccccccccccccccccccccccccccccccccccccccct3W .... aYxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxp ivvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvwyYA==\\n -----END RSA PRIVATE KEY-----\u0026#34; volumes: - ./:/app Run:\ndocker-compose -f docker-compose.production.yml up -d docker ps Truy c·∫≠p v√†o VM public IP port 8080, example: http://111.222.333.444:8080/\nN·∫øu tr·∫£ v·ªÅ: Hello from Staticman version 3.0.0! nghƒ©a l√† ƒë√£ OK\nMu·ªën s·ª≠a port ko d√πng 8080 m√† d√πng port kh√°c (8811 ch·∫≥ng h·∫°n) th√¨ s·ª≠a 2 file:\n file docker-compose.yml: - '8811:3000'. file config.production.json: \u0026quot;port\u0026quot;: 8811.  L√†m l·∫°i step 2.2.8. Gi·ªù add account Github c·ªßa con bot as collaborator v√† accept invitation\nN·∫øu tr∆∞·ªõc ƒë√¢y ƒë√£ add account con bot r·ªìi th√¨ remove ƒëi v√† add l·∫°i nh√©:\nƒê·ªÉ m√†n h√¨nh n√†y l√† OK:\nAccept b·∫±ng c√°ch truy c·∫≠p link c·ªßa Staticman server:\nL√†m l·∫°i step 2.2.9. Ch√∫ √Ω apiEndpoint trong file config.toml:\n# Staticman [params.comments.staticman] enable = true apiEndpoint = \u0026#34;http://VM_PUBLIC_IP:PORT/v2/entry\u0026#34; maxDepth = 2 Gi·ªù ti·∫øp t·ª•c l√†m ph·∫ßn reCapcha\nV√¨ m√¨nh ƒë√£ l√†m step 2.3.1. n√™n ƒë√£ c√≥ Secret r·ªìi\nL√†m l·∫°i step 2.3.2. th√¥i:\nTruy c·∫≠p link staticman Server ƒë·ªÉ encrypt Secret: http://VM_PUBLIC_IP:PORT/v3/encrypt/\u0026lt;Secret\u0026gt;\ns·∫Ω nh·∫≠n ƒë∆∞·ª£c 1 string, copy v√† s·ª≠a file staticman.yml nh∆∞ step 2.3.3.\nConfirm l·∫°i step 2.3.4. r·ªìi push l√™n nh√°nh master ƒë·ªÉ test.\nGi·ªù push nh·ªØng thay ƒë·ªïi v·ª´a xong l√™n nh√°nh master th√¥i.\nC√°ch #3 n√†y c√≥ 1 nh∆∞·ª£c di·ªÉm l√† B·ªüi v√¨ m√¨nh expose ra endpoint ko c√≥ HTTPS (ch·ªâ HTTP th√¥i), n√™n m·ªói khi comment s·∫Ω c√≥ warning nh∆∞ n√†y:\nREFERENCES https://github.com/eduardoboucas/staticman/issues/296\nhttps://dreambooker.site/2019/08/17/Hugo-Staticman-Travis/\nhttps://minimo.netlify.com/docs/comments-support/\nhttps://github.com/eduardoboucas/staticman\nhttps://github.com/eduardoboucas/staticman/issues/243\nhttps://yasoob.me/posts/running_staticman_on_static_hugo_blog_with_nested_comments/# https://vincenttam.gitlab.io/post/2018-09-16-staticman-powered-gitlab-pages/2/\nhttps://www.datascienceblog.net/post/other/staticman_comments/\nhttps://github.com/eduardoboucas/staticman-recaptcha\n","href":"/bk/encrypt-apply-staticman-v3-for-hugo-blog-github-cmt-feature/","title":"Apply Staticman v3/v2 for Hugo blog Github to enable comment feature"},{"content":"Y√™u C·∫ßu B·∫°n ƒë√£ c√≥ 1 Github Pages Blog d·ª±ng b·∫±ng Hugo, s·ª≠ d·ª•ng theme Minimo ho·∫∑c nh·ªØng theme t∆∞∆°ng t·ª± nh∆∞ Minimo\nGi·ªõi thi·ªáu N·∫øu b·∫°n ƒëang host 1 Hugo blog tr√™n Github, h·∫≥n b·∫°n s·∫Ω mu·ªën ng∆∞·ªùi kh√°c c√≥ th·ªÉ comment tr√™n blog c·ªßa m√¨nh\nNh∆∞ng v√¨ blog c·ªßa b·∫°n l√† static site. N·∫øu ng∆∞·ªùi kh√°c c√≥ th·ªÉ comment dc th√¨ ch·∫≥ng ph·∫£i l√† dynamic site sao :v\nƒê·ªÉ l√†m ƒëi·ªÅu n√†y b·∫°n c·∫ßn 1 b√™n th·ª© 3 ƒë·ªÉ host c√°i ch·ª©c nƒÉng n√†y, ·ªü ƒë√¢y m√¨nh ch·ªâ ra 2 c√°ch:\n-C√°ch 1: S·ª≠ d·ª•ng staticmanlab c·ªßa @VincentTam\n-C√°ch 2: T·ª± setup 1 h·ªá th·ªëng Staticman l√†m comment system ri√™ng tr√™n Heroku free plan\n-C√°ch 3 (Recommend): T·ª± setup 1 h·ªá th·ªëng Staticman l√†m comment system ri√™ng tr√™n 1 VM\nC√°ch 1 th√¨ do ch√∫ng ta s·ª≠ d·ª•ng API c·ªßa ƒë·ªìng ch√≠ @VincentTam t·∫°o ra tr√™n Heroku n√™n n√≥ ph·ª• thu·ªôc v√†o ƒë·ªìng ch√≠ ·∫•y, n·∫øu v√¨ 1 l√Ω do n√†o ƒë√≥ anh ta x√≥a app th√¨ ch·ª©c nƒÉng comment s·∫Ω b·ªã disable. Th·∫ø n√™n c√°ch n√†y hi·ªán t·∫°i d√πng ƒë∆∞·ª£c, nh∆∞ng ko ho√†n to√†n ƒë·∫£m b·∫£o trong t∆∞∆°ng lai.\nC√°ch 2 th√¨ b·∫°n s·∫Ω t·ª± host app ƒë√≥ tr√™n Heroku server c·ªßa b·∫°n n√™n bao gi·ªù b·∫°n ko d√πng Heroku n·ªØa th√¨ m·ªõi th√¥i (nhi·ªÅu kh·∫£ nƒÉng v√¨ Heroku ko cho x√†i free n√™n b·∫°n ko d√πng n·ªØa ü§£)\nC√°ch 3 th√¨ b·∫°n s·∫Ω t·ª± host app ƒë√≥ tr√™n server ri√™ng c·ªßa b·∫°n n√™n bao gi·ªù b·∫°n stop server th√¨ m·ªõi th√¥i.\nGi·ªù m√¨nh s·∫Ω gi·ªõi thi·ªáu C√°ch 1:\n1. C√°ch 1: S·ª≠ d·ª•ng staticmanlab c·ªßa @VincentTam 1- Add @staticmanlab v√†o collaborator github setting c·ªßa github repo Hugo blog:\n2- Access v√†o link sau ƒë·ªÉ active c√°i @staticmanlab:\nhttps://staticman3.herokuapp.com/v3/connect/github/\u0026lt;GITHUB_USER\u0026gt;/\u0026lt;REPO_NAME\u0026gt;/\nnh·ªõ s·ª≠a \u0026lt;GITHUB_USER\u0026gt;,\u0026lt;REPO_NAME\u0026gt; l√† c·ªßa b·∫°n M√†n h√¨nh tr·∫£ v·ªÅ n·∫øu th·∫•y ch·ªØ OK! th√¨ m·ªõi l√† th√†nh c√¥ng nh√©\n3- Add file staticman.yml v√†o root folder (c√πng v·ªã tr√≠ v·ªõi file config.toml), n·ªôi dung file staticman.yml nh∆∞ sau, nh·ªõ s·ª≠a l·∫°i d√≤ng name: '\u0026lt;REPO_NAME\u0026gt;' t√πy theo repository name c·ªßa b·∫°n:\ncomments: allowedFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;parent_id\u0026#39;, \u0026#39;permalink\u0026#39;, \u0026#39;site\u0026#39;] branch: \u0026#39;master\u0026#39; commitMessage: \u0026#34;add [comment]: by {fields.author} \u0026lt;Staticman\u0026gt;\\n\\n{fields.permalink}#comment-{@id}\u0026#34; filename: \u0026#39;{@id}\u0026#39; format: \u0026#39;yaml\u0026#39; generatedFields: date: type: date options: format: \u0026#39;timestamp\u0026#39; moderation: false name: \u0026#39;\u0026lt;REPO_NAME\u0026gt;\u0026#39; path: \u0026#39;data/comments/{options.postId}\u0026#39; requiredFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;] reCaptcha: enabled: false # siteKey: \u0026#39;\u0026#39; # secret: \u0026#39;\u0026#39; transforms: email: md5 4- Update file config.toml nh∆∞ ƒëo·∫°n n√†y, nh·ªõ s·ª≠a \u0026lt;GITHUB_USER\u0026gt; v√† \u0026lt;REPO_NAME\u0026gt; ph√π h·ª£p v·ªõi th√¥ng tin c·ªßa b·∫°n:\n# Staticman [params.comments.staticman] enable = true apiEndpoint = \u0026#34;https://staticman3.herokuapp.com/v3/entry/github\u0026#34; maxDepth = 2 [params.comments.staticman.github] username = \u0026#34;\u0026lt;GITHUB_USER\u0026gt;\u0026#34; repository = \u0026#34;\u0026lt;REPO_NAME\u0026gt;\u0026#34; branch = \u0026#34;master\u0026#34; V·∫≠y l√† xong, gi·ªù push nh·ªØng thay ƒë·ªïi v·ª´a xong l√™n nh√°nh master th√¥i.\nGi·∫£i th√≠ch:\nM·ªói khi c√≥ ng∆∞·ªùi comment, ƒë·ªìng nghƒ©a v·ªõi vi·ªác h·ªç s·∫Ω g·ª≠i 1 POST request ƒë·∫øn Staticman API c·ªßa ƒë·ªìng ch√≠ @VincentTam ƒë√£ t·∫°o ra tr√™n Heroku.\nSau khi API ƒë√≥ nh·∫≠n ƒë∆∞·ª£c n·ªôi dung comment, n√≥ s·∫Ω commit n·ªôi dung comment ƒë√≥ v√†o branch master c·ªßa Blog ch√∫ng ta (C·ª• th·ªÉ th√¨ n·ªôi dung comment ƒë√≥ s·∫Ω dc push v√†o folder /data/comments).\nKhi nh√°nh master ph√°t hi·ªán commit m·ªõi ƒë√≥, n√≥ s·∫Ω build l·∫°i page v√† hi·ªÉn th·ªã l·∫°i n·ªôi dung comment tr√™n page. ƒê∆°n gi·∫£n th·∫ø th√¥i. üòÜ\n2. C√°ch 2: T·ª± setup 1 h·ªá th·ªëng Staticman v2 (d√πng Heroku free plan) 2.1. Y√™u c·∫ßu B·∫°n ƒëang d√πng Windows, ƒë√£ c√†i Gitbash, Visual Studio Code, ƒë√£ ƒëƒÉng k√Ω t√†i kho·∫£n Heroku (r·∫•t ƒë∆°n gi·∫£n, ƒëƒÉng k√Ω c≈©ng free), ƒë√£ c√†i Heroku CLI v√†o m√°y\n2.2. C√°ch l√†m 2.2.1. T·∫°o RSA Private key M·ªü GitBash l√™n, ch·∫°y command sau ƒë·ªÉ generate ra RSA Private key\nopenssl genrsa -out key.pem N√≥ s·∫Ω t·∫°o ra 1 file key.pem\n2.2.2. T·∫°o 1 account Github ri√™ng (g·ªçi l√† bot account) ph·ª•c v·ª• m·ª•c ƒë√≠ch d√πng Staticman Login v√†o bot account, d√πng link n√†y ƒë·ªÉ t·∫°o Personal Access Token:\nhttps://github.com/settings/tokens\nH√£y ch·ªçn nh·ªØng quy·ªÅn nh∆∞ read:packages, repo, write:packages\nNh·ªõ copy c√°i chu·ªói token ƒë√≥ ra ƒë√¢u ƒë√≥ l√°t n·ªØa d√πng nh√©\n2.2.3. Clone source staticman v·ªÅ v√† cd v√†o n√≥ git clone https://github.com/eduardoboucas/staticman cd staticman M·ªü folder staticman ƒë√≥ trong Visual Studio Code (VSC) ƒë·ªÉ d√πng Powershell terminal c·ªßa VSC (t·ª´ b∆∞·ªõc n√†y ko d√πng GitBash n·ªØa v√¨ GitBash ko ƒëƒÉng nh·∫≠p Heroku ƒë∆∞·ª£c)\n2.2.4. T·∫°o 1 file t√™n l√† Procfile trong folder staticman n·ªôi dung file ch·ªâ c√≥ 1 d√≤ng l√†:\nweb: npm start 2.2.5. T·∫°o file config.production.json trong folder staticman n·ªôi dung file:\n{ \u0026quot;githubToken\u0026quot;: process.env.GITHUB_TOKEN, \u0026quot;rsaPrivateKey\u0026quot;: JSON.stringify(process.env.RSA_PRIVATE_KEY), \u0026quot;port\u0026quot;: 8080 } 2.2.6. S·ª≠a file .gitignore add th√™m d√≤ng sau:\n!config.production.json 2.2.7. Login v√†o Heroku heroku login T·∫°o app tr√™n Heroku\nheroku create \u0026lt;app_name\u0026gt; T·∫°o bi·∫øn m√¥i tr∆∞·ªùng c·ªßa app tr√™n Heroku\nheroku config:set GITHUB_TOKEN=\u0026#34;\u0026lt;Paste token l·∫•y ƒë∆∞·ª£c ·ªü step 2\u0026gt;\u0026#34; heroku config:set RSA_PRIVATE_KEY=\u0026#34;$(cat ../key.pem)\u0026#34; heroku config:set NODE_ENV=\u0026#34;production\u0026#34; check logs nh√¨n ƒë·∫πp ƒë·∫πp, ko c√≥ ERROR g√¨ l√† ƒë∆∞·ª£c:\nheroku logs --tail T·∫°o branch production:\ngit checkout -b production Commit thay ƒë·ªïi l√™n Heroku:\ngit add . git commit -m \u0026#34;Setup Staticman v2 to Heroku\u0026#34; git push heroku production:master check logs nh√¨n ƒë·∫πp ƒë·∫πp ki·ªÉu n√†y l√† ok:\nheroku logs --tail Truy c·∫≠p v√†o link n√†y ƒë·ªÉ check app ƒë√£ ho·∫°t ƒë·ªông ch∆∞a: https://\u0026lt;app_name\u0026gt;.herokuapp.com\nN·∫øu tr·∫£ v·ªÅ Hello from Staticman version 3.0.0! nghƒ©a l√† ƒë√£ deploy th√†nh c√¥ng\n2.2.8. Add bot account v√†o l√†m Collaborator Login v√†o Github ch√≠nh ch·ªß c·ªßa b·∫°n (ko ph·∫£i bot account nh√©)\nV√†o repository ch·ª©a blog c·ªßa b·∫°n, ch·ªçn m·ª•c Settings -\u0026gt; Manage Access -\u0026gt; Invite a Collaborator\nG√µ username c·ªßa bot account v√†o, v√† Invite n√≥\nƒê·∫øn ƒë√¢y th√¨ ƒê·ª™NG login v√†o con bot account ƒë·ªÉ accept l·ªùi m·ªùi b·∫±ng tay nh√©!\nB·∫°n c·∫ßn ph·∫£i truy c·∫≠p v√†o link n√†y ƒë·ªÉ accept m·ªõi ƒë∆∞·ª£c:\n(ch√∫ √Ω thay ƒë·ªïi \u0026lt;app_name\u0026gt;, \u0026lt;GITHUB_USER\u0026gt;, \u0026lt;REPO_NAME\u0026gt; cho ph√π h·ª£p) https://\u0026lt;app_name\u0026gt;.herokuapp.com/v2/connect/\u0026lt;GITHUB_USER\u0026gt;/\u0026lt;REPO_NAME\u0026gt;/ N·∫øu hi·ªán ch·ªØ OK! c√≥ nghƒ©a l√† ƒë√£ accept invitation th√†nh c√¥ng.\n2.2.9. Gi·ªù c·∫ßn s·ª≠a 1 ch√∫t c√°i Blog repository c·ªßa b·∫°n n·ªØa 9.1- Add file staticman.yml v√†o root folder (c√πng v·ªã tr√≠ v·ªõi file config.toml),\nn·ªôi dung file staticman.yml nh∆∞ sau, nh·ªõ s·ª≠a l·∫°i d√≤ng name: '\u0026lt;REPO_NAME\u0026gt;' t√πy theo repo name c·ªßa b·∫°n:\ncomments: allowedFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;parent_id\u0026#39;, \u0026#39;permalink\u0026#39;, \u0026#39;site\u0026#39;] branch: \u0026#39;master\u0026#39; commitMessage: \u0026#34;add [comment]: by {fields.author} \u0026lt;Staticman\u0026gt;\\n\\n{fields.permalink}#comment-{@id}\u0026#34; filename: \u0026#39;{@id}\u0026#39; format: \u0026#39;yaml\u0026#39; generatedFields: date: type: date options: format: \u0026#39;timestamp\u0026#39; moderation: false name: \u0026#39;\u0026lt;REPO_NAME\u0026gt;\u0026#39; path: \u0026#39;data/comments/{options.postId}\u0026#39; requiredFields: [\u0026#39;author\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;email\u0026#39;] reCaptcha: enabled: false # siteKey: \u0026#39;\u0026#39; # secret: \u0026#39;\u0026#39; transforms: email: md5 9.2- Update file config.toml nh∆∞ ƒëo·∫°n n√†y, nh·ªõ s·ª≠a \u0026lt;GITHUB_USER\u0026gt;, \u0026lt;app_name\u0026gt; v√† \u0026lt;REPO_NAME\u0026gt; ph√π h·ª£p v·ªõi th√¥ng tin c·ªßa b·∫°n:\n# Staticman [params.comments.staticman] enable = true apiEndpoint = \u0026#34;https://\u0026lt;app_name\u0026gt;.herokuapp.com/v2/entry\u0026#34; maxDepth = 2 [params.comments.staticman.github] username = \u0026#34;\u0026lt;GITHUB_USER\u0026gt;\u0026#34; repository = \u0026#34;\u0026lt;REPO_NAME\u0026gt;\u0026#34; branch = \u0026#34;master\u0026#34; Gi·ªù push nh·ªØng thay ƒë·ªïi v·ª´a xong l√™n nh√°nh master th√¥i.\nGi·ªù b·∫°n ƒë√£ c√≥ th·ªÉ comment v√†o blog ƒë·ªÉ xem Staticman c·ªßa ri√™ng b·∫°n ho·∫°t ƒë·ªông th·∫ø n√†o r·ªìi üòÜüòÜ\nTuy nhi√™n th√¨ Heroku c≈©ng c√≥ 1 s·ªë nh∆∞·ª£c ƒëi·ªÉm ng∆∞·ªùi ta li·ªát k√™ nh∆∞ sau:\n Limitations of Heroku apps Free Heroku apps come with some limitations. The most important limitation is that the number of free dyno hours is limited. A dyno is the isolated container in which your application is running. A free dyno begins to sleep after 30 mins of inactivity. Otherwise, it is always on as long as you still have remaining dyno hours (currently 550 free dyno hours per month).\n  When your free dyno hours are exhausted, all of your free dynos start sleeping. You will receive a notification from Heroku before this happens though. If the only app that you are running is Staticman, however, it is unlikely that you will use up you free dyno hours because this would mean that you are receiving comments for more than 18 hours per day, which would be quite a lot.\n  So, the main disadvantage of free dynos when is that posting comments will be slow (delay of a few seconds) when no one has posted anything within a while (30 minutes). In my opinion, this limitation is sufferable for a free service. If you want to get rid of this limitation or if you need more dyno hours, you can always upgrade to a paid dyno.\n Trong t∆∞∆°ng lai c√≥ th·ªÉ m√¨nh s·∫Ω d√πng th√™m 1 s·ªë feature n·ªØa, nh∆∞ Captcha, Mailgun\n2.3. B·∫≠t t√≠nh nƒÉng Captcha cho c√°c comment (cho C√°ch 2 d√πng Heroku free plan) G·∫ßn ƒë√¢y m√¨nh ph√°t hi·ªán c√≥ 1 s·ªë comment tr√™n blog c·ªßa m√¨nh c√≥ n·ªôi dung r·∫•t l·∫°, comment kh√¥ng li√™n quan b√†i vi·∫øt, ho·∫∑c comment ch·ª©a nh·ªØng ƒë∆∞·ªùng link m√£ ƒë·ªôc r·∫•t nguy hi·ªÉm. Thi tho·∫£ng m√¨nh ph·∫£i v√†o v√† x√≥a comment ƒë√≥ ƒëi.\nTh·∫ø n√™n m√¨nh quy·∫øt ƒë·ªãnh apply reCaptcha cho t√≠nh nƒÉng comment n√†y. √çt nh·∫•t th√¨ c≈©ng l√†m kh√≥ 1 ch√∫t cho b·ªçn bot.\n2.3.1. ƒêƒÉng k√Ω ReCaptcha ƒë·ªÉ l·∫•y key v√†o link sau ƒë·ªÉ ƒëƒÉng k√Ω nh√©:\nhttps://www.google.com/recaptcha/admin\nNh·∫≠p c√°c th√¥ng tin nh∆∞ n√†y v√† ·∫•n Submit:\nSau khi submit b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c SiteKey v√† Secret, h√£y gi·ªØ ch√∫ng c·∫©n th·∫≠n\n2.3.2. Encrypt Secret b·∫±ng c√°ch truy c·∫≠p link sau https://\u0026lt;app_name\u0026gt;.herokuapp.com/v3/encrypt/\u0026lt;Secret\u0026gt;\nV√¨ m√¨nh build Static tr√™n Heroku n√™n d∆∞·ªùng link c·ªßa m√¨nh n√≥ tr√¥ng nh∆∞ v·∫≠y.\nB·∫°n xem Staticman ApiEnpoint c·ªßa b·∫°n l√† g√¨ ƒë·ªÉ call ƒë·∫øn ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ·ª©ng nh√©.\nN√≥ s·∫Ω tr·∫£ v·ªÅ k·∫øt qu·∫£ l√† 1 chu·ªói k√≠ t·ª± ƒë√£ m√£ h√≥a h√£y copy chu·ªói ·∫•y.\n2.3.3. S·ª≠a file staticman.yml ~~~ reCaptcha: enabled: true siteKey: \u0026#39;65AAAAAABkgfodgkeMK_VmkgmkKkf3\u0026#39; secret: \u0026#39;PxQcp............\u0026#39; ~~~ 2.3.4. H√£y ch·∫Øc ch·∫Øn r·∫±ng theme m√† b·∫°n ƒëang d√πng c√≥ h·ªó tr·ª£ reCaptcha V√≠ d·ª• m√¨nh ƒëang d√πng theme Minimo. Trong ƒë∆∞·ªùng link themes/minimo/layouts/partials/comments/staticman/form.html, m√¨nh c√≥ th·ªÉ th·∫•y t·ª´ nƒÉm 2019 h·ªç m·ªõi add th√™m c√°c m√£ html ƒë·ªÉ h·ªó tr·ª£ reCaptcha (tr∆∞·ªõc ƒë√≥ l√† kh√¥ng c√≥, n·∫øu mu·ªën ph·∫£i t·ª± vi·∫øt)\nGi·ªù push t·∫•t c·∫£ l√™n nh√°nh master v√† t·∫≠n h∆∞·ªüng th√†nh qu·∫£ th√¥i. üòÜüòÜ 3. C√°ch 3: T·ª± setup 1 h·ªá th·ªëng Staticman v2 (d√πng Cloud VM) Ph·∫ßn n√†y ƒë∆∞·ª£c update v√†o 12/2022, khi m√† Heroku ko cho d√πng free n·ªØa. M√¨nh ph·∫£i chuy·ªÉn server Staticman qua 1 VM free kh√°c, l√† Oracle Cloud VM.\nC√°ch n√†y c√≥ 1 nh∆∞·ª£c ƒëi·ªÉm l√† do Server expose ra IP endpoint th√¥i n√™n ko c√≥ HTTPS, d·∫´n ƒë·∫øn khi comment s·∫Ω c√≥ warning nh∆∞ ·∫£nh ·ªü cu·ªëi b√†i.\nM√¨nh s·∫Ω d√πng Docker ƒë·ªÉ run app staticman.\nClone source v·ªÅ /opt/devops/\nCopy file key.pem v√†o trong folder source.\nT·∫°o file config.development.json:\n{ \u0026#34;githubToken\u0026#34;: process.env.GITHUB_TOKEN, \u0026#34;rsaPrivateKey\u0026#34;: JSON.stringify(process.env.RSA_PRIVATE_KEY), \u0026#34;port\u0026#34;: 8080 } S·ª≠a file docker-compose.development.yml, add th√™m c√°c bi·∫øn ENV, ch√∫ √Ω bi·∫øn RSA_PRIVATE_KEY h√£y paste n·ªôi d√πng file key.pem v√†o:\nversion: \u0026#39;2\u0026#39; services: staticman: container_name: staticman extends: file: docker-compose.yml service: staticman environment: PORT: 3000 NODE_ENV: development GITHUB_TOKEN: \u0026#34;cb9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx26\u0026#34; RSA_PRIVATE_KEY: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\n MAIDpcccccccccccccccccccccccccccccccccccccccccccccccccccccccct3W .... aYxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxp ivvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvwyYA==\\n -----END RSA PRIVATE KEY-----\u0026#34; volumes: - ./:/app Run:\ndocker-compose -f docker-compose.development.yml up -d docker ps Truy c·∫≠p v√†o Browser, VM public IP port 8080, example: http://111.222.333.444:8080/\nN·∫øu tr·∫£ v·ªÅ: Hello from Staticman version 3.0.0! nghƒ©a l√† ƒë√£ OK\nChuy·ªÉn sang l√†m tr√™n production, stop m√¥i tr∆∞·ªùng development:\ndocker stop staticman T·∫°o file config.production.json:\n{ \u0026#34;githubToken\u0026#34;: process.env.GITHUB_TOKEN, \u0026#34;rsaPrivateKey\u0026#34;: JSON.stringify(process.env.RSA_PRIVATE_KEY), \u0026#34;port\u0026#34;: 8080 } T·∫°o file docker-compose.production.yml:\nversion: \u0026#39;2\u0026#39; services: staticman: container_name: staticman extends: file: docker-compose.yml service: staticman environment: PORT: 3000 NODE_ENV: production GITHUB_TOKEN: \u0026#34;cbxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx26\u0026#34; RSA_PRIVATE_KEY: \u0026#34;-----BEGIN RSA PRIVATE KEY-----\\n MAIDpcccccccccccccccccccccccccccccccccccccccccccccccccccccccct3W .... aYxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxp ivvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvwyYA==\\n -----END RSA PRIVATE KEY-----\u0026#34; volumes: - ./:/app Run:\ndocker-compose -f docker-compose.production.yml up -d docker ps Truy c·∫≠p v√†o VM public IP port 8080, example: http://111.222.333.444:8080/\nN·∫øu tr·∫£ v·ªÅ: Hello from Staticman version 3.0.0! nghƒ©a l√† ƒë√£ OK\nMu·ªën s·ª≠a port ko d√πng 8080 m√† d√πng port kh√°c (8811 ch·∫≥ng h·∫°n) th√¨ s·ª≠a 2 file:\n file docker-compose.yml: - '8811:3000'. file config.production.json: \u0026quot;port\u0026quot;: 8811.  L√†m l·∫°i step 2.2.8. Gi·ªù add account Github c·ªßa con bot as collaborator v√† accept invitation\nN·∫øu tr∆∞·ªõc ƒë√¢y ƒë√£ add account con bot r·ªìi th√¨ remove ƒëi v√† add l·∫°i nh√©:\nƒê·ªÉ m√†n h√¨nh n√†y l√† OK:\nAccept b·∫±ng c√°ch truy c·∫≠p link c·ªßa Staticman server:\nL√†m l·∫°i step 2.2.9. Ch√∫ √Ω apiEndpoint trong file config.toml:\n# Staticman [params.comments.staticman] enable = true apiEndpoint = \u0026#34;http://VM_PUBLIC_IP:PORT/v2/entry\u0026#34; maxDepth = 2 Gi·ªù ti·∫øp t·ª•c l√†m ph·∫ßn reCapcha\nV√¨ m√¨nh ƒë√£ l√†m step 2.3.1. n√™n ƒë√£ c√≥ Secret r·ªìi\nL√†m l·∫°i step 2.3.2. th√¥i:\nTruy c·∫≠p link staticman Server ƒë·ªÉ encrypt Secret: http://VM_PUBLIC_IP:PORT/v3/encrypt/\u0026lt;Secret\u0026gt;\ns·∫Ω nh·∫≠n ƒë∆∞·ª£c 1 string, copy v√† s·ª≠a file staticman.yml nh∆∞ step 2.3.3.\nConfirm l·∫°i step 2.3.4. r·ªìi push l√™n nh√°nh master ƒë·ªÉ test.\nGi·ªù push nh·ªØng thay ƒë·ªïi v·ª´a xong l√™n nh√°nh master th√¥i.\nC√°ch #3 n√†y c√≥ 1 nh∆∞·ª£c di·ªÉm l√† B·ªüi v√¨ m√¨nh expose ra endpoint ko c√≥ HTTPS (ch·ªâ HTTP th√¥i), n√™n m·ªói khi comment s·∫Ω c√≥ warning nh∆∞ n√†y:\nREFERENCES https://github.com/eduardoboucas/staticman/issues/296\nhttps://dreambooker.site/2019/08/17/Hugo-Staticman-Travis/\nhttps://minimo.netlify.com/docs/comments-support/\nhttps://github.com/eduardoboucas/staticman\nhttps://github.com/eduardoboucas/staticman/issues/243\nhttps://yasoob.me/posts/running_staticman_on_static_hugo_blog_with_nested_comments/# https://vincenttam.gitlab.io/post/2018-09-16-staticman-powered-gitlab-pages/2/\nhttps://www.datascienceblog.net/post/other/staticman_comments/\nhttps://github.com/eduardoboucas/staticman-recaptcha\n","href":"/posts/encrypt-apply-staticman-v3-for-hugo-blog-github-cmt-feature/","title":"Apply Staticman v3/v2 for Hugo blog Github to enable comment feature"},{"content":"","href":"/tags/blog/","title":"Blog"},{"content":"T·∫°o Gitlab v·ªõi y√™u c·∫ßu l√†:\n disable MinIO, d√πng GCS, disable cert-manager, d√πng cert-manager m√¨nh t·ª± t·∫°o ri√™ng, c√°c config v·ªÅ resource c·ªßa gitlab ·ªü m·ª©c minimal, gitlab s·∫Ω v√†o qua link gitlab.your-subdomain.your-domain.net v√† c√≥ HTTPS  Y√™u C·∫ßu  ƒê√£ t·∫°o GKE Cluster c√≥ √≠t nh·∫•t l√† 3 vCPU ƒê√£ install Helm 2 ƒê√£ l√†m theo b√†i n√†y ƒë·ªÉ setup cert-manager:\nK8S 9: Setup External DNS + Cert Manager + Nginx Ingress Controller Wilcard  C√°ch L√†m 1. Setup environment variables export PROJECT_ID=\u0026#34;YOUR_PROJECT_ID\u0026#34; export CLUSTER_NAME=\u0026#34;YOUR_CLUSTER_NAME\u0026#34; export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; gcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a 2. T·∫°o namespace gitlab v√† add helm repo kubectl create namespace gitlab helm repo add gitlab https://charts.gitlab.io/ helm repo update 3. D√πng Kubed ƒë·ªÉ synchronize TLS secret gi·ªØa c√°c namespaces helm repo add appscode https://charts.appscode.com/stable/ helm repo update helm install -n kubed appscode/kubed --version 0.12.0 --namespace kube-system --wait --set config.clusterName=${CLUSTER_NAME} Gi·∫£ s·ª≠ nh·ªù b√†i tr∆∞·ªõc m√† m√¨nh ƒë√£ issue ƒë∆∞·ª£c 1 TLS Secret th√†nh c√¥ng t√™n l√† echo-tls-secret-prod ·ªü namespace default,\nk get secret -A default default-token-7hsrc kubernetes.io/service-account-token 3 3h11m default echo-tls-secret-prod kubernetes.io/tls 3 157m default echo-tls-secret-staging kubernetes.io/tls 3 164m gitlab default-token-t728z kubernetes.io/service-account-token 3 99s gi·ªù m√¨nh c·∫ßn sync secret ƒë√≥ sang namespace gitlab kh√°c th√¨ c·∫ßn l√†m nh∆∞ sau:\nannotate secret m√† m√¨nh mu·ªën sync gi·ªØa c√°c namespaces\n# ƒê√°nh label `app=kubed` cho namespace gitlab kubectl label namespace gitlab app=kubed # Annotate ƒë·ªÉ secret m√¨nh mu·ªën s·∫Ω sync sang c√°c namespace c√≥ label `app=kubed`  kubectl annotate secret echo-tls-secret-prod -n default kubed.appscode.com/sync=\u0026#34;app=kubed\u0026#34; check: (B·∫°n s·∫Ω th·∫•y namespace gitlab c≈©ng c√≥ secret echo-tls-secret-prod)\nk get secret -A default default-token-7hsrc kubernetes.io/service-account-token 3 3h15m default echo-tls-secret-prod kubernetes.io/tls 3 161m default echo-tls-secret-staging kubernetes.io/tls 3 168m gitlab default-token-t728z kubernetes.io/service-account-token 3 5m43s gitlab echo-tls-secret-prod kubernetes.io/tls 3 6s 4. GCP - T·∫°o Service Account gcloud iam service-accounts create gitlab-gcs \\  --display-name \u0026#34;Gitlab Cloud Storage\u0026#34; 5. GCP - T·∫°o Service Account Key gcloud iam service-accounts keys create --iam-account \\  gitlab-gcs@${PROJECT_ID}.iam.gserviceaccount.com /tmp/gitlab-gcs-key.json 6. GCP - Binding role Storage Admin v√†o Service Account gcloud projects add-iam-policy-binding --role roles/storage.admin ${PROJECT_ID} \\  --member=serviceAccount:gitlab-gcs@${PROJECT_ID}.iam.gserviceaccount.com 7. GCP - Create GCS Buckets gsutil mb -c standard gs://${PROJECT_ID}-gitlab-uploads gsutil mb -c standard gs://${PROJECT_ID}-gitlab-artifacts gsutil mb -c standard gs://${PROJECT_ID}-gitlab-lfs gsutil mb -c standard gs://${PROJECT_ID}-gitlab-packages gsutil mb -c standard gs://${PROJECT_ID}-gitlab-registry gsutil mb -c standard gs://${PROJECT_ID}-gitlab-runner-cache gsutil mb -c nearline gs://${PROJECT_ID}-gitlab-backups 8. Create a Secret to hold your Cloud Storage credentials kubectl create secret generic google-application-credentials \\  --from-file=gcs-application-credentials-file=/tmp/gitlab-gcs-key.json \\  --namespace gitlab 9. T·∫°o file rails.yaml cat \u0026gt; /tmp/rails.yaml \u0026lt;\u0026lt;EOF provider: Google google_project: ${PROJECT_ID} google_client_email: gitlab-gcs@${PROJECT_ID}.iam.gserviceaccount.com google_json_key_string: \u0026#39;$(cat /tmp/gitlab-gcs-key.json)\u0026#39; EOF 10. T·∫°o secret t·ª´ file rails.yaml kubectl create secret generic gitlab-rails-storage --from-file=connection=/tmp/rails.yaml --namespace gitlab 11. (Optional) X√≥a key ƒëi cho secure rm -rf /tmp/gitlab-gcs-key.json rm -rf /tmp/rails.yaml 12. T·∫°o Gitlab helm values file cat \u0026gt; ./gitlab-values.yaml \u0026lt;\u0026lt;EOF # Values for gitlab/gitlab chart on GKE global: edition: ce ## doc/charts/globals.md#configure-ingress-settings ingress: configureCertmanager: false tls: enabled: true secretName: echo-tls-secret-prod hosts: domain: ${SUBDOMAIN} https: false gitlab: https: false registry: https: false minio: https: false ## doc/charts/globals.md#configure-minio-settings minio: enabled: false registry: bucket: ${PROJECT_ID}-gitlab-registry ## doc/charts/globals.md#configure-appconfig-settings ## Rails based portions of this chart share many settings appConfig: ## doc/charts/globals.md#general-application-settings enableUsagePing: false defaultTheme: 2 defaultProjectsFeatures: issues: true mergeRequests: true wiki: true snippets: true builds: true containerRegistry: true ## doc/charts/globals.md#lfs-artifacts-uploads-packages backups: bucket: ${PROJECT_ID}-gitlab-backups lfs: bucket: ${PROJECT_ID}-gitlab-lfs connection: secret: gitlab-rails-storage key: connection artifacts: bucket: ${PROJECT_ID}-gitlab-artifacts connection: secret: gitlab-rails-storage key: connection uploads: bucket: ${PROJECT_ID}-gitlab-uploads connection: secret: gitlab-rails-storage key: connection packages: bucket: ${PROJECT_ID}-gitlab-packages connection: secret: gitlab-rails-storage key: connection gitlab: webservice: minReplicas: 1 resources: limits: memory: 1.5G requests: cpu: 100m memory: 900M workhorse: resources: limits: memory: 100M requests: cpu: 10m memory: 10M task-runner: backups: objectStorage: backend: gcs config: secret: \u0026#34;google-application-credentials\u0026#34; key: gcs-application-credentials-file gcpProject: ${PROJECT_ID} sidekiq: minReplicas: 1 resources: limits: memory: 1.5G requests: cpu: 50m memory: 625M gitlab-shell: minReplicas: 1 nginx-ingress: controller: replicaCount: 1 minAvailable: 0 resources: requests: cpu: 50m memory: 100Mi defaultBackend: replicaCount: 1 minAvailable: 0 resources: requests: cpu: 5m memory: 5Mi redis: resources: requests: cpu: 10m memory: 64Mi certmanager: install: false prometheus: install: false registry: enabled: true certificate: secret: gitlab-rails-storage key: connection ingress: tls: enabled: true secretName: echo-tls-secret-prod gitlab-runner: install: false rbac: create: true runners: locked: false cache: cacheType: gcs gcsBucketname: ${PROJECT_ID}-gitlab-runner-cache secretName: gitlab-rails-storage cacheShared: true EOF 13. Install Gitlab helm chart helm upgrade --install gitlab gitlab/gitlab \\  --namespace gitlab \\  --values gitlab-values.yaml Check gitlab tr√™n link gitlab.your-subdomain.your-domain.net ƒë∆∞·ª£c nh∆∞ sau l√† OK:\nƒêƒÉng nh·∫≠p v√†o gitlab b·∫±ng user \u0026ldquo;root\u0026rdquo;, pass l·∫•y t·ª´ command sau:\nkubectl get secret gitlab-gitlab-initial-root-password -ojsonpath={.data.password} -n gitlab | base64 --decode ; echo Check xem c√≥ th·ªÉ Backup v√† Restore Gitlab b√¨nh th∆∞·ªùng ko l√† OK:\nkubectl exec gitlab-task-runner-7d686c6474-w8mnt -it backup-utility -n gitlab WARNING: This version of GitLab depends on gitlab-shell 11.0.0, but you're running Unknown. Please update gitlab-shell. 2020-02-21 09:26:19 +0000 -- Dumping database ... Dumping PostgreSQL database gitlabhq_production ... [DONE] 2020-02-21 09:26:22 +0000 -- done WARNING: This version of GitLab depends on gitlab-shell 11.0.0, but you're running Unknown. Please update gitlab-shell. 2020-02-21 09:26:32 +0000 -- Dumping repositories ... 2020-02-21 09:26:32 +0000 -- done Dumping registry ... empty Dumping uploads ... empty Dumping artifacts ... empty Dumping lfs ... empty Dumping packages ... empty WARNING: This version of GitLab depends on gitlab-shell 11.0.0, but you're running Unknown. Please update gitlab-shell. Packing up backup tar Copying file:///srv/gitlab/tmp/backup_tars/1582277165_2020_02_21_12.7.6_gitlab_backup.tar [Content-Type=application/x-tar]... / [1 files][150.0 KiB/150.0 KiB] Operation completed over 1 objects/150.0 KiB. [DONE] Backup can be found at gs://xxx-gitlab-backups/1582277165_2020_02_21_12.7.6_gitlab_backup.tar N·∫øu backup b·ªã l·ªói ƒë·ªè ho·∫∑c c√≥ l·ªói ki·ªÉu Bucket not found: registry. Skipping backup of registry th√¨ c√≥ nghƒ©a l√† registry bucket kh√¥ng nh√¨n th·∫•y, c·∫ßn check l·∫°i file values nh√©\nDONE! üéâüéâüéâ\n14. Clean up  N√™n ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ x√≥a Cluster, VPC V√†o Service Cloud DNS, x√≥a c√°c record ƒë∆∞·ª£c sinh ra b·ªüi ExternalDNS\n V√†o IAM - Service Account x√≥a service account gitlab m√† b·∫°n ƒë√£ t·∫°o\n V√†o Compute Engine - Disk x√≥a c√°c disk ƒë∆∞·ª£c t·∫°o b·ªüi Cluster\n V√†o Storage, x√≥a c√°c bucket *-gitlab-* b·∫°n ƒë√£ t·∫°o ra  REFERENCES https://docs.gitlab.com/charts/ https://docs.gitlab.com/charts/charts/globals.html#connection https://ruzickap.github.io/k8s-harbor/part-03/#install-nginx-ingress https://gitlab.com/gitlab-org/charts/gitlab/issues/1464 https://docs.gitlab.com/charts/backup-restore/backup.html\n","href":"/bk/k8s-x-setup-gitlab-on-gke-cluster/","title":"K8S 10: Setup Gitlab Self Hosted on GKE Cluster"},{"content":"T·∫°o Gitlab v·ªõi y√™u c·∫ßu l√†:\n disable MinIO, d√πng GCS, disable cert-manager, d√πng cert-manager m√¨nh t·ª± t·∫°o ri√™ng, c√°c config v·ªÅ resource c·ªßa gitlab ·ªü m·ª©c minimal, gitlab s·∫Ω v√†o qua link gitlab.your-subdomain.your-domain.net v√† c√≥ HTTPS  Y√™u C·∫ßu  ƒê√£ t·∫°o GKE Cluster c√≥ √≠t nh·∫•t l√† 3 vCPU ƒê√£ install Helm 2 ƒê√£ l√†m theo b√†i n√†y ƒë·ªÉ setup cert-manager:\nK8S 9: Setup External DNS + Cert Manager + Nginx Ingress Controller Wilcard  C√°ch L√†m 1. Setup environment variables export PROJECT_ID=\u0026#34;YOUR_PROJECT_ID\u0026#34; export CLUSTER_NAME=\u0026#34;YOUR_CLUSTER_NAME\u0026#34; export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; gcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a 2. T·∫°o namespace gitlab v√† add helm repo kubectl create namespace gitlab helm repo add gitlab https://charts.gitlab.io/ helm repo update 3. D√πng Kubed ƒë·ªÉ synchronize TLS secret gi·ªØa c√°c namespaces helm repo add appscode https://charts.appscode.com/stable/ helm repo update helm install -n kubed appscode/kubed --version 0.12.0 --namespace kube-system --wait --set config.clusterName=${CLUSTER_NAME} Gi·∫£ s·ª≠ nh·ªù b√†i tr∆∞·ªõc m√† m√¨nh ƒë√£ issue ƒë∆∞·ª£c 1 TLS Secret th√†nh c√¥ng t√™n l√† echo-tls-secret-prod ·ªü namespace default,\nk get secret -A default default-token-7hsrc kubernetes.io/service-account-token 3 3h11m default echo-tls-secret-prod kubernetes.io/tls 3 157m default echo-tls-secret-staging kubernetes.io/tls 3 164m gitlab default-token-t728z kubernetes.io/service-account-token 3 99s gi·ªù m√¨nh c·∫ßn sync secret ƒë√≥ sang namespace gitlab kh√°c th√¨ c·∫ßn l√†m nh∆∞ sau:\nannotate secret m√† m√¨nh mu·ªën sync gi·ªØa c√°c namespaces\n# ƒê√°nh label `app=kubed` cho namespace gitlab kubectl label namespace gitlab app=kubed # Annotate ƒë·ªÉ secret m√¨nh mu·ªën s·∫Ω sync sang c√°c namespace c√≥ label `app=kubed`  kubectl annotate secret echo-tls-secret-prod -n default kubed.appscode.com/sync=\u0026#34;app=kubed\u0026#34; check: (B·∫°n s·∫Ω th·∫•y namespace gitlab c≈©ng c√≥ secret echo-tls-secret-prod)\nk get secret -A default default-token-7hsrc kubernetes.io/service-account-token 3 3h15m default echo-tls-secret-prod kubernetes.io/tls 3 161m default echo-tls-secret-staging kubernetes.io/tls 3 168m gitlab default-token-t728z kubernetes.io/service-account-token 3 5m43s gitlab echo-tls-secret-prod kubernetes.io/tls 3 6s 4. GCP - T·∫°o Service Account gcloud iam service-accounts create gitlab-gcs \\  --display-name \u0026#34;Gitlab Cloud Storage\u0026#34; 5. GCP - T·∫°o Service Account Key gcloud iam service-accounts keys create --iam-account \\  gitlab-gcs@${PROJECT_ID}.iam.gserviceaccount.com /tmp/gitlab-gcs-key.json 6. GCP - Binding role Storage Admin v√†o Service Account gcloud projects add-iam-policy-binding --role roles/storage.admin ${PROJECT_ID} \\  --member=serviceAccount:gitlab-gcs@${PROJECT_ID}.iam.gserviceaccount.com 7. GCP - Create GCS Buckets gsutil mb -c standard gs://${PROJECT_ID}-gitlab-uploads gsutil mb -c standard gs://${PROJECT_ID}-gitlab-artifacts gsutil mb -c standard gs://${PROJECT_ID}-gitlab-lfs gsutil mb -c standard gs://${PROJECT_ID}-gitlab-packages gsutil mb -c standard gs://${PROJECT_ID}-gitlab-registry gsutil mb -c standard gs://${PROJECT_ID}-gitlab-runner-cache gsutil mb -c nearline gs://${PROJECT_ID}-gitlab-backups 8. Create a Secret to hold your Cloud Storage credentials kubectl create secret generic google-application-credentials \\  --from-file=gcs-application-credentials-file=/tmp/gitlab-gcs-key.json \\  --namespace gitlab 9. T·∫°o file rails.yaml cat \u0026gt; /tmp/rails.yaml \u0026lt;\u0026lt;EOF provider: Google google_project: ${PROJECT_ID} google_client_email: gitlab-gcs@${PROJECT_ID}.iam.gserviceaccount.com google_json_key_string: \u0026#39;$(cat /tmp/gitlab-gcs-key.json)\u0026#39; EOF 10. T·∫°o secret t·ª´ file rails.yaml kubectl create secret generic gitlab-rails-storage --from-file=connection=/tmp/rails.yaml --namespace gitlab 11. (Optional) X√≥a key ƒëi cho secure rm -rf /tmp/gitlab-gcs-key.json rm -rf /tmp/rails.yaml 12. T·∫°o Gitlab helm values file cat \u0026gt; ./gitlab-values.yaml \u0026lt;\u0026lt;EOF # Values for gitlab/gitlab chart on GKE global: edition: ce ## doc/charts/globals.md#configure-ingress-settings ingress: configureCertmanager: false tls: enabled: true secretName: echo-tls-secret-prod hosts: domain: ${SUBDOMAIN} https: false gitlab: https: false registry: https: false minio: https: false ## doc/charts/globals.md#configure-minio-settings minio: enabled: false registry: bucket: ${PROJECT_ID}-gitlab-registry ## doc/charts/globals.md#configure-appconfig-settings ## Rails based portions of this chart share many settings appConfig: ## doc/charts/globals.md#general-application-settings enableUsagePing: false defaultTheme: 2 defaultProjectsFeatures: issues: true mergeRequests: true wiki: true snippets: true builds: true containerRegistry: true ## doc/charts/globals.md#lfs-artifacts-uploads-packages backups: bucket: ${PROJECT_ID}-gitlab-backups lfs: bucket: ${PROJECT_ID}-gitlab-lfs connection: secret: gitlab-rails-storage key: connection artifacts: bucket: ${PROJECT_ID}-gitlab-artifacts connection: secret: gitlab-rails-storage key: connection uploads: bucket: ${PROJECT_ID}-gitlab-uploads connection: secret: gitlab-rails-storage key: connection packages: bucket: ${PROJECT_ID}-gitlab-packages connection: secret: gitlab-rails-storage key: connection gitlab: webservice: minReplicas: 1 resources: limits: memory: 1.5G requests: cpu: 100m memory: 900M workhorse: resources: limits: memory: 100M requests: cpu: 10m memory: 10M task-runner: backups: objectStorage: backend: gcs config: secret: \u0026#34;google-application-credentials\u0026#34; key: gcs-application-credentials-file gcpProject: ${PROJECT_ID} sidekiq: minReplicas: 1 resources: limits: memory: 1.5G requests: cpu: 50m memory: 625M gitlab-shell: minReplicas: 1 nginx-ingress: controller: replicaCount: 1 minAvailable: 0 resources: requests: cpu: 50m memory: 100Mi defaultBackend: replicaCount: 1 minAvailable: 0 resources: requests: cpu: 5m memory: 5Mi redis: resources: requests: cpu: 10m memory: 64Mi certmanager: install: false prometheus: install: false registry: enabled: true certificate: secret: gitlab-rails-storage key: connection ingress: tls: enabled: true secretName: echo-tls-secret-prod gitlab-runner: install: false rbac: create: true runners: locked: false cache: cacheType: gcs gcsBucketname: ${PROJECT_ID}-gitlab-runner-cache secretName: gitlab-rails-storage cacheShared: true EOF 13. Install Gitlab helm chart helm upgrade --install gitlab gitlab/gitlab \\  --namespace gitlab \\  --values gitlab-values.yaml Check gitlab tr√™n link gitlab.your-subdomain.your-domain.net ƒë∆∞·ª£c nh∆∞ sau l√† OK:\nƒêƒÉng nh·∫≠p v√†o gitlab b·∫±ng user \u0026ldquo;root\u0026rdquo;, pass l·∫•y t·ª´ command sau:\nkubectl get secret gitlab-gitlab-initial-root-password -ojsonpath={.data.password} -n gitlab | base64 --decode ; echo Check xem c√≥ th·ªÉ Backup v√† Restore Gitlab b√¨nh th∆∞·ªùng ko l√† OK:\nkubectl exec gitlab-task-runner-7d686c6474-w8mnt -it backup-utility -n gitlab WARNING: This version of GitLab depends on gitlab-shell 11.0.0, but you're running Unknown. Please update gitlab-shell. 2020-02-21 09:26:19 +0000 -- Dumping database ... Dumping PostgreSQL database gitlabhq_production ... [DONE] 2020-02-21 09:26:22 +0000 -- done WARNING: This version of GitLab depends on gitlab-shell 11.0.0, but you're running Unknown. Please update gitlab-shell. 2020-02-21 09:26:32 +0000 -- Dumping repositories ... 2020-02-21 09:26:32 +0000 -- done Dumping registry ... empty Dumping uploads ... empty Dumping artifacts ... empty Dumping lfs ... empty Dumping packages ... empty WARNING: This version of GitLab depends on gitlab-shell 11.0.0, but you're running Unknown. Please update gitlab-shell. Packing up backup tar Copying file:///srv/gitlab/tmp/backup_tars/1582277165_2020_02_21_12.7.6_gitlab_backup.tar [Content-Type=application/x-tar]... / [1 files][150.0 KiB/150.0 KiB] Operation completed over 1 objects/150.0 KiB. [DONE] Backup can be found at gs://xxx-gitlab-backups/1582277165_2020_02_21_12.7.6_gitlab_backup.tar N·∫øu backup b·ªã l·ªói ƒë·ªè ho·∫∑c c√≥ l·ªói ki·ªÉu Bucket not found: registry. Skipping backup of registry th√¨ c√≥ nghƒ©a l√† registry bucket kh√¥ng nh√¨n th·∫•y, c·∫ßn check l·∫°i file values nh√©\nDONE! üéâüéâüéâ\n14. Clean up  N√™n ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ x√≥a Cluster, VPC V√†o Service Cloud DNS, x√≥a c√°c record ƒë∆∞·ª£c sinh ra b·ªüi ExternalDNS\n V√†o IAM - Service Account x√≥a service account gitlab m√† b·∫°n ƒë√£ t·∫°o\n V√†o Compute Engine - Disk x√≥a c√°c disk ƒë∆∞·ª£c t·∫°o b·ªüi Cluster\n V√†o Storage, x√≥a c√°c bucket *-gitlab-* b·∫°n ƒë√£ t·∫°o ra  REFERENCES https://docs.gitlab.com/charts/ https://docs.gitlab.com/charts/charts/globals.html#connection https://ruzickap.github.io/k8s-harbor/part-03/#install-nginx-ingress https://gitlab.com/gitlab-org/charts/gitlab/issues/1464 https://docs.gitlab.com/charts/backup-restore/backup.html\n","href":"/posts/k8s-x-setup-gitlab-on-gke-cluster/","title":"K8S 10: Setup Gitlab Self Hosted on GKE Cluster"},{"content":"2 ƒë·ª©a r·ªß nhau ƒëi ch∆°i v√†o 1 cu·ªëi tu·∫ßn trong th√°ng 6, l√∫c m√† c·∫£ 2 ƒë·ªÅu r·∫£nh üòÑ\nH·ªèi ch·ªã m√¨nh 1 h·ªìi ko ch·ªçn ƒë∆∞·ª£c ƒë·ªãa ƒëi·ªÉm n√†o, cu·ªëi c√πng quy·∫øt ƒë·ªãnh ch·ªçn Ninh B√¨nh ƒë·ªÉ ƒëi\nTi√™u ch√≠ l√† ƒëi ngh·ªâ d∆∞·ª°ng cho th∆∞ gi√£n ƒë·∫ßu √≥c l√† ch√≠nh, ko ph·∫£i ƒëi l·∫°i nhi·ªÅu, y√™n tƒ©nh, √≠t ng∆∞·ªùi üòÑ\n20/6/2020 Xu·∫•t ph√°t t·ª´ c·ªïng sau Big C ThƒÉng Long\nCheckin The Reed Hotel\nthu√™ 1 con xe m√°y ƒëi d·∫°o loanh quanh (n·ª≠a ng√†y gi√° 125k)\nƒêi l·∫°c v√†o v√πng ƒë·ªìng ko m√¥ng qu·∫°nh, ngiu lon ton ƒëi t√¨m ch√¢u ch·∫•u :v tr√®o l√™n 1 c√°i tr·∫°m b∆°m n∆∞·ªõc, ƒë√°ng nh·∫Ω g·ªçi ng iu quay l·∫°i ch·ª•p ·∫£nh s·∫Ω ƒë·∫πp h∆°n H√°i hoa b·∫ª c√†nh, m√¨nh ƒë·ª©ng tr√¥ng üëÄüëÄ\ntr·ªùi t·ªëi r√πi ƒëi v·ªÅ ƒÉn th·ªãt D√™ qu√°n Ch√≠nh Th∆∞ ƒë∆∞·ªùng Tr√†ng An\nCh√¢n dung th·ªß ph·∫°m h√°i tr·ªôm hoa ph∆∞·ª£ng :))\nR√πi ƒëi ƒÉn kem x√¥i ·ªü qu√°n Thanh H·∫±ng ƒë∆∞·ªùng Nguy·ªÖn Th√°i H·ªçc v·ªõi b·∫°n Linh\n21/6/2020 s√°ng h√¥m sau xu·ªëng l√∫c 8h45 ƒÉn s√°ng R·ªìi 11h checkout tr·∫£ ph√≤ng, ti·∫øp t·ª•c ƒë·∫øn nh√† s√†n Dream Up c≈©ng c·ªßa 1 ch·ªã b·∫°n Linh 2 anh ch·ªã ng·∫Øm Nh·∫≠t th·ª±c :))) S·ª£ ch√≥ girl üê∂ üê∂ ƒë·∫±ng sau nh√† 1 con s√¥ng r·∫•t r·ªông Ham h·ªë ƒë√≤i b∆°i nh∆∞ng m√¨nh ko cho =)) ch·ªâ ƒë∆∞·ª£c ngh·ªãch n∆∞·ªõc\n  ƒêi sang 1 kia s√¥ng xem b√£i c·ªè to√†n ph√¢n b√≤\nng·∫Øm nh·∫≠t th·ª±c R√πi sau ƒë√≥ nh·ªù anh ch·ªã DA g·ªçi xe cho ƒë·ªÉ v·ªÅ HN\nK·∫øt th√∫c 1 chuy·∫øn ƒëi ng·∫Øn ng√†y h∆°i n√≥ng nh∆∞ng m√† zui :))) üéàüéà\n","href":"/love/encrypt-linh-and-ninh-binh-2020/","title":"Linh and Ninh Binh 6.2020"},{"content":"","href":"/love/","title":"Loves"},{"content":"","href":"/tags/cert-manager/","title":"Cert-manager"},{"content":"","href":"/tags/externaldns/","title":"ExternalDNS"},{"content":"Y√™u C·∫ßu  ƒê√£ t·∫°o GKE Cluster ƒê√£ mua 1 domain ri√™ng, ki·ªÉu your-domain.net ƒê√£ setup service CloudDNS trong GCP console, ƒë·ªÉ s·ª≠ d·ª•ng dc your-domain.net:\n  C√°ch L√†m 0. Setup environment variables C√°c bi·∫øn n√†y s·∫Ω d√πng xuy√™n su·ªët trong b√†i:\nexport PROJECT_ID=\u0026#34;your-project-id\u0026#34; export DOMAIN=\u0026#34;your-domain.net\u0026#34; export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; export YOUR_EMAIL_ADDRESS=\u0026#34;your-mail-address\u0026#34; # Cloud DNS service account n√™n l√† unique ƒë·ªÉ tr√°nh l·ªói khi issue Certificate, n√™n m√¨nh cho th√™m h·∫≠u t·ªë `date` v√†o nh∆∞ sau:  export CLOUD_DNS_SA=\u0026#34;certmng-cdns-$(date +%d%m%Y-%H)\u0026#34; 1. Install Helm 2 mkdir ~/environment cd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF Sau m·ªói l·∫ßn x√≥a ƒëi t·∫°o l·∫°i cluster, b·∫°n ƒë·ªÅu c·∫ßn l√†m b∆∞·ªõc sau ƒë·ªÉ install Tiller (c√≤n g·ªçi l√† helm server-side) l√™n cluster\nkubectl apply -f ~/environment/rbac.yaml helm init --service-account tiller check version:\nhelm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Server: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} 2. Install Nginx Ingress Controller helm install -n nginx-ingress stable/nginx-ingress --namespace nginx-ingress k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE nginx-ingress nginx-ingress-controller-5cbd846c5f-nmhwz 1/1 Running 0 20s nginx-ingress nginx-ingress-default-backend-576b86996d-5c4dh 1/1 Running 0 20s 3. Install External DNS cat \u0026gt; ./externaldns-values.yaml \u0026lt;\u0026lt;EOF rbac: create: true provider: google interval: \u0026#34;1m\u0026#34; policy: sync # or upsert-only domainFilters: [ \u0026#39;${DOMAIN}\u0026#39; ] source: ingress registry: txt txt-owner-id: my-identifier EOF install by helm:\nhelm install -n external-dns stable/external-dns -f externaldns-values.yaml --namespace nginx-ingress check:\nk get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE nginx-ingress external-dns-6df4c8c96d-cwvl2 1/1 Running 0 47s N·∫øu b·∫°n d·ª± ƒë·ªãnh d√πng link \u0026ldquo;your-subdomain.your-domain.net\u0026rdquo; v√† wildcard cho \u0026ldquo;*.your-subdomain.your-domain.net\u0026rdquo; th√¨ c·∫ßn annotate n√≥ v√†o service ingress-controller nh∆∞ sau:\nkubectl annotate service nginx-ingress-controller \u0026#34;external-dns.alpha.kubernetes.io/hostname=${SUBDOMAIN}.,*.${SUBDOMAIN}.\u0026#34; -n nginx-ingress --overwrite Sau khi annotate th√¨ b·∫°n s·∫Ω th·∫•y tr√™n CloudDNS t·ª± ƒë·ªông xu·∫•t hi·ªán c√°c A record v√† TXT record c·ªßa domain (·∫£nh):\n4. Setup sample app (Echo App) T·∫°o 1 web app ƒë∆°n gi·∫£n ƒë·ªÉ test vi·ªác access v√†o domain:\ncat \u0026gt; ./echo-app.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: echo spec: ports: - port: 80 targetPort: 5678 selector: app: echo --- apiVersion: apps/v1 kind: Deployment metadata: name: echo spec: selector: matchLabels: app: echo replicas: 1 template: metadata: labels: app: echo spec: containers: - name: echo image: hashicorp/http-echo args: - \u0026#34;-text=Echo!\u0026#34; ports: - containerPort: 5678 EOF apply ƒë·ªÉ install app:\nk apply -f echo-app.yaml check:\nk get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default echo-84bb76dddf-pgtdl 1/1 Running 0 11s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/echo ClusterIP 10.68.64.124 \u0026lt;none\u0026gt; 80/TCP 24s 5. Create Ingress Ingress s·∫Ω route traffic t·ª´ 1 domain c·ª• th·ªÉ v√†o service c·ªßa Echo App\ncat \u0026gt; ./echo-ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: echo-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: ${SUBDOMAIN} http: paths: - path: / backend: serviceName: echo servicePort: 80 - host: \u0026#34;*.${SUBDOMAIN}\u0026#34; http: paths: - path: / backend: serviceName: echo servicePort: 80 EOF apply ingress:\nk apply -f echo-ingress.yaml check tr√™n browser (v√†o link sau your-subdomain.your-domain.net), hi·ªÉn th·ªã nh∆∞ sau l√† ok (·∫£nh):\nüéâ Nh∆∞ v·∫≠y web ƒë√£ c√≥ HTTP, ƒë·ªÉ c√≥ HTTPS th√¨ c·∫ßn d√πng Cert Manager, c√°c b∆∞·ªõc ti·∫øp theo s·∫Ω setup HTTPS\n6. Install Cert Manager kubectl apply --validate=false -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml kubectl create namespace cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update helm install \\  -n cert-manager \\  --namespace cert-manager \\  --version v0.12.0 \\  jetstack/cert-manager check:\nkubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5fd56d487b-j862g 1/1 Running 0 2m5s cert-manager-cainjector-6bdbb96457-9scgc 1/1 Running 0 2m5s cert-manager-webhook-6f78788cd-x2rrs 1/1 Running 0 2m5s T·∫°o key c·ªßa Service Account v·ªõi quy·ªÅn \u0026ldquo;DNS Admin\u0026rdquo;:\ngcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a T·∫°o service account:\ngcloud iam service-accounts create ${CLOUD_DNS_SA} \\  --display-name \u0026#34;Service Account to support ACME DNS-01 challenge.\u0026#34; T·∫°o service account Key:\ngcloud iam service-accounts keys create --iam-account \\  ${CLOUD_DNS_SA}@${PROJECT_ID}.iam.gserviceaccount.com /tmp/cloud-dns-key.json Binding c√°i role DNS Admin v√†o Service account ƒë√≥:\ngcloud projects add-iam-policy-binding --role roles/dns.admin ${PROJECT_ID} \\  --member=serviceAccount:${CLOUD_DNS_SA}@${PROJECT_ID}.iam.gserviceaccount.com T·∫°o secret d√πng ƒë·ªÉ issue Certificate:\nkubectl create secret generic clouddns-service-account --from-file=service-account-key.json=/tmp/cloud-dns-key.json --namespace=cert-manager check:\nkubectl describe secret clouddns-service-account -n cert-manager Name: clouddns-service-account Namespace: cert-manager Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== service-account-key.json: 2335 bytes t·∫°o ClusterIssuer staging\ncat \u0026gt; ./staging-issuer.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: ${YOUR_EMAIL_ADDRESS} # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # ACME DNS-01 provider configurations solvers: - dns01: clouddns: # The Google Cloud project in which to update the DNS zone project: ${PROJECT_ID} # A secretKeyRef to a google cloud json service account serviceAccountSecretRef: name: clouddns-service-account key: service-account-key.json EOF apply issuer:\nkubectl apply -f staging-issuer.yaml t·∫°o ClusterIssuer production:\ncat \u0026gt; ./production-issuer.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: ${YOUR_EMAIL_ADDRESS} # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prod # ACME DNS-01 provider configurations solvers: - dns01: clouddns: # The Google Cloud project in which to update the DNS zone project: ${PROJECT_ID} # A secretKeyRef to a google cloud json service account serviceAccountSecretRef: name: clouddns-service-account key: service-account-key.json EOF apply issuer:\nkubectl apply -f production-issuer.yaml check:\nk get clusterissuer -A NAME READY AGE letsencrypt-prod True 20s letsencrypt-staging True 99s Issue staging Certificate cho domain:\ncat \u0026gt; ./echo-certificate-staging.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: echo-tls-cert-staging namespace: default spec: secretName: echo-tls-secret-staging issuerRef: name: letsencrypt-staging kind: ClusterIssuer commonName: ${SUBDOMAIN} dnsNames: - ${SUBDOMAIN} - \u0026#34;*.${SUBDOMAIN}\u0026#34; EOF apply ƒë·ªÉ issue certificate:\nk apply -f echo-certificate-staging.yaml Qu√° tr√¨nh issue certificate ph·∫£i ch·ªù kho·∫£ng 5 ph√∫t th√¨ m·ªõi Issued th√†nh c√¥ng ƒë∆∞·ª£c, v√¨ Issue cho wildcard m·∫•t th·ªùi gian\nN·∫øu th√†nh c√¥ng s·∫Ω hi·ªán nh∆∞ sau:\nk describe cert echo-tls-cert-staging Status: Conditions: Last Transition Time: 2019-12-14T14:54:05Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2020-03-13T13:54:05Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal GeneratedKey 5m25s cert-manager Generated a new private key Normal Requested 5m25s cert-manager Created new CertificateRequest resource \u0026quot;echo-tls-cert-staging-2906129357\u0026quot; Normal Issued 61s cert-manager Certificate issued successfully Khi ƒë√£ confirm th√†nh c√¥ng th√¨ c√≥ th·ªÉ l√†m b∆∞·ªõc sau (Ch√∫ √Ω l√† ph·∫£i confirm ch·∫Øc ch·∫Øn l√† staging Certificate echo-tls-cert-staging ƒë√£ ƒëc issue th√†nh c√¥ng th√¨ m·ªõi l√†m ti·∫øp production Certificate, b·ªüi v√¨ letsencrypt production Certificate r·∫•t h·∫°n ch·∫ø s·ªë l·∫ßn issue, l√†m ƒëi l√†m l·∫°i v·ªõi 1 subdomain c√≥ th·ªÉ b·ªã l·ªói ko th·ªÉ l√†m l·∫°i)\nIssue production Certificate cho domain:\ncat \u0026gt; ./echo-certificate-prod.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: echo-tls-cert-prod namespace: default spec: secretName: echo-tls-secret-prod issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: ${SUBDOMAIN} dnsNames: - ${SUBDOMAIN} - \u0026#34;*.${SUBDOMAIN}\u0026#34; EOF apply ƒë·ªÉ issue certificate:\nk apply -f echo-certificate-prod.yaml check :\nk describe cert echo-tls-cert-prod\tStatus:\tConditions:\tLast Transition Time: 2019-12-14T14:54:05Z\tMessage: Certificate is up to date and has not expired\tReason: Ready\tStatus: True\tType: Ready\tNot After: 2020-03-13T13:54:05Z\tEvents:\tType Reason Age From Message\t---- ------ ---- ---- -------\tNormal GeneratedKey 4m18s cert-manager Generated a new private key Normal Requested 4m18s cert-manager Created new CertificateRequest resource \u0026quot;echo-tls-cert-prod-2591535419\u0026quot; Normal Issued 5s cert-manager Certificate issued successfully\t7. Create Ingress with TLS TLS ·ªü ƒë√¢y s·∫Ω s·ª≠ d·ª•ng Certificate m√† ch√∫ng ta ƒë√£ issue th√†nh c√¥ng ·ªü b∆∞·ªõc tr∆∞·ªõc, ƒë·ªÉ secure cho app c·ªßa m√¨nh:\ncat \u0026gt; ./echo-ingress-prod.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: echo-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: tls: - hosts: - ${SUBDOMAIN} - \u0026#34;*.${SUBDOMAIN}\u0026#34; secretName: echo-tls-secret-prod rules: - host: ${SUBDOMAIN} http: paths: - path: / backend: serviceName: echo servicePort: 80 - host: \u0026#34;*.${SUBDOMAIN}\u0026#34; http: paths: - path: / backend: serviceName: echo servicePort: 80 EOF apply ingress:\nk apply -f echo-ingress-prod.yaml Check tr√™n browser (v√†o link your-subdomain.your-domain.net), confirm c√≥ HTTPS v√† Certificate Valid nh∆∞ h√¨nh sau l√† OK:\nCheck wildcard ho·∫°t ƒë·ªông OK b·∫±ng c√°ch th√™m 1 prefix b·∫•t k·ª≥ (v√≠ d·ª• nh∆∞ test) v√†o link tr√™n ƒë·ªÉ th√†nh nh∆∞ sau: test.your-subdomain.your-domain.net, n·∫øu v·∫´n c√≥ HTTPS v√† Certificate Valid th√¨ c√≥ nghƒ©a l√† wildcard ƒë√£ ho·∫°t ƒë·ªông th√†nh c√¥ng:\n8. Clean up  Ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ x√≥a Cluster, VPC V√†o Service Cloud DNS, x√≥a c√°c record ƒë∆∞·ª£c sinh ra b·ªüi ExternalDNS\n V√†o IAM - IAM x√≥a member certmng-cdns-* m√† b·∫°n ƒë√£ t·∫°o\n V√†o IAM - Service Account x√≥a service account certmng-cdns-* m√† b·∫°n ƒë√£ t·∫°o\n  Done!\n","href":"/bk/k8s-ix-setup-extdns-certmanager-nginxingress-wilcard/","title":"K8S 9: Setup External DNS + Cert Manager + Nginx Ingress Controller Wilcard"},{"content":"Y√™u C·∫ßu  ƒê√£ t·∫°o GKE Cluster ƒê√£ mua 1 domain ri√™ng, ki·ªÉu your-domain.net ƒê√£ setup service CloudDNS trong GCP console, ƒë·ªÉ s·ª≠ d·ª•ng dc your-domain.net:\n  C√°ch L√†m 0. Setup environment variables C√°c bi·∫øn n√†y s·∫Ω d√πng xuy√™n su·ªët trong b√†i:\nexport PROJECT_ID=\u0026#34;your-project-id\u0026#34; export DOMAIN=\u0026#34;your-domain.net\u0026#34; export SUBDOMAIN=\u0026#34;your-subdomain.your-domain.net\u0026#34; export YOUR_EMAIL_ADDRESS=\u0026#34;your-mail-address\u0026#34; # Cloud DNS service account n√™n l√† unique ƒë·ªÉ tr√°nh l·ªói khi issue Certificate, n√™n m√¨nh cho th√™m h·∫≠u t·ªë `date` v√†o nh∆∞ sau:  export CLOUD_DNS_SA=\u0026#34;certmng-cdns-$(date +%d%m%Y-%H)\u0026#34; 1. Install Helm 2 mkdir ~/environment cd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF Sau m·ªói l·∫ßn x√≥a ƒëi t·∫°o l·∫°i cluster, b·∫°n ƒë·ªÅu c·∫ßn l√†m b∆∞·ªõc sau ƒë·ªÉ install Tiller (c√≤n g·ªçi l√† helm server-side) l√™n cluster\nkubectl apply -f ~/environment/rbac.yaml helm init --service-account tiller check version:\nhelm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Server: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} 2. Install Nginx Ingress Controller helm install -n nginx-ingress stable/nginx-ingress --namespace nginx-ingress k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE nginx-ingress nginx-ingress-controller-5cbd846c5f-nmhwz 1/1 Running 0 20s nginx-ingress nginx-ingress-default-backend-576b86996d-5c4dh 1/1 Running 0 20s 3. Install External DNS cat \u0026gt; ./externaldns-values.yaml \u0026lt;\u0026lt;EOF rbac: create: true provider: google interval: \u0026#34;1m\u0026#34; policy: sync # or upsert-only domainFilters: [ \u0026#39;${DOMAIN}\u0026#39; ] source: ingress registry: txt txt-owner-id: my-identifier EOF install by helm:\nhelm install -n external-dns stable/external-dns -f externaldns-values.yaml --namespace nginx-ingress check:\nk get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE nginx-ingress external-dns-6df4c8c96d-cwvl2 1/1 Running 0 47s N·∫øu b·∫°n d·ª± ƒë·ªãnh d√πng link \u0026ldquo;your-subdomain.your-domain.net\u0026rdquo; v√† wildcard cho \u0026ldquo;*.your-subdomain.your-domain.net\u0026rdquo; th√¨ c·∫ßn annotate n√≥ v√†o service ingress-controller nh∆∞ sau:\nkubectl annotate service nginx-ingress-controller \u0026#34;external-dns.alpha.kubernetes.io/hostname=${SUBDOMAIN}.,*.${SUBDOMAIN}.\u0026#34; -n nginx-ingress --overwrite Sau khi annotate th√¨ b·∫°n s·∫Ω th·∫•y tr√™n CloudDNS t·ª± ƒë·ªông xu·∫•t hi·ªán c√°c A record v√† TXT record c·ªßa domain (·∫£nh):\n4. Setup sample app (Echo App) T·∫°o 1 web app ƒë∆°n gi·∫£n ƒë·ªÉ test vi·ªác access v√†o domain:\ncat \u0026gt; ./echo-app.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: echo spec: ports: - port: 80 targetPort: 5678 selector: app: echo --- apiVersion: apps/v1 kind: Deployment metadata: name: echo spec: selector: matchLabels: app: echo replicas: 1 template: metadata: labels: app: echo spec: containers: - name: echo image: hashicorp/http-echo args: - \u0026#34;-text=Echo!\u0026#34; ports: - containerPort: 5678 EOF apply ƒë·ªÉ install app:\nk apply -f echo-app.yaml check:\nk get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default echo-84bb76dddf-pgtdl 1/1 Running 0 11s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/echo ClusterIP 10.68.64.124 \u0026lt;none\u0026gt; 80/TCP 24s 5. Create Ingress Ingress s·∫Ω route traffic t·ª´ 1 domain c·ª• th·ªÉ v√†o service c·ªßa Echo App\ncat \u0026gt; ./echo-ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: echo-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: ${SUBDOMAIN} http: paths: - path: / backend: serviceName: echo servicePort: 80 - host: \u0026#34;*.${SUBDOMAIN}\u0026#34; http: paths: - path: / backend: serviceName: echo servicePort: 80 EOF apply ingress:\nk apply -f echo-ingress.yaml check tr√™n browser (v√†o link sau your-subdomain.your-domain.net), hi·ªÉn th·ªã nh∆∞ sau l√† ok (·∫£nh):\nüéâ Nh∆∞ v·∫≠y web ƒë√£ c√≥ HTTP, ƒë·ªÉ c√≥ HTTPS th√¨ c·∫ßn d√πng Cert Manager, c√°c b∆∞·ªõc ti·∫øp theo s·∫Ω setup HTTPS\n6. Install Cert Manager kubectl apply --validate=false -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml kubectl create namespace cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update helm install \\  -n cert-manager \\  --namespace cert-manager \\  --version v0.12.0 \\  jetstack/cert-manager check:\nkubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5fd56d487b-j862g 1/1 Running 0 2m5s cert-manager-cainjector-6bdbb96457-9scgc 1/1 Running 0 2m5s cert-manager-webhook-6f78788cd-x2rrs 1/1 Running 0 2m5s T·∫°o key c·ªßa Service Account v·ªõi quy·ªÅn \u0026ldquo;DNS Admin\u0026rdquo;:\ngcloud config set project ${PROJECT_ID} gcloud config set compute/region asia-northeast1 gcloud config set compute/zone asia-northeast1-a T·∫°o service account:\ngcloud iam service-accounts create ${CLOUD_DNS_SA} \\  --display-name \u0026#34;Service Account to support ACME DNS-01 challenge.\u0026#34; T·∫°o service account Key:\ngcloud iam service-accounts keys create --iam-account \\  ${CLOUD_DNS_SA}@${PROJECT_ID}.iam.gserviceaccount.com /tmp/cloud-dns-key.json Binding c√°i role DNS Admin v√†o Service account ƒë√≥:\ngcloud projects add-iam-policy-binding --role roles/dns.admin ${PROJECT_ID} \\  --member=serviceAccount:${CLOUD_DNS_SA}@${PROJECT_ID}.iam.gserviceaccount.com T·∫°o secret d√πng ƒë·ªÉ issue Certificate:\nkubectl create secret generic clouddns-service-account --from-file=service-account-key.json=/tmp/cloud-dns-key.json --namespace=cert-manager check:\nkubectl describe secret clouddns-service-account -n cert-manager Name: clouddns-service-account Namespace: cert-manager Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== service-account-key.json: 2335 bytes t·∫°o ClusterIssuer staging\ncat \u0026gt; ./staging-issuer.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: ${YOUR_EMAIL_ADDRESS} # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # ACME DNS-01 provider configurations solvers: - dns01: clouddns: # The Google Cloud project in which to update the DNS zone project: ${PROJECT_ID} # A secretKeyRef to a google cloud json service account serviceAccountSecretRef: name: clouddns-service-account key: service-account-key.json EOF apply issuer:\nkubectl apply -f staging-issuer.yaml t·∫°o ClusterIssuer production:\ncat \u0026gt; ./production-issuer.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: ${YOUR_EMAIL_ADDRESS} # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prod # ACME DNS-01 provider configurations solvers: - dns01: clouddns: # The Google Cloud project in which to update the DNS zone project: ${PROJECT_ID} # A secretKeyRef to a google cloud json service account serviceAccountSecretRef: name: clouddns-service-account key: service-account-key.json EOF apply issuer:\nkubectl apply -f production-issuer.yaml check:\nk get clusterissuer -A NAME READY AGE letsencrypt-prod True 20s letsencrypt-staging True 99s Issue staging Certificate cho domain:\ncat \u0026gt; ./echo-certificate-staging.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: echo-tls-cert-staging namespace: default spec: secretName: echo-tls-secret-staging issuerRef: name: letsencrypt-staging kind: ClusterIssuer commonName: ${SUBDOMAIN} dnsNames: - ${SUBDOMAIN} - \u0026#34;*.${SUBDOMAIN}\u0026#34; EOF apply ƒë·ªÉ issue certificate:\nk apply -f echo-certificate-staging.yaml Qu√° tr√¨nh issue certificate ph·∫£i ch·ªù kho·∫£ng 5 ph√∫t th√¨ m·ªõi Issued th√†nh c√¥ng ƒë∆∞·ª£c, v√¨ Issue cho wildcard m·∫•t th·ªùi gian\nN·∫øu th√†nh c√¥ng s·∫Ω hi·ªán nh∆∞ sau:\nk describe cert echo-tls-cert-staging Status: Conditions: Last Transition Time: 2019-12-14T14:54:05Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2020-03-13T13:54:05Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal GeneratedKey 5m25s cert-manager Generated a new private key Normal Requested 5m25s cert-manager Created new CertificateRequest resource \u0026quot;echo-tls-cert-staging-2906129357\u0026quot; Normal Issued 61s cert-manager Certificate issued successfully Khi ƒë√£ confirm th√†nh c√¥ng th√¨ c√≥ th·ªÉ l√†m b∆∞·ªõc sau (Ch√∫ √Ω l√† ph·∫£i confirm ch·∫Øc ch·∫Øn l√† staging Certificate echo-tls-cert-staging ƒë√£ ƒëc issue th√†nh c√¥ng th√¨ m·ªõi l√†m ti·∫øp production Certificate, b·ªüi v√¨ letsencrypt production Certificate r·∫•t h·∫°n ch·∫ø s·ªë l·∫ßn issue, l√†m ƒëi l√†m l·∫°i v·ªõi 1 subdomain c√≥ th·ªÉ b·ªã l·ªói ko th·ªÉ l√†m l·∫°i)\nIssue production Certificate cho domain:\ncat \u0026gt; ./echo-certificate-prod.yaml \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: echo-tls-cert-prod namespace: default spec: secretName: echo-tls-secret-prod issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: ${SUBDOMAIN} dnsNames: - ${SUBDOMAIN} - \u0026#34;*.${SUBDOMAIN}\u0026#34; EOF apply ƒë·ªÉ issue certificate:\nk apply -f echo-certificate-prod.yaml check :\nk describe cert echo-tls-cert-prod\tStatus:\tConditions:\tLast Transition Time: 2019-12-14T14:54:05Z\tMessage: Certificate is up to date and has not expired\tReason: Ready\tStatus: True\tType: Ready\tNot After: 2020-03-13T13:54:05Z\tEvents:\tType Reason Age From Message\t---- ------ ---- ---- -------\tNormal GeneratedKey 4m18s cert-manager Generated a new private key Normal Requested 4m18s cert-manager Created new CertificateRequest resource \u0026quot;echo-tls-cert-prod-2591535419\u0026quot; Normal Issued 5s cert-manager Certificate issued successfully\t7. Create Ingress with TLS TLS ·ªü ƒë√¢y s·∫Ω s·ª≠ d·ª•ng Certificate m√† ch√∫ng ta ƒë√£ issue th√†nh c√¥ng ·ªü b∆∞·ªõc tr∆∞·ªõc, ƒë·ªÉ secure cho app c·ªßa m√¨nh:\ncat \u0026gt; ./echo-ingress-prod.yaml \u0026lt;\u0026lt;EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: echo-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: tls: - hosts: - ${SUBDOMAIN} - \u0026#34;*.${SUBDOMAIN}\u0026#34; secretName: echo-tls-secret-prod rules: - host: ${SUBDOMAIN} http: paths: - path: / backend: serviceName: echo servicePort: 80 - host: \u0026#34;*.${SUBDOMAIN}\u0026#34; http: paths: - path: / backend: serviceName: echo servicePort: 80 EOF apply ingress:\nk apply -f echo-ingress-prod.yaml Check tr√™n browser (v√†o link your-subdomain.your-domain.net), confirm c√≥ HTTPS v√† Certificate Valid nh∆∞ h√¨nh sau l√† OK:\nCheck wildcard ho·∫°t ƒë·ªông OK b·∫±ng c√°ch th√™m 1 prefix b·∫•t k·ª≥ (v√≠ d·ª• nh∆∞ test) v√†o link tr√™n ƒë·ªÉ th√†nh nh∆∞ sau: test.your-subdomain.your-domain.net, n·∫øu v·∫´n c√≥ HTTPS v√† Certificate Valid th√¨ c√≥ nghƒ©a l√† wildcard ƒë√£ ho·∫°t ƒë·ªông th√†nh c√¥ng:\n8. Clean up  Ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ x√≥a Cluster, VPC V√†o Service Cloud DNS, x√≥a c√°c record ƒë∆∞·ª£c sinh ra b·ªüi ExternalDNS\n V√†o IAM - IAM x√≥a member certmng-cdns-* m√† b·∫°n ƒë√£ t·∫°o\n V√†o IAM - Service Account x√≥a service account certmng-cdns-* m√† b·∫°n ƒë√£ t·∫°o\n  Done!\n","href":"/posts/k8s-ix-setup-extdns-certmanager-nginxingress-wilcard/","title":"K8S 9: Setup External DNS + Cert Manager + Nginx Ingress Controller Wilcard"},{"content":"","href":"/tags/efk/","title":"EFK"},{"content":"","href":"/tags/elasticsearch/","title":"ElasticSearch"},{"content":"","href":"/tags/fluentd/","title":"Fluentd"},{"content":"","href":"/tags/gcp/","title":"GCP"},{"content":"Gi·ªõi thi·ªáu B√†i n√†y h∆∞·ªõng d·∫´n d·ª±ng EFK stack ph·ª©c t·∫°p h∆°n, d√πng ConfigMap,\nElasticSearch chia l√†m c√°c role \u0026ldquo;master, client, data\u0026rdquo;,\nc√≥ √°p d·ª•ng authentication cho Kibana ƒë·ªÉ \u0026ldquo;more secure\u0026rdquo;\nC√°ch l√†m ƒê·∫ßu ti√™n c·∫ßn checkout source code n√†y:\ngit clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series/efk-stack-advanced 1. T·∫°o cluster Tr√™n gcp th√¨ t·∫°o cluster b·∫±ng console, m√¨nh ƒë√£ ch·ªçn lo·∫°i N1 standard 2 (2vCPU,7.5GB memory), t·∫°o cluster ch·ª©a 2 node nh∆∞ v·∫≠y\n2. T·∫°o namespace kubectl create -f kube-logging.yaml 3. T·∫°o ElasticSearch master  The first node of the cluster we\u0026rsquo;re going to setup is the master which is responsible of controlling the cluster.\n kubectl create -f elasticsearch-master.yaml 4. T·∫°o ElasticSearch data  The second node of the cluster we\u0026rsquo;re going to setup is the data which is responsible of hosting the data and executing the queries (CRUD, search, aggregation).\n kubectl create -f elasticsearch-data.yaml 5. T·∫°o ElasticSearch client  The last but not least node of the cluster is the client which is responsible of exposing an HTTP interface and pass queries to the data node.\n kubectl create -f elasticsearch-client.yaml Wait for all pods in state READY 1/1\n$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default docker-postgres-cf447b874-p8gbq 1/1 Running 0 25m default docker-spring-boot-containers-7c694df96b-qznth 1/1 Running 0 25m kube-logging elasticsearch-client-784cbb477-s7mkq 1/1 Running 0 2m8s kube-logging elasticsearch-data-0 1/1 Running 0 2m18s kube-logging elasticsearch-master-56fd947c4c-8brls 1/1 Running 0 2m25s kube-system kube-dns-5f886bf8d8-5km8c 4/4 Running 0 28m kube-system kube-dns-5f886bf8d8-vknfr 4/4 Running 0 28m kube-system kube-dns-autoscaler-85f8bdb54-d946p 1/1 Running 0 28m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-3b51bccc-lzpf 1/1 Running 0 28m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-d645c580-blss 1/1 Running 0 28m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-e1ba0508-lk35 1/1 Running 0 28m kube-system l7-default-backend-8f479dd9-fx4wg 1/1 Running 0 28m kube-system metrics-server-v0.3.1-8d4c5db46-lqrmp 2/2 Running 0 28m kube-system tiller-deploy-9bf6fb76d-wj229 1/1 Running 0 26m Ch·ªù 1 v√†i ph√∫t ƒë·ªÉ c√°c pods c·ªßa elastic READY:\nkubectl logs -f -n kube-logging \\  $(kubectl get pods -n kube-logging | grep elasticsearch-master | sed -n 1p | awk \u0026#39;{print $1}\u0026#39;) \\  | grep \u0026#34;Cluster health status changed from \\[YELLOW\\] to \\[GREEN\\]\u0026#34; 6. Generate a password and store in a k8s secret  We enabled the xpack security module to secure our cluster, so we need to initialise the passwords. Execute the following command which runs the program bin/elasticsearch-setup-passwords within the client node container (any node would work) to generate default users and passwords.\n kubectl exec $(kubectl get pods -n kube-logging | grep elasticsearch-client | sed -n 1p | awk \u0026#39;{print $1}\u0026#39;) \\  -n kube-logging \\  -- bin/elasticsearch-setup-passwords auto -b Output\n$ kubectl exec $(kubectl get pods -n kube-logging | grep elasticsearch-client | sed -n 1p | awk '{print $1}') \\ -n kube-logging \\ -- bin/elasticsearch-setup-passwords auto -b Changed password for user apm_system PASSWORD apm_system = eo83yYSKxR49QTx5eNx3 Changed password for user kibana PASSWORD kibana = tdrr6dMWenSJiOX2eGDN Changed password for user logstash_system PASSWORD logstash_system = qf5dXCaYfLJflugUpGtB Changed password for user beats_system PASSWORD beats_system = PF75DeqUbWU5TusncL1l Changed password for user remote_monitoring_user PASSWORD remote_monitoring_user = 54Nmz6CSaejkcJwgijtP Changed password for user elastic PASSWORD elastic = S7kuf4HCinVdMp3Vtrkx Note the elastic user password and add it into a k8s secret like this:\nkubectl create secret generic elasticsearch-pw-elastic \\  -n kube-logging \\  --from-literal password=S7kuf4HCinVdMp3Vtrkx 7. T·∫°o Kibana kubectl create -f kibana.yaml 8. T·∫°o Fluentd kubectl create -f fluentd.yaml Wait for all pods in state READY 1/1\n$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default docker-postgres-cf447b874-p8gbq 1/1 Running 0 29m default docker-spring-boot-containers-7c694df96b-qznth 1/1 Running 0 29m kube-logging elasticsearch-client-784cbb477-s7mkq 1/1 Running 0 5m31s kube-logging elasticsearch-data-0 1/1 Running 0 5m41s kube-logging elasticsearch-master-56fd947c4c-8brls 1/1 Running 0 5m48s kube-logging fluentd-c48hz 1/1 Running 0 78s kube-logging fluentd-j9g7m 1/1 Running 0 78s kube-logging fluentd-xtk8k 1/1 Running 0 78s kube-logging kibana-5f8cb9b596-jvllj 1/1 Running 0 85s kube-system kube-dns-5f886bf8d8-5km8c 4/4 Running 0 32m kube-system kube-dns-5f886bf8d8-vknfr 4/4 Running 0 31m kube-system kube-dns-autoscaler-85f8bdb54-d946p 1/1 Running 0 31m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-3b51bccc-lzpf 1/1 Running 0 31m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-d645c580-blss 1/1 Running 0 31m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-e1ba0508-lk35 1/1 Running 0 31m kube-system l7-default-backend-8f479dd9-fx4wg 1/1 Running 0 32m kube-system metrics-server-v0.3.1-8d4c5db46-lqrmp 2/2 Running 0 31m kube-system tiller-deploy-9bf6fb76d-wj229 1/1 Running 0 29m Check logs c·ªßa Fluentd Pod n·∫øu show ra t∆∞∆°ng t·ª± nh∆∞ sau, ko c√≥ error g√¨ l√† ok\nkubectl logs -n kube-logging \u0026lt;POD_NAME\u0026gt; -n kube-logging # kubectl logs -n kube-logging fluentd-c48hz-n kube-logging output:\n2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;fluent.**\u0026quot; type=\u0026quot;null\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding filter pattern=\u0026quot;kubernetes.**\u0026quot; type=\u0026quot;kubernetes_metadata\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;kubernetes.var.log.containers.**kube-logging**.log\u0026quot; type=\u0026quot;null\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;kubernetes.var.log.containers.**kube-system**.log\u0026quot; type=\u0026quot;null\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;kubernetes.**\u0026quot; type=\u0026quot;elasticsearch\u0026quot; 2019-11-25 03:16:40 +0000 [warn]: #0 Detected ES 7.x or above: `_doc` will be used as the document `_type`. 2019-11-25 03:16:40 +0000 [info]: adding source type=\u0026quot;tail\u0026quot; 2019-11-25 03:16:40 +0000 [info]: #0 starting fluentd worker pid=11 ppid=6 worker=0 9. T·∫°o 1 app li√™n t·ª•c out ra log ƒë·ªÉ test kubectl create -f counter.yaml 10. Test Make sure all pods are in Running state and READY 1/1, 2/2, 3/3, 4/4 \u0026hellip;.\nkubectl get pods --all-namespaces get NodePort of service Kibana\nkubectl get svc --all-namespaces output\nNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.116.0.1 \u0026lt;none\u0026gt; 443/TCP 10m kube-logging service/elasticsearch-client ClusterIP 10.116.9.216 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 6m34s kube-logging service/elasticsearch-data ClusterIP 10.116.14.205 \u0026lt;none\u0026gt; 9300/TCP 6m46s kube-logging service/elasticsearch-master ClusterIP 10.116.10.136 \u0026lt;none\u0026gt; 9300/TCP 6m56s kube-logging service/kibana NodePort 10.116.1.237 \u0026lt;none\u0026gt; 5601:32141/TCP 2m15s kube-system service/default-http-backend NodePort 10.116.3.142 \u0026lt;none\u0026gt; 80:32636/TCP 10m kube-system service/heapster ClusterIP 10.116.10.12 \u0026lt;none\u0026gt; 80/TCP 10m kube-system service/kube-dns ClusterIP 10.116.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 10m kube-system service/metrics-server ClusterIP 10.116.12.232 \u0026lt;none\u0026gt; 443/TCP 10m Ta th·∫•y NodePort c·ªßa Kibana l√† 32141\nget external ip of nodes\nkubectl get nodes --output wide output\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-efk-stack-advanced-default-pool-eeaed3c6-kx6f Ready \u0026lt;none\u0026gt; 9m44s v1.13.11-gke.14 10.128.0.11 34.67.152.105 Container-Optimized OS from Google 4.14.138+ docker://18.9.7 gke-efk-stack-advanced-default-pool-eeaed3c6-rj79 Ready \u0026lt;none\u0026gt; 9m44s v1.13.11-gke.14 10.128.0.12 34.66.70.31 Container-Optimized OS from Google 4.14.138+ docker://18.9.7 Ta c√≥ th·ªÉ ch·ªçn 1 Node IP ƒë·ªÉ truy c·∫≠p, v√≠ d·ª• tr√™n th√¨\n\u0026lt;EXTERNAL-IP\u0026gt; = 34.67.152.105\n\u0026lt;NODOE_PORT\u0026gt; = 32141\nCreate a firewall rule to allow TCP traffic on your node port:\ngcloud compute firewall-rules create kibana-node-port --allow tcp:\u0026lt;NODOE_PORT\u0026gt; N·∫øu ƒë√£ c√≥ s·∫µn th√¨ c·∫ßn update ch·ª© ko th·ªÉ t·∫°o m·ªõi\ngcloud compute firewall-rules update kibana-node-port --allow tcp:\u0026lt;NODOE_PORT\u0026gt; Ho·∫∑c c√°c b·∫°n n√™n m·ªü all NodePort t·ª´ 30000-32767 b·∫±ng command sau:\ngcloud compute firewall-rules update kibana-node-port --allow tcp:30000-32767 r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODOE_PORT\u0026gt; ƒë·ªÉ check\nLogin b·∫±ng user elastic v√† passw ƒë√£ generate ·ªü step tr√™n\nT·∫°o index-pattern : logstash-*\nV√†o Discover, check logs N·∫øu ch·ªâ show log c·ªßa namespace default l√† Done!\nN·∫øu mu·ªën s·ª≠a ConfigMap c·ªßa Fluentd th√¨ c·∫ßn ch√∫ √Ω l√† sau khi s·ª≠a xong th√¨ sau khi apply template m·ªõi,\nb·∫°n ph·∫£i delete c√°c pods fluentd c≈© ƒë·ªÉ n√≥ apply c√°c config v·ª´a thay ƒë·ªïi\n11. D√πng Helm ƒë·ªÉ deploy ·ªû c√°c b∆∞·ªõc t·ª´ 1-10 tr√™n ƒë√£ h∆∞·ªõng d·∫´n setup EFK b·∫±ng tay r·ªìi.\nGi·ªù n·∫øu mu·ªën d√πng Helm th√¨ c·∫ßn thay ƒë·ªïi 1 s·ªë ch·ªó. C·ª• th·ªÉ c√°c b∆∞·ªõc tr√™n th√¨ c√≥ b∆∞·ªõc 6: Generate ra password l√† lo·∫≥ng ngo·∫±ng nh·∫•t,\nmu·ªën chuy·ªÉn sang t·ª± ƒë·ªông b·∫±ng Helm th√¨ c·∫ßn s·ª≠a c√°c file:\nelasticsearch-client.yaml (ch·ªß y·∫øu file n√†y),\nfluentd.yaml,\nkibana.yaml\nSau khi s·ª≠a xong th√¨ ch·ªâ c·∫ßn l√†m nh∆∞ sau l√† deploy c·∫£ h·ªá th·ªëng EFK advanced l√™n trong 1 n·ªët nh·∫°c üòÜ\n# Ch√∫ √Ω l√† c·∫ßn c√†i ƒë·∫∑t helm v√† tiller ƒë√£ (c√≥ th·ªÉ tham kh·∫£o b√†i tr∆∞·ªõc K8S 5: Using Helm ...) cd kubernetes-series/efk-stack-advanced-helm helm install -n efk-stack . Ch·ªù kho·∫£ng 5ph√∫t, r·ªìi quay l·∫°i step 10 ƒë·ªÉ Test h·ªá th·ªëng, ch√∫ √Ω l√† ƒëƒÉng nh·∫≠p kibana b·∫±ng user my_admin\nDONE! üéâüéâüéâ\nREFERENCES:\nhttps://kauri.io/article/e5b86351f38940b8a071267062f052cb/v2/monitoring-kubernetes-with-elastic-stack-2.-elasticsearch-and-kibana\nhttps://github.com/gjeanmart/kauri-content/blob/master/spring-boot-simple/k8s\nhttps://blog.ptrk.io/tweaking-an-efk-stack-on-kubernetes/\nhttps://github.com/GoogleCloudPlatform/click-to-deploy/tree/master/k8s/elastic-gke-logging\n","href":"/bk/k8s-viii-setup-advanced-efk-stack-on-gcp/","title":"K8S 8: Setup Advanced EFK Stack on GCP cluster (ElasticSearch, Fluentd, Kibana)"},{"content":"Gi·ªõi thi·ªáu B√†i n√†y h∆∞·ªõng d·∫´n d·ª±ng EFK stack ph·ª©c t·∫°p h∆°n, d√πng ConfigMap,\nElasticSearch chia l√†m c√°c role \u0026ldquo;master, client, data\u0026rdquo;,\nc√≥ √°p d·ª•ng authentication cho Kibana ƒë·ªÉ \u0026ldquo;more secure\u0026rdquo;\nC√°ch l√†m ƒê·∫ßu ti√™n c·∫ßn checkout source code n√†y:\ngit clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series/efk-stack-advanced 1. T·∫°o cluster Tr√™n gcp th√¨ t·∫°o cluster b·∫±ng console, m√¨nh ƒë√£ ch·ªçn lo·∫°i N1 standard 2 (2vCPU,7.5GB memory), t·∫°o cluster ch·ª©a 2 node nh∆∞ v·∫≠y\n2. T·∫°o namespace kubectl create -f kube-logging.yaml 3. T·∫°o ElasticSearch master  The first node of the cluster we\u0026rsquo;re going to setup is the master which is responsible of controlling the cluster.\n kubectl create -f elasticsearch-master.yaml 4. T·∫°o ElasticSearch data  The second node of the cluster we\u0026rsquo;re going to setup is the data which is responsible of hosting the data and executing the queries (CRUD, search, aggregation).\n kubectl create -f elasticsearch-data.yaml 5. T·∫°o ElasticSearch client  The last but not least node of the cluster is the client which is responsible of exposing an HTTP interface and pass queries to the data node.\n kubectl create -f elasticsearch-client.yaml Wait for all pods in state READY 1/1\n$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default docker-postgres-cf447b874-p8gbq 1/1 Running 0 25m default docker-spring-boot-containers-7c694df96b-qznth 1/1 Running 0 25m kube-logging elasticsearch-client-784cbb477-s7mkq 1/1 Running 0 2m8s kube-logging elasticsearch-data-0 1/1 Running 0 2m18s kube-logging elasticsearch-master-56fd947c4c-8brls 1/1 Running 0 2m25s kube-system kube-dns-5f886bf8d8-5km8c 4/4 Running 0 28m kube-system kube-dns-5f886bf8d8-vknfr 4/4 Running 0 28m kube-system kube-dns-autoscaler-85f8bdb54-d946p 1/1 Running 0 28m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-3b51bccc-lzpf 1/1 Running 0 28m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-d645c580-blss 1/1 Running 0 28m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-e1ba0508-lk35 1/1 Running 0 28m kube-system l7-default-backend-8f479dd9-fx4wg 1/1 Running 0 28m kube-system metrics-server-v0.3.1-8d4c5db46-lqrmp 2/2 Running 0 28m kube-system tiller-deploy-9bf6fb76d-wj229 1/1 Running 0 26m Ch·ªù 1 v√†i ph√∫t ƒë·ªÉ c√°c pods c·ªßa elastic READY:\nkubectl logs -f -n kube-logging \\  $(kubectl get pods -n kube-logging | grep elasticsearch-master | sed -n 1p | awk \u0026#39;{print $1}\u0026#39;) \\  | grep \u0026#34;Cluster health status changed from \\[YELLOW\\] to \\[GREEN\\]\u0026#34; 6. Generate a password and store in a k8s secret  We enabled the xpack security module to secure our cluster, so we need to initialise the passwords. Execute the following command which runs the program bin/elasticsearch-setup-passwords within the client node container (any node would work) to generate default users and passwords.\n kubectl exec $(kubectl get pods -n kube-logging | grep elasticsearch-client | sed -n 1p | awk \u0026#39;{print $1}\u0026#39;) \\  -n kube-logging \\  -- bin/elasticsearch-setup-passwords auto -b Output\n$ kubectl exec $(kubectl get pods -n kube-logging | grep elasticsearch-client | sed -n 1p | awk '{print $1}') \\ -n kube-logging \\ -- bin/elasticsearch-setup-passwords auto -b Changed password for user apm_system PASSWORD apm_system = eo83yYSKxR49QTx5eNx3 Changed password for user kibana PASSWORD kibana = tdrr6dMWenSJiOX2eGDN Changed password for user logstash_system PASSWORD logstash_system = qf5dXCaYfLJflugUpGtB Changed password for user beats_system PASSWORD beats_system = PF75DeqUbWU5TusncL1l Changed password for user remote_monitoring_user PASSWORD remote_monitoring_user = 54Nmz6CSaejkcJwgijtP Changed password for user elastic PASSWORD elastic = S7kuf4HCinVdMp3Vtrkx Note the elastic user password and add it into a k8s secret like this:\nkubectl create secret generic elasticsearch-pw-elastic \\  -n kube-logging \\  --from-literal password=S7kuf4HCinVdMp3Vtrkx 7. T·∫°o Kibana kubectl create -f kibana.yaml 8. T·∫°o Fluentd kubectl create -f fluentd.yaml Wait for all pods in state READY 1/1\n$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default docker-postgres-cf447b874-p8gbq 1/1 Running 0 29m default docker-spring-boot-containers-7c694df96b-qznth 1/1 Running 0 29m kube-logging elasticsearch-client-784cbb477-s7mkq 1/1 Running 0 5m31s kube-logging elasticsearch-data-0 1/1 Running 0 5m41s kube-logging elasticsearch-master-56fd947c4c-8brls 1/1 Running 0 5m48s kube-logging fluentd-c48hz 1/1 Running 0 78s kube-logging fluentd-j9g7m 1/1 Running 0 78s kube-logging fluentd-xtk8k 1/1 Running 0 78s kube-logging kibana-5f8cb9b596-jvllj 1/1 Running 0 85s kube-system kube-dns-5f886bf8d8-5km8c 4/4 Running 0 32m kube-system kube-dns-5f886bf8d8-vknfr 4/4 Running 0 31m kube-system kube-dns-autoscaler-85f8bdb54-d946p 1/1 Running 0 31m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-3b51bccc-lzpf 1/1 Running 0 31m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-d645c580-blss 1/1 Running 0 31m kube-system kube-proxy-gke-efk-stack-tf7-default-pool-e1ba0508-lk35 1/1 Running 0 31m kube-system l7-default-backend-8f479dd9-fx4wg 1/1 Running 0 32m kube-system metrics-server-v0.3.1-8d4c5db46-lqrmp 2/2 Running 0 31m kube-system tiller-deploy-9bf6fb76d-wj229 1/1 Running 0 29m Check logs c·ªßa Fluentd Pod n·∫øu show ra t∆∞∆°ng t·ª± nh∆∞ sau, ko c√≥ error g√¨ l√† ok\nkubectl logs -n kube-logging \u0026lt;POD_NAME\u0026gt; -n kube-logging # kubectl logs -n kube-logging fluentd-c48hz-n kube-logging output:\n2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;fluent.**\u0026quot; type=\u0026quot;null\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding filter pattern=\u0026quot;kubernetes.**\u0026quot; type=\u0026quot;kubernetes_metadata\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;kubernetes.var.log.containers.**kube-logging**.log\u0026quot; type=\u0026quot;null\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;kubernetes.var.log.containers.**kube-system**.log\u0026quot; type=\u0026quot;null\u0026quot; 2019-11-25 03:16:39 +0000 [info]: adding match pattern=\u0026quot;kubernetes.**\u0026quot; type=\u0026quot;elasticsearch\u0026quot; 2019-11-25 03:16:40 +0000 [warn]: #0 Detected ES 7.x or above: `_doc` will be used as the document `_type`. 2019-11-25 03:16:40 +0000 [info]: adding source type=\u0026quot;tail\u0026quot; 2019-11-25 03:16:40 +0000 [info]: #0 starting fluentd worker pid=11 ppid=6 worker=0 9. T·∫°o 1 app li√™n t·ª•c out ra log ƒë·ªÉ test kubectl create -f counter.yaml 10. Test Make sure all pods are in Running state and READY 1/1, 2/2, 3/3, 4/4 \u0026hellip;.\nkubectl get pods --all-namespaces get NodePort of service Kibana\nkubectl get svc --all-namespaces output\nNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.116.0.1 \u0026lt;none\u0026gt; 443/TCP 10m kube-logging service/elasticsearch-client ClusterIP 10.116.9.216 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 6m34s kube-logging service/elasticsearch-data ClusterIP 10.116.14.205 \u0026lt;none\u0026gt; 9300/TCP 6m46s kube-logging service/elasticsearch-master ClusterIP 10.116.10.136 \u0026lt;none\u0026gt; 9300/TCP 6m56s kube-logging service/kibana NodePort 10.116.1.237 \u0026lt;none\u0026gt; 5601:32141/TCP 2m15s kube-system service/default-http-backend NodePort 10.116.3.142 \u0026lt;none\u0026gt; 80:32636/TCP 10m kube-system service/heapster ClusterIP 10.116.10.12 \u0026lt;none\u0026gt; 80/TCP 10m kube-system service/kube-dns ClusterIP 10.116.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 10m kube-system service/metrics-server ClusterIP 10.116.12.232 \u0026lt;none\u0026gt; 443/TCP 10m Ta th·∫•y NodePort c·ªßa Kibana l√† 32141\nget external ip of nodes\nkubectl get nodes --output wide output\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gke-efk-stack-advanced-default-pool-eeaed3c6-kx6f Ready \u0026lt;none\u0026gt; 9m44s v1.13.11-gke.14 10.128.0.11 34.67.152.105 Container-Optimized OS from Google 4.14.138+ docker://18.9.7 gke-efk-stack-advanced-default-pool-eeaed3c6-rj79 Ready \u0026lt;none\u0026gt; 9m44s v1.13.11-gke.14 10.128.0.12 34.66.70.31 Container-Optimized OS from Google 4.14.138+ docker://18.9.7 Ta c√≥ th·ªÉ ch·ªçn 1 Node IP ƒë·ªÉ truy c·∫≠p, v√≠ d·ª• tr√™n th√¨\n\u0026lt;EXTERNAL-IP\u0026gt; = 34.67.152.105\n\u0026lt;NODOE_PORT\u0026gt; = 32141\nCreate a firewall rule to allow TCP traffic on your node port:\ngcloud compute firewall-rules create kibana-node-port --allow tcp:\u0026lt;NODOE_PORT\u0026gt; N·∫øu ƒë√£ c√≥ s·∫µn th√¨ c·∫ßn update ch·ª© ko th·ªÉ t·∫°o m·ªõi\ngcloud compute firewall-rules update kibana-node-port --allow tcp:\u0026lt;NODOE_PORT\u0026gt; Ho·∫∑c c√°c b·∫°n n√™n m·ªü all NodePort t·ª´ 30000-32767 b·∫±ng command sau:\ngcloud compute firewall-rules update kibana-node-port --allow tcp:30000-32767 r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODOE_PORT\u0026gt; ƒë·ªÉ check\nLogin b·∫±ng user elastic v√† passw ƒë√£ generate ·ªü step tr√™n\nT·∫°o index-pattern : logstash-*\nV√†o Discover, check logs N·∫øu ch·ªâ show log c·ªßa namespace default l√† Done!\nN·∫øu mu·ªën s·ª≠a ConfigMap c·ªßa Fluentd th√¨ c·∫ßn ch√∫ √Ω l√† sau khi s·ª≠a xong th√¨ sau khi apply template m·ªõi,\nb·∫°n ph·∫£i delete c√°c pods fluentd c≈© ƒë·ªÉ n√≥ apply c√°c config v·ª´a thay ƒë·ªïi\n11. D√πng Helm ƒë·ªÉ deploy ·ªû c√°c b∆∞·ªõc t·ª´ 1-10 tr√™n ƒë√£ h∆∞·ªõng d·∫´n setup EFK b·∫±ng tay r·ªìi.\nGi·ªù n·∫øu mu·ªën d√πng Helm th√¨ c·∫ßn thay ƒë·ªïi 1 s·ªë ch·ªó. C·ª• th·ªÉ c√°c b∆∞·ªõc tr√™n th√¨ c√≥ b∆∞·ªõc 6: Generate ra password l√† lo·∫≥ng ngo·∫±ng nh·∫•t,\nmu·ªën chuy·ªÉn sang t·ª± ƒë·ªông b·∫±ng Helm th√¨ c·∫ßn s·ª≠a c√°c file:\nelasticsearch-client.yaml (ch·ªß y·∫øu file n√†y),\nfluentd.yaml,\nkibana.yaml\nSau khi s·ª≠a xong th√¨ ch·ªâ c·∫ßn l√†m nh∆∞ sau l√† deploy c·∫£ h·ªá th·ªëng EFK advanced l√™n trong 1 n·ªët nh·∫°c üòÜ\n# Ch√∫ √Ω l√† c·∫ßn c√†i ƒë·∫∑t helm v√† tiller ƒë√£ (c√≥ th·ªÉ tham kh·∫£o b√†i tr∆∞·ªõc K8S 5: Using Helm ...) cd kubernetes-series/efk-stack-advanced-helm helm install -n efk-stack . Ch·ªù kho·∫£ng 5ph√∫t, r·ªìi quay l·∫°i step 10 ƒë·ªÉ Test h·ªá th·ªëng, ch√∫ √Ω l√† ƒëƒÉng nh·∫≠p kibana b·∫±ng user my_admin\nDONE! üéâüéâüéâ\nREFERENCES:\nhttps://kauri.io/article/e5b86351f38940b8a071267062f052cb/v2/monitoring-kubernetes-with-elastic-stack-2.-elasticsearch-and-kibana\nhttps://github.com/gjeanmart/kauri-content/blob/master/spring-boot-simple/k8s\nhttps://blog.ptrk.io/tweaking-an-efk-stack-on-kubernetes/\nhttps://github.com/GoogleCloudPlatform/click-to-deploy/tree/master/k8s/elastic-gke-logging\n","href":"/posts/k8s-viii-setup-advanced-efk-stack-on-gcp/","title":"K8S 8: Setup Advanced EFK Stack on GCP cluster (ElasticSearch, Fluentd, Kibana)"},{"content":"","href":"/tags/kibana/","title":"Kibana"},{"content":"","href":"/tags/eks/","title":"EKS"},{"content":"Y√™u c·∫ßu ƒê√£ c√†i ƒë·∫∑t eksctl, kubectl\nC√°ch l√†m git clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series/efk-stack 1. T·∫°o cluster 1.1. eks tr√™n eks ph·∫£i t·∫°o cluster b·∫±ng file n√†y cluster-efk.yaml\nc√≥ th·ªÉ ƒë·ªïi t√™n cluster, t·∫°o th√™m node, ƒë·ªïi type c·ªßa node trong file ƒë√≥, nh∆∞ng n√™n ch·ªçn 1 node c√≥ 4vCPU, \u0026gt;8GB (t2.xlarge), ƒë√£ test tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng t2.large (2 vCPU, 8 GB Mem) c≈©ng ok\neksctl create cluster -f cluster-efk.yaml 1.2. gcp tr√™n gcp th√¨ t·∫°o cluster b·∫±ng console, m√¨nh ƒë√£ ch·ªçn lo·∫°i N1 standard 4 (4vCPU,15GB memory), ch∆∞a test lo·∫°i kh√°c\n2. T·∫°o namespace kubectl create -f kube-logging.yaml 3. T·∫°o service ElasticSearch kubectl create -f elasticsearch_svc.yaml 4. T·∫°o statefulset ElasticSearch c√≥ v√†i ch·ªó n√™n s·ª≠a tr∆∞·ªõc khi t·∫°o\nc·ª• th·ªÉ l√† xem file n√†y elasticsearch_statefulset.yaml\nnano elasticsearch_statefulset.yaml ‚ñ† s·ª≠a t√™n c·ªßa cluster ƒëang d√πng, n·∫øu b·∫°n ƒëang d√πng cluster name kh√°c th√¨ n√™n s·ª≠a gi√° tr·ªã efk-stack\nenv: - name: cluster.name value: efk-stack ‚ñ† s·ª≠a namespace c·ªßa volume m√† m√¨nh s·∫Ω s·ª≠ d·ª•ng, ·ªü ƒë√¢y m√¨nh d√πng namespace \u0026ldquo;default\u0026rdquo;\n‚ñ† s·ª≠a storageClass c·ªßa volume m√¨nh s·∫Ω s·ª≠ d·ª•ng, ·ªü ƒë√¢y m√¨nh d√πng \u0026ldquo;gp2\u0026rdquo;:\n n·∫øu ƒëang d√πng cluster tr√™n eks th√¨ volume ·ªü namespace \u0026ldquo;default\u0026rdquo; l√† \u0026ldquo;gp2\u0026rdquo; n·∫øu ƒëang d√πng cluster tr√™n gcp th√¨ volume ·ªü namespace \u0026ldquo;default\u0026rdquo; l√† \u0026ldquo;standard\u0026rdquo; n·∫øu ƒëang d√πng cluster tr√™n DigitalOcean th√¨ volume ·ªü namespace \u0026ldquo;default\u0026rdquo; l√† \u0026ldquo;do-block-storage\u0026rdquo;  ‚ñ† s·ª≠a dung l∆∞·ª£ng c·ªßa storage n·∫øu ƒëang mu·ªën test th√¨ d√πng 5-10GB th√¥i cho ti·∫øt ki·ªám, ·ªü ƒë√¢y m√¨nh d√πng \u0026ldquo;10Gi\u0026rdquo;\nvolumeClaimTemplates: - metadata: name: data namespace: default labels: app: elasticsearch spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] storageClassName: gp2 resources: requests: storage: 10Gi Ngo√†i ra c√≤n c√°c th√¥ng s·ªë kh√°c, xem k·ªπ ƒë·ªÉ n·∫øu mu·ªën thay ƒë·ªïi\ns·ª≠a xong h·∫øt th√¨ m·ªõi t·∫°o statefulset b·∫±ng command sau:\nkubectl create -f elasticsearch_statefulset.yaml d√πng command sau ƒë·ªÉ ki·ªÉm tra pod ƒë√£ t·∫°o h·∫øt ch∆∞a\nkubectl describe pod \u0026lt;POD_NAME\u0026gt; N·∫øu b·ªã l·ªói pod has unbound immediate PersistentVolumeClaims th√¨ c·∫ßn ph·∫£i fix, th·ª≠ nh·ªØng command sau:\nkubectl get storageclass --all-namespaces kubectl get pvc --all-namespaces N·∫øu c√≥ c√°i n√†o c·ª© ·ªü tr·∫°ng th√°i pending m√£i th√¨ n√™n x√≥a n√≥ ƒëi,\nkubectl delete pvc \u0026lt;PVC_NAME\u0026gt; -n \u0026lt;NAME_SPACE\u0026gt; r·ªìi ch·∫°y l·∫°i c√°c command\nkubectl delete -f elasticsearch_statefulset.yaml kubectl create -f elasticsearch_statefulset.yaml N·∫øu t·∫•t c·∫£ pod elasticsearch ƒë·ªÅu l√™n Running nghƒ©a l√† ok\n[ec2-user@ip-172-31-84-250 efk-stack]$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-logging es-cluster-0 1/1 Running 0 2m6s kube-logging es-cluster-1 1/1 Running 0 85s kube-logging es-cluster-2 1/1 Running 0 43s kube-system aws-node-j28lc 1/1 Running 0 14m kube-system aws-node-n87zk 1/1 Running 0 14m kube-system coredns-8455f84f99-2cjf8 1/1 Running 0 20m kube-system coredns-8455f84f99-p7fv9 1/1 Running 0 20m kube-system kube-proxy-dm77l 1/1 Running 0 14m kube-system kube-proxy-nfqgn 1/1 Running 0 14m 4.1. (Optional) ki·ªÉm tra ElasticSearch b·∫±ng port-forward 4.1.a. Tr√™n eks kubectl port-forward es-cluster-0 9200:9200 --namespace=kube-logging --address 0.0.0.0 r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:9200 ƒë·ªÉ check\nCh√∫ √Ω M·ªü Security Group cho port 9200\nN·∫øu tr·∫£ v·ªÅ 1 chu·ªói JSON c√≥ ch·ª©a \u0026ldquo;You know, For Search\u0026rdquo; nghƒ©a l√† ok\n4.1.b. Tr√™n gcp mu·ªën check ph·∫£i edit service th√†nh NodePort r·ªìi m·ªõi port-forward ƒë∆∞·ª£c\nkubectl edit service \u0026lt;ELASTIC_SVC\u0026gt; -n kube-logging mu·ªën check NodePort ƒëang s·ª≠ d·ª•ng l√† port bao nhi√™u th√¨:\nkubectl describe svc \u0026lt;ELASTIC_SVC\u0026gt; -n kube-logging edit xong th√¨ port-forward:\nkubectl port-forward -n kube-logging service/\u0026lt;ELASTICSEARCH_SVC\u0026gt; \u0026lt;NODEPORT\u0026gt;:9200 --address 0.0.0.0 L·∫•y external IP c·ªßa Node\nkubectl get nodes --output wide r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODOE_PORT\u0026gt; ƒë·ªÉ check\nCh√∫ √Ω M·ªü Firewall cho \u0026lt;NODE_PORT\u0026gt;:\ngcloud compute firewall-rules create elasticsearch-node-port --allow tcp:\u0026lt;NODE_PORT\u0026gt; # If rules exist you need to using `update` instead of `create` N·∫øu tr·∫£ v·ªÅ 1 chu·ªói JSON c√≥ ch·ª©a \u0026ldquo;You know, For Search\u0026rdquo; nghƒ©a l√† ok\n5. T·∫°o kibana kubectl create -f kibana.yaml 5.1. ki·ªÉm tra Kibana b·∫±ng port-forward 5.1.a. Tr√™n eks kubectl port-forward \u0026lt;KIBANA_POD_NAME\u0026gt; 5601:5601 --namespace=kube-logging --address 0.0.0.0 r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:5601 ƒë·ªÉ check\nCh√∫ √Ω M·ªü Security Group cho port 5601\n5.1.b. Tr√™n gcp mu·ªën check ph·∫£i edit service th√†nh NodePort r·ªìi m·ªõi port-forward ƒë∆∞·ª£c\nkubectl edit service \u0026lt;KIBANA_SVC\u0026gt; -n kube-logging mu·ªën check NodePort ƒëang s·ª≠ d·ª•ng l√† port bao nhi√™u th√¨:\nkubectl describe svc \u0026lt;KIBANA_SVC\u0026gt; -n kube-logging edit xong th√¨ port-forward:\nkubectl port-forward -n kube-logging service/\u0026lt;KIBANA_SVC\u0026gt; \u0026lt;NODEPORT\u0026gt;:5601 --address 0.0.0.0 L·∫•y external IP c·ªßa Node\nkubectl get nodes --output wide r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODOE_PORT\u0026gt; ƒë·ªÉ check\nCh√∫ √Ω M·ªü Firewall cho port NodePort c·ªßa Kibana service:\ngcloud compute firewall-rules create kibana-node-port --allow tcp:\u0026lt;NODE_PORT\u0026gt; # If rules exist you need to using `update` instead of `create` 6. T·∫°o DaemonSet fluentd kubectl create -f fluentd.yaml Check xem tr·∫°ng th√°i c√°c pods:\nkubectl get pods -A N·∫øu t·∫•t c·∫£ pod ƒë·ªÅu l√™n Running nghƒ©a l√† ok\n7. Check h·ªá th·ªëng EFK ho·∫°t ƒë·ªông ra sao Gi·ªù c√≥ th·ªÉ port-forward ƒë·ªÉ v√†o l·∫°i link kibana xem log http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:5601\nkubectl port-forward \u0026lt;KIBANA_POD_NAME\u0026gt; 5601:5601 --namespace=kube-logging --address 0.0.0.0 select Discover Enter¬†logstash-*¬†in the text box and click on¬†Next step. select @timestamp back to Discover t·∫°o app Counter ƒë·ªÉ test vi·ªác xu·∫•t logs\nkubectl create -f counter.yaml N·∫øu mu·ªën filter log theo pod name t√™n l√† \u0026ldquo;counter\u0026rdquo;, th√¨ trong tab Discover, ƒëi·ªÅn v√†o khung search kubernetes.pod_name:counter\nREFERENCES:\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes https://jmartinezxp.gitlab.io/post/config-kubernetes-efk/ https://chris-vermeulen.com/how-to-monitor-distributed-logs-in-kubernetes-with-the-efk-stack-/ https://github.com/GoogleCloudPlatform/click-to-deploy/tree/master/k8s/elastic-gke-logging https://github.com/mjhea0/efk-kubernetes/blob/master/kubernetes/elastic.yaml https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/ https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch#storage\n","href":"/bk/k8s-vii-setup-efk-stack-on-eks-gcp/","title":"K8S 7: Setup EFK Stack on EKS/GCP cluster (ElasticSearch, Fluentd, Kibana)"},{"content":"Y√™u c·∫ßu ƒê√£ c√†i ƒë·∫∑t eksctl, kubectl\nC√°ch l√†m git clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series/efk-stack 1. T·∫°o cluster 1.1. eks tr√™n eks ph·∫£i t·∫°o cluster b·∫±ng file n√†y cluster-efk.yaml\nc√≥ th·ªÉ ƒë·ªïi t√™n cluster, t·∫°o th√™m node, ƒë·ªïi type c·ªßa node trong file ƒë√≥, nh∆∞ng n√™n ch·ªçn 1 node c√≥ 4vCPU, \u0026gt;8GB (t2.xlarge), ƒë√£ test tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng t2.large (2 vCPU, 8 GB Mem) c≈©ng ok\neksctl create cluster -f cluster-efk.yaml 1.2. gcp tr√™n gcp th√¨ t·∫°o cluster b·∫±ng console, m√¨nh ƒë√£ ch·ªçn lo·∫°i N1 standard 4 (4vCPU,15GB memory), ch∆∞a test lo·∫°i kh√°c\n2. T·∫°o namespace kubectl create -f kube-logging.yaml 3. T·∫°o service ElasticSearch kubectl create -f elasticsearch_svc.yaml 4. T·∫°o statefulset ElasticSearch c√≥ v√†i ch·ªó n√™n s·ª≠a tr∆∞·ªõc khi t·∫°o\nc·ª• th·ªÉ l√† xem file n√†y elasticsearch_statefulset.yaml\nnano elasticsearch_statefulset.yaml ‚ñ† s·ª≠a t√™n c·ªßa cluster ƒëang d√πng, n·∫øu b·∫°n ƒëang d√πng cluster name kh√°c th√¨ n√™n s·ª≠a gi√° tr·ªã efk-stack\nenv: - name: cluster.name value: efk-stack ‚ñ† s·ª≠a namespace c·ªßa volume m√† m√¨nh s·∫Ω s·ª≠ d·ª•ng, ·ªü ƒë√¢y m√¨nh d√πng namespace \u0026ldquo;default\u0026rdquo;\n‚ñ† s·ª≠a storageClass c·ªßa volume m√¨nh s·∫Ω s·ª≠ d·ª•ng, ·ªü ƒë√¢y m√¨nh d√πng \u0026ldquo;gp2\u0026rdquo;:\n n·∫øu ƒëang d√πng cluster tr√™n eks th√¨ volume ·ªü namespace \u0026ldquo;default\u0026rdquo; l√† \u0026ldquo;gp2\u0026rdquo; n·∫øu ƒëang d√πng cluster tr√™n gcp th√¨ volume ·ªü namespace \u0026ldquo;default\u0026rdquo; l√† \u0026ldquo;standard\u0026rdquo; n·∫øu ƒëang d√πng cluster tr√™n DigitalOcean th√¨ volume ·ªü namespace \u0026ldquo;default\u0026rdquo; l√† \u0026ldquo;do-block-storage\u0026rdquo;  ‚ñ† s·ª≠a dung l∆∞·ª£ng c·ªßa storage n·∫øu ƒëang mu·ªën test th√¨ d√πng 5-10GB th√¥i cho ti·∫øt ki·ªám, ·ªü ƒë√¢y m√¨nh d√πng \u0026ldquo;10Gi\u0026rdquo;\nvolumeClaimTemplates: - metadata: name: data namespace: default labels: app: elasticsearch spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] storageClassName: gp2 resources: requests: storage: 10Gi Ngo√†i ra c√≤n c√°c th√¥ng s·ªë kh√°c, xem k·ªπ ƒë·ªÉ n·∫øu mu·ªën thay ƒë·ªïi\ns·ª≠a xong h·∫øt th√¨ m·ªõi t·∫°o statefulset b·∫±ng command sau:\nkubectl create -f elasticsearch_statefulset.yaml d√πng command sau ƒë·ªÉ ki·ªÉm tra pod ƒë√£ t·∫°o h·∫øt ch∆∞a\nkubectl describe pod \u0026lt;POD_NAME\u0026gt; N·∫øu b·ªã l·ªói pod has unbound immediate PersistentVolumeClaims th√¨ c·∫ßn ph·∫£i fix, th·ª≠ nh·ªØng command sau:\nkubectl get storageclass --all-namespaces kubectl get pvc --all-namespaces N·∫øu c√≥ c√°i n√†o c·ª© ·ªü tr·∫°ng th√°i pending m√£i th√¨ n√™n x√≥a n√≥ ƒëi,\nkubectl delete pvc \u0026lt;PVC_NAME\u0026gt; -n \u0026lt;NAME_SPACE\u0026gt; r·ªìi ch·∫°y l·∫°i c√°c command\nkubectl delete -f elasticsearch_statefulset.yaml kubectl create -f elasticsearch_statefulset.yaml N·∫øu t·∫•t c·∫£ pod elasticsearch ƒë·ªÅu l√™n Running nghƒ©a l√† ok\n[ec2-user@ip-172-31-84-250 efk-stack]$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-logging es-cluster-0 1/1 Running 0 2m6s kube-logging es-cluster-1 1/1 Running 0 85s kube-logging es-cluster-2 1/1 Running 0 43s kube-system aws-node-j28lc 1/1 Running 0 14m kube-system aws-node-n87zk 1/1 Running 0 14m kube-system coredns-8455f84f99-2cjf8 1/1 Running 0 20m kube-system coredns-8455f84f99-p7fv9 1/1 Running 0 20m kube-system kube-proxy-dm77l 1/1 Running 0 14m kube-system kube-proxy-nfqgn 1/1 Running 0 14m 4.1. (Optional) ki·ªÉm tra ElasticSearch b·∫±ng port-forward 4.1.a. Tr√™n eks kubectl port-forward es-cluster-0 9200:9200 --namespace=kube-logging --address 0.0.0.0 r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:9200 ƒë·ªÉ check\nCh√∫ √Ω M·ªü Security Group cho port 9200\nN·∫øu tr·∫£ v·ªÅ 1 chu·ªói JSON c√≥ ch·ª©a \u0026ldquo;You know, For Search\u0026rdquo; nghƒ©a l√† ok\n4.1.b. Tr√™n gcp mu·ªën check ph·∫£i edit service th√†nh NodePort r·ªìi m·ªõi port-forward ƒë∆∞·ª£c\nkubectl edit service \u0026lt;ELASTIC_SVC\u0026gt; -n kube-logging mu·ªën check NodePort ƒëang s·ª≠ d·ª•ng l√† port bao nhi√™u th√¨:\nkubectl describe svc \u0026lt;ELASTIC_SVC\u0026gt; -n kube-logging edit xong th√¨ port-forward:\nkubectl port-forward -n kube-logging service/\u0026lt;ELASTICSEARCH_SVC\u0026gt; \u0026lt;NODEPORT\u0026gt;:9200 --address 0.0.0.0 L·∫•y external IP c·ªßa Node\nkubectl get nodes --output wide r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODOE_PORT\u0026gt; ƒë·ªÉ check\nCh√∫ √Ω M·ªü Firewall cho \u0026lt;NODE_PORT\u0026gt;:\ngcloud compute firewall-rules create elasticsearch-node-port --allow tcp:\u0026lt;NODE_PORT\u0026gt; # If rules exist you need to using `update` instead of `create` N·∫øu tr·∫£ v·ªÅ 1 chu·ªói JSON c√≥ ch·ª©a \u0026ldquo;You know, For Search\u0026rdquo; nghƒ©a l√† ok\n5. T·∫°o kibana kubectl create -f kibana.yaml 5.1. ki·ªÉm tra Kibana b·∫±ng port-forward 5.1.a. Tr√™n eks kubectl port-forward \u0026lt;KIBANA_POD_NAME\u0026gt; 5601:5601 --namespace=kube-logging --address 0.0.0.0 r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:5601 ƒë·ªÉ check\nCh√∫ √Ω M·ªü Security Group cho port 5601\n5.1.b. Tr√™n gcp mu·ªën check ph·∫£i edit service th√†nh NodePort r·ªìi m·ªõi port-forward ƒë∆∞·ª£c\nkubectl edit service \u0026lt;KIBANA_SVC\u0026gt; -n kube-logging mu·ªën check NodePort ƒëang s·ª≠ d·ª•ng l√† port bao nhi√™u th√¨:\nkubectl describe svc \u0026lt;KIBANA_SVC\u0026gt; -n kube-logging edit xong th√¨ port-forward:\nkubectl port-forward -n kube-logging service/\u0026lt;KIBANA_SVC\u0026gt; \u0026lt;NODEPORT\u0026gt;:5601 --address 0.0.0.0 L·∫•y external IP c·ªßa Node\nkubectl get nodes --output wide r·ªìi tr√™n tr√¨nh duy·ªát v√†o http://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODOE_PORT\u0026gt; ƒë·ªÉ check\nCh√∫ √Ω M·ªü Firewall cho port NodePort c·ªßa Kibana service:\ngcloud compute firewall-rules create kibana-node-port --allow tcp:\u0026lt;NODE_PORT\u0026gt; # If rules exist you need to using `update` instead of `create` 6. T·∫°o DaemonSet fluentd kubectl create -f fluentd.yaml Check xem tr·∫°ng th√°i c√°c pods:\nkubectl get pods -A N·∫øu t·∫•t c·∫£ pod ƒë·ªÅu l√™n Running nghƒ©a l√† ok\n7. Check h·ªá th·ªëng EFK ho·∫°t ƒë·ªông ra sao Gi·ªù c√≥ th·ªÉ port-forward ƒë·ªÉ v√†o l·∫°i link kibana xem log http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:5601\nkubectl port-forward \u0026lt;KIBANA_POD_NAME\u0026gt; 5601:5601 --namespace=kube-logging --address 0.0.0.0 select Discover Enter¬†logstash-*¬†in the text box and click on¬†Next step. select @timestamp back to Discover t·∫°o app Counter ƒë·ªÉ test vi·ªác xu·∫•t logs\nkubectl create -f counter.yaml N·∫øu mu·ªën filter log theo pod name t√™n l√† \u0026ldquo;counter\u0026rdquo;, th√¨ trong tab Discover, ƒëi·ªÅn v√†o khung search kubernetes.pod_name:counter\nREFERENCES:\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes https://jmartinezxp.gitlab.io/post/config-kubernetes-efk/ https://chris-vermeulen.com/how-to-monitor-distributed-logs-in-kubernetes-with-the-efk-stack-/ https://github.com/GoogleCloudPlatform/click-to-deploy/tree/master/k8s/elastic-gke-logging https://github.com/mjhea0/efk-kubernetes/blob/master/kubernetes/elastic.yaml https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/ https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch#storage\n","href":"/posts/k8s-vii-setup-efk-stack-on-eks-gcp/","title":"K8S 7: Setup EFK Stack on EKS/GCP cluster (ElasticSearch, Fluentd, Kibana)"},{"content":" If you delete the database pod all data is lost. We\u0026rsquo;ll fix this by using a database that lives externally to our cluster.\n Y√™u c·∫ßu Workplace: Amazon EC2 Linux\nƒê√£ t·∫°o m√¥i tr∆∞·ªùng, cluster c·ªßa ri√™ng b·∫°n, c√≥ th·ªÉ d√πng eksctl t·∫°o t·ª´ file cluster.yaml sau\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: base-project region: us-east-1 availabilityZones: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1d\u0026quot;] nodeGroups: - name: nodegrp-1 instanceType: t2.medium desiredCapacity: 1 ssh: # import public key from file publicKeyPath: /home/ec2-user/.ssh/id_rsa.pub Ch√∫ √Ω:\nS·ª≠ d·ª•ng ssh-keygen ƒë·ªÉ generate ra b·ªô key (id_rsa.pub, id_rsa) d√πng ƒë·ªÉ SSH v√†o Node\nT·∫°o cluster eksctl create cluster -f cluster.yaml Clone project cd ~ git clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series ll B√†i n√†y s·∫Ω d·ª±a tr√™n 2 folder spring-maven-postgres-docker-k8s-helm v√† spring-maven-postgres-docker-k8s\nCu·ªëi c√πng s·∫Ω l∆∞u l·∫°i s·∫£n ph·∫©m trong folder spring-maven-postgres-docker-k8s-helm-externaldb\nC√°ch l√†m Kh√°i qu√°t  tr∆∞·ªõc ti√™n m·ª•c ti√™u m√¨nh s·∫Ω ƒë·ªÉ PostgreSQL DB ch·∫°y tr√™n con workplace EC2 b·∫±ng docker. R·ªìi tr√™n cluster s·∫Ω ch·ªâ ch·∫°y app Spring boot th√¥i, v√† app ƒë√≥ s·∫Ω ch·ªçc v√†o PostgreSQL DB m√¨nh ƒë√£ t·∫°o\n C·ª• th·ªÉ Chu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng\nexport AWS_DEFAULT_REGION=us-east-1 export DOCKER_USERNAME=AAAAAA export DOCKER_PASSWORD=BBBBBB export DOCKER_USER_ID=CCCCCC docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Gi·ªù s·∫Ω ch·∫°y con postgreSQL DB\nTrong tr∆∞·ªùng h·ª£p ch∆∞a c√≥ image th√¨ c·∫ßn build docker image tr∆∞·ªõc:\ncd spring-maven-postgres-docker-k8s/docker/postgres docker build -f Dockerfile -t $DOCKER_USERNAME/docker_postgres . V√¨ c√°i image Postgres c·∫ßn truy·ªÅn v√†o c√°c bi·∫øn m√¥i tr∆∞·ªùng env n√™n m√¨nh ch·∫°y l·ªánh sau:\ndocker run --env POSTGRES_USER=dbuser --env POSTGRES_PASSWORD=password --env POSTGRES_DB=store -d -p 5432:5432 hoangmnsd/docker_postgres Sau khi ch·∫°y xong mu·ªën check vi·ªác t·∫°o db ƒë√£ ok ch∆∞a th√¨ ssh v√†o container ƒë√≥\ndocker ps docker exec -it \u0026lt;CONTAINER_NAME\u0026gt; bash su postgres psql -p 5432 store -U dbuser # show all database \\list # show all table \\d N·∫øu th·∫•y database \u0026ldquo;store\u0026rdquo; v√† table \u0026ldquo;product\u0026rdquo; c√≥ nghƒ©a l√† ƒë√£ run db th√†nh c√¥ng\nroot@7b00518cc6c3:/# su postgres postgres@7b00518cc6c3:/$ psql -p 5432 store -U dbuser psql (12.0 (Debian 12.0-2.pgdg100+1)) Type \u0026quot;help\u0026quot; for help. store=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- postgres | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | store | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | =Tc/dbuser + | | | | | dbuser=CTc/dbuser template0 | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | =c/dbuser + | | | | | dbuser=CTc/dbuser template1 | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | =c/dbuser + | | | | | dbuser=CTc/dbuser (4 rows) store=# \\d List of relations Schema | Name | Type | Owner --------+--------------------+----------+-------- public | hibernate_sequence | sequence | dbuser public | product | table | dbuser (2 rows) Gi·ªù n√™n test connection gi·ªØa Node v√† Host (Workplace EC2) xem ƒë√£ connect dc t·ªõi DB hay ch∆∞a\nSSH v√†o Node\nssh -i ~/.ssh/id_rsa ec2-user@\u0026lt;EC2-NODE-PUBLIC-IP\u0026gt; B·ªüi v√¨ Node l√† Amazon linux 2, n√™n c·∫ßn install psql client b·∫±ng command sau:\nsudo amazon-linux-extras install postgresql10 -y Sau ƒë√≥ d√πng psql ƒë·ªÉ connect v√†o DB ƒëang ch·∫°y tr√™n Workplace xem ƒë∆∞·ª£c ko\n# psql -h \u0026lt;EC2-PUBLIC-IP\u0026gt; -p 5432 \u0026lt;DB_NAME\u0026gt; \u0026lt;DB_USER\u0026gt; psql -h 34.238.123.20 -p 5432 store dbuser nh·∫≠p password la` \u0026ldquo;password\u0026rdquo; (c√°i n√†y m√¨nh define trong bi·∫øn m√¥i tr∆∞·ªùng khi truy·ªÅn v√†o v√† run docker image postgres)\nN·∫øu connect th√†nh c√¥ng th·ª≠ list db v√† list table xem\n\\list \\d N·∫øu th·∫•y database \u0026ldquo;store\u0026rdquo; v√† table \u0026ldquo;product\u0026rdquo; c√≥ nghƒ©a l√† ƒë√£ connect db th√†nh c√¥ng\nN·∫øu ko th·∫•y nghƒ©a l√† connect ko th√†nh c√¥ng th√¨ n√™n check Security Group xem Workplace ƒë√£ m·ªü port 5432 cho con Node connect v√†o ch∆∞a\nGi·ªù c·∫ßn s·ª≠a project Spring v√† Helm chart ƒë·ªÉ khi deploy l√™n cluster n√≥ c√≥ th·ªÉ connect v√†o DB ƒëang ·ªü 1 m√°y EC2 kh√°c (ch√≠nh l√† m√°y workplace c·ªßa m√¨nh)\nnano spring-maven-postgres-docker-k8s/src/main/resources/application.yml s·ª≠a URL c·ªßa PostgreSQL DB nh∆∞ sau:\n#below is config for Docker compose #ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for postgresql in local windows #ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for k8s, using service_name:5432 to connect db #ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store #below is config for k8s, using a external db running in another machine ENV_DATASOURCE_URL: jdbc:postgresql://34.238.123.20:5432/store #ƒë√¢y l√† EC2-PUBLIC-IP c·ªßa con m√°y m√† m√¨nh ƒëang run Postgres DB sau ƒë√≥ build l√† file jar v√† ƒë√≥ng docker image m·ªõi\ncd spring-maven-postgres-docker-k8s mvn clean package docker build -f Dockerfile -t $DOCKER_USERNAME/docker_spring-boot-containers . c·∫ßn push l√™n Docker Hub ƒë·ªÉ sau n√†y helm chart pull v·ªÅ\ndocker push $DOCKER_USERNAME/docker_spring-boot-containers Ti·∫øp l√† s·ª≠a helm chart, ƒë·∫ßu ti√™n s·ª≠a file Service c·ªßa Postgres\nnano spring-maven-postgres-docker-k8s-helm/templates/docker_postgres-service-external.yaml apiVersion: v1 kind: Service metadata: name: docker-postgres labels: app: docker-postgres spec: type: ExternalName externalName: {{ .Values.postgresService.externalName }} selector: app: docker-postgres b·ªüi v√¨ ko t·∫°o pod cho Postgresql m√† s·∫Ω d√πng b√™n ngo√†i n√™n c·∫ßn x√≥a file docker_postgres-deployment.yaml, b·∫°n s·∫Ω th·∫•y file ƒë√≥ ko c√≤n trong folder spring-maven-postgres-docker-k8s-helm-externaldb n·ªØa\nc·∫ßn s·ª≠a file values.yaml n·ªØa\npostgresService: type: ClusterIP port: 5432 targetPort: 5432 externalName: 34.238.123.20 #ƒë√¢y l√† IP c·ªßa con m√°y m√† m√¨nh ƒëang run Postgres DB v·∫≠y l√† chu·∫©n b·ªã xong gi·ªù c√≥ th·ªÉ install helm chart\ncd spring-maven-postgres-docker-k8s-helm helm install -n spring-postgres . $ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/docker-spring-boot-containers-54566ddbc8-8dpkf 1/1 Running 0 39s kube-system pod/aws-node-jpx87 1/1 Running 0 27m kube-system pod/coredns-8455f84f99-7wzm8 1/1 Running 0 32m kube-system pod/coredns-8455f84f99-dpxld 1/1 Running 0 32m kube-system pod/kube-proxy-r9tsx 1/1 Running 0 27m kube-system pod/tiller-deploy-586965d498-58gn9 1/1 Running 0 50s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/docker-postgres ExternalName \u0026lt;none\u0026gt; 34.238.123.20 \u0026lt;none\u0026gt; 39s default service/docker-spring-boot-containers NodePort 10.100.248.57 \u0026lt;none\u0026gt; 12345:32594/TCP 39s default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 32m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 32m kube-system service/tiller-deploy ClusterIP 10.100.5.254 \u0026lt;none\u0026gt; 44134/TCP 50s Check logs c·ªßa pod n·∫øu ko c√≥ l·ªói k·∫øt n·ªëi DB l√† th√†nh c√¥ng\nkubectl logs pod/docker-spring-boot-containers-54566ddbc8-8dpkf v√¨ ·ªü tr√™n Spring pod ƒëang d√πng ki·ªÉu NodePort n√™n c·∫ßn forward port ƒë·ªÉ m·ªçi n∆°i c√≥ th·ªÉ d√πng:\nkubectl port-forward -n default service/docker-spring-boot-containers 32594:12345 --address 0.0.0.0 Sau ƒë√≥ c√≥ th·ªÉ insert DB b·∫±ng c√°ch d√πng POSTMAN send POST request ƒë·∫øn http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:32594/v1/product v·ªõi body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} B√¢y gi·ªù khi x√≥a cluster t·∫°o l·∫°i th√¨ DB v·∫´n c√≤n ƒë√≥, b·ªüi v√¨ n√≥ ƒëc gi·ªØ ·ªü 1 con EC2 kh√°c\nTuy nhi√™n tr√™n workplace n·∫øu m√¨nh stop docker th√¨ DB s·∫Ω m·∫•t, n√™n c·∫ßn mount volume d·ªÉ l∆∞u DB ra b√™n ngo√†i docker\nvi·ªác n√†y thu·ªôc v·ªÅ kƒ© thu·∫≠t d√πng docker\nt·∫•t c·∫£ nh·ªØng thay ƒë·ªïi v·ª´a xong ƒë∆∞·ª£c l∆∞u trong folder spring-maven-postgres-docker-k8s-helm-externaldb\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s-helm-externaldb\nREFERENCES:\nhttps://github.com/red-gate/ks/blob/master/ks8-2/ks8-2.md https://stackoverflow.com/questions/49573258/installing-postgresql-client-v10-on-aws-amazon-linux-ec2-ami https://stackoverflow.com/questions/37694987/connecting-to-postgresql-in-a-docker-container-from-outside https://severalnines.com/database-blog/using-kubernetes-deploy-postgresql https://stackoverflow.com/questions/26040493/how-to-show-data-in-a-table-by-using-psql-command-line-interface\n","href":"/bk/k8s-vi-using-postgres-run-outside-cluster-in-another-host-ec2/","title":"K8S 6: Using Postgresql Run Outside Cluster (in Another Host Ec2)"},{"content":" If you delete the database pod all data is lost. We\u0026rsquo;ll fix this by using a database that lives externally to our cluster.\n Y√™u c·∫ßu Workplace: Amazon EC2 Linux\nƒê√£ t·∫°o m√¥i tr∆∞·ªùng, cluster c·ªßa ri√™ng b·∫°n, c√≥ th·ªÉ d√πng eksctl t·∫°o t·ª´ file cluster.yaml sau\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: base-project region: us-east-1 availabilityZones: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1d\u0026quot;] nodeGroups: - name: nodegrp-1 instanceType: t2.medium desiredCapacity: 1 ssh: # import public key from file publicKeyPath: /home/ec2-user/.ssh/id_rsa.pub Ch√∫ √Ω:\nS·ª≠ d·ª•ng ssh-keygen ƒë·ªÉ generate ra b·ªô key (id_rsa.pub, id_rsa) d√πng ƒë·ªÉ SSH v√†o Node\nT·∫°o cluster eksctl create cluster -f cluster.yaml Clone project cd ~ git clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series ll B√†i n√†y s·∫Ω d·ª±a tr√™n 2 folder spring-maven-postgres-docker-k8s-helm v√† spring-maven-postgres-docker-k8s\nCu·ªëi c√πng s·∫Ω l∆∞u l·∫°i s·∫£n ph·∫©m trong folder spring-maven-postgres-docker-k8s-helm-externaldb\nC√°ch l√†m Kh√°i qu√°t  tr∆∞·ªõc ti√™n m·ª•c ti√™u m√¨nh s·∫Ω ƒë·ªÉ PostgreSQL DB ch·∫°y tr√™n con workplace EC2 b·∫±ng docker. R·ªìi tr√™n cluster s·∫Ω ch·ªâ ch·∫°y app Spring boot th√¥i, v√† app ƒë√≥ s·∫Ω ch·ªçc v√†o PostgreSQL DB m√¨nh ƒë√£ t·∫°o\n C·ª• th·ªÉ Chu·∫©n b·ªã bi·∫øn m√¥i tr∆∞·ªùng\nexport AWS_DEFAULT_REGION=us-east-1 export DOCKER_USERNAME=AAAAAA export DOCKER_PASSWORD=BBBBBB export DOCKER_USER_ID=CCCCCC docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Gi·ªù s·∫Ω ch·∫°y con postgreSQL DB\nTrong tr∆∞·ªùng h·ª£p ch∆∞a c√≥ image th√¨ c·∫ßn build docker image tr∆∞·ªõc:\ncd spring-maven-postgres-docker-k8s/docker/postgres docker build -f Dockerfile -t $DOCKER_USERNAME/docker_postgres . V√¨ c√°i image Postgres c·∫ßn truy·ªÅn v√†o c√°c bi·∫øn m√¥i tr∆∞·ªùng env n√™n m√¨nh ch·∫°y l·ªánh sau:\ndocker run --env POSTGRES_USER=dbuser --env POSTGRES_PASSWORD=password --env POSTGRES_DB=store -d -p 5432:5432 hoangmnsd/docker_postgres Sau khi ch·∫°y xong mu·ªën check vi·ªác t·∫°o db ƒë√£ ok ch∆∞a th√¨ ssh v√†o container ƒë√≥\ndocker ps docker exec -it \u0026lt;CONTAINER_NAME\u0026gt; bash su postgres psql -p 5432 store -U dbuser # show all database \\list # show all table \\d N·∫øu th·∫•y database \u0026ldquo;store\u0026rdquo; v√† table \u0026ldquo;product\u0026rdquo; c√≥ nghƒ©a l√† ƒë√£ run db th√†nh c√¥ng\nroot@7b00518cc6c3:/# su postgres postgres@7b00518cc6c3:/$ psql -p 5432 store -U dbuser psql (12.0 (Debian 12.0-2.pgdg100+1)) Type \u0026quot;help\u0026quot; for help. store=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- postgres | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | store | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | =Tc/dbuser + | | | | | dbuser=CTc/dbuser template0 | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | =c/dbuser + | | | | | dbuser=CTc/dbuser template1 | dbuser | UTF8 | en_US.utf8 | en_US.utf8 | =c/dbuser + | | | | | dbuser=CTc/dbuser (4 rows) store=# \\d List of relations Schema | Name | Type | Owner --------+--------------------+----------+-------- public | hibernate_sequence | sequence | dbuser public | product | table | dbuser (2 rows) Gi·ªù n√™n test connection gi·ªØa Node v√† Host (Workplace EC2) xem ƒë√£ connect dc t·ªõi DB hay ch∆∞a\nSSH v√†o Node\nssh -i ~/.ssh/id_rsa ec2-user@\u0026lt;EC2-NODE-PUBLIC-IP\u0026gt; B·ªüi v√¨ Node l√† Amazon linux 2, n√™n c·∫ßn install psql client b·∫±ng command sau:\nsudo amazon-linux-extras install postgresql10 -y Sau ƒë√≥ d√πng psql ƒë·ªÉ connect v√†o DB ƒëang ch·∫°y tr√™n Workplace xem ƒë∆∞·ª£c ko\n# psql -h \u0026lt;EC2-PUBLIC-IP\u0026gt; -p 5432 \u0026lt;DB_NAME\u0026gt; \u0026lt;DB_USER\u0026gt; psql -h 34.238.123.20 -p 5432 store dbuser nh·∫≠p password la` \u0026ldquo;password\u0026rdquo; (c√°i n√†y m√¨nh define trong bi·∫øn m√¥i tr∆∞·ªùng khi truy·ªÅn v√†o v√† run docker image postgres)\nN·∫øu connect th√†nh c√¥ng th·ª≠ list db v√† list table xem\n\\list \\d N·∫øu th·∫•y database \u0026ldquo;store\u0026rdquo; v√† table \u0026ldquo;product\u0026rdquo; c√≥ nghƒ©a l√† ƒë√£ connect db th√†nh c√¥ng\nN·∫øu ko th·∫•y nghƒ©a l√† connect ko th√†nh c√¥ng th√¨ n√™n check Security Group xem Workplace ƒë√£ m·ªü port 5432 cho con Node connect v√†o ch∆∞a\nGi·ªù c·∫ßn s·ª≠a project Spring v√† Helm chart ƒë·ªÉ khi deploy l√™n cluster n√≥ c√≥ th·ªÉ connect v√†o DB ƒëang ·ªü 1 m√°y EC2 kh√°c (ch√≠nh l√† m√°y workplace c·ªßa m√¨nh)\nnano spring-maven-postgres-docker-k8s/src/main/resources/application.yml s·ª≠a URL c·ªßa PostgreSQL DB nh∆∞ sau:\n#below is config for Docker compose #ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for postgresql in local windows #ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for k8s, using service_name:5432 to connect db #ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store #below is config for k8s, using a external db running in another machine ENV_DATASOURCE_URL: jdbc:postgresql://34.238.123.20:5432/store #ƒë√¢y l√† EC2-PUBLIC-IP c·ªßa con m√°y m√† m√¨nh ƒëang run Postgres DB sau ƒë√≥ build l√† file jar v√† ƒë√≥ng docker image m·ªõi\ncd spring-maven-postgres-docker-k8s mvn clean package docker build -f Dockerfile -t $DOCKER_USERNAME/docker_spring-boot-containers . c·∫ßn push l√™n Docker Hub ƒë·ªÉ sau n√†y helm chart pull v·ªÅ\ndocker push $DOCKER_USERNAME/docker_spring-boot-containers Ti·∫øp l√† s·ª≠a helm chart, ƒë·∫ßu ti√™n s·ª≠a file Service c·ªßa Postgres\nnano spring-maven-postgres-docker-k8s-helm/templates/docker_postgres-service-external.yaml apiVersion: v1 kind: Service metadata: name: docker-postgres labels: app: docker-postgres spec: type: ExternalName externalName: {{ .Values.postgresService.externalName }} selector: app: docker-postgres b·ªüi v√¨ ko t·∫°o pod cho Postgresql m√† s·∫Ω d√πng b√™n ngo√†i n√™n c·∫ßn x√≥a file docker_postgres-deployment.yaml, b·∫°n s·∫Ω th·∫•y file ƒë√≥ ko c√≤n trong folder spring-maven-postgres-docker-k8s-helm-externaldb n·ªØa\nc·∫ßn s·ª≠a file values.yaml n·ªØa\npostgresService: type: ClusterIP port: 5432 targetPort: 5432 externalName: 34.238.123.20 #ƒë√¢y l√† IP c·ªßa con m√°y m√† m√¨nh ƒëang run Postgres DB v·∫≠y l√† chu·∫©n b·ªã xong gi·ªù c√≥ th·ªÉ install helm chart\ncd spring-maven-postgres-docker-k8s-helm helm install -n spring-postgres . $ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/docker-spring-boot-containers-54566ddbc8-8dpkf 1/1 Running 0 39s kube-system pod/aws-node-jpx87 1/1 Running 0 27m kube-system pod/coredns-8455f84f99-7wzm8 1/1 Running 0 32m kube-system pod/coredns-8455f84f99-dpxld 1/1 Running 0 32m kube-system pod/kube-proxy-r9tsx 1/1 Running 0 27m kube-system pod/tiller-deploy-586965d498-58gn9 1/1 Running 0 50s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/docker-postgres ExternalName \u0026lt;none\u0026gt; 34.238.123.20 \u0026lt;none\u0026gt; 39s default service/docker-spring-boot-containers NodePort 10.100.248.57 \u0026lt;none\u0026gt; 12345:32594/TCP 39s default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 32m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 32m kube-system service/tiller-deploy ClusterIP 10.100.5.254 \u0026lt;none\u0026gt; 44134/TCP 50s Check logs c·ªßa pod n·∫øu ko c√≥ l·ªói k·∫øt n·ªëi DB l√† th√†nh c√¥ng\nkubectl logs pod/docker-spring-boot-containers-54566ddbc8-8dpkf v√¨ ·ªü tr√™n Spring pod ƒëang d√πng ki·ªÉu NodePort n√™n c·∫ßn forward port ƒë·ªÉ m·ªçi n∆°i c√≥ th·ªÉ d√πng:\nkubectl port-forward -n default service/docker-spring-boot-containers 32594:12345 --address 0.0.0.0 Sau ƒë√≥ c√≥ th·ªÉ insert DB b·∫±ng c√°ch d√πng POSTMAN send POST request ƒë·∫øn http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:32594/v1/product v·ªõi body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} B√¢y gi·ªù khi x√≥a cluster t·∫°o l·∫°i th√¨ DB v·∫´n c√≤n ƒë√≥, b·ªüi v√¨ n√≥ ƒëc gi·ªØ ·ªü 1 con EC2 kh√°c\nTuy nhi√™n tr√™n workplace n·∫øu m√¨nh stop docker th√¨ DB s·∫Ω m·∫•t, n√™n c·∫ßn mount volume d·ªÉ l∆∞u DB ra b√™n ngo√†i docker\nvi·ªác n√†y thu·ªôc v·ªÅ kƒ© thu·∫≠t d√πng docker\nt·∫•t c·∫£ nh·ªØng thay ƒë·ªïi v·ª´a xong ƒë∆∞·ª£c l∆∞u trong folder spring-maven-postgres-docker-k8s-helm-externaldb\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s-helm-externaldb\nREFERENCES:\nhttps://github.com/red-gate/ks/blob/master/ks8-2/ks8-2.md https://stackoverflow.com/questions/49573258/installing-postgresql-client-v10-on-aws-amazon-linux-ec2-ami https://stackoverflow.com/questions/37694987/connecting-to-postgresql-in-a-docker-container-from-outside https://severalnines.com/database-blog/using-kubernetes-deploy-postgresql https://stackoverflow.com/questions/26040493/how-to-show-data-in-a-table-by-using-psql-command-line-interface\n","href":"/posts/k8s-vi-using-postgres-run-outside-cluster-in-another-host-ec2/","title":"K8S 6: Using Postgresql Run Outside Cluster (in Another Host Ec2)"},{"content":"","href":"/tags/postgresql/","title":"Postgresql"},{"content":"","href":"/tags/helm/","title":"Helm"},{"content":"Gi·ªõi thi·ªáu Tr∆∞·ªõc khi d√πng helm, m√¨nh ƒë√£ d√πng kubectl ƒë·ªÉ run app n√†y ok:\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s\ntuy nhi√™n vi·ªác ch·∫°y ri√™ng t·ª´ng command kubectl apply -f ‚Ä¶ v√† vi·ªác qu·∫£n l√Ω version t·∫≠p trung c·ªßa kubectl b·∫•t ti·ªán ƒë√£ d·∫´n ƒë·∫øn vi·ªác c·∫ßn d√πng Helm ƒë·ªÉ qu·∫£n l√Ω kubernetes cluster\nV·∫≠y n√™n gi·ªù m√¨nh s·∫Ω c·∫•u tr√∫c l·∫°i folder https://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s ƒë·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c Helm,\nSau khi c·∫•u tr√∫c l·∫°i th√¨ k·∫øt qu·∫£ cu·ªëi c√πng l√† project n√†y https://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s-helm\nGi·ªù m√¨nh s·∫Ω m√¥ t·∫£ l·∫°i c√°c b∆∞·ªõc ƒë√£ l√†m ƒë·ªÉ c·∫•u tr√∫c l·∫°i project ƒë√≥\nChu·∫©n b·ªã Workplace: Amazon EC2 Linux\nƒê·∫ßu ti√™n c·∫ßn ƒë·∫£m b·∫£o ƒë√£ t·∫°o ra cluster, c√≥ th·ªÉ t·∫°o b·∫±ng eksctl ch·∫≥ng h·∫°n\nsau ƒë√≥ l√† install helm + tiller\n(https://eksworkshop.com/helm_root/helm_intro/install/)\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh T·∫°o rbac.yaml\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF Sau m·ªói l·∫ßn x√≥a ƒëi t·∫°o l·∫°i cluster, b·∫°n ƒë·ªÅu c·∫ßn l√†m b∆∞·ªõc n√†y ƒë·ªÉ install Tiller (c√≤n g·ªçi l√† helm server-side) l√™n cluster\nkubectl apply -f ~/environment/rbac.yaml helm init --service-account tiller $ helm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Server: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} [ec2-user@ip-172-31-84-250 environment]$ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/aws-node-rjgp7 1/1 Running 0 12m kube-system pod/coredns-8455f84f99-kjxvs 1/1 Running 0 17m kube-system pod/coredns-8455f84f99-tlmql 1/1 Running 0 17m kube-system pod/kube-proxy-5hpd7 1/1 Running 0 12m kube-system pod/tiller-deploy-586965d498-q9pc4 1/1 Running 0 8m52s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 17m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 17m kube-system service/tiller-deploy ClusterIP 10.100.162.227 \u0026lt;none\u0026gt; 44134/TCP 8m52s B·∫Øt ƒë·∫ßu d√πng Helm helm repo update create 1 chart t√™n t·ª± define nh∆∞ sau:\nhelm create spring-maven-postgres-docker-k8s-helm chart m·ªõi t·∫°o c√≥ c·∫•u tr√∫c th∆∞ m·ª•c nh∆∞ sau\ncd spring-maven-postgres-docker-k8s-helm tree ‚îú‚îÄ‚îÄ charts ‚îú‚îÄ‚îÄ Chart.yaml ‚îú‚îÄ‚îÄ templates ‚îÇ¬†‚îú‚îÄ‚îÄ configmap.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ _helpers.tpl ‚îÇ¬†‚îú‚îÄ‚îÄ ingress.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ NOTES.txt ‚îÇ¬†‚îú‚îÄ‚îÄ serviceaccount.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ service.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ tests ‚îÇ¬†‚îî‚îÄ‚îÄ test-connection.yaml ‚îî‚îÄ‚îÄ values.yaml trong folder /templates, Helm ƒë√£ t·∫°o cho ch√∫ng ta nh·ªØng default resource, nh∆∞ng m√¨nh c·∫ßn d√πng nh·ªØng resource m√¨nh ƒë√£ t·∫°o t·ª´ tr∆∞·ªõc c∆°, n√™n h√£y x√≥a h·∫øt c√°c file trong /templates ƒëi ch·ªâ c·∫ßn gi·ªØ l·∫°i c√°i serviceaccount.yaml th√¥i\nCopy nh·ªØng file template yaml c≈© (c√≥ s·∫µn) v√†o th∆∞ m·ª•c templates\nc·∫•u tr√∫c m·ªõi nh∆∞ sau:\n‚îú‚îÄ‚îÄ charts ‚îú‚îÄ‚îÄ Chart.yaml ‚îú‚îÄ‚îÄ templates ‚îÇ¬†‚îú‚îÄ‚îÄ docker_postgres-deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ docker_postgres-service.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ docker_spring-boot-containers-deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ docker_spring-boot-containers-service.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ _helpers.tpl ‚îÇ¬†‚îú‚îÄ‚îÄ serviceaccount.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ tests ‚îÇ¬†‚îî‚îÄ‚îÄ test-connection.yaml ‚îî‚îÄ‚îÄ values.yaml C·∫ßn ch·ªânh s·ª≠a file values.yaml ƒë·ªÉ s·ª≠ d·ª•ng n√≥\nc√°c file yaml c≈© c≈©ng c·∫ßn ƒë∆∞·ª£c ch·ªânh s·ª≠a l·∫°i, v√≠ d·ª• nh∆∞ file docker_postgres-deployment.yaml\nc·ª• th·ªÉ th√¨ c√°c b·∫°n xem file ƒë√£ s·ª≠a ·ªü ƒë√¢y\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s-helm/templates\nGi·ªù c√≥ th·ªÉ deploy helm chart\nhelm install --name spring-maven-postgres-docker-k8s-helm . $ helm install --name spring-maven-postgres-docker-k8s-helm . NAME: spring-maven-postgres-docker-k8s-helm LAST DEPLOYED: Mon Nov 18 14:17:37 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Deployment NAME AGE docker-postgres 0s docker-spring-boot-containers 0s ==\u0026gt; v1/Pod(related) NAME AGE docker-postgres-748bcf79db-tcgp8 0s docker-spring-boot-containers-6f8c4fbbbb-pnkng 0s ==\u0026gt; v1/Service NAME AGE docker-postgres 0s docker-spring-boot-containers 0s ==\u0026gt; v1/ServiceAccount NAME AGE spring-maven-postgres-docker-k8s-helm 0s $ kubectl get pods,svc,serviceaccount -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/docker-postgres-748bcf79db-tcgp8 1/1 Running 0 19s default pod/docker-spring-boot-containers-6f8c4fbbbb-pnkng 1/1 Running 0 19s kube-system pod/aws-node-wg64q 1/1 Running 0 78m kube-system pod/coredns-8455f84f99-r78gk 1/1 Running 0 85m kube-system pod/coredns-8455f84f99-szqsl 1/1 Running 0 85m kube-system pod/kube-proxy-ddjhk 1/1 Running 0 78m kube-system pod/tiller-deploy-586965d498-64pt8 1/1 Running 0 69m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/docker-postgres ClusterIP 10.100.193.192 \u0026lt;none\u0026gt; 5432/TCP 19s default service/docker-spring-boot-containers LoadBalancer 10.100.24.212 a2c8bb9720a0e11eaaeb412190640976-542595414.us-east-1.elb.amazonaws.com 80:30911/TCP 19s default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 85m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 85m kube-system service/tiller-deploy ClusterIP 10.100.14.180 \u0026lt;none\u0026gt; 44134/TCP 69m NAMESPACE NAME SECRETS AGE default serviceaccount/default 1 85m default serviceaccount/spring-maven-postgres-docker-k8s-helm 1 19s mu·ªën get logs trong 1 pod n√†o ƒë√≥\nkubectl logs \u0026lt;POD_NAME\u0026gt; sau m·ªói l·∫ßn edit file yaml, c·∫ßn ph·∫£i upgrade helm chart\nhelm upgrade spring-maven-postgres-docker-k8s-helm . mu·ªën x√≥a helm chart th√¨ helm ls ƒë·ªÉ l·∫•y t√™n\nhelm delete \u0026lt;CHART_NAME\u0026gt; command tr√™n ch∆∞a ph·∫£i x√≥a tri·ªát ƒë·ªÉ, v·∫´n c√≥ th·ªÉ th·∫•y b·∫±ng c√°ch \u0026ldquo;helm list \u0026ndash;all\u0026rdquo;\nN·∫øu mu·ªën rollback l·∫°i c√°i helm chart v·ª´a x√≥a th√¨\nhelm rollback \u0026lt;CHART_NAME\u0026gt; \u0026lt;REVISON\u0026gt; N·∫øu mu·ªën th·ª±c s·ª± x√≥a th√¨\nhelm delete --purge \u0026lt;CHART_NAME\u0026gt; ƒë√≥ng c·∫£ c√°i chart th√†nh package\nhelm package . n√≥ s·∫Ω t·∫°o th√†nh 1 file package .tgz\nt·∫°o repo index\nhelm repo index . N√≥ s·∫Ω t·∫°o file index.html\nGit push l√™n 1 repo n√†o ƒë·∫•y tr√™n github\nL·∫•y link raw https://raw.githubusercontent.com/\u0026lt;ACCOUNT_NAME\u0026gt;/\u0026lt;REPO\u0026gt;/\u0026lt;BRANCH\u0026gt;\nadd repo l·∫•y raw t·ª´ github\nhelm repo add spring-postgres https://raw.githubusercontent.com/hoangmnsd/spring-maven-postgres-docker-k8s-helm/master list all repo\nhelm repo list sau n√†y c√≥ th·ªÉ install b·∫±ng command\nhelm install --name spring-maven-postgres-docker-k8s-helm spring-postgres/spring-maven-postgres-docker-k8s-helm REFERENCES:\nhttps://github.com/red-gate/ks/blob/master/ks5/ks5.md\nhttps://medium.com/ingeniouslysimple/deploying-kubernetes-applications-with-helm-81c9c931f9d3\n","href":"/bk/k8s-v-using-helm-chart-w-kubectl/","title":"K8S 5: Using Helm Chart With Kubectl"},{"content":"Gi·ªõi thi·ªáu Tr∆∞·ªõc khi d√πng helm, m√¨nh ƒë√£ d√πng kubectl ƒë·ªÉ run app n√†y ok:\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s\ntuy nhi√™n vi·ªác ch·∫°y ri√™ng t·ª´ng command kubectl apply -f ‚Ä¶ v√† vi·ªác qu·∫£n l√Ω version t·∫≠p trung c·ªßa kubectl b·∫•t ti·ªán ƒë√£ d·∫´n ƒë·∫øn vi·ªác c·∫ßn d√πng Helm ƒë·ªÉ qu·∫£n l√Ω kubernetes cluster\nV·∫≠y n√™n gi·ªù m√¨nh s·∫Ω c·∫•u tr√∫c l·∫°i folder https://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s ƒë·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c Helm,\nSau khi c·∫•u tr√∫c l·∫°i th√¨ k·∫øt qu·∫£ cu·ªëi c√πng l√† project n√†y https://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s-helm\nGi·ªù m√¨nh s·∫Ω m√¥ t·∫£ l·∫°i c√°c b∆∞·ªõc ƒë√£ l√†m ƒë·ªÉ c·∫•u tr√∫c l·∫°i project ƒë√≥\nChu·∫©n b·ªã Workplace: Amazon EC2 Linux\nƒê·∫ßu ti√™n c·∫ßn ƒë·∫£m b·∫£o ƒë√£ t·∫°o ra cluster, c√≥ th·ªÉ t·∫°o b·∫±ng eksctl ch·∫≥ng h·∫°n\nsau ƒë√≥ l√† install helm + tiller\n(https://eksworkshop.com/helm_root/helm_intro/install/)\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh T·∫°o rbac.yaml\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF Sau m·ªói l·∫ßn x√≥a ƒëi t·∫°o l·∫°i cluster, b·∫°n ƒë·ªÅu c·∫ßn l√†m b∆∞·ªõc n√†y ƒë·ªÉ install Tiller (c√≤n g·ªçi l√† helm server-side) l√™n cluster\nkubectl apply -f ~/environment/rbac.yaml helm init --service-account tiller $ helm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Server: \u0026amp;version.Version{SemVer:\u0026quot;v2.16.1\u0026quot;, GitCommit:\u0026quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} [ec2-user@ip-172-31-84-250 environment]$ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/aws-node-rjgp7 1/1 Running 0 12m kube-system pod/coredns-8455f84f99-kjxvs 1/1 Running 0 17m kube-system pod/coredns-8455f84f99-tlmql 1/1 Running 0 17m kube-system pod/kube-proxy-5hpd7 1/1 Running 0 12m kube-system pod/tiller-deploy-586965d498-q9pc4 1/1 Running 0 8m52s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 17m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 17m kube-system service/tiller-deploy ClusterIP 10.100.162.227 \u0026lt;none\u0026gt; 44134/TCP 8m52s B·∫Øt ƒë·∫ßu d√πng Helm helm repo update create 1 chart t√™n t·ª± define nh∆∞ sau:\nhelm create spring-maven-postgres-docker-k8s-helm chart m·ªõi t·∫°o c√≥ c·∫•u tr√∫c th∆∞ m·ª•c nh∆∞ sau\ncd spring-maven-postgres-docker-k8s-helm tree ‚îú‚îÄ‚îÄ charts ‚îú‚îÄ‚îÄ Chart.yaml ‚îú‚îÄ‚îÄ templates ‚îÇ¬†‚îú‚îÄ‚îÄ configmap.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ _helpers.tpl ‚îÇ¬†‚îú‚îÄ‚îÄ ingress.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ NOTES.txt ‚îÇ¬†‚îú‚îÄ‚îÄ serviceaccount.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ service.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ tests ‚îÇ¬†‚îî‚îÄ‚îÄ test-connection.yaml ‚îî‚îÄ‚îÄ values.yaml trong folder /templates, Helm ƒë√£ t·∫°o cho ch√∫ng ta nh·ªØng default resource, nh∆∞ng m√¨nh c·∫ßn d√πng nh·ªØng resource m√¨nh ƒë√£ t·∫°o t·ª´ tr∆∞·ªõc c∆°, n√™n h√£y x√≥a h·∫øt c√°c file trong /templates ƒëi ch·ªâ c·∫ßn gi·ªØ l·∫°i c√°i serviceaccount.yaml th√¥i\nCopy nh·ªØng file template yaml c≈© (c√≥ s·∫µn) v√†o th∆∞ m·ª•c templates\nc·∫•u tr√∫c m·ªõi nh∆∞ sau:\n‚îú‚îÄ‚îÄ charts ‚îú‚îÄ‚îÄ Chart.yaml ‚îú‚îÄ‚îÄ templates ‚îÇ¬†‚îú‚îÄ‚îÄ docker_postgres-deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ docker_postgres-service.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ docker_spring-boot-containers-deployment.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ docker_spring-boot-containers-service.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ _helpers.tpl ‚îÇ¬†‚îú‚îÄ‚îÄ serviceaccount.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ tests ‚îÇ¬†‚îî‚îÄ‚îÄ test-connection.yaml ‚îî‚îÄ‚îÄ values.yaml C·∫ßn ch·ªânh s·ª≠a file values.yaml ƒë·ªÉ s·ª≠ d·ª•ng n√≥\nc√°c file yaml c≈© c≈©ng c·∫ßn ƒë∆∞·ª£c ch·ªânh s·ª≠a l·∫°i, v√≠ d·ª• nh∆∞ file docker_postgres-deployment.yaml\nc·ª• th·ªÉ th√¨ c√°c b·∫°n xem file ƒë√£ s·ª≠a ·ªü ƒë√¢y\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker-k8s-helm/templates\nGi·ªù c√≥ th·ªÉ deploy helm chart\nhelm install --name spring-maven-postgres-docker-k8s-helm . $ helm install --name spring-maven-postgres-docker-k8s-helm . NAME: spring-maven-postgres-docker-k8s-helm LAST DEPLOYED: Mon Nov 18 14:17:37 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Deployment NAME AGE docker-postgres 0s docker-spring-boot-containers 0s ==\u0026gt; v1/Pod(related) NAME AGE docker-postgres-748bcf79db-tcgp8 0s docker-spring-boot-containers-6f8c4fbbbb-pnkng 0s ==\u0026gt; v1/Service NAME AGE docker-postgres 0s docker-spring-boot-containers 0s ==\u0026gt; v1/ServiceAccount NAME AGE spring-maven-postgres-docker-k8s-helm 0s $ kubectl get pods,svc,serviceaccount -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/docker-postgres-748bcf79db-tcgp8 1/1 Running 0 19s default pod/docker-spring-boot-containers-6f8c4fbbbb-pnkng 1/1 Running 0 19s kube-system pod/aws-node-wg64q 1/1 Running 0 78m kube-system pod/coredns-8455f84f99-r78gk 1/1 Running 0 85m kube-system pod/coredns-8455f84f99-szqsl 1/1 Running 0 85m kube-system pod/kube-proxy-ddjhk 1/1 Running 0 78m kube-system pod/tiller-deploy-586965d498-64pt8 1/1 Running 0 69m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/docker-postgres ClusterIP 10.100.193.192 \u0026lt;none\u0026gt; 5432/TCP 19s default service/docker-spring-boot-containers LoadBalancer 10.100.24.212 a2c8bb9720a0e11eaaeb412190640976-542595414.us-east-1.elb.amazonaws.com 80:30911/TCP 19s default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 85m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 85m kube-system service/tiller-deploy ClusterIP 10.100.14.180 \u0026lt;none\u0026gt; 44134/TCP 69m NAMESPACE NAME SECRETS AGE default serviceaccount/default 1 85m default serviceaccount/spring-maven-postgres-docker-k8s-helm 1 19s mu·ªën get logs trong 1 pod n√†o ƒë√≥\nkubectl logs \u0026lt;POD_NAME\u0026gt; sau m·ªói l·∫ßn edit file yaml, c·∫ßn ph·∫£i upgrade helm chart\nhelm upgrade spring-maven-postgres-docker-k8s-helm . mu·ªën x√≥a helm chart th√¨ helm ls ƒë·ªÉ l·∫•y t√™n\nhelm delete \u0026lt;CHART_NAME\u0026gt; command tr√™n ch∆∞a ph·∫£i x√≥a tri·ªát ƒë·ªÉ, v·∫´n c√≥ th·ªÉ th·∫•y b·∫±ng c√°ch \u0026ldquo;helm list \u0026ndash;all\u0026rdquo;\nN·∫øu mu·ªën rollback l·∫°i c√°i helm chart v·ª´a x√≥a th√¨\nhelm rollback \u0026lt;CHART_NAME\u0026gt; \u0026lt;REVISON\u0026gt; N·∫øu mu·ªën th·ª±c s·ª± x√≥a th√¨\nhelm delete --purge \u0026lt;CHART_NAME\u0026gt; ƒë√≥ng c·∫£ c√°i chart th√†nh package\nhelm package . n√≥ s·∫Ω t·∫°o th√†nh 1 file package .tgz\nt·∫°o repo index\nhelm repo index . N√≥ s·∫Ω t·∫°o file index.html\nGit push l√™n 1 repo n√†o ƒë·∫•y tr√™n github\nL·∫•y link raw https://raw.githubusercontent.com/\u0026lt;ACCOUNT_NAME\u0026gt;/\u0026lt;REPO\u0026gt;/\u0026lt;BRANCH\u0026gt;\nadd repo l·∫•y raw t·ª´ github\nhelm repo add spring-postgres https://raw.githubusercontent.com/hoangmnsd/spring-maven-postgres-docker-k8s-helm/master list all repo\nhelm repo list sau n√†y c√≥ th·ªÉ install b·∫±ng command\nhelm install --name spring-maven-postgres-docker-k8s-helm spring-postgres/spring-maven-postgres-docker-k8s-helm REFERENCES:\nhttps://github.com/red-gate/ks/blob/master/ks5/ks5.md\nhttps://medium.com/ingeniouslysimple/deploying-kubernetes-applications-with-helm-81c9c931f9d3\n","href":"/posts/k8s-v-using-helm-chart-w-kubectl/","title":"K8S 5: Using Helm Chart With Kubectl"},{"content":"","href":"/tags/kubectl/","title":"kubectl"},{"content":"","href":"/tags/football/","title":"Football"},{"content":"ƒê·∫ßu ti√™n c√°c b·∫°n c√≥ th·ªÉ check l·ªãch thi ƒë·∫•u c·ªßa c√°c ƒë·ªôi b√≥ng ·ªü livescore\n(https://www.livescore.com/soccer/japan/j-league/)\n·ªü h√¨nh tr√™n m√¨nh th·∫•y v√†o ng√†y 23/11/2019 FC Tokyo s·∫Ω ƒë√° v·ªõi FC Shonan tr√™n s√¢n nh√† Ajinomoto Stadium\nM√¨nh c≈©ng ·ªü Tokyo, n√™n m√¨nh s·∫Ω ƒëi xem tr·∫≠n n√†y\nC√≥ 2 c√°ch ƒë·ªÉ mua v√©, 1 l√† c√°c b·∫°n ƒë·∫øn ng√†y ƒë√≥ th√¨ ƒëi ƒë·∫øn s√¢n t√¨m qu·∫ßy b√°n v√© r·ªìi v√†o mua v√† xem lu√¥n (ch√∫ √Ω l√† gi√° v√© s·∫Ω th√™m 300-400 y√™n), c√°ch 2 l√† mua online, h√¥m nay m√¨nh s·∫Ω mua online (mua online c≈©ng c√≥ 2 h√¨nh th·ª©c, mua tr√™n trang ch·ªß c·ªßa ƒë·ªôi b√≥ng, ho·∫∑c mua ·ªü trang https://l-tike.com, nay m√¨nh ch·ªçn c√°ch mua ·ªü trang l-tike.com)\nƒë·∫ßu ti√™n c√°c b·∫°n c·∫ßn t·∫°o t√†i kho·∫£n tr√™n trang n√†y:\nhttps://l-tike.com\nsau ƒë√≥, b·∫°n c√≥ th·ªÉ v√†o link n√†y ƒë·ªÉ mua v√©:\nhttps://l-tike.com/sports/football/\nk√©o xu·ªëng d∆∞·ªõi, t√¨m bi·ªÉu t∆∞·ª£ng c·ªßa FC Tokyo, ·∫•n v√†o\nhi·ªán ra th√¥ng tin c√°c tr·∫≠n FC Tokyo s·∫Øp ƒë√°, ch·ªçn tr·∫≠n ng√†y 23/11, ·∫•n n√∫t m√†u v√†ng\nch·ªçn gh·∫ø t√πy b·∫°n, m√¨nh ch·ªçn gh·∫ø ng·ªìi ch·ªó ƒë·ªôi ch·ªß nh√†, ch·ªó ng·ªìi t·ª± do, kick v√†o n√∫t m√†u ƒë·ªè\nch·ªçn s·ªë l∆∞·ª£ng v√©, v√† ·∫•n n√∫t m√†u ƒë·ªè\nc√≥ th·ªÉ s·∫Ω b·∫Øt ƒëƒÉng nh·∫≠p, b·∫°n nh·∫≠p password l√∫c t·∫°o account, r·ªìi c√≥ th·ªÉ ph·∫£i authen b·∫±ng c√°ch r·∫•t vui m·∫Øt nh∆∞ n√†y\nsau ƒë√≥ ·∫•n next, s·∫Ω hi·ªán ra qu√° tr√¨nh th·ª±c hi·ªán thanh to√°n, ·ªü ƒë√¢y m√¨nh ch·ªçn th√†nh to√°n b·∫±ng credit card\nGi√° v√© s·∫Ω th√™m 2 lo·∫°i chi ph√≠ th√†nh ra 2300 y√™n ti·ªÅn v√© + chi ph√≠ = 2630 y√™n 1 v√©\nsau khi thanh to√°n xong h·ªç s·∫Ω g·ª≠i mail v·ªÅ cho b·∫°n, c√≥ ch·ª©a ‰∫àÁ¥ÑÁï™Âè∑\nb·∫°n ra ngo√†i LAWSON ho·∫∑c MINISTOP, t√¨m c√¢y Loppi\ntr√™n m√†n h√¨nh Loppi, ·∫•n n√∫t „É≠„Éº„ÇΩ„É≥„ÉÅ„Ç±„ÉÉ„Éà\nti·∫øp t·ª•c ch·ªçn ‰∫àÁ¥ÑÊ∏à„Åø„ÉÅ„Ç±„ÉÉ„Éà„ÅÆ„ÅäÂºïÂèñ„Çä/„ÅäÊîØÊâï„ÅÑ\nnh·∫≠p 10 k√≠ t·ª± trong mail v√†o ph·∫ßn ‰∫àÁ¥ÑÁï™Âè∑\nnh·∫≠p s·ªë ƒëi·ªán tho·∫°i ÈõªË©±Áï™Âè∑, t√™n ÂêçÂâç\nNh·∫≠n ƒëc h√≥a ƒë∆°n r·ªìi th√¨ ƒëem ra cho nh√¢n vi√™n LAWSON ƒë·ªÉ h·ªç ƒë∆∞a v√© cho m√¨nh\nC·∫ßm v√© ƒë√≥ ƒë·∫øn s√¢n v√† xem ƒë√° b√≥ng th√¥iüòÜ\nL·∫ßn sau m√¨nh s·∫Ω mua v√© tr√™n website c·ªßa ƒë·ªôi xem sao\n","href":"/bk/review-buy-ticket-to-wach-football-match-tokyo/","title":"H∆∞·ªõng d·∫´n mua v√© xem ƒë√° b√≥ng ·ªü Tokyo"},{"content":"ƒê·∫ßu ti√™n c√°c b·∫°n c√≥ th·ªÉ check l·ªãch thi ƒë·∫•u c·ªßa c√°c ƒë·ªôi b√≥ng ·ªü livescore\n(https://www.livescore.com/soccer/japan/j-league/)\n·ªü h√¨nh tr√™n m√¨nh th·∫•y v√†o ng√†y 23/11/2019 FC Tokyo s·∫Ω ƒë√° v·ªõi FC Shonan tr√™n s√¢n nh√† Ajinomoto Stadium\nM√¨nh c≈©ng ·ªü Tokyo, n√™n m√¨nh s·∫Ω ƒëi xem tr·∫≠n n√†y\nC√≥ 2 c√°ch ƒë·ªÉ mua v√©, 1 l√† c√°c b·∫°n ƒë·∫øn ng√†y ƒë√≥ th√¨ ƒëi ƒë·∫øn s√¢n t√¨m qu·∫ßy b√°n v√© r·ªìi v√†o mua v√† xem lu√¥n (ch√∫ √Ω l√† gi√° v√© s·∫Ω th√™m 300-400 y√™n), c√°ch 2 l√† mua online, h√¥m nay m√¨nh s·∫Ω mua online (mua online c≈©ng c√≥ 2 h√¨nh th·ª©c, mua tr√™n trang ch·ªß c·ªßa ƒë·ªôi b√≥ng, ho·∫∑c mua ·ªü trang https://l-tike.com, nay m√¨nh ch·ªçn c√°ch mua ·ªü trang l-tike.com)\nƒë·∫ßu ti√™n c√°c b·∫°n c·∫ßn t·∫°o t√†i kho·∫£n tr√™n trang n√†y:\nhttps://l-tike.com\nsau ƒë√≥, b·∫°n c√≥ th·ªÉ v√†o link n√†y ƒë·ªÉ mua v√©:\nhttps://l-tike.com/sports/football/\nk√©o xu·ªëng d∆∞·ªõi, t√¨m bi·ªÉu t∆∞·ª£ng c·ªßa FC Tokyo, ·∫•n v√†o\nhi·ªán ra th√¥ng tin c√°c tr·∫≠n FC Tokyo s·∫Øp ƒë√°, ch·ªçn tr·∫≠n ng√†y 23/11, ·∫•n n√∫t m√†u v√†ng\nch·ªçn gh·∫ø t√πy b·∫°n, m√¨nh ch·ªçn gh·∫ø ng·ªìi ch·ªó ƒë·ªôi ch·ªß nh√†, ch·ªó ng·ªìi t·ª± do, kick v√†o n√∫t m√†u ƒë·ªè\nch·ªçn s·ªë l∆∞·ª£ng v√©, v√† ·∫•n n√∫t m√†u ƒë·ªè\nc√≥ th·ªÉ s·∫Ω b·∫Øt ƒëƒÉng nh·∫≠p, b·∫°n nh·∫≠p password l√∫c t·∫°o account, r·ªìi c√≥ th·ªÉ ph·∫£i authen b·∫±ng c√°ch r·∫•t vui m·∫Øt nh∆∞ n√†y\nsau ƒë√≥ ·∫•n next, s·∫Ω hi·ªán ra qu√° tr√¨nh th·ª±c hi·ªán thanh to√°n, ·ªü ƒë√¢y m√¨nh ch·ªçn th√†nh to√°n b·∫±ng credit card\nGi√° v√© s·∫Ω th√™m 2 lo·∫°i chi ph√≠ th√†nh ra 2300 y√™n ti·ªÅn v√© + chi ph√≠ = 2630 y√™n 1 v√©\nsau khi thanh to√°n xong h·ªç s·∫Ω g·ª≠i mail v·ªÅ cho b·∫°n, c√≥ ch·ª©a ‰∫àÁ¥ÑÁï™Âè∑\nb·∫°n ra ngo√†i LAWSON ho·∫∑c MINISTOP, t√¨m c√¢y Loppi\ntr√™n m√†n h√¨nh Loppi, ·∫•n n√∫t „É≠„Éº„ÇΩ„É≥„ÉÅ„Ç±„ÉÉ„Éà\nti·∫øp t·ª•c ch·ªçn ‰∫àÁ¥ÑÊ∏à„Åø„ÉÅ„Ç±„ÉÉ„Éà„ÅÆ„ÅäÂºïÂèñ„Çä/„ÅäÊîØÊâï„ÅÑ\nnh·∫≠p 10 k√≠ t·ª± trong mail v√†o ph·∫ßn ‰∫àÁ¥ÑÁï™Âè∑\nnh·∫≠p s·ªë ƒëi·ªán tho·∫°i ÈõªË©±Áï™Âè∑, t√™n ÂêçÂâç\nNh·∫≠n ƒëc h√≥a ƒë∆°n r·ªìi th√¨ ƒëem ra cho nh√¢n vi√™n LAWSON ƒë·ªÉ h·ªç ƒë∆∞a v√© cho m√¨nh\nC·∫ßm v√© ƒë√≥ ƒë·∫øn s√¢n v√† xem ƒë√° b√≥ng th√¥iüòÜ\nL·∫ßn sau m√¨nh s·∫Ω mua v√© tr√™n website c·ªßa ƒë·ªôi xem sao\n","href":"/posts/review-buy-ticket-to-wach-football-match-tokyo/","title":"H∆∞·ªõng d·∫´n mua v√© xem ƒë√° b√≥ng ·ªü Tokyo"},{"content":"","href":"/tags/eksctl/","title":"eksctl"},{"content":"Run Spring Boot + Postgresql App by Docker Compose/Kubernetes ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh ƒëang ·ªü c√°ch 2, s·∫Ω h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ d·ª±ng 1 app micro-service (SpringBoot + PostgreSQL) tr√™n AWS b·∫±ng 2 ph∆∞∆°ng ph√°p l√† Docker Compose v√† Kubernetes\n1. Prepare Launch 1 EC2 Amazon Linux, t2.micro l√† ƒë·ªß, ssh v√†o r·ªìi l√†m vi·ªác\nƒê·ªÉ v√†o b√†i n√†y th√¨ c√°c b·∫°n c·∫ßn chu·∫©n b·ªã m√¥i tr∆∞·ªùng v·ªõi eksctl, kubectl, m√¨nh ƒë√£ h∆∞·ªõng d·∫´n ·ªü link n√†y:\nK8S 3: Using eksctl on Amazon Linux EC2\nv√†o link tr√™n c√°c b·∫°n l√†m ƒë·∫øn h·∫øt b∆∞·ªõc C√°ch t·∫°o 1 cluster b·∫±ng eksctl l√† ƒë∆∞·ª£c, r·ªìi quay l·∫°i ƒë√¢y l√†m ti·∫øp\nMake sure you installed maven, java, docker, docker compose:\nmaven, java: https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html\ndocker, docker compose:\nsudo yum install git -y sudo yum update -y sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user sudo -i curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose exit #Logout and Login clone project sau:\ncd ~ git clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series/spring-maven-postgres-docker-k8s 2. How to run on Kubernetes 2.1. Setup set env of Docker Hub account\nexport AWS_DEFAULT_REGION=us-east-1 export DOCKER_USERNAME=XXXXX export DOCKER_PASSWORD=YYYYY docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Edit file n√†y ƒë·ªÉ config db spring-maven-postgres-docker-k8s/src/main/resources, n·ªôi d√πng edit nh∆∞ sau:\n#below is config for running in Docker compose #ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for running in postgresql in local windows #ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for running in K8s, using service_name:5432 to connect db ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store build jar file\ncd spring-maven-postgres-docker-k8s mvn clean package build Docker image of Spring App and push image to Docker Hub\ncd spring-maven-postgres-docker-k8s docker build -f Dockerfile -t $DOCKER_USERNAME/docker_spring-boot-containers . docker push $DOCKER_USERNAME/docker_spring-boot-containers build Docker image of Postgresql and push image to Docker Hub\ncd spring-maven-postgres-docker-k8s/docker/postgres docker build -f Dockerfile -t $DOCKER_USERNAME/docker_postgres . docker push $DOCKER_USERNAME/docker_postgres 2.2. Run app run app by kubectl:\ntr∆∞·ªõc khi run th√¨ c√°c file trong folder spring-maven-postgres-docker-k8s/resource-manifests c·∫ßn ƒë∆∞·ª£c edit ƒë·ªÉ s·ª≠ d·ª•ng images c·ªßa ri√™ng c√°c b·∫°n (hi·ªán t·∫°i n√≥ ƒëang l·∫•y t·ª´ Docker Hub repository c·ªßa m√¨nh), c√°c b·∫°n edit c√°c file yaml ch·ªó image n√†y:\nspec: containers: - image: hoangmnsd/docker_spring-boot-containers thay b·∫±ng Docker Hub account c·ªßa b·∫°n:\nspec: containers: - image: XXXXX/docker_spring-boot-containers sau ƒë√≥ apply c√°c file config:\ncd spring-maven-postgres-docker-k8s/resource-manifests kubectl apply -f docker_postgres-deployment.yaml kubectl apply -f docker_postgres-service.yaml kubectl apply -f docker_spring-boot-containers-deployment.yaml kubectl apply -f docker_spring-boot-containers-service.yaml kubectl get pods,svc -A output:\nNAMESPACE NAME READY STATUS RESTARTS AGE default pod/docker-postgres-dd794d8bd-wjhhc 1/1 Running 0 74s default pod/docker-postgres-dd794d8bd-xncpf 1/1 Running 0 74s default pod/docker-spring-boot-containers-5cb7c5d684-jc6nq 1/1 Running 0 38s default pod/docker-spring-boot-containers-5cb7c5d684-xv2dt 1/1 Running 0 38s kube-system pod/aws-node-ntc7n 1/1 Running 0 12m kube-system pod/coredns-8455f84f99-2b46q 1/1 Running 0 17m kube-system pod/coredns-8455f84f99-zndgw 1/1 Running 0 17m kube-system pod/kube-proxy-h2rw6 1/1 Running 0 12m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/docker-postgres ClusterIP 10.100.143.50 \u0026lt;none\u0026gt; 5432/TCP 52s default service/docker-spring-boot-containers LoadBalancer 10.100.197.147 aff5c4d4f086e11eab08c128b5593723-1463578799.us-east-1.elb.amazonaws.com 80:32700/TCP 17s default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 17m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 17m C√≥ th·ªÉ th·∫•y l√† kubectl ƒë√£ t·∫°o ra 4 pod, v√† 2 service m·ªõi\n2.3. How to test Using Postman, send POST request to http://aff5c4d4f086e11eab08c128b5593723-1463578799.us-east-1.elb.amazonaws.com/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} Ch√∫ √Ω l√† body c·ªßa request c·∫ßn set l√† raw v√† JSON: (·∫£nh)\nif success, Postman s·∫Ω tr·∫£ v·ªÅ response d·∫°ng n√†y:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://aff5c4d4f086e11eab08c128b5593723-1463578799.us-east-1.elb.amazonaws.com/v1/product/product001\nif success browser s·∫Ω tr·∫£ v·ªÅ chu·ªói n√†y (qu√° tr√¨nh ch·ªù ELB available t·ªën kh√° nhi·ªÅu th·ªùi gian, ch·∫Øc kho·∫£ng 3 ph√∫t):\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n(·∫£nh)\nV·∫≠y l√† xong, gi·ªù c√°c b·∫°n ch·ªâ c·∫ßn l·ª•c l·ªçi c√°i project ƒë√≥ ƒë·ªÉ xem k8s n√≥ ho·∫°t ƒë·ªông nh∆∞ n√†o th√¥iüòÜ\n2.4. Using NodePort instead of LoadBalancer ·ªû tr√™n service/docker-spring-boot-containers ƒëang d√πng type LoadBalancer ki·ªÉu n√†y t·∫•t nhi√™n s·∫Ω t·ªën ti·ªÅn LoadBalancer (0.025$ per hour, 0.008$ per GB) n√™n n·∫øu b·∫°n mu·ªën ti·∫øt ki·ªám th√¨ n√™n d√πng NodePort\nkubectedit svc docker-spring-boot-containers s·ª≠a th√†nh nh∆∞ sau(ch·ªß y·∫øu s·ª≠a type v√† ports):\n externalTrafficPolicy: Cluster ports: - nodePort: 31138 #ch·ªó n√†y ch·ªâ c√≥ th·ªÉ ch·ªçn t·ª´ 30000 ƒë·∫øn 32267 port: 12345 protocol: TCP targetPort: 12345 selector: app: docker-spring-boot-containers sessionAffinity: None type: NodePort nodePort: port m√† s·∫Ω outside c√≥ th·ªÉ access ƒë∆∞·ª£c v√†o\nport: gi·ªØa c√°c service s·∫Ω n√≥i chuy·ªán v·ªõi nhau qua port n√†y\ntargetPort: port m√† pod th·ª±c s·ª± ƒëang running\nsau khi s·ª≠a xong th√¨ t·ª´ b√™n ngo√†i c√≥ th·ªÉ access v√†o b·∫±ng c√°ch port-forward\nkubectl port-forward -n default service/docker-spring-boot-containers 31138:12345 --address 0.0.0.0 2.5. Test on NodePort Using Postman, send POST request to http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:31138/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} if success, Postman s·∫Ω tr·∫£ v·ªÅ response d·∫°ng n√†y:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:31138/v1/product/product001\nif success browser s·∫Ω tr·∫£ v·ªÅ chu·ªói n√†y:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n3. How to run by Docker Compose 3.1. Setup Make sure below config in spring-maven-postgres-docker-k8s/src/main/resources :\n#below is config for running in Docker compose ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for running in postgresql in local windows #ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for running in K8s, using service_name:5432 to connect db #ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store 3.2. Run app cd spring-maven-postgres-docker-k8s sh cleanRun.sh 3.3. How to test Using Postman, send POST request to http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:12345/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} if success:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:12345/v1/product/product001\nif success:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n4. How to run in local windows machine 4.1. Setup Make sure you install maven, java8, Intelij IDEA, Postgresql.\nConfig postgresql open port 5432, create database store, and user dbuser/password password\nexecute below query on PgAdmin4:\nGRANT ALL PRIVILEGES ON DATABASE \u0026quot;store\u0026quot; TO \u0026quot;dbuser\u0026quot;; create table if not exists product ( id bigint not null constraint product_pkey primary key, name varchar(255) UNIQUE ); CREATE SEQUENCE IF NOT EXISTS hibernate_sequence START 1; Edit src/main/resources/application.yml, uncomment set config like this\n#below is config for Docker compose #ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for postgresql in local windows ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for k8s, using service_name:5432 to connect db #ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store 4.2. Run app cd spring-maven-postgres-docker-k8s mvn spring-boot:run 4.3. How to test Using Postman, send POST request to http://localhost:12345/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} if success:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://localhost:12345/v1/product/product001\nif success:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\nBonus D∆∞·ªõi ƒë√¢y l√† h∆∞·ªõng d·∫´n c√°ch x√¢y d·ª±ng 1 base project ch·∫°y app Spring Boot + PostgreSQL b·∫±ng Docker Compose\nN·∫øu mu·ªën 1 project ch·∫°y b·∫±ng Maven, th√¨ h√£y v√†o folder n√†y r·ªìi ƒë·ªçc file README.md:\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker\nN·∫øu mu·ªën 1 project ch·∫°y b·∫±ng Gradle, th√¨ h√£y v√†o folder n√†y r·ªìi ƒë·ªçc file README.md:\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-gradle-postgres-docker\nReference:\nSpecial thank to muzir because this\nhttps://muzir.github.io/spring/docker/docker-compose/postgres/2019/03/24/Spring-Boot-Docker.html\n","href":"/bk/k8s-iv-app-micro-services-on-aws-w-eksctl/","title":"K8S 4: App Micro Services on AWS with eksctl"},{"content":"Run Spring Boot + Postgresql App by Docker Compose/Kubernetes ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh ƒëang ·ªü c√°ch 2, s·∫Ω h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ d·ª±ng 1 app micro-service (SpringBoot + PostgreSQL) tr√™n AWS b·∫±ng 2 ph∆∞∆°ng ph√°p l√† Docker Compose v√† Kubernetes\n1. Prepare Launch 1 EC2 Amazon Linux, t2.micro l√† ƒë·ªß, ssh v√†o r·ªìi l√†m vi·ªác\nƒê·ªÉ v√†o b√†i n√†y th√¨ c√°c b·∫°n c·∫ßn chu·∫©n b·ªã m√¥i tr∆∞·ªùng v·ªõi eksctl, kubectl, m√¨nh ƒë√£ h∆∞·ªõng d·∫´n ·ªü link n√†y:\nK8S 3: Using eksctl on Amazon Linux EC2\nv√†o link tr√™n c√°c b·∫°n l√†m ƒë·∫øn h·∫øt b∆∞·ªõc C√°ch t·∫°o 1 cluster b·∫±ng eksctl l√† ƒë∆∞·ª£c, r·ªìi quay l·∫°i ƒë√¢y l√†m ti·∫øp\nMake sure you installed maven, java, docker, docker compose:\nmaven, java: https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html\ndocker, docker compose:\nsudo yum install git -y sudo yum update -y sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user sudo -i curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose exit #Logout and Login clone project sau:\ncd ~ git clone https://github.com/hoangmnsd/kubernetes-series cd kubernetes-series/spring-maven-postgres-docker-k8s 2. How to run on Kubernetes 2.1. Setup set env of Docker Hub account\nexport AWS_DEFAULT_REGION=us-east-1 export DOCKER_USERNAME=XXXXX export DOCKER_PASSWORD=YYYYY docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Edit file n√†y ƒë·ªÉ config db spring-maven-postgres-docker-k8s/src/main/resources, n·ªôi d√πng edit nh∆∞ sau:\n#below is config for running in Docker compose #ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for running in postgresql in local windows #ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for running in K8s, using service_name:5432 to connect db ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store build jar file\ncd spring-maven-postgres-docker-k8s mvn clean package build Docker image of Spring App and push image to Docker Hub\ncd spring-maven-postgres-docker-k8s docker build -f Dockerfile -t $DOCKER_USERNAME/docker_spring-boot-containers . docker push $DOCKER_USERNAME/docker_spring-boot-containers build Docker image of Postgresql and push image to Docker Hub\ncd spring-maven-postgres-docker-k8s/docker/postgres docker build -f Dockerfile -t $DOCKER_USERNAME/docker_postgres . docker push $DOCKER_USERNAME/docker_postgres 2.2. Run app run app by kubectl:\ntr∆∞·ªõc khi run th√¨ c√°c file trong folder spring-maven-postgres-docker-k8s/resource-manifests c·∫ßn ƒë∆∞·ª£c edit ƒë·ªÉ s·ª≠ d·ª•ng images c·ªßa ri√™ng c√°c b·∫°n (hi·ªán t·∫°i n√≥ ƒëang l·∫•y t·ª´ Docker Hub repository c·ªßa m√¨nh), c√°c b·∫°n edit c√°c file yaml ch·ªó image n√†y:\nspec: containers: - image: hoangmnsd/docker_spring-boot-containers thay b·∫±ng Docker Hub account c·ªßa b·∫°n:\nspec: containers: - image: XXXXX/docker_spring-boot-containers sau ƒë√≥ apply c√°c file config:\ncd spring-maven-postgres-docker-k8s/resource-manifests kubectl apply -f docker_postgres-deployment.yaml kubectl apply -f docker_postgres-service.yaml kubectl apply -f docker_spring-boot-containers-deployment.yaml kubectl apply -f docker_spring-boot-containers-service.yaml kubectl get pods,svc -A output:\nNAMESPACE NAME READY STATUS RESTARTS AGE default pod/docker-postgres-dd794d8bd-wjhhc 1/1 Running 0 74s default pod/docker-postgres-dd794d8bd-xncpf 1/1 Running 0 74s default pod/docker-spring-boot-containers-5cb7c5d684-jc6nq 1/1 Running 0 38s default pod/docker-spring-boot-containers-5cb7c5d684-xv2dt 1/1 Running 0 38s kube-system pod/aws-node-ntc7n 1/1 Running 0 12m kube-system pod/coredns-8455f84f99-2b46q 1/1 Running 0 17m kube-system pod/coredns-8455f84f99-zndgw 1/1 Running 0 17m kube-system pod/kube-proxy-h2rw6 1/1 Running 0 12m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/docker-postgres ClusterIP 10.100.143.50 \u0026lt;none\u0026gt; 5432/TCP 52s default service/docker-spring-boot-containers LoadBalancer 10.100.197.147 aff5c4d4f086e11eab08c128b5593723-1463578799.us-east-1.elb.amazonaws.com 80:32700/TCP 17s default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 17m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 17m C√≥ th·ªÉ th·∫•y l√† kubectl ƒë√£ t·∫°o ra 4 pod, v√† 2 service m·ªõi\n2.3. How to test Using Postman, send POST request to http://aff5c4d4f086e11eab08c128b5593723-1463578799.us-east-1.elb.amazonaws.com/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} Ch√∫ √Ω l√† body c·ªßa request c·∫ßn set l√† raw v√† JSON: (·∫£nh)\nif success, Postman s·∫Ω tr·∫£ v·ªÅ response d·∫°ng n√†y:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://aff5c4d4f086e11eab08c128b5593723-1463578799.us-east-1.elb.amazonaws.com/v1/product/product001\nif success browser s·∫Ω tr·∫£ v·ªÅ chu·ªói n√†y (qu√° tr√¨nh ch·ªù ELB available t·ªën kh√° nhi·ªÅu th·ªùi gian, ch·∫Øc kho·∫£ng 3 ph√∫t):\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n(·∫£nh)\nV·∫≠y l√† xong, gi·ªù c√°c b·∫°n ch·ªâ c·∫ßn l·ª•c l·ªçi c√°i project ƒë√≥ ƒë·ªÉ xem k8s n√≥ ho·∫°t ƒë·ªông nh∆∞ n√†o th√¥iüòÜ\n2.4. Using NodePort instead of LoadBalancer ·ªû tr√™n service/docker-spring-boot-containers ƒëang d√πng type LoadBalancer ki·ªÉu n√†y t·∫•t nhi√™n s·∫Ω t·ªën ti·ªÅn LoadBalancer (0.025$ per hour, 0.008$ per GB) n√™n n·∫øu b·∫°n mu·ªën ti·∫øt ki·ªám th√¨ n√™n d√πng NodePort\nkubectedit svc docker-spring-boot-containers s·ª≠a th√†nh nh∆∞ sau(ch·ªß y·∫øu s·ª≠a type v√† ports):\n externalTrafficPolicy: Cluster ports: - nodePort: 31138 #ch·ªó n√†y ch·ªâ c√≥ th·ªÉ ch·ªçn t·ª´ 30000 ƒë·∫øn 32267 port: 12345 protocol: TCP targetPort: 12345 selector: app: docker-spring-boot-containers sessionAffinity: None type: NodePort nodePort: port m√† s·∫Ω outside c√≥ th·ªÉ access ƒë∆∞·ª£c v√†o\nport: gi·ªØa c√°c service s·∫Ω n√≥i chuy·ªán v·ªõi nhau qua port n√†y\ntargetPort: port m√† pod th·ª±c s·ª± ƒëang running\nsau khi s·ª≠a xong th√¨ t·ª´ b√™n ngo√†i c√≥ th·ªÉ access v√†o b·∫±ng c√°ch port-forward\nkubectl port-forward -n default service/docker-spring-boot-containers 31138:12345 --address 0.0.0.0 2.5. Test on NodePort Using Postman, send POST request to http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:31138/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} if success, Postman s·∫Ω tr·∫£ v·ªÅ response d·∫°ng n√†y:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:31138/v1/product/product001\nif success browser s·∫Ω tr·∫£ v·ªÅ chu·ªói n√†y:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n3. How to run by Docker Compose 3.1. Setup Make sure below config in spring-maven-postgres-docker-k8s/src/main/resources :\n#below is config for running in Docker compose ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for running in postgresql in local windows #ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for running in K8s, using service_name:5432 to connect db #ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store 3.2. Run app cd spring-maven-postgres-docker-k8s sh cleanRun.sh 3.3. How to test Using Postman, send POST request to http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:12345/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} if success:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:12345/v1/product/product001\nif success:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\n4. How to run in local windows machine 4.1. Setup Make sure you install maven, java8, Intelij IDEA, Postgresql.\nConfig postgresql open port 5432, create database store, and user dbuser/password password\nexecute below query on PgAdmin4:\nGRANT ALL PRIVILEGES ON DATABASE \u0026quot;store\u0026quot; TO \u0026quot;dbuser\u0026quot;; create table if not exists product ( id bigint not null constraint product_pkey primary key, name varchar(255) UNIQUE ); CREATE SEQUENCE IF NOT EXISTS hibernate_sequence START 1; Edit src/main/resources/application.yml, uncomment set config like this\n#below is config for Docker compose #ENV_DATASOURCE_URL: jdbc:postgresql://postgres/store #below is config for postgresql in local windows ENV_DATASOURCE_URL: jdbc:postgresql://localhost:5432/store #below is config for k8s, using service_name:5432 to connect db #ENV_DATASOURCE_URL: jdbc:postgresql://docker-postgres:5432/store 4.2. Run app cd spring-maven-postgres-docker-k8s mvn spring-boot:run 4.3. How to test Using Postman, send POST request to http://localhost:12345/v1/product with body:\n{\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;} if success:\n{ \u0026quot;product\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;product001\u0026quot;, \u0026quot;new\u0026quot;: false }, \u0026quot;result\u0026quot;: { \u0026quot;success\u0026quot;: true, \u0026quot;message\u0026quot;: \u0026quot;Success\u0026quot; } } In Browser, check this link http://localhost:12345/v1/product/product001\nif success:\n{\u0026quot;product\u0026quot;:{\u0026quot;id\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;product001\u0026quot;,\u0026quot;new\u0026quot;:false},\u0026quot;result\u0026quot;:{\u0026quot;success\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Success\u0026quot;}}\nBonus D∆∞·ªõi ƒë√¢y l√† h∆∞·ªõng d·∫´n c√°ch x√¢y d·ª±ng 1 base project ch·∫°y app Spring Boot + PostgreSQL b·∫±ng Docker Compose\nN·∫øu mu·ªën 1 project ch·∫°y b·∫±ng Maven, th√¨ h√£y v√†o folder n√†y r·ªìi ƒë·ªçc file README.md:\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-maven-postgres-docker\nN·∫øu mu·ªën 1 project ch·∫°y b·∫±ng Gradle, th√¨ h√£y v√†o folder n√†y r·ªìi ƒë·ªçc file README.md:\nhttps://github.com/hoangmnsd/kubernetes-series/tree/master/spring-gradle-postgres-docker\nReference:\nSpecial thank to muzir because this\nhttps://muzir.github.io/spring/docker/docker-compose/postgres/2019/03/24/Spring-Boot-Docker.html\n","href":"/posts/k8s-iv-app-micro-services-on-aws-w-eksctl/","title":"K8S 4: App Micro Services on AWS with eksctl"},{"content":"Gi·ªõi thi·ªáu ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh s·∫Ω h∆∞·ªõng d·∫´n c√°ch 2, d√πng eksctl, c√°c b√†i tr∆∞·ªõc ƒë√£ n√≥i v·ªÅ c√°ch 3 r·ªìi\nChu·∫©n b·ªã Launch 1 EC2 Amazon Linux, t2.micro l√† ƒë·ªß, ssh v√†o r·ªìi l√†m vi·ªác\ninstall kubectl\ncurl -o kubectl https://amazon-eks.s3-us-west-2.amazonaws.com/1.14.6/2019-08-22/bin/linux/amd64/kubectl chmod +x ./kubectl mkdir -p $HOME/bin \u0026amp;\u0026amp; cp ./kubectl $HOME/bin/kubectl \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH echo \u0026#39;export PATH=$HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc kubectl version --short --client install¬†aws-iam-authenticator¬†on Linux\ncurl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.14.6/2019-08-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator mkdir -p $HOME/bin \u0026amp;\u0026amp; cp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH echo \u0026#39;export PATH=$HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc aws-iam-authenticator help install eksctl\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin eksctl version set default region, v√† set account Docker Hub\nexport AWS_DEFAULT_REGION=us-east-1 export DOCKER_USERNAME=AAAAAAA export DOCKER_PASSWORD=BBBBBBB export DOCKER_USER_ID=CCCCCC Use AWS Console, create IAM Role and attach to EC2 with this policy:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreateInstanceProfile\u0026quot;, \u0026quot;iam:DeleteInstanceProfile\u0026quot;, \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:GetInstanceProfile\u0026quot;, \u0026quot;iam:ListRoleTags\u0026quot;, \u0026quot;iam:UntagRole\u0026quot;, \u0026quot;iam:TagRole\u0026quot;, \u0026quot;iam:RemoveRoleFromInstanceProfile\u0026quot;, \u0026quot;iam:CreateRole\u0026quot;, \u0026quot;iam:DeleteRole\u0026quot;, \u0026quot;iam:AttachRolePolicy\u0026quot;, \u0026quot;iam:PutRolePolicy\u0026quot;, \u0026quot;iam:ListInstanceProfiles\u0026quot;, \u0026quot;iam:AddRoleToInstanceProfile\u0026quot;, \u0026quot;iam:ListInstanceProfilesForRole\u0026quot;, \u0026quot;iam:PassRole\u0026quot;, \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:DetachRolePolicy\u0026quot;, \u0026quot;iam:DeleteRolePolicy\u0026quot;, \u0026quot;iam:DeleteServiceLinkedRole\u0026quot;, \u0026quot;ec2:DeleteInternetGateway\u0026quot;, \u0026quot;iam:GetOpenIDConnectProvider\u0026quot;, \u0026quot;iam:GetRolePolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:iam::793459850633:instance-profile/eksctl-*\u0026quot;, \u0026quot;arn:aws:iam::*:oidc-provider/*\u0026quot;, \u0026quot;arn:aws:iam::793459850633:role/eksctl-*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:internet-gateway/*\u0026quot; ] }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:DeleteSubnet\u0026quot;, \u0026quot;ec2:AttachInternetGateway\u0026quot;, \u0026quot;ec2:DeleteRouteTable\u0026quot;, \u0026quot;ec2:AssociateRouteTable\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;ec2:CreateRoute\u0026quot;, \u0026quot;ec2:CreateInternetGateway\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupEgress\u0026quot;, \u0026quot;autoscaling:UpdateAutoScalingGroup\u0026quot;, \u0026quot;ec2:DeleteInternetGateway\u0026quot;, \u0026quot;ec2:DescribeKeyPairs\u0026quot;, \u0026quot;ec2:DescribeRouteTables\u0026quot;, \u0026quot;ec2:ImportKeyPair\u0026quot;, \u0026quot;ec2:DescribeLaunchTemplates\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:CreateRouteTable\u0026quot;, \u0026quot;cloudformation:*\u0026quot;, \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;ec2:DetachInternetGateway\u0026quot;, \u0026quot;ec2:DisassociateRouteTable\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:DescribeImageAttribute\u0026quot;, \u0026quot;ec2:DeleteNatGateway\u0026quot;, \u0026quot;autoscaling:DeleteAutoScalingGroup\u0026quot;, \u0026quot;ec2:DeleteVpc\u0026quot;, \u0026quot;ec2:CreateSubnet\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;eks:*\u0026quot;, \u0026quot;autoscaling:CreateAutoScalingGroup\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot;, \u0026quot;ec2:CreateNatGateway\u0026quot;, \u0026quot;autoscaling:DescribeLaunchConfigurations\u0026quot;, \u0026quot;ec2:CreateVpc\u0026quot;, \u0026quot;ec2:DescribeVpcAttribute\u0026quot;, \u0026quot;autoscaling:DescribeScalingActivities\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:CreateSecurityGroup\u0026quot;, \u0026quot;ec2:ModifyVpcAttribute\u0026quot;, \u0026quot;ec2:ReleaseAddress\u0026quot;, \u0026quot;ec2:AuthorizeSecurityGroupEgress\u0026quot;, \u0026quot;ec2:DeleteLaunchTemplate\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DeleteRoute\u0026quot;, \u0026quot;ec2:DescribeLaunchTemplateVersions\u0026quot;, \u0026quot;ec2:DescribeNatGateways\u0026quot;, \u0026quot;ec2:AllocateAddress\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;autoscaling:CreateLaunchConfiguration\u0026quot;, \u0026quot;ec2:DescribeImages\u0026quot;, \u0026quot;ec2:CreateLaunchTemplate\u0026quot;, \u0026quot;autoscaling:DeleteLaunchConfiguration\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } C√°ch t·∫°o 1 cluster b·∫±ng eksctl Create a file cluster.yaml\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: base-project region: us-east-1 availabilityZones: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1d\u0026quot;] nodeGroups: - name: nodegrp-1 instanceType: t2.medium desiredCapacity: 1 Apply above config file to create cluster:\neksctl create cluster -f cluster.yaml ch·ªù kho·∫£ng 10 ph√∫t v√¨ eks s·∫Ω provision ra network r·∫•t t·ªën time:\nmore sample template https://github.com/weaveworks/eksctl/blob/master/examples\nGet pods, service, nodes, is running\nkubectl get pods,svc,node -A [ec2-user@ip-172-31-84-250 ~]$ kubectl get pods,svc,node -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/aws-node-6k2qq 1/1 Running 0 3h28m kube-system pod/coredns-8455f84f99-rzn44 1/1 Running 0 3h34m kube-system pod/coredns-8455f84f99-xw2rv 1/1 Running 0 3h34m kube-system pod/kube-proxy-w2t7v 1/1 Running 0 3h28m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 3h34m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 3h34m NAMESPACE NAME STATUS ROLES AGE VERSION node/ip-192-168-62-230.ec2.internal Ready \u0026lt;none\u0026gt; 3h28m v1.14.7-eks-1861c5 Run app micro-service on k8s clone source v·ªÅ\ncd ~ git clone https://github.com/hoangmnsd/k8s-mastery t·∫°o pod frontend:\ncd /home/ec2-user/k8s-mastery/resource-manifests kubectl apply -f sa-frontend-deployment.yaml sau khi apply th√¨ pods v√† services, node nh∆∞ sau:\nip-172-31-84-250 ~]$ kubectl get pods,svc,node -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/sa-frontend-54789d8b7d-2whc4 1/1 Running 0 2m36s default pod/sa-frontend-54789d8b7d-dbh4p 1/1 Running 0 2m36s kube-system pod/aws-node-7t2df 1/1 Running 0 9m5s kube-system pod/coredns-8455f84f99-w8dlf 1/1 Running 0 15m kube-system pod/coredns-8455f84f99-xqk2f 1/1 Running 0 15m kube-system pod/kube-proxy-xzhwd 1/1 Running 0 9m5s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 15m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 15m NAMESPACE NAME STATUS ROLES AGE VERSION node/ip-192-168-34-208.ec2.internal Ready \u0026lt;none\u0026gt; 9m5s v1.14.7-eks-1861c5 ip-172-31-84-250 ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 483M 60K 483M 1% /dev tmpfs 493M 0 493M 0% /dev/shm /dev/xvda1 7.9G 2.8G 5.0G 36% / t·∫°o service frondend:\nkubectl create -f service-sa-frontend-lb.yaml get services:\nkubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 21m default sa-frontend-lb LoadBalancer 10.100.64.73 a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com 80:31360/TCP 102s kube-system kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 21m check frontend app tr√™n LB URL: a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com\nT·∫°o pod v√† service c·ªßa backend logic:\ncd /home/ec2-user/k8s-mastery/resource-manifests kubectl apply -f sa-logic-deployment.yaml --record kubectl apply -f service-sa-logic.yaml get pods, svc:\nip-172-31-84-250 resource-manifests]$ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/sa-frontend-54789d8b7d-2whc4 1/1 Running 0 19m default pod/sa-frontend-54789d8b7d-dbh4p 1/1 Running 0 19m default pod/sa-logic-7d7ff8f6dc-r9tcz 1/1 Running 0 118s default pod/sa-logic-7d7ff8f6dc-zhwv2 1/1 Running 0 118s kube-system pod/aws-node-7t2df 1/1 Running 0 25m kube-system pod/coredns-8455f84f99-w8dlf 1/1 Running 0 31m kube-system pod/coredns-8455f84f99-xqk2f 1/1 Running 0 31m kube-system pod/kube-proxy-xzhwd 1/1 Running 0 25m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 31m default service/sa-frontend-lb LoadBalancer 10.100.64.73 a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com 80:31360/TCP 11m default service/sa-logic ClusterIP 10.100.97.45 \u0026lt;none\u0026gt; 80/TCP 4s kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 31m T·∫°o pod v√† service c·ªßa backend webapp:\ncd /home/ec2-user/k8s-mastery/resource-manifests kubectl apply -f sa-web-app-deployment.yaml --record kubectl apply -f service-sa-web-app-lb.yaml get pods,svc:\nip-172-31-84-250 resource-manifests]$ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/sa-frontend-54789d8b7d-2whc4 1/1 Running 0 21m default pod/sa-frontend-54789d8b7d-dbh4p 1/1 Running 0 21m default pod/sa-logic-7d7ff8f6dc-r9tcz 1/1 Running 0 4m8s default pod/sa-logic-7d7ff8f6dc-zhwv2 1/1 Running 0 4m8s default pod/sa-web-app-5f7d8fd94d-lbzj8 1/1 Running 0 27s default pod/sa-web-app-5f7d8fd94d-ql5k2 1/1 Running 0 27s kube-system pod/aws-node-7t2df 1/1 Running 0 27m kube-system pod/coredns-8455f84f99-w8dlf 1/1 Running 0 33m kube-system pod/coredns-8455f84f99-xqk2f 1/1 Running 0 33m kube-system pod/kube-proxy-xzhwd 1/1 Running 0 27m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 33m default service/sa-frontend-lb LoadBalancer 10.100.64.73 a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com 80:31360/TCP 13m default service/sa-logic ClusterIP 10.100.97.45 \u0026lt;none\u0026gt; 80/TCP 2m14s default service/sa-web-app-lb LoadBalancer 10.100.34.198 a289bb56f061a11ea86df120e903ee59-714828784.us-east-1.elb.amazonaws.com 80:32321/TCP 5s kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 33m s·ª≠a c√°i App.js ƒë·ªÉ n√≥ fetch URL c·ªßa sa-webapp\nnano ~/k8s-mastery/sa-frontend/src/App.js s·ª≠a nh∆∞ sau (d√πng ELB URL c·ªßa sa-webapp):\nfetch('http://a7cf3208e067911ea90de1247d6da376-1135942870.us-east-1.elb.amazonaws.com/sentiment', { build docker images v√† push l·∫°i l√™n Docker Hub:\nnpm run build sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:28 . sudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend:28 s·ª≠a deployment config ƒë·ªÉ d√πng image m·ªõi tag:28\nnano ~/k8s-mastery/resource-manifests/sa-frontend-deployment-update.yaml s·ª≠a ch·ªó image th√†nh d√πng b·∫£n tag :28,\nc·∫ßn ch√∫ √Ω ch·ªó hoangmnsd ƒë√≥ l√† account Docker Hub c·ªßa m√¨nh, b·∫°n c·∫ßn d√πng account Docker Hub c·ªßa b·∫°n, v√≠ d·ª• l√† CCCCCC m√† b·∫°n ƒë√£ setting ·ªü tr√™n\nimage: hoangmnsd/sentiment-analysis-frontend:28 apply l·∫°i:\nkubectl apply -f sa-frontend-deployment-update.yaml --record check l·∫°i app ƒë·ªÉ th√¥ng lu·ªìng t·ª´ frontend -\u0026gt; backend:\na40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com\nSSH v√†o Node trong Cluster N·∫øu mu·ªën c√≥ th·ªÉ ssh v√†o Node, tr∆∞·ªõc ti√™n c·∫ßn t·∫°o ssh key ssh-keygen:\nip-172-31-84-250 ~]$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/ec2-user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ec2-user/.ssh/id_rsa. Your public key has been saved in /home/ec2-user/.ssh/id_rsa.pub. khi t·∫°o cluster c·∫ßn t·∫°o v·ªõi file config nh∆∞ sau:\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: base-project region: us-east-1 availabilityZones: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1d\u0026quot;] nodeGroups: - name: nodegrp-1 instanceType: t2.medium desiredCapacity: 1 ssh: # import public key from file publicKeyPath: /home/ec2-user/.ssh/id_rsa.pub Khi ssh v√†o th√¨ d√πng command sau:\nssh -i ~/.ssh/id_rsa ec2-user@\u0026lt;EC2-PUBLIC-IP\u0026gt; T·∫°o k8s dashboard Do m√¨nh d√πng b·∫£n recommend b·ªã ko v√†o dc t·ª´ windows browser, n√™n d√πng b·∫£n alternative:\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/alternative.yaml t·∫°o serviceaccount:\nkubectl create serviceaccount my-dashboard-sa -n kubernetes-dashboard t·∫°o clusterrolebinding:\nkubectl create clusterrolebinding my-dashboard-sa \\ --clusterrole=cluster-admin \\ --serviceaccount=kubernetes-dashboard:my-dashboard-sa get secret:\nkubectl get secrets -n kubernetes-dashboard L·∫•y token:\nkubectl describe secret -n kubernetes-dashboard \u0026lt;TOKEN_NAME\u0026gt; d√πng extension REQUESTLY c·ªßa Chrome, link c√†i:\nhttps://chrome.google.com/webstore/detail/requestly-redirect-url-mo/mdnleldcmiljblolnjhpnblkcekpdkpa config nh∆∞ h√¨nh sau, m·ªói khi v√†o c√°i IP kia th√¨ n√≥ s·∫Ω t·ª± ƒë·ªông modify Header v√† add th√™m TOKEN v√†o cho m√¨nh\nport-forward ƒë·ªÉ b√™n ngo√†i c√≥ th·ªÉ access:\nkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 0.0.0.0 Done! B·∫°n s·∫Ω v√†o ƒë∆∞·ª£c k8s Dashboard t·ª´ Chrome browser tr√™n windows v·ªõi link http://\u0026lt;EC-PUBLIC-IP\u0026gt;:9090\nL·ªói c√≥ th·ªÉ g·∫∑p Sau khi install helm v√† app spring postgres,\nsau ƒë√≥ th√™m k8s dashboard th√¨ b·ªã l·ªói POD m·ªõi t·∫°o lu√¥n ·ªü tr·∫°ng th√°i ContainerCreating\nxem logs:\nkubectl describe pod \u0026lt;pod_name\u0026gt; th√¨ ph√°t hi·ªán l·ªói network: add cmd: failed to assign an IP address to container\ngoogle 1 l√∫c th√¨ th·∫•y (https://github.com/aws/amazon-vpc-cni-k8s/issues/59),\nnguy√™n nh√¢n c√≥ th·ªÉ do thi·∫øu IP trong ENI\n\u0026ldquo;After checking AWS documentation, it seems that there is an IP limit per instance: 18 for t3.medium, 36 for t3.large\u0026rdquo;\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\n","href":"/bk/k8s-iii-using-eksctl-on-amazon-linux/","title":"K8S 3: Using eksctl on Amazon Linux EC2"},{"content":"Gi·ªõi thi·ªáu ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh s·∫Ω h∆∞·ªõng d·∫´n c√°ch 2, d√πng eksctl, c√°c b√†i tr∆∞·ªõc ƒë√£ n√≥i v·ªÅ c√°ch 3 r·ªìi\nChu·∫©n b·ªã Launch 1 EC2 Amazon Linux, t2.micro l√† ƒë·ªß, ssh v√†o r·ªìi l√†m vi·ªác\ninstall kubectl\ncurl -o kubectl https://amazon-eks.s3-us-west-2.amazonaws.com/1.14.6/2019-08-22/bin/linux/amd64/kubectl chmod +x ./kubectl mkdir -p $HOME/bin \u0026amp;\u0026amp; cp ./kubectl $HOME/bin/kubectl \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH echo \u0026#39;export PATH=$HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc kubectl version --short --client install¬†aws-iam-authenticator¬†on Linux\ncurl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.14.6/2019-08-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator mkdir -p $HOME/bin \u0026amp;\u0026amp; cp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH echo \u0026#39;export PATH=$HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc aws-iam-authenticator help install eksctl\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin eksctl version set default region, v√† set account Docker Hub\nexport AWS_DEFAULT_REGION=us-east-1 export DOCKER_USERNAME=AAAAAAA export DOCKER_PASSWORD=BBBBBBB export DOCKER_USER_ID=CCCCCC Use AWS Console, create IAM Role and attach to EC2 with this policy:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreateInstanceProfile\u0026quot;, \u0026quot;iam:DeleteInstanceProfile\u0026quot;, \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:GetInstanceProfile\u0026quot;, \u0026quot;iam:ListRoleTags\u0026quot;, \u0026quot;iam:UntagRole\u0026quot;, \u0026quot;iam:TagRole\u0026quot;, \u0026quot;iam:RemoveRoleFromInstanceProfile\u0026quot;, \u0026quot;iam:CreateRole\u0026quot;, \u0026quot;iam:DeleteRole\u0026quot;, \u0026quot;iam:AttachRolePolicy\u0026quot;, \u0026quot;iam:PutRolePolicy\u0026quot;, \u0026quot;iam:ListInstanceProfiles\u0026quot;, \u0026quot;iam:AddRoleToInstanceProfile\u0026quot;, \u0026quot;iam:ListInstanceProfilesForRole\u0026quot;, \u0026quot;iam:PassRole\u0026quot;, \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:DetachRolePolicy\u0026quot;, \u0026quot;iam:DeleteRolePolicy\u0026quot;, \u0026quot;iam:DeleteServiceLinkedRole\u0026quot;, \u0026quot;ec2:DeleteInternetGateway\u0026quot;, \u0026quot;iam:GetOpenIDConnectProvider\u0026quot;, \u0026quot;iam:GetRolePolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:iam::793459850633:instance-profile/eksctl-*\u0026quot;, \u0026quot;arn:aws:iam::*:oidc-provider/*\u0026quot;, \u0026quot;arn:aws:iam::793459850633:role/eksctl-*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:internet-gateway/*\u0026quot; ] }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:DeleteSubnet\u0026quot;, \u0026quot;ec2:AttachInternetGateway\u0026quot;, \u0026quot;ec2:DeleteRouteTable\u0026quot;, \u0026quot;ec2:AssociateRouteTable\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;ec2:CreateRoute\u0026quot;, \u0026quot;ec2:CreateInternetGateway\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupEgress\u0026quot;, \u0026quot;autoscaling:UpdateAutoScalingGroup\u0026quot;, \u0026quot;ec2:DeleteInternetGateway\u0026quot;, \u0026quot;ec2:DescribeKeyPairs\u0026quot;, \u0026quot;ec2:DescribeRouteTables\u0026quot;, \u0026quot;ec2:ImportKeyPair\u0026quot;, \u0026quot;ec2:DescribeLaunchTemplates\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:CreateRouteTable\u0026quot;, \u0026quot;cloudformation:*\u0026quot;, \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;ec2:DetachInternetGateway\u0026quot;, \u0026quot;ec2:DisassociateRouteTable\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:DescribeImageAttribute\u0026quot;, \u0026quot;ec2:DeleteNatGateway\u0026quot;, \u0026quot;autoscaling:DeleteAutoScalingGroup\u0026quot;, \u0026quot;ec2:DeleteVpc\u0026quot;, \u0026quot;ec2:CreateSubnet\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;eks:*\u0026quot;, \u0026quot;autoscaling:CreateAutoScalingGroup\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot;, \u0026quot;ec2:CreateNatGateway\u0026quot;, \u0026quot;autoscaling:DescribeLaunchConfigurations\u0026quot;, \u0026quot;ec2:CreateVpc\u0026quot;, \u0026quot;ec2:DescribeVpcAttribute\u0026quot;, \u0026quot;autoscaling:DescribeScalingActivities\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:CreateSecurityGroup\u0026quot;, \u0026quot;ec2:ModifyVpcAttribute\u0026quot;, \u0026quot;ec2:ReleaseAddress\u0026quot;, \u0026quot;ec2:AuthorizeSecurityGroupEgress\u0026quot;, \u0026quot;ec2:DeleteLaunchTemplate\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DeleteRoute\u0026quot;, \u0026quot;ec2:DescribeLaunchTemplateVersions\u0026quot;, \u0026quot;ec2:DescribeNatGateways\u0026quot;, \u0026quot;ec2:AllocateAddress\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;autoscaling:CreateLaunchConfiguration\u0026quot;, \u0026quot;ec2:DescribeImages\u0026quot;, \u0026quot;ec2:CreateLaunchTemplate\u0026quot;, \u0026quot;autoscaling:DeleteLaunchConfiguration\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } C√°ch t·∫°o 1 cluster b·∫±ng eksctl Create a file cluster.yaml\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: base-project region: us-east-1 availabilityZones: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1d\u0026quot;] nodeGroups: - name: nodegrp-1 instanceType: t2.medium desiredCapacity: 1 Apply above config file to create cluster:\neksctl create cluster -f cluster.yaml ch·ªù kho·∫£ng 10 ph√∫t v√¨ eks s·∫Ω provision ra network r·∫•t t·ªën time:\nmore sample template https://github.com/weaveworks/eksctl/blob/master/examples\nGet pods, service, nodes, is running\nkubectl get pods,svc,node -A [ec2-user@ip-172-31-84-250 ~]$ kubectl get pods,svc,node -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/aws-node-6k2qq 1/1 Running 0 3h28m kube-system pod/coredns-8455f84f99-rzn44 1/1 Running 0 3h34m kube-system pod/coredns-8455f84f99-xw2rv 1/1 Running 0 3h34m kube-system pod/kube-proxy-w2t7v 1/1 Running 0 3h28m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 3h34m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 3h34m NAMESPACE NAME STATUS ROLES AGE VERSION node/ip-192-168-62-230.ec2.internal Ready \u0026lt;none\u0026gt; 3h28m v1.14.7-eks-1861c5 Run app micro-service on k8s clone source v·ªÅ\ncd ~ git clone https://github.com/hoangmnsd/k8s-mastery t·∫°o pod frontend:\ncd /home/ec2-user/k8s-mastery/resource-manifests kubectl apply -f sa-frontend-deployment.yaml sau khi apply th√¨ pods v√† services, node nh∆∞ sau:\nip-172-31-84-250 ~]$ kubectl get pods,svc,node -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/sa-frontend-54789d8b7d-2whc4 1/1 Running 0 2m36s default pod/sa-frontend-54789d8b7d-dbh4p 1/1 Running 0 2m36s kube-system pod/aws-node-7t2df 1/1 Running 0 9m5s kube-system pod/coredns-8455f84f99-w8dlf 1/1 Running 0 15m kube-system pod/coredns-8455f84f99-xqk2f 1/1 Running 0 15m kube-system pod/kube-proxy-xzhwd 1/1 Running 0 9m5s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 15m kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 15m NAMESPACE NAME STATUS ROLES AGE VERSION node/ip-192-168-34-208.ec2.internal Ready \u0026lt;none\u0026gt; 9m5s v1.14.7-eks-1861c5 ip-172-31-84-250 ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 483M 60K 483M 1% /dev tmpfs 493M 0 493M 0% /dev/shm /dev/xvda1 7.9G 2.8G 5.0G 36% / t·∫°o service frondend:\nkubectl create -f service-sa-frontend-lb.yaml get services:\nkubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 21m default sa-frontend-lb LoadBalancer 10.100.64.73 a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com 80:31360/TCP 102s kube-system kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 21m check frontend app tr√™n LB URL: a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com\nT·∫°o pod v√† service c·ªßa backend logic:\ncd /home/ec2-user/k8s-mastery/resource-manifests kubectl apply -f sa-logic-deployment.yaml --record kubectl apply -f service-sa-logic.yaml get pods, svc:\nip-172-31-84-250 resource-manifests]$ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/sa-frontend-54789d8b7d-2whc4 1/1 Running 0 19m default pod/sa-frontend-54789d8b7d-dbh4p 1/1 Running 0 19m default pod/sa-logic-7d7ff8f6dc-r9tcz 1/1 Running 0 118s default pod/sa-logic-7d7ff8f6dc-zhwv2 1/1 Running 0 118s kube-system pod/aws-node-7t2df 1/1 Running 0 25m kube-system pod/coredns-8455f84f99-w8dlf 1/1 Running 0 31m kube-system pod/coredns-8455f84f99-xqk2f 1/1 Running 0 31m kube-system pod/kube-proxy-xzhwd 1/1 Running 0 25m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 31m default service/sa-frontend-lb LoadBalancer 10.100.64.73 a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com 80:31360/TCP 11m default service/sa-logic ClusterIP 10.100.97.45 \u0026lt;none\u0026gt; 80/TCP 4s kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 31m T·∫°o pod v√† service c·ªßa backend webapp:\ncd /home/ec2-user/k8s-mastery/resource-manifests kubectl apply -f sa-web-app-deployment.yaml --record kubectl apply -f service-sa-web-app-lb.yaml get pods,svc:\nip-172-31-84-250 resource-manifests]$ kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/sa-frontend-54789d8b7d-2whc4 1/1 Running 0 21m default pod/sa-frontend-54789d8b7d-dbh4p 1/1 Running 0 21m default pod/sa-logic-7d7ff8f6dc-r9tcz 1/1 Running 0 4m8s default pod/sa-logic-7d7ff8f6dc-zhwv2 1/1 Running 0 4m8s default pod/sa-web-app-5f7d8fd94d-lbzj8 1/1 Running 0 27s default pod/sa-web-app-5f7d8fd94d-ql5k2 1/1 Running 0 27s kube-system pod/aws-node-7t2df 1/1 Running 0 27m kube-system pod/coredns-8455f84f99-w8dlf 1/1 Running 0 33m kube-system pod/coredns-8455f84f99-xqk2f 1/1 Running 0 33m kube-system pod/kube-proxy-xzhwd 1/1 Running 0 27m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 33m default service/sa-frontend-lb LoadBalancer 10.100.64.73 a40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com 80:31360/TCP 13m default service/sa-logic ClusterIP 10.100.97.45 \u0026lt;none\u0026gt; 80/TCP 2m14s default service/sa-web-app-lb LoadBalancer 10.100.34.198 a289bb56f061a11ea86df120e903ee59-714828784.us-east-1.elb.amazonaws.com 80:32321/TCP 5s kube-system service/kube-dns ClusterIP 10.100.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 33m s·ª≠a c√°i App.js ƒë·ªÉ n√≥ fetch URL c·ªßa sa-webapp\nnano ~/k8s-mastery/sa-frontend/src/App.js s·ª≠a nh∆∞ sau (d√πng ELB URL c·ªßa sa-webapp):\nfetch('http://a7cf3208e067911ea90de1247d6da376-1135942870.us-east-1.elb.amazonaws.com/sentiment', { build docker images v√† push l·∫°i l√™n Docker Hub:\nnpm run build sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:28 . sudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend:28 s·ª≠a deployment config ƒë·ªÉ d√πng image m·ªõi tag:28\nnano ~/k8s-mastery/resource-manifests/sa-frontend-deployment-update.yaml s·ª≠a ch·ªó image th√†nh d√πng b·∫£n tag :28,\nc·∫ßn ch√∫ √Ω ch·ªó hoangmnsd ƒë√≥ l√† account Docker Hub c·ªßa m√¨nh, b·∫°n c·∫ßn d√πng account Docker Hub c·ªßa b·∫°n, v√≠ d·ª• l√† CCCCCC m√† b·∫°n ƒë√£ setting ·ªü tr√™n\nimage: hoangmnsd/sentiment-analysis-frontend:28 apply l·∫°i:\nkubectl apply -f sa-frontend-deployment-update.yaml --record check l·∫°i app ƒë·ªÉ th√¥ng lu·ªìng t·ª´ frontend -\u0026gt; backend:\na40ec71f8061811ea91b70a830ca64b1-169405131.us-east-1.elb.amazonaws.com\nSSH v√†o Node trong Cluster N·∫øu mu·ªën c√≥ th·ªÉ ssh v√†o Node, tr∆∞·ªõc ti√™n c·∫ßn t·∫°o ssh key ssh-keygen:\nip-172-31-84-250 ~]$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/ec2-user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ec2-user/.ssh/id_rsa. Your public key has been saved in /home/ec2-user/.ssh/id_rsa.pub. khi t·∫°o cluster c·∫ßn t·∫°o v·ªõi file config nh∆∞ sau:\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: base-project region: us-east-1 availabilityZones: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1d\u0026quot;] nodeGroups: - name: nodegrp-1 instanceType: t2.medium desiredCapacity: 1 ssh: # import public key from file publicKeyPath: /home/ec2-user/.ssh/id_rsa.pub Khi ssh v√†o th√¨ d√πng command sau:\nssh -i ~/.ssh/id_rsa ec2-user@\u0026lt;EC2-PUBLIC-IP\u0026gt; T·∫°o k8s dashboard Do m√¨nh d√πng b·∫£n recommend b·ªã ko v√†o dc t·ª´ windows browser, n√™n d√πng b·∫£n alternative:\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/alternative.yaml t·∫°o serviceaccount:\nkubectl create serviceaccount my-dashboard-sa -n kubernetes-dashboard t·∫°o clusterrolebinding:\nkubectl create clusterrolebinding my-dashboard-sa \\ --clusterrole=cluster-admin \\ --serviceaccount=kubernetes-dashboard:my-dashboard-sa get secret:\nkubectl get secrets -n kubernetes-dashboard L·∫•y token:\nkubectl describe secret -n kubernetes-dashboard \u0026lt;TOKEN_NAME\u0026gt; d√πng extension REQUESTLY c·ªßa Chrome, link c√†i:\nhttps://chrome.google.com/webstore/detail/requestly-redirect-url-mo/mdnleldcmiljblolnjhpnblkcekpdkpa config nh∆∞ h√¨nh sau, m·ªói khi v√†o c√°i IP kia th√¨ n√≥ s·∫Ω t·ª± ƒë·ªông modify Header v√† add th√™m TOKEN v√†o cho m√¨nh\nport-forward ƒë·ªÉ b√™n ngo√†i c√≥ th·ªÉ access:\nkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 0.0.0.0 Done! B·∫°n s·∫Ω v√†o ƒë∆∞·ª£c k8s Dashboard t·ª´ Chrome browser tr√™n windows v·ªõi link http://\u0026lt;EC-PUBLIC-IP\u0026gt;:9090\nL·ªói c√≥ th·ªÉ g·∫∑p Sau khi install helm v√† app spring postgres,\nsau ƒë√≥ th√™m k8s dashboard th√¨ b·ªã l·ªói POD m·ªõi t·∫°o lu√¥n ·ªü tr·∫°ng th√°i ContainerCreating\nxem logs:\nkubectl describe pod \u0026lt;pod_name\u0026gt; th√¨ ph√°t hi·ªán l·ªói network: add cmd: failed to assign an IP address to container\ngoogle 1 l√∫c th√¨ th·∫•y (https://github.com/aws/amazon-vpc-cni-k8s/issues/59),\nnguy√™n nh√¢n c√≥ th·ªÉ do thi·∫øu IP trong ENI\n\u0026ldquo;After checking AWS documentation, it seems that there is an IP limit per instance: 18 for t3.medium, 36 for t3.large\u0026rdquo;\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\n","href":"/posts/k8s-iii-using-eksctl-on-amazon-linux/","title":"K8S 3: Using eksctl on Amazon Linux EC2"},{"content":"Gi·ªõi thi·ªáu ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh ƒëang ·ªü c√°ch 3, v√† h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ access v√†o k8s dashboard\nMu·ªën access v√†o k8s dashboard th√¨ b·∫°n c√≥ nhi·ªÅu c√°ch, b√†i n√†y s·∫Ω ƒë∆∞a ra v√†i c√°ch, theo flow m√† m√¨nh ƒë√£ t√¨m hi·ªÉu trong nh·ªØng ng√†y ƒë·∫ßu v·ªçc k8s n√†y\nChu·∫©n b·ªã Tr∆∞·ªõc ti√™n th√¨ gi·ªëng nh∆∞ b√†i tr∆∞·ªõc, ta c·∫ßn chu·∫©n b·ªã AWS EC2 Ubuntu 18.04 LTS, t2.medium\ninstall docker\nsudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install docker.io -y t·∫°o bi·∫øn m√¥i tr∆∞·ªùng truy·ªÅn v√†o th√¥ng tin c·ªßa account Docker Hub c·ªßa b·∫°n\nexport DOCKER_USERNAME=AAAABBBB export DOCKER_PASSWORD=CCCCDDDD export DOCKER_USER_ID=AAAABBBB Login v√†o Docker Hub v·ªõi account v√† password c·ªßa b·∫°n\nsudo docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; install kubectl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl install minikube\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ check minikube version:\nubuntu@ip-172-31-17-59:~$ minikube version minikube version: v1.5.2 commit: 792dbf92a1de583fcee76f8791cff12e0c9440ad-dirty Start minikube l√™n, t·ª´ ƒë√¢y tr·ªü xu·ªëng s·∫Ω switch sang user root\nsudo -i minikube start --vm-driver=none Check status c·ªßa minikube\nroot@ip-172-31-16-165:~# minikube status host: Running kubelet: Running apiserver: Running kubeconfig: Configured Tr·∫°ng th√°i khi v·ª´a install minikube xong s·∫Ω nh∆∞ sau:\nip-172-31-80-166:/home/ubuntu/k8s-mastery# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-4vtz4 1/1 Running 0 115s kube-system pod/coredns-5644d7b6d9-svxqw 1/1 Running 0 115s kube-system pod/etcd-minikube 1/1 Running 0 57s kube-system pod/kube-addon-manager-minikube 1/1 Running 0 42s kube-system pod/kube-apiserver-minikube 1/1 Running 0 54s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 48s kube-system pod/kube-proxy-2bw77 1/1 Running 0 115s kube-system pod/kube-scheduler-minikube 1/1 Running 0 66s kube-system pod/storage-provisioner 1/1 Running 0 112s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2m4s kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 2m2s root@ip-172-31-80-166:/home/ubuntu/k8s-mastery# minikube service list |-------------|------------|--------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|------------|--------------|-----| | default | kubernetes | No node port | | kube-system | kube-dns | No node port | |-------------|------------|--------------|-----| ƒê·∫øn ƒë√¢y, m·ªçi th·ª© ƒë√£ gi·ªëng v·ªõi b√†i tr∆∞·ªõc, mu·ªën access v√†o k8s dashboard c√≥ 2 c√°ch:\nC√°ch 1: D√πng minikube dashboard C√¢u l·ªánh minikube dashboard s·∫Ω t·∫°o ra 2 namespace m·ªõi nh∆∞ sau:\n(qu√° tr√¨nh ch·∫°y c√≥ th·ªÉ ph·∫£i ch·ªù 1 l√∫c ko th·∫•y l·ªói g√¨ ·ªü ph·∫ßn Verifying proxy health ‚Ä¶ th√¨ c√≥ th·ªÉ Ctrl+C lu√¥n)\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-4vtz4 1/1 Running 0 9m55s kube-system pod/coredns-5644d7b6d9-svxqw 1/1 Running 0 9m55s kube-system pod/etcd-minikube 1/1 Running 0 8m57s kube-system pod/kube-addon-manager-minikube 1/1 Running 0 8m42s kube-system pod/kube-apiserver-minikube 1/1 Running 0 8m54s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 8m48s kube-system pod/kube-proxy-2bw77 1/1 Running 0 9m55s kube-system pod/kube-scheduler-minikube 1/1 Running 0 9m6s kube-system pod/storage-provisioner 1/1 Running 0 9m52s kubernetes-dashboard pod/dashboard-metrics-scraper-76585494d8-n67wm 1/1 Running 0 4m4s kubernetes-dashboard pod/kubernetes-dashboard-57f4cb4545-xzdfd 1/1 Running 0 4m4s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 10m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 10m kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.98.238.240 \u0026lt;none\u0026gt; 8000/TCP 4m10s kubernetes-dashboard service/kubernetes-dashboard ClusterIP 10.102.179.238 \u0026lt;none\u0026gt; 80/TCP 4m10s Sau ƒë√≥ d√πng port-forward ƒë·ªÉ m·ªü port 9090\nkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 0.0.0.0 N·∫øu port-forward b·ªã l·ªói socat th√¨ c·∫ßn install socat apt-get install -y socat n·ªØa, sau ƒë√≥ ch·∫°y l·∫°i command port-forward b√™n tr√™n\nT·ª´ ƒë√¢y s·∫Ω v√†o dc dashboard b√¨nh th∆∞·ªùng http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:9090/,\nnh∆∞ v·∫≠y l√† d√πng minikube dashboard s·∫Ω t·∫°o dashboard r·∫•t nhanh\nC≈©ng ko c·∫ßn t·∫°o admin-user hay ClusterRoleBinding g√¨ c·∫£,\nTuy nhi√™n c√°ch n√†y ko secure l·∫Øm,\nv√¨ c√°ch n√†y, trong serivce dashboard c·ªßa minikube ko th·ªÉ x√≥a c√°i flag \u0026ndash;enable-skip-login ƒë∆∞·ª£c\nc√°c b·∫°n c√≥ th·ªÉ th·ª≠ b·∫±ng command sau:\nkubectl edit deployment kubernetes-dashboard -n kubernetes-dashboard  containers: - args: - --namespace=kubernetes-dashboard - --enable-skip-login # \u0026lt;----x√≥a 2 d√≤ng n√†y hay 1 d√≤ng ƒë·ªÅu ko ƒÉn thua - --disable-settings-authorizer # \u0026lt;----x√≥a 2 d√≤ng n√†y hay 1 d√≤ng ƒë·ªÅu ko ƒÉn thua C·ª© x√≥a ƒëi n√≥ l·∫°i t·ª± t·∫°o l·∫°i\nx√≥a ƒëi r·ªìi th√¨ khi port-forward th√¨ v·∫´n v√†o dc dashoboard b√¨nh th∆∞·ªùng, kh√¥ng c√≥ secure g√¨ c·∫£\nC√≤n 1 ƒëi·ªÉm n·ªØa l√†: ban ƒë·∫ßu m√¨nh t∆∞·ªüng c√≥ th·ªÉ ch·ªâ ƒë·ªãnh 1 IP ƒë·ªÉ open ·ªü ch·ªó --address 0.0.0.0,\nnh∆∞ng th·ª±c ra l√† ko th·ªÉ ch·ªâ ƒë·ªãnh IP ƒë∆∞·ª£c, b·∫Øt bu·ªôc ph·∫£i --address 0.0.0.0\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 219.117.237.243 Unable to listen on port 9090: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 219.117.237.243:9090: bind: cannot assign requested address] error: unable to listen on any of the requested ports: [{9090 9090}] ip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 219.117.237.243/32 error: 219.117.237.243/32 is not a valid IP Trong c√°ch 1 n√†y th√¨ b·∫°n c√≥ th·ªÉ expose dashboard ra public b·∫±ng c√°ch d√πng port-forward ho·∫∑c d√πng service type NodePort\nM√¨nh v·ª´a gi·ªõi thi·ªáu ·ªü ph√≠a tr√™n l√† c√°ch port-forward\nC√≤n n·∫øu d√πng c√°ch NodePort, ch·ªâ n√™n d√πng cho m√¥i tr∆∞·ªùng develop (theo link sau n√≥i v·∫≠y)\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md\nD√πng command sau s·ª≠a type c·ªßa Service t∆∞ ClusterIP th√†nh NodePort\nkubectl -n kubernetes-dashboard edit service kubernetes-dashboard Nh∆∞ log sau b·∫°n c√≥ th·ªÉ th·∫•y Service ƒë√£ thay ƒë·ªïi th√†nh NodePort\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-4vtz4 1/1 Running 0 30m kube-system pod/coredns-5644d7b6d9-svxqw 1/1 Running 0 30m kube-system pod/etcd-minikube 1/1 Running 0 29m kube-system pod/kube-addon-manager-minikube 1/1 Running 0 28m kube-system pod/kube-apiserver-minikube 1/1 Running 0 29m kube-system pod/kube-controller-manager-minikube 1/1 Running 0 29m kube-system pod/kube-proxy-2bw77 1/1 Running 0 30m kube-system pod/kube-scheduler-minikube 1/1 Running 0 29m kube-system pod/storage-provisioner 1/1 Running 0 30m kubernetes-dashboard pod/dashboard-metrics-scraper-76585494d8-n67wm 1/1 Running 0 24m kubernetes-dashboard pod/kubernetes-dashboard-57f4cb4545-xzdfd 1/1 Running 0 24m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 30m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 30m kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.98.238.240 \u0026lt;none\u0026gt; 8000/TCP 24m kubernetes-dashboard service/kubernetes-dashboard NodePort 10.102.179.238 \u0026lt;none\u0026gt; 80:32536/TCP 24m root@ip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# minikube service list |----------------------|---------------------------|----------------------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |----------------------|---------------------------|----------------------------|-----| | default | kubernetes | No node port | | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | http://172.31.80.166:32536 | |----------------------|---------------------------|----------------------------|-----| ƒê·∫øn ƒë√¢y th√¨ s·∫Ω v√†o dc dashboard ·ªü link http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:32536\nTuy nhi√™n c·∫£ c√°ch NodePort n√†y c≈©ng ko secure\nN√≥ open to the world, ai c≈©ng c√≥ th·ªÉ access ƒë∆∞·ª£c\nN·∫øu mu·ªën chuy·ªÉn l·∫°i t·ª´ NodePort v·ªÅ ClusterIP\nth√¨ b·∫°n c·∫ßn ph·∫£i x√≥a h·∫≥n c√°i service ƒë√≥ ƒëi, minikube s·∫Ω t·ª± ƒë·ªông t·∫°o l·∫°i 1 c√°i service m·ªõi m·∫∑c ƒë·ªãnh l√† ClusterIP\ndelete c√°i svc ƒëang config NodePort ƒëi b·∫±ng command sau:\nkubectl delete svc kubernetes-dashboard -n kubernetes-dashboard log nh∆∞ sau:\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 46m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 46m kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.98.238.240 \u0026lt;none\u0026gt; 8000/TCP 40m kubernetes-dashboard kubernetes-dashboard ClusterIP 10.99.59.17 \u0026lt;none\u0026gt; 80/TCP 25s =\u0026raquo; V·∫≠y l√† c√°ch d√πng minikube t·∫°o dashboard ch·ªâ c√≥ th·∫ø th√¥i, c√≥ ∆∞u v√† nh∆∞·ª£c nh∆∞ v·∫≠y, m√¨nh kh√¥ng khuy√™n d√πng c√°ch 1 v√¨ ko t√πy bi·∫øn ƒë∆∞·ª£c nhi·ªÅu\nGi·ªù n·∫øu mu·ªën X√≥a c√°i minikube dashboard ƒëi th√¨ ch·ªâ c√≥ c√°ch l√† x√≥a h·∫≥n cluster vm minikube ƒëi th√¥i, bu·ªìn l√† v·∫≠y:\nminikube stop; minikube delete docker stop (docker ps -aq) rm -r ~/.kube ~/.minikube sudo rm /usr/local/bin/localkube /usr/local/bin/minikube systemctl stop \u0026#39;*kubelet*.mount\u0026#39; sudo rm -rf /etc/kubernetes/ docker system prune -af --volumes Sau ƒë√≥ install l·∫°i kubectl v√† minikube nh∆∞ ph√≠a tr√™n c√πng nh√©\nSau ƒë√≥ v√†o start l·∫°i th√¨ s·∫Ω th·∫•y m·∫•t c√°i service dashboard\nC√°ch 2: T·∫°o Dashboard t·ª´ file YAML C√°ch n√†y l√† t·∫°o Dashboard from scratch lu√¥n, b·∫°n s·∫Ω c√≥ th·ªÉ t√πy ch·ªânh ƒë∆∞·ª£c nhi·ªÅu h∆°n\ntr∆∞·ªõc khi l√†m c√°ch n√†y th√¨ n√™n x√≥a minikube tri·ªát ƒë·ªÉ ƒëi, r·ªìi t·∫°o l·∫°i nh√©\nth·ª≠ c√°i YAML th·ª© 1 Recommended:\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml N√≥ s·∫Ω t·∫°o dashboard trong namespace kube-system\nch√∫ √Ω l√† n·∫øu c√≥ s·ª≠a file ƒë·ªÉ thay ƒë·ªïi namespace th√¨ c≈©ng s·∫Ω apply fail th√¥i, n√™n ko n√™n s·ª≠a l√†m g√¨, m√¨nh ƒë√£ th·ª≠ r·ªìi\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-m8ltq 1/1 Running 0 44m kube-system pod/coredns-5644d7b6d9-qqhqm 1/1 Running 0 44m kube-system pod/etcd-minikube 1/1 Running 0 43m kube-system pod/kube-addon-manager-minikube 1/1 Running 0 43m kube-system pod/kube-apiserver-minikube 1/1 Running 0 43m kube-system pod/kube-controller-manager-minikube 1/1 Running 0 43m kube-system pod/kube-proxy-mz6ns 1/1 Running 0 44m kube-system pod/kube-scheduler-minikube 1/1 Running 0 43m kube-system pod/kubernetes-dashboard-7c54d59f66-2k6cx 1/1 Running 0 30s kube-system pod/storage-provisioner 1/1 Running 0 44m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 44m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 44m kube-system service/kubernetes-dashboard ClusterIP 10.111.180.197 \u0026lt;none\u0026gt; 443/TCP 30s Gi·ªù s·∫Ω d√πng port-forward\nkubectl port-forward -n kube-system service/kubernetes-dashboard 8443:443 --address 0.0.0.0 nh∆∞ng s·∫Ω b·ªã l·ªói:\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl port-forward -n kube-system service/kubernetes-dashboard 8443:443 --address 0.0.0.0 Unable to listen on port 8443: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 0.0.0.0:8443: bind: address already in use] error: unable to listen on any of the requested ports: [{8443 8443}] Do service m·∫∑c ƒë·ªãnh ƒëang ƒë·ªÉ 8443:443 n√™n ph·∫£i s·ª≠a th√†nh port kh√°c, 8443 th√†nh 6443 ch·∫≥ng h·∫°n, b·∫±ng command sau:\nkubectl edit service kubernetes-dashboard -n kube-system R·ªìi l·∫°i port-forward l·∫ßn n·ªØa:\nkubectl port-forward -n kube-system service/kubernetes-dashboard 6443:443 --address 0.0.0.0 tuy nhi√™n khi v√†o tr√¨nh duy·ªát ·ªü IP: http://54.209.126.239:6443/ th√¨ v·∫´n ko ƒëc, terminal c√≥ l·ªói sau:\nE1111 14:10:55.337284 7357 portforward.go:400] an error occurred forwarding 6443 -\u0026gt; 6443: error forwarding port 6443 to pod 230ca1a80532352a973c7da2136127da565232b93f98e76de1a60e0d63d5610e, uid : exit status 1: 2019/11/11 14:10:55 socat[8210] E connect(5, AF=2 127.0.0.1:6443, 16): Connection refused Ch·ªãu lu√¥n ko th·ªÉ fix ƒë∆∞·ª£c l·ªói connection refused n√†y\nTh·∫ø l√† m√¨nh ƒë√£ t·ª´ b·ªè v√† chuy·ªÉn sang c√°i YAML th·ª© 2 Alternative\ntr∆∞·ªõc ti√™n ph·∫£i delete c√°i config c≈© ƒëi\nkubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml T·∫°o c√°i dashboard b·∫±ng fiel YAML 2:\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/alternative.yaml YAML n√†y s·∫Ω t·∫°o ra dashboard ·ªü trong namespace m·ªõi l√† kubernetes-dashboard\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get svc,pods -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 68m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 68m kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.108.238.95 \u0026lt;none\u0026gt; 8000/TCP 16s kubernetes-dashboard service/kubernetes-dashboard ClusterIP 10.101.65.242 \u0026lt;none\u0026gt; 80/TCP 16s NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-m8ltq 1/1 Running 0 68m kube-system pod/coredns-5644d7b6d9-qqhqm 1/1 Running 0 68m kube-system pod/etcd-minikube 1/1 Running 0 67m kube-system pod/kube-addon-manager-minikube 1/1 Running 0 67m kube-system pod/kube-apiserver-minikube 1/1 Running 0 67m kube-system pod/kube-controller-manager-minikube 1/1 Running 0 67m kube-system pod/kube-proxy-mz6ns 1/1 Running 0 68m kube-system pod/kube-scheduler-minikube 1/1 Running 0 67m kube-system pod/storage-provisioner 1/1 Running 0 68m kubernetes-dashboard pod/dashboard-metrics-scraper-76585494d8-5b5w9 1/1 Running 0 16s kubernetes-dashboard pod/kubernetes-dashboard-565564f797-sb2qg 1/1 Running 0 16s sau ƒë√≥ th·ª≠ port-forward ƒë·ªÉ xem c√≥ login ƒë∆∞·ª£c kh√¥ng\nkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 0.0.0.0 fix b·∫±ng c√°ch sau:\nkubectl create serviceaccount my-dashboard-sa -n kubernetes-dashboard kubectl create clusterrolebinding my-dashboard-sa \\ --clusterrole=cluster-admin \\ --serviceaccount=kubernetes-dashboard:my-dashboard-sa kubectl get secrets -n kubernetes-dashboard ip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get secrets -n kubernetes-dashboard NAME TYPE DATA AGE default-token-w9sk5 kubernetes.io/service-account-token 3 8m40s kubernetes-dashboard-csrf Opaque 1 8m40s kubernetes-dashboard-key-holder Opaque 2 8m40s kubernetes-dashboard-token-d7tt4 kubernetes.io/service-account-token 3 8m40s my-dashboard-sa-token-tms9g kubernetes.io/service-account-token 3 15s l·∫•y TOKEN b·∫±ng command sau:\nkubectl describe secret -n kubernetes-dashboard my-dashboard-sa-token-tms9g d√πng extension REQUESTLY c·ªßa Chrome, link c√†i:\nhttps://chrome.google.com/webstore/detail/requestly-redirect-url-mo/mdnleldcmiljblolnjhpnblkcekpdkpa config nh∆∞ h√¨nh sau, m·ªói khi v√†o c√°i IP kia th√¨ n√≥ s·∫Ω t·ª± ƒë·ªông modify Header v√† add th√™m TOKEN v√†o cho m√¨nh\nsau ƒë√≥ port-forward th√¨ s·∫Ω t·ª± ƒë·ªông login dc v√†o k8s dashboard\nTuy nhi√™n v√†o dashboard b·ªã l·ªói secrets is forbidden: User \u0026quot;system:serviceaccount:kubernetes-dashboard:kubernetes-dashboard\u0026quot; cannot list resource \u0026quot;secrets\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;default\u0026quot;\t\nnh∆∞ng ph·∫£i s·ª≠a file YAML ClusterRole nh∆∞ sau m·ªõi full quy·ªÅn:\nnano dashboard-cluster-role-test.yaml # Â¢ûÂä†‰∏Ä‰∫õÊÉ≥Ë¶ÅÂú®dashboard‰∏äÊìç‰ΩúÁöÑË≥áÊ∫ê kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;metrics.k8s.io\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;,\u0026quot;pods\u0026quot;, \u0026quot;nodes\u0026quot;,\u0026quot;namespaces\u0026quot;,\u0026quot;secrets\u0026quot;,\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;apps\u0026quot;] resources: [\u0026quot;statefulsets\u0026quot;,\u0026quot;replicationcontrollers\u0026quot;, \u0026quot;ingresses\u0026quot;,\u0026quot;services\u0026quot;,\u0026quot;daemonsets\u0026quot;,\u0026quot;configmaps\u0026quot;,\u0026quot;pods\u0026quot;, \u0026quot;nodes\u0026quot;,\u0026quot;namespaces\u0026quot;,\u0026quot;secrets\u0026quot;,\u0026quot;persistentvolumeclaims\u0026quot;,\u0026quot;replicasets\u0026quot;,\u0026quot;deployments\u0026quot;,\u0026quot;events\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;extensions\u0026quot;] resources: [\u0026quot;ingresses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;batch\u0026quot;] resources: [\u0026quot;jobs\u0026quot;, \u0026quot;cronjobs\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;, \u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;rbac.authorization.k8s.io\u0026quot;] resources: [\u0026quot;clusterroles\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] Sau ƒë√≥ apply file ClusterRole v·ª´a t·∫°o:\nkubectl apply -f dashboard-cluster-role-test.yaml B·∫°n c√≥ th·ªÉ s·ª≠a file alternative.yaml, th√™m c√°i n·ªôi dung b√™n tr√™n v√†o file th√¨ khi apply ch·ªâ c·∫ßn apply file alternative.yaml th√¥i l√† ƒë·ªß\nTuy·ªát v·ªùi!!, v·∫≠y l√† ƒë√£ secure dc k8s dashboard b·∫±ng ServiceAccount Token: Authorization Bearer Token\nC√°ch n√†y hay ·ªü ch·ªó b·∫°n c√≥ th·ªÉ t·∫°o nhi·ªÅu service account ƒë·ªÉ gi·ªõi h·∫°n quy·ªÅn c·ªßa user\nTuy nhi√™n l·∫°i y√™u c·∫ßu User ph·∫£i d√πng extension ƒë·ªÉ modify Header b·∫±ng TOKEN\nC√≤n 1 c√°ch n·ªØa l√† D√πng oauth2_proxy\nC√°ch n√†y th√¨ \u0026ldquo;best secure\u0026rdquo;\nhttps://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca\nC√°ch n√†y s·∫Ω b·∫Øt User authen qua 1 b√™n th·ª© 3 ·ªü ƒë√¢y l√† GitHub\nth√¨ User ko c·∫ßn ph·∫£i modify Header b·∫±ng Requestly nh∆∞ tr√™n (c√≥ th·ªùi gian m√¨nh s·∫Ω l√†m)\nREFERENCES:\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/installation.md\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md\nhttps://kknews.cc/code/56mvxx3.html\nhttps://chrome.google.com/webstore/detail/requestly-redirect-url-mo/mdnleldcmiljblolnjhpnblkcekpdkpa\nhttps://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca\nhttps://unofficialism.info/posts/accessing-rbac-enabled-kubernetes-dashboard/\n","href":"/bk/k8s-ii-how-to-access-dashboard-on-aws-ec2-ubuntu/","title":"K8S 2: How to Access K8s Dashboard On Aws Ec2 Ubuntu"},{"content":"Gi·ªõi thi·ªáu ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh ƒëang ·ªü c√°ch 3, v√† h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ access v√†o k8s dashboard\nMu·ªën access v√†o k8s dashboard th√¨ b·∫°n c√≥ nhi·ªÅu c√°ch, b√†i n√†y s·∫Ω ƒë∆∞a ra v√†i c√°ch, theo flow m√† m√¨nh ƒë√£ t√¨m hi·ªÉu trong nh·ªØng ng√†y ƒë·∫ßu v·ªçc k8s n√†y\nChu·∫©n b·ªã Tr∆∞·ªõc ti√™n th√¨ gi·ªëng nh∆∞ b√†i tr∆∞·ªõc, ta c·∫ßn chu·∫©n b·ªã AWS EC2 Ubuntu 18.04 LTS, t2.medium\ninstall docker\nsudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install docker.io -y t·∫°o bi·∫øn m√¥i tr∆∞·ªùng truy·ªÅn v√†o th√¥ng tin c·ªßa account Docker Hub c·ªßa b·∫°n\nexport DOCKER_USERNAME=AAAABBBB export DOCKER_PASSWORD=CCCCDDDD export DOCKER_USER_ID=AAAABBBB Login v√†o Docker Hub v·ªõi account v√† password c·ªßa b·∫°n\nsudo docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; install kubectl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl install minikube\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ check minikube version:\nubuntu@ip-172-31-17-59:~$ minikube version minikube version: v1.5.2 commit: 792dbf92a1de583fcee76f8791cff12e0c9440ad-dirty Start minikube l√™n, t·ª´ ƒë√¢y tr·ªü xu·ªëng s·∫Ω switch sang user root\nsudo -i minikube start --vm-driver=none Check status c·ªßa minikube\nroot@ip-172-31-16-165:~# minikube status host: Running kubelet: Running apiserver: Running kubeconfig: Configured Tr·∫°ng th√°i khi v·ª´a install minikube xong s·∫Ω nh∆∞ sau:\nip-172-31-80-166:/home/ubuntu/k8s-mastery# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-4vtz4 1/1 Running 0 115s kube-system pod/coredns-5644d7b6d9-svxqw 1/1 Running 0 115s kube-system pod/etcd-minikube 1/1 Running 0 57s kube-system pod/kube-addon-manager-minikube 1/1 Running 0 42s kube-system pod/kube-apiserver-minikube 1/1 Running 0 54s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 48s kube-system pod/kube-proxy-2bw77 1/1 Running 0 115s kube-system pod/kube-scheduler-minikube 1/1 Running 0 66s kube-system pod/storage-provisioner 1/1 Running 0 112s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2m4s kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 2m2s root@ip-172-31-80-166:/home/ubuntu/k8s-mastery# minikube service list |-------------|------------|--------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|------------|--------------|-----| | default | kubernetes | No node port | | kube-system | kube-dns | No node port | |-------------|------------|--------------|-----| ƒê·∫øn ƒë√¢y, m·ªçi th·ª© ƒë√£ gi·ªëng v·ªõi b√†i tr∆∞·ªõc, mu·ªën access v√†o k8s dashboard c√≥ 2 c√°ch:\nC√°ch 1: D√πng minikube dashboard C√¢u l·ªánh minikube dashboard s·∫Ω t·∫°o ra 2 namespace m·ªõi nh∆∞ sau:\n(qu√° tr√¨nh ch·∫°y c√≥ th·ªÉ ph·∫£i ch·ªù 1 l√∫c ko th·∫•y l·ªói g√¨ ·ªü ph·∫ßn Verifying proxy health ‚Ä¶ th√¨ c√≥ th·ªÉ Ctrl+C lu√¥n)\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-4vtz4 1/1 Running 0 9m55s kube-system pod/coredns-5644d7b6d9-svxqw 1/1 Running 0 9m55s kube-system pod/etcd-minikube 1/1 Running 0 8m57s kube-system pod/kube-addon-manager-minikube 1/1 Running 0 8m42s kube-system pod/kube-apiserver-minikube 1/1 Running 0 8m54s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 8m48s kube-system pod/kube-proxy-2bw77 1/1 Running 0 9m55s kube-system pod/kube-scheduler-minikube 1/1 Running 0 9m6s kube-system pod/storage-provisioner 1/1 Running 0 9m52s kubernetes-dashboard pod/dashboard-metrics-scraper-76585494d8-n67wm 1/1 Running 0 4m4s kubernetes-dashboard pod/kubernetes-dashboard-57f4cb4545-xzdfd 1/1 Running 0 4m4s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 10m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 10m kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.98.238.240 \u0026lt;none\u0026gt; 8000/TCP 4m10s kubernetes-dashboard service/kubernetes-dashboard ClusterIP 10.102.179.238 \u0026lt;none\u0026gt; 80/TCP 4m10s Sau ƒë√≥ d√πng port-forward ƒë·ªÉ m·ªü port 9090\nkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 0.0.0.0 N·∫øu port-forward b·ªã l·ªói socat th√¨ c·∫ßn install socat apt-get install -y socat n·ªØa, sau ƒë√≥ ch·∫°y l·∫°i command port-forward b√™n tr√™n\nT·ª´ ƒë√¢y s·∫Ω v√†o dc dashboard b√¨nh th∆∞·ªùng http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:9090/,\nnh∆∞ v·∫≠y l√† d√πng minikube dashboard s·∫Ω t·∫°o dashboard r·∫•t nhanh\nC≈©ng ko c·∫ßn t·∫°o admin-user hay ClusterRoleBinding g√¨ c·∫£,\nTuy nhi√™n c√°ch n√†y ko secure l·∫Øm,\nv√¨ c√°ch n√†y, trong serivce dashboard c·ªßa minikube ko th·ªÉ x√≥a c√°i flag \u0026ndash;enable-skip-login ƒë∆∞·ª£c\nc√°c b·∫°n c√≥ th·ªÉ th·ª≠ b·∫±ng command sau:\nkubectl edit deployment kubernetes-dashboard -n kubernetes-dashboard  containers: - args: - --namespace=kubernetes-dashboard - --enable-skip-login # \u0026lt;----x√≥a 2 d√≤ng n√†y hay 1 d√≤ng ƒë·ªÅu ko ƒÉn thua - --disable-settings-authorizer # \u0026lt;----x√≥a 2 d√≤ng n√†y hay 1 d√≤ng ƒë·ªÅu ko ƒÉn thua C·ª© x√≥a ƒëi n√≥ l·∫°i t·ª± t·∫°o l·∫°i\nx√≥a ƒëi r·ªìi th√¨ khi port-forward th√¨ v·∫´n v√†o dc dashoboard b√¨nh th∆∞·ªùng, kh√¥ng c√≥ secure g√¨ c·∫£\nC√≤n 1 ƒëi·ªÉm n·ªØa l√†: ban ƒë·∫ßu m√¨nh t∆∞·ªüng c√≥ th·ªÉ ch·ªâ ƒë·ªãnh 1 IP ƒë·ªÉ open ·ªü ch·ªó --address 0.0.0.0,\nnh∆∞ng th·ª±c ra l√† ko th·ªÉ ch·ªâ ƒë·ªãnh IP ƒë∆∞·ª£c, b·∫Øt bu·ªôc ph·∫£i --address 0.0.0.0\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 219.117.237.243 Unable to listen on port 9090: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 219.117.237.243:9090: bind: cannot assign requested address] error: unable to listen on any of the requested ports: [{9090 9090}] ip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 219.117.237.243/32 error: 219.117.237.243/32 is not a valid IP Trong c√°ch 1 n√†y th√¨ b·∫°n c√≥ th·ªÉ expose dashboard ra public b·∫±ng c√°ch d√πng port-forward ho·∫∑c d√πng service type NodePort\nM√¨nh v·ª´a gi·ªõi thi·ªáu ·ªü ph√≠a tr√™n l√† c√°ch port-forward\nC√≤n n·∫øu d√πng c√°ch NodePort, ch·ªâ n√™n d√πng cho m√¥i tr∆∞·ªùng develop (theo link sau n√≥i v·∫≠y)\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md\nD√πng command sau s·ª≠a type c·ªßa Service t∆∞ ClusterIP th√†nh NodePort\nkubectl -n kubernetes-dashboard edit service kubernetes-dashboard Nh∆∞ log sau b·∫°n c√≥ th·ªÉ th·∫•y Service ƒë√£ thay ƒë·ªïi th√†nh NodePort\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-4vtz4 1/1 Running 0 30m kube-system pod/coredns-5644d7b6d9-svxqw 1/1 Running 0 30m kube-system pod/etcd-minikube 1/1 Running 0 29m kube-system pod/kube-addon-manager-minikube 1/1 Running 0 28m kube-system pod/kube-apiserver-minikube 1/1 Running 0 29m kube-system pod/kube-controller-manager-minikube 1/1 Running 0 29m kube-system pod/kube-proxy-2bw77 1/1 Running 0 30m kube-system pod/kube-scheduler-minikube 1/1 Running 0 29m kube-system pod/storage-provisioner 1/1 Running 0 30m kubernetes-dashboard pod/dashboard-metrics-scraper-76585494d8-n67wm 1/1 Running 0 24m kubernetes-dashboard pod/kubernetes-dashboard-57f4cb4545-xzdfd 1/1 Running 0 24m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 30m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 30m kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.98.238.240 \u0026lt;none\u0026gt; 8000/TCP 24m kubernetes-dashboard service/kubernetes-dashboard NodePort 10.102.179.238 \u0026lt;none\u0026gt; 80:32536/TCP 24m root@ip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# minikube service list |----------------------|---------------------------|----------------------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |----------------------|---------------------------|----------------------------|-----| | default | kubernetes | No node port | | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | http://172.31.80.166:32536 | |----------------------|---------------------------|----------------------------|-----| ƒê·∫øn ƒë√¢y th√¨ s·∫Ω v√†o dc dashboard ·ªü link http://\u0026lt;EC2-PUBLIC-IP\u0026gt;:32536\nTuy nhi√™n c·∫£ c√°ch NodePort n√†y c≈©ng ko secure\nN√≥ open to the world, ai c≈©ng c√≥ th·ªÉ access ƒë∆∞·ª£c\nN·∫øu mu·ªën chuy·ªÉn l·∫°i t·ª´ NodePort v·ªÅ ClusterIP\nth√¨ b·∫°n c·∫ßn ph·∫£i x√≥a h·∫≥n c√°i service ƒë√≥ ƒëi, minikube s·∫Ω t·ª± ƒë·ªông t·∫°o l·∫°i 1 c√°i service m·ªõi m·∫∑c ƒë·ªãnh l√† ClusterIP\ndelete c√°i svc ƒëang config NodePort ƒëi b·∫±ng command sau:\nkubectl delete svc kubernetes-dashboard -n kubernetes-dashboard log nh∆∞ sau:\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 46m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 46m kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.98.238.240 \u0026lt;none\u0026gt; 8000/TCP 40m kubernetes-dashboard kubernetes-dashboard ClusterIP 10.99.59.17 \u0026lt;none\u0026gt; 80/TCP 25s =\u0026raquo; V·∫≠y l√† c√°ch d√πng minikube t·∫°o dashboard ch·ªâ c√≥ th·∫ø th√¥i, c√≥ ∆∞u v√† nh∆∞·ª£c nh∆∞ v·∫≠y, m√¨nh kh√¥ng khuy√™n d√πng c√°ch 1 v√¨ ko t√πy bi·∫øn ƒë∆∞·ª£c nhi·ªÅu\nGi·ªù n·∫øu mu·ªën X√≥a c√°i minikube dashboard ƒëi th√¨ ch·ªâ c√≥ c√°ch l√† x√≥a h·∫≥n cluster vm minikube ƒëi th√¥i, bu·ªìn l√† v·∫≠y:\nminikube stop; minikube delete docker stop (docker ps -aq) rm -r ~/.kube ~/.minikube sudo rm /usr/local/bin/localkube /usr/local/bin/minikube systemctl stop \u0026#39;*kubelet*.mount\u0026#39; sudo rm -rf /etc/kubernetes/ docker system prune -af --volumes Sau ƒë√≥ install l·∫°i kubectl v√† minikube nh∆∞ ph√≠a tr√™n c√πng nh√©\nSau ƒë√≥ v√†o start l·∫°i th√¨ s·∫Ω th·∫•y m·∫•t c√°i service dashboard\nC√°ch 2: T·∫°o Dashboard t·ª´ file YAML C√°ch n√†y l√† t·∫°o Dashboard from scratch lu√¥n, b·∫°n s·∫Ω c√≥ th·ªÉ t√πy ch·ªânh ƒë∆∞·ª£c nhi·ªÅu h∆°n\ntr∆∞·ªõc khi l√†m c√°ch n√†y th√¨ n√™n x√≥a minikube tri·ªát ƒë·ªÉ ƒëi, r·ªìi t·∫°o l·∫°i nh√©\nth·ª≠ c√°i YAML th·ª© 1 Recommended:\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml N√≥ s·∫Ω t·∫°o dashboard trong namespace kube-system\nch√∫ √Ω l√† n·∫øu c√≥ s·ª≠a file ƒë·ªÉ thay ƒë·ªïi namespace th√¨ c≈©ng s·∫Ω apply fail th√¥i, n√™n ko n√™n s·ª≠a l√†m g√¨, m√¨nh ƒë√£ th·ª≠ r·ªìi\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get pods,svc -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-m8ltq 1/1 Running 0 44m kube-system pod/coredns-5644d7b6d9-qqhqm 1/1 Running 0 44m kube-system pod/etcd-minikube 1/1 Running 0 43m kube-system pod/kube-addon-manager-minikube 1/1 Running 0 43m kube-system pod/kube-apiserver-minikube 1/1 Running 0 43m kube-system pod/kube-controller-manager-minikube 1/1 Running 0 43m kube-system pod/kube-proxy-mz6ns 1/1 Running 0 44m kube-system pod/kube-scheduler-minikube 1/1 Running 0 43m kube-system pod/kubernetes-dashboard-7c54d59f66-2k6cx 1/1 Running 0 30s kube-system pod/storage-provisioner 1/1 Running 0 44m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 44m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 44m kube-system service/kubernetes-dashboard ClusterIP 10.111.180.197 \u0026lt;none\u0026gt; 443/TCP 30s Gi·ªù s·∫Ω d√πng port-forward\nkubectl port-forward -n kube-system service/kubernetes-dashboard 8443:443 --address 0.0.0.0 nh∆∞ng s·∫Ω b·ªã l·ªói:\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl port-forward -n kube-system service/kubernetes-dashboard 8443:443 --address 0.0.0.0 Unable to listen on port 8443: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 0.0.0.0:8443: bind: address already in use] error: unable to listen on any of the requested ports: [{8443 8443}] Do service m·∫∑c ƒë·ªãnh ƒëang ƒë·ªÉ 8443:443 n√™n ph·∫£i s·ª≠a th√†nh port kh√°c, 8443 th√†nh 6443 ch·∫≥ng h·∫°n, b·∫±ng command sau:\nkubectl edit service kubernetes-dashboard -n kube-system R·ªìi l·∫°i port-forward l·∫ßn n·ªØa:\nkubectl port-forward -n kube-system service/kubernetes-dashboard 6443:443 --address 0.0.0.0 tuy nhi√™n khi v√†o tr√¨nh duy·ªát ·ªü IP: http://54.209.126.239:6443/ th√¨ v·∫´n ko ƒëc, terminal c√≥ l·ªói sau:\nE1111 14:10:55.337284 7357 portforward.go:400] an error occurred forwarding 6443 -\u0026gt; 6443: error forwarding port 6443 to pod 230ca1a80532352a973c7da2136127da565232b93f98e76de1a60e0d63d5610e, uid : exit status 1: 2019/11/11 14:10:55 socat[8210] E connect(5, AF=2 127.0.0.1:6443, 16): Connection refused Ch·ªãu lu√¥n ko th·ªÉ fix ƒë∆∞·ª£c l·ªói connection refused n√†y\nTh·∫ø l√† m√¨nh ƒë√£ t·ª´ b·ªè v√† chuy·ªÉn sang c√°i YAML th·ª© 2 Alternative\ntr∆∞·ªõc ti√™n ph·∫£i delete c√°i config c≈© ƒëi\nkubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml T·∫°o c√°i dashboard b·∫±ng fiel YAML 2:\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/alternative.yaml YAML n√†y s·∫Ω t·∫°o ra dashboard ·ªü trong namespace m·ªõi l√† kubernetes-dashboard\nip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get svc,pods -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 68m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 68m kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.108.238.95 \u0026lt;none\u0026gt; 8000/TCP 16s kubernetes-dashboard service/kubernetes-dashboard ClusterIP 10.101.65.242 \u0026lt;none\u0026gt; 80/TCP 16s NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5644d7b6d9-m8ltq 1/1 Running 0 68m kube-system pod/coredns-5644d7b6d9-qqhqm 1/1 Running 0 68m kube-system pod/etcd-minikube 1/1 Running 0 67m kube-system pod/kube-addon-manager-minikube 1/1 Running 0 67m kube-system pod/kube-apiserver-minikube 1/1 Running 0 67m kube-system pod/kube-controller-manager-minikube 1/1 Running 0 67m kube-system pod/kube-proxy-mz6ns 1/1 Running 0 68m kube-system pod/kube-scheduler-minikube 1/1 Running 0 67m kube-system pod/storage-provisioner 1/1 Running 0 68m kubernetes-dashboard pod/dashboard-metrics-scraper-76585494d8-5b5w9 1/1 Running 0 16s kubernetes-dashboard pod/kubernetes-dashboard-565564f797-sb2qg 1/1 Running 0 16s sau ƒë√≥ th·ª≠ port-forward ƒë·ªÉ xem c√≥ login ƒë∆∞·ª£c kh√¥ng\nkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 9090:80 --address 0.0.0.0 fix b·∫±ng c√°ch sau:\nkubectl create serviceaccount my-dashboard-sa -n kubernetes-dashboard kubectl create clusterrolebinding my-dashboard-sa \\ --clusterrole=cluster-admin \\ --serviceaccount=kubernetes-dashboard:my-dashboard-sa kubectl get secrets -n kubernetes-dashboard ip-172-31-80-166:/home/ubuntu/k8s-mastery/resource-manifests# kubectl get secrets -n kubernetes-dashboard NAME TYPE DATA AGE default-token-w9sk5 kubernetes.io/service-account-token 3 8m40s kubernetes-dashboard-csrf Opaque 1 8m40s kubernetes-dashboard-key-holder Opaque 2 8m40s kubernetes-dashboard-token-d7tt4 kubernetes.io/service-account-token 3 8m40s my-dashboard-sa-token-tms9g kubernetes.io/service-account-token 3 15s l·∫•y TOKEN b·∫±ng command sau:\nkubectl describe secret -n kubernetes-dashboard my-dashboard-sa-token-tms9g d√πng extension REQUESTLY c·ªßa Chrome, link c√†i:\nhttps://chrome.google.com/webstore/detail/requestly-redirect-url-mo/mdnleldcmiljblolnjhpnblkcekpdkpa config nh∆∞ h√¨nh sau, m·ªói khi v√†o c√°i IP kia th√¨ n√≥ s·∫Ω t·ª± ƒë·ªông modify Header v√† add th√™m TOKEN v√†o cho m√¨nh\nsau ƒë√≥ port-forward th√¨ s·∫Ω t·ª± ƒë·ªông login dc v√†o k8s dashboard\nTuy nhi√™n v√†o dashboard b·ªã l·ªói secrets is forbidden: User \u0026quot;system:serviceaccount:kubernetes-dashboard:kubernetes-dashboard\u0026quot; cannot list resource \u0026quot;secrets\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;default\u0026quot;\t\nnh∆∞ng ph·∫£i s·ª≠a file YAML ClusterRole nh∆∞ sau m·ªõi full quy·ªÅn:\nnano dashboard-cluster-role-test.yaml # Â¢ûÂä†‰∏Ä‰∫õÊÉ≥Ë¶ÅÂú®dashboard‰∏äÊìç‰ΩúÁöÑË≥áÊ∫ê kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;metrics.k8s.io\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;,\u0026quot;pods\u0026quot;, \u0026quot;nodes\u0026quot;,\u0026quot;namespaces\u0026quot;,\u0026quot;secrets\u0026quot;,\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;apps\u0026quot;] resources: [\u0026quot;statefulsets\u0026quot;,\u0026quot;replicationcontrollers\u0026quot;, \u0026quot;ingresses\u0026quot;,\u0026quot;services\u0026quot;,\u0026quot;daemonsets\u0026quot;,\u0026quot;configmaps\u0026quot;,\u0026quot;pods\u0026quot;, \u0026quot;nodes\u0026quot;,\u0026quot;namespaces\u0026quot;,\u0026quot;secrets\u0026quot;,\u0026quot;persistentvolumeclaims\u0026quot;,\u0026quot;replicasets\u0026quot;,\u0026quot;deployments\u0026quot;,\u0026quot;events\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;extensions\u0026quot;] resources: [\u0026quot;ingresses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;batch\u0026quot;] resources: [\u0026quot;jobs\u0026quot;, \u0026quot;cronjobs\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;, \u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;rbac.authorization.k8s.io\u0026quot;] resources: [\u0026quot;clusterroles\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] Sau ƒë√≥ apply file ClusterRole v·ª´a t·∫°o:\nkubectl apply -f dashboard-cluster-role-test.yaml B·∫°n c√≥ th·ªÉ s·ª≠a file alternative.yaml, th√™m c√°i n·ªôi dung b√™n tr√™n v√†o file th√¨ khi apply ch·ªâ c·∫ßn apply file alternative.yaml th√¥i l√† ƒë·ªß\nTuy·ªát v·ªùi!!, v·∫≠y l√† ƒë√£ secure dc k8s dashboard b·∫±ng ServiceAccount Token: Authorization Bearer Token\nC√°ch n√†y hay ·ªü ch·ªó b·∫°n c√≥ th·ªÉ t·∫°o nhi·ªÅu service account ƒë·ªÉ gi·ªõi h·∫°n quy·ªÅn c·ªßa user\nTuy nhi√™n l·∫°i y√™u c·∫ßu User ph·∫£i d√πng extension ƒë·ªÉ modify Header b·∫±ng TOKEN\nC√≤n 1 c√°ch n·ªØa l√† D√πng oauth2_proxy\nC√°ch n√†y th√¨ \u0026ldquo;best secure\u0026rdquo;\nhttps://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca\nC√°ch n√†y s·∫Ω b·∫Øt User authen qua 1 b√™n th·ª© 3 ·ªü ƒë√¢y l√† GitHub\nth√¨ User ko c·∫ßn ph·∫£i modify Header b·∫±ng Requestly nh∆∞ tr√™n (c√≥ th·ªùi gian m√¨nh s·∫Ω l√†m)\nREFERENCES:\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/installation.md\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md\nhttps://kknews.cc/code/56mvxx3.html\nhttps://chrome.google.com/webstore/detail/requestly-redirect-url-mo/mdnleldcmiljblolnjhpnblkcekpdkpa\nhttps://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca\nhttps://unofficialism.info/posts/accessing-rbac-enabled-kubernetes-dashboard/\n","href":"/posts/k8s-ii-how-to-access-dashboard-on-aws-ec2-ubuntu/","title":"K8S 2: How to Access K8s Dashboard On Aws Ec2 Ubuntu"},{"content":"Gi·ªõi thi·ªáu ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh ƒëang ·ªü c√°ch 3, s·∫Ω h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ d·ª±ng 1 app k8s micro-service tr√™n Amazon EC2 Ubuntu\nTr∆∞·ªõc ti√™n h√£y d·ª±a v√†o m·ª•c l·ª•c, n·∫øu b·∫°n mu·ªën l√†m 1 app micro-services b·∫±ng Kubernetes th√¨ h√£y l√†m l·∫ßn l∆∞·ª£t c√°c m·ª•c 1.b.‚Üí2.b.‚Üí3. nh√© (tr√°nh 1 ph√°t v√†o l√†m b∆∞·ªõc 3. lu√¥n r·ªìi s·∫Ω r∆°i v√†o c·∫£nh ko hi·ªÉu m√¨nh thi·∫øu c√°i g√¨ m√† b·ªã l·ªói),\nn√™n l√†m l·∫ßn l∆∞·ª£t nh∆∞ v·∫≠y ƒë·ªÉ hi·ªÉu v·∫•n ƒë·ªÅ, khi ƒë√£ quen r·ªìi th√¨ c√≥ th·ªÉ b·ªè nh·ªØng b∆∞·ªõc ko c·∫ßn thi·∫øt.\n1. Deploy 1 Micro-service app tr√™n AWS EC2 1.a. Linux Launch 1 EC2 Amazon Linux\ncd ~ git clone https://github.com/hoangmnsd/k8s-mastery Ph·∫ßn frontend\ninstall nodejs:\nsudo yum install -y gcc-c++ make curl -sL https://rpm.nodesource.com/setup_13.x | sudo -E bash - sudo yum install -y nodejs node -v npm -v s·ª≠a c√°i localhost b·∫±ng Public IP c·ªßa EC2 (gi·∫£ s·ª≠ l√† ip 52.90.234.33)\nnano ~/k8s-mastery/sa-frontend/src/App.js analyzeSentence() { fetch('http://52.90.234.33:8080/sentiment', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({sentence: this.textField.getValue()}) }) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; this.setState(data)); } R·ªìi verify n·∫øu mu·ªën\ncd ~/k8s-mastery/sa-frontend npm install npm start Verify xong, gi·ªù th√¨ build\nnpm run build Install nginx\nsudo yum install nginx S·ª≠a file config nginx, c·ª• th·ªÉ l√† s·ª≠a c√°i path ƒë·∫øn folder ch·ª©a html c·ªßa app v√† add th√™m c√°i ph·∫ßn location ƒë·∫øn path /sentiment\nsudo nano /etc/nginx/nginx.conf server { listen 80 default_server; listen [::]:80 default_server; server_name localhost; #root /usr/share/nginx/html; root /home/ec2-user/k8s-mastery/sa-frontend/build; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { } location /sentiment { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://localhost:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } Start nginx ƒë·ªÉ apply thay ƒë·ªïi\nchmod 755 /home/ec2-user/ sudo /etc/init.d/nginx restart Xong ph·∫ßn frontend, d√πng browser ƒë·ªÉ v√†o Public IP c·ªßa EC2 ƒë·ªÉ check frontend:\nhttp://52.90.234.33\nSang ph·∫ßn backend Java webapp\ninstall maven v√† java 8 theo link https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html\nInstall dependencies\ncd ~/k8s-mastery/sa-webapp mvn install Build ph·∫ßn webapp\ncd ~/k8s-mastery/sa-webapp/target java -jar sentiment-analysis-web-0.0.1-SNAPSHOT.jar --sa.logic.api.url=http://localhost:5000 Sang ph·∫ßn backend Python logic\nInstall dependencies\ncd ~/k8s-mastery/sa-logic/sa sudo python -m pip install -r requirements.txt python -m textblob.download_corpora Build ph·∫ßn backend logic\ncd ~/k8s-mastery/sa-logic/sa python sentiment_analysis.py Gi·ªù v√†o browser test app ch·∫°y ok l√† ƒëc\nXong ph·∫ßn n√†y\n1.b. Ubuntu Launch EC2 Ubuntu 18.04\ncd ~ git clone https://github.com/rinormaloku/k8s-mastery Ph·∫ßn frontend\ns·ª≠a c√°i ip localhost b·∫±ng public ip c·ªßa EC2\nnano ~/k8s-mastery/sa-frontend/src/App.js analyzeSentence() { fetch('http://52.90.234.33:8080/sentiment', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({sentence: this.textField.getValue()}) }) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; this.setState(data)); } Install nodejs, nh·ªõ l√† b·∫°n ƒëang ·ªü user ubuntu, ko ph·∫£i root\ncurl -sL https://deb.nodesource.com/setup_10.x | sudo bash - sudo apt-get install -y nodejs R·ªìi verify n·∫øu mu·ªën\ncd ~/k8s-mastery/sa-frontend npm install npm start Verify xong, gi·ªù th√¨ build\nnpm run build Install nginx\nsudo apt install nginx S·ª≠a config c·ªßa nginx\nsudo nano /etc/nginx/sites-available/default # Default server configuration # server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration # # listen 443 ssl default_server; # listen [::]:443 ssl default_server; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # # include snippets/snakeoil.conf; #root /var/www/html; root /home/ubuntu/k8s-mastery/sa-frontend/build; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } location /sentiment { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://localhost:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } Gi·ªù restart nginx\nchmod 755 /home/ubuntu/ sudo /etc/init.d/nginx restart Xong ph·∫ßn frontend, d√πng browser ƒë·ªÉ v√†o Public IP c·ªßa EC2 ƒë·ªÉ check frontend:\nhttp://52.90.234.33\nSang ph·∫ßn backend Java webapp\nC·∫ßn install maven https://www.hostinger.com/tutorials/how-to-install-maven-on-ubuntu-18-04/\nsudo apt-get -y install maven install dependencies\ncd ~/k8s-mastery/sa-webapp mvn install build ph·∫ßn webapp\ncd ~/k8s-mastery/sa-webapp/target java -jar sentiment-analysis-web-0.0.1-SNAPSHOT.jar --sa.logic.api.url=http://localhost:5000 C·ª© ƒë·ªÉ c·ª≠a s·ªï terminal ƒë√≥, b·∫≠t th√™m c√°i n·ªØa ƒë·ªÉ ch·∫°y ph·∫ßn backend Python logic\nSang ph·∫ßn backend Python logic\nC·∫ßn install pip n·∫øu ch∆∞a c√≥\nsudo apt install python3-pip Install dependencies\ncd ~/k8s-mastery/sa-logic/sa sudo python3 -m pip install -r requirements.txt python3 -m textblob.download_corpora Build ph·∫ßn backend\ncd ~/k8s-mastery/sa-logic/sa python3 sentiment_analysis.py C·ª© ƒë·ªÉ c·ª≠a s·ªï terminal ƒë√≥, gi·ªù v√†o browser (http://52.90.234.33) test app ch·∫°y ok t·ª´ frontend ƒë·∫øn backend l√† ƒëc\nXong ph·∫ßn n√†y\n2. Deploy 1 Micro-service app tr√™n AWS EC2 b·∫±ng Docker Tr∆∞·ªõc ti√™n c·∫ßn install docker, c√°i n√†y kh√° d·ªÖ n√™n s·∫Ω vi·∫øt sau\nSau ƒë√≥ ƒëƒÉng k√Ω 1 account Docker Hub\nSet c√°c bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ d√πng v·ªÅ sau\nexport DOCKER_USERNAME=AAAABBBB export DOCKER_PASSWORD=CCCCDDDD export DOCKER_USER_ID=AAAABBBB 2.a. Linux Install docker (bonus c·∫£ docker compose)\nsudo yum install git -y sudo yum update -y sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user sudo -i curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose exit #Sau ƒë√≥ log out and log in tr·ªü l·∫°i EC2 ƒë·ªÉ EC2 apply docker user group Login v√†o Docker Hub v·ªõi account v√† password ƒë√£ ƒëƒÉng k√Ω\ndocker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Dockerize ph·∫ßn frontend\nAdd c√°c line sau v√†o ƒë·ªÉ khi build docker s·∫Ω ignore nh·ªØng c√°i n√†y\ncd ~/k8s-mastery/sa-frontend nano .dockerignore node_modules src public Build Docker image\ndocker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend . Push docker image ƒë√≥ l√™n Docker Hub\ndocker push $DOCKER_USER_ID/sentiment-analysis-frontend l√∫c n√†y docker image m√¨nh build ra ƒë√£ dc push l√™n Docker Hub, ai c≈©ng c√≥ th·ªÉ d√πng dc\nRun container\ndocker pull $DOCKER_USER_ID/sentiment-analysis-frontend docker run -d -p 80:80 $DOCKER_USER_ID/sentiment-analysis-frontend n·∫øu b·ªã l·ªói tcp 0.0.0.0:80: bind: address already in use\nth√¨ kill process root nginx theo b√†i n√†y: https://github.com/hadim/docker-omero/issues/9\nsudo netstat -tulpn | grep :80 ps aux | grep nginx sudo kill 1027 Test v√†o Frontend b√¨nh th∆∞·ªùng l√† ok\nDockerize ph·∫ßn backend Python logic\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-logic docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-logic . docker push $DOCKER_USER_ID/sentiment-analysis-logic Run container\ndocker pull $DOCKER_USER_ID/sentiment-analysis-logic docker run -d -p 5050:5000 $DOCKER_USER_ID/sentiment-analysis-logic Dockerize ph·∫ßn backend Java webapp\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-webapp docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-webapp . docker push $DOCKER_USER_ID/sentiment-analysis-webapp Run container\ndocker pull $DOCKER_USER_ID/sentiment-analysis-webapp L·∫•y container ip c·ªßa sa-logic ƒë·ªÉ d√πng b√™n d∆∞·ªõi (s·∫Ω th√™m h√¨nh ·∫£nh minh h·ªça sau)\ndocker container list docker inspect \u0026lt;container_id c·ªßa sa-logic\u0026gt; #ch·ªó n√†y c·∫ßn thay b·∫±ng container_id c·ªßa sa-logic nh√¨n th·∫•y ·ªü command \u0026#34;docker container list\u0026#34; Copy c√°i Containers IP address ·ªü NetworkSettings.IPAddress\nGi·∫£ s·ª≠ l√† 172.17.0.3\ndocker run -d -p 8080:8080 -e SA_LOGIC_API_URL=\u0026#39;http://172.17.0.3:5000\u0026#39; $DOCKER_USER_ID/sentiment-analysis-webapp Test app b√¨nh th∆∞·ªùng tr√™n port 8080\nXong ph·∫ßn Dockerize cho app\n2.b. Ubuntu *Tr∆∞·ªõc khi l√†m ph·∫ßn n√†y th√¨ h√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒë√£ l√†m ph·∫ßn 1.b. Ubuntu (ƒë·ªÉ hi·ªÉu 1 app ƒë·ªÉ build ch·∫°y ƒë∆∞·ª£c c·∫ßn l√†m nh·ªØng g√¨)\nInstall docker\nsudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install docker.io -y Login v√†o Docker Hub v·ªõi account v√† password ƒë√£ ƒëƒÉng k√Ω\nsudo docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Hi·ªán nh∆∞ n√†y l√† ƒë√£ login th√†nh c√¥ng:\nubuntu@ip-172-31-28-137:~$ sudo docker login -u=\u0026quot;$DOCKER_USERNAME\u0026quot; -p=\u0026quot;$DOCKER_PASSWORD\u0026quot; WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Dockerize ph·∫ßn frontend\nAdd c√°c line sau v√†o ƒë·ªÉ khi build docker s·∫Ω ignore nh·ªØng c√°i n√†y\ncd ~/k8s-mastery/sa-frontend nano .dockerignore node_modules src public Build Docker image\nsudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend . Push docker image ƒë√≥ l√™n Docker Hub\nsudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend L√∫c n√†y docker image m√¨nh build ra ƒë√£ dc push l√™n Docker Hub, ai c≈©ng c√≥ th·ªÉ d√πng dc\nRun container\nsudo docker pull $DOCKER_USER_ID/sentiment-analysis-frontend sudo docker run -d -p 80:80 $DOCKER_USER_ID/sentiment-analysis-frontend N·∫øu b·ªã l·ªói tcp 0.0.0.0:80: bind: address already in use\nth√¨ kill process root nginx theo b√†i n√†y: https://github.com/hadim/docker-omero/issues/9\nsudo netstat -tulpn | grep :80 ps aux | grep nginx sudo kill 1027 Test v√†o Frontend b√¨nh th∆∞·ªùng l√† ok\nDockerize ph·∫ßn backend Python logic\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-logic sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-logic . sudo docker push $DOCKER_USER_ID/sentiment-analysis-logic Run container\nsudo docker pull $DOCKER_USER_ID/sentiment-analysis-logic sudo docker run -d -p 5050:5000 $DOCKER_USER_ID/sentiment-analysis-logic Dockerize ph·∫ßn backend Java webapp\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-webapp sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-webapp . sudo docker push $DOCKER_USER_ID/sentiment-analysis-webapp Run container\nsudo docker pull $DOCKER_USER_ID/sentiment-analysis-webapp l·∫•y container ip c·ªßa sa-logic ƒë·ªÉ d√πng b√™n d∆∞·ªõi (s·∫Ω th√™m h√¨nh ·∫£nh minh h·ªça sau)\nsudo docker container list sudo docker inspect \u0026lt;container_id c·ªßa sa-logic\u0026gt; #ch·ªó n√†y c·∫ßn thay b·∫±ng container_id c·ªßa sa-logic nh√¨n th·∫•y ·ªü command \u0026#34;docker container list\u0026#34; copy c√°i Containers IP address ·ªü NetworkSettings.IPAddress\nGi·∫£ s·ª≠ l√† 172.17.0.3\nsudo docker run -d -p 8080:8080 -e SA_LOGIC_API_URL=\u0026#39;http://172.17.0.3:5000\u0026#39; $DOCKER_USER_ID/sentiment-analysis-webapp Test app b√¨nh th∆∞·ªùng tr√™n port 8080\nxong ph·∫ßn Dockerize cho app\n3. Deploy 1 Micro-service app tr√™n AWS EC2 b·∫±ng Kubernetes (Ubuntu) Kh√¥ng th·ªÉ ch·∫°y minikube tr√™n Amazon EC2 Linux ƒë∆∞·ª£c\n(C√≥ 1 c√°ch ƒë·ªÉ d·ª±ng app k8s tr√™n Amazon EC2 Linux ƒë√≥ l√† d√πng eksctl, tuy nhi√™n c√°ch n√†y b·∫Øt bu·ªôc ph·∫£i ƒë·ªÉ eksctl d·ª±ng network, t·ªën kh√° nhi·ªÅu ti·ªÅn - s·∫Ω c√≥ 1 b√†i v·ªÅ d√πng c√°ch eksctl n√†y)\nN√™n n·∫øu mu·ªën d√πng minikube tr√™n Amazon EC2 th√¨ ph·∫£i ch·ªçn Ubuntu - 18.04 LTS - t2.medium\n*Tr∆∞·ªõc khi l√†m ph·∫ßn n√†y th√¨ h√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒë√£ c√≥ nh·ªØng docker images ri√™ng tr√™n Docker Hub (c√≥ th·ªÉ l√†m ph·∫ßn 2.b. Ubuntu ·ªü ph√≠a tr√™n ƒë·ªÉ t·∫°o Docker images ri√™ng)\nInstall kubectl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Install docker\nsudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install docker.io -y Install minikube\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ check minikube version:\nubuntu@ip-172-31-17-59:~$ minikube version minikube version: v1.5.2 commit: 792dbf92a1de583fcee76f8791cff12e0c9440ad-dirty Start minikube l√™n, t·ª´ ƒë√¢y tr·ªü xu·ªëng s·∫Ω switch sang user root\nsudo -i minikube start --vm-driver=none Check status c·ªßa minikube\nminikube status Hi·ªán nh∆∞ n√†y l√† ok:\nroot@ip-172-31-16-165:~# minikube status host: Running kubelet: Running apiserver: Running kubeconfig: Configured Check c√°c node c·ªßa kubectl\nkubectl get nodes Direct v√†o th∆∞ m·ª•c project:\ncd /home/ubuntu/k8s-mastery/resource-manifests Gi·ªù s·∫Ω t·∫°o 2 pods v√† 1 Service ƒë·ªÉ loadbalancing 2 pods ƒë√≥\nBest practice th√¨ c·∫ßn d√πng deployment template ƒë·ªÉ t·∫°o c√°c pods\nnano sa-frontend-deployment.yaml S·ª≠a ph·∫ßn \u0026ldquo;images\u0026rdquo; b·∫±ng Username tr√™n Docker Hub c·ªßa b·∫°n:\napiVersion: apps/v1 kind: Deployment # 1 metadata: name: sa-frontend spec: replicas: 2 # 2: ƒë√¢y l√† s·ªë l∆∞·ª£ng pods selector: # 3: Pods matching the selector will be taken under the management of this deployment. matchLabels: app: sa-frontend minReadySeconds: 15 strategy: type: RollingUpdate # 4 rollingUpdate: maxUnavailable: 1 # 5 maxSurge: 1 # 6 template: # 7 metadata: labels: app: sa-frontend # 8 the label to use for the pods created by this template. spec: containers: - image: hoangmnsd/sentiment-analysis-frontend imagePullPolicy: Always # 9 name: sa-frontend ports: - containerPort: 80 Apply c√°i deployment template m√† m√¨nh v·ª´a t·∫°o\nkubectl apply -f sa-frontend-deployment.yaml Get c√°c pods hi·ªán ƒëang ch·∫°y\nkubectl get pods N·∫øu mu·ªën x√≥a pods\nkubectl delete pod \u0026lt;pod-name\u0026gt; T·∫°o file service load balancer config\nnano service-sa-frontend-lb.yaml apiVersion: v1 kind: Service # 1 metadata: name: sa-frontend-lb spec: type: LoadBalancer # 2 Specification type, we choose LoadBalancer because we want to balance the load between the pods. ports: - port: 80 # 3 Specifies the port in which the service gets requests. protocol: TCP # 4 targetPort: 80 # 5 The port at which incomming requests are forwarded. selector: # 6 app: sa-frontend # 7 app Defines which pods to target, only pods that are labeled with ‚Äúapp: sa-frontend‚Äù T·∫°o service t·ª´ file config ƒë√£ t·∫°o\nkubectl create -f service-sa-frontend-lb.yaml Check c√°c service kubectl ƒëang ch·∫°y\nkubectl get svc V√†o browser g√µ http://\u0026lt;public-ip-ec2\u0026gt;:32613\nth√¨ s·∫Ω ra dc app frontend\nTr∆∞·ªõc ƒë√≥ th√¨ SG c·ªßa EC2 n√™n m·ªü port TCP 32613 v·ªõi source l√† 0.0.0.0/0 ƒë·ªÉ c√≥ th·ªÉ access v√†o dc t·ª´ everywhere\nN·∫øu mu·ªën delete services th√¨\nkubectl delete services sa-frontend-lb Gi·ªù gi·∫£ s·ª≠ b·∫°n mu·ªën update D√≤ng ch·ªØ \u0026ldquo;Sentiment Analyser\u0026rdquo; th√†nh \u0026ldquo;Sentiment Analyser Update\u0026rdquo;\nth√¨ b·∫°n s·∫Ω s·ª≠a code file /home/ubuntu/k8s-mastery/sa-frontend/src/App.js\nsau ƒë√≥ build l·∫°i frontend\ncd /home/ubuntu/k8s-mastery/sa-frontend npm run build sau ƒë√≥ build th√†nh image m·ªõi g·∫Øn lu√¥n tag l√† 19 ch·∫≥ng h·∫°n\nsudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:19 . list c√°c images ra\ndocker images N·∫øu mu·ªën ƒë√°nh l·∫°i tag cho image m√† m√¨nh v·ª´a t·∫°o t·ª´ :19 th√†nh :20 ch·∫≥ng h·∫°n\ndocker tag hoangmnsd/sentiment-analysis-frontend:19 hoangmnsd/sentiment-analysis-frontend:20 push image v·ª´a build xong l√™n Docker Hub\nsudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend:19 t·∫°o deployment config file m·ªõi, s·ª≠a c√°i image m√¨nh s·∫Ω d√πng l√† c√°i c√≥ tag:19\nnano sa-frontend-deployment-update.yaml apiVersion: apps/v1 kind: Deployment # 1 metadata: name: sa-frontend spec: replicas: 2 # 2 selector: matchLabels: app: sa-frontend minReadySeconds: 15 strategy: type: RollingUpdate # 3 rollingUpdate: maxUnavailable: 1 # 4 maxSurge: 1 # 5 template: # 6 metadata: labels: app: sa-frontend # 7 spec: containers: - image: hoangmnsd/sentiment-analysis-frontend:19 imagePullPolicy: Always # 8 name: sa-frontend ports: - containerPort: 80 sau ƒë√≥ d√πng kubectl ƒë·ªÉ apply l·∫°i file deployment yaml v·ªõi t√™n m·ªõi\nkubectl apply -f sa-frontend-deployment-update.yaml --record l·∫ßn l∆∞·ª£t 2 pods c≈© s·∫Ω b·ªã terminate:\ncheck status c·ªßa qu√° tr√¨nh rollout images\nkubectl rollout status deployment sa-frontend V√†o link http://\u0026lt;public-ip-ec2\u0026gt;:32613 ƒë·ªÉ confirm s·ª± thay ƒë·ªïi c·ªßa frontend: \u0026ldquo;Sentiment Analyser Update\u0026rdquo;\ncheck l·∫°i l·ªãch s·ª≠ c√°c revision ƒë√£ dc deploy\nkubectl rollout history deployment sa-frontend rollback l·∫°i b·∫£n deploy tr∆∞·ªõc , v√≠ d·ª• b·∫£n 14\nkubectl rollout undo deployment sa-frontend --to-revision=14 Gi·ªù s·∫Ω t·∫°o backend SA-Logic pods\nDeployment SA-Logic\nb·ªüi v√¨ trong folder resource-manifests ƒë√£ c√≥ s·∫µn file config k8s ƒë·ªÉ t·∫°o backend SA-logic r·ªìi nh∆∞ng c≈©ng n√™n s·ª≠a l·∫°i ƒë·ªÉ d√πng image c·ªßa m√¨nh t·∫°o ra:\nimage: hoangmnsd/sentiment-analysis-logic cd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f sa-logic-deployment.yaml --record R·ªìi t·∫°o backend SA-Logic service ƒë·ªÉ load balance traffic gi·ªØa c√°c pods c·ªßa n√≥\ndeploy Service SA-Logic\ncd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f service-sa-logic.yaml check c√°c pods v√† services ƒë√£ ƒë∆∞·ª£c t·∫°o ra b·∫±ng command kubectl get pods v√† kubectl get svc\nGi·ªù s·∫Ω t·∫°o backend SA-WebApp pods\nDeployment SA-WebApp\nb·ªüi v√¨ trong folder resource-manifests ƒë√£ c√≥ s·∫µn file config k8s ƒë·ªÉ t·∫°o backend SA-WebApp r·ªìi nh∆∞ng c≈©ng n√™n s·ª≠a l·∫°i ƒë·ªÉ d√πng image c·ªßa m√¨nh t·∫°o ra:\nimage: hoangmnsd/sentiment-analysis-webapp cd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f sa-web-app-deployment.yaml --record R·ªìi t·∫°o backend SA-WebApp service ƒë·ªÉ load balance traffic gi·ªØa c√°c pods c·ªßa n√≥\nService SA-WebApp\ncd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f service-sa-web-app-lb.yaml list c√°c services c·ªßa minikube\nminikube service list th·∫•y l√† sa-web-app-lb ƒëang m·ªü port 30079 (c·ªßa m√¨nh l√† v·∫≠y, c·ªßa c√°c b·∫°n c√≥ th·ªÉ port kh√°c), n√™n frontend khi fetch request c·∫ßn s·ª≠a sang port ƒë√≥\nnano /home/ubuntu/k8s-mastery/sa-frontend/src/App.js  analyzeSentence() { // fetch('http://54.236.201.29:8080/sentiment', { fetch('http://54.236.201.29:30079/sentiment', { method: 'POST', Build l·∫°i source code, images v·ªõi tag t√πy b·∫°n (·ªü ƒë√¢y m√¨nh ch·ªçn 20) v√† push l√™n Docker Hub\ncd /home/ubuntu/k8s-mastery/sa-frontend npm run build sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:20 . sudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend:20 S·ª≠a l·∫°i file config sa-frontend-deployment-update.yaml b·∫±ng image c√≥ tag:20, r·ªìi apply n√≥\nimage: hoangmnsd/sentiment-analysis-frontend:20 kubectl apply -f sa-frontend-deployment-update.yaml --record B·∫±ng kubectl get pods b·∫°n s·∫Ω th·∫•y 2 pods c≈© s·∫Ω d·∫ßn d·∫ßn b·ªã terminate, thay b·∫±ng 2 pods m·ªõi ch·∫°y t·ª´ c√°i docker image:20\nNh·ªõ s·ª≠a Security Group m·ªü port cho port 30079\nV√†o l·∫°i frontend v·ªõi port c·ªßa frontend test app ho√†n ch·ªânh\nB√†i sau s·∫Ω n√≥i v·ªÅ Kubernetes Dashboard\nReferences:\nhttps://www.freecodecamp.org/news/learn-kubernetes-in-under-3-hours-a-detailed-guide-to-orchestrating-containers-114ff420e882/\nhttps://www.radishlogic.com/kubernetes/running-minikube-in-aws-ec2-ubuntu/\nhttps://blog.kornelondigital.com/2019/04/28/setting-up-a-kubernetes-sandbox-in-aws-ec2/\nhttps://stackoverflow.com/questions/51912879/is-it-possible-to-setup-a-local-kubernetes-development-environment-minikube-et\nhttps://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md\n","href":"/bk/k8s-i-app-micro-services-on-aws-w-minikube/","title":"K8S 1: App Micro Services on AWS with Minikube"},{"content":"Gi·ªõi thi·ªáu ƒê·ªÉ v·ªçc Kubernetes tr√™n AWS, c√≥ nhi·ªÅu c√°ch:\nc√°ch 1 l√† d√πng Service EKS c·ªßa AWS, l√†m vi·ªác tr√™n Console lu√¥n, r·∫•t tr·ª±c quan\nc√°ch 2 l√† d√πng eksctl l√† CLI c·ªßa AWS ph√°t tri·ªÉn, nhi·ªám v·ª• t∆∞∆°ng t·ª± nh∆∞ Service EKS, nh∆∞ng ta l√†m vi·ªác v·ªõi n√≥ tr√™n CLI/terminal\nc√°ch 3 l√† t·∫°o 1 EC2 Ubuntu 18.04 LTS (t2.medium tr·ªü l√™n), c√†i minikube l√™n n√≥, d·ª±ng 1 cluster\n=\u0026gt; c√°ch 1 v√† 2 kh√° t·ªën k√©m, nh∆∞ng b·∫°n c√≥ th·ªÉ d√πng full service, g·∫ßn v·ªõi m√¥i tr∆∞·ªùng production nh·∫•t,\nc√°ch 3 th√¨ r·∫ª h∆°n nhi·ªÅu, c√°c b·∫°n ch·ªâ t·ªën ph√≠ duy tr√¨ con EC2 Ubuntu th√¥i, tuy nhi√™n c√°ch n√†y ch·ªâ n√™n d√πng ƒë·ªÉ v·ªçc v·∫°ch, d√πng \u0026ldquo;cho bi·∫øt\u0026rdquo; th·∫ø n√†o l√† k8s th√¥i üòÜ\nB√†i n√†y m√¨nh ƒëang ·ªü c√°ch 3, s·∫Ω h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ d·ª±ng 1 app k8s micro-service tr√™n Amazon EC2 Ubuntu\nTr∆∞·ªõc ti√™n h√£y d·ª±a v√†o m·ª•c l·ª•c, n·∫øu b·∫°n mu·ªën l√†m 1 app micro-services b·∫±ng Kubernetes th√¨ h√£y l√†m l·∫ßn l∆∞·ª£t c√°c m·ª•c 1.b.‚Üí2.b.‚Üí3. nh√© (tr√°nh 1 ph√°t v√†o l√†m b∆∞·ªõc 3. lu√¥n r·ªìi s·∫Ω r∆°i v√†o c·∫£nh ko hi·ªÉu m√¨nh thi·∫øu c√°i g√¨ m√† b·ªã l·ªói),\nn√™n l√†m l·∫ßn l∆∞·ª£t nh∆∞ v·∫≠y ƒë·ªÉ hi·ªÉu v·∫•n ƒë·ªÅ, khi ƒë√£ quen r·ªìi th√¨ c√≥ th·ªÉ b·ªè nh·ªØng b∆∞·ªõc ko c·∫ßn thi·∫øt.\n1. Deploy 1 Micro-service app tr√™n AWS EC2 1.a. Linux Launch 1 EC2 Amazon Linux\ncd ~ git clone https://github.com/hoangmnsd/k8s-mastery Ph·∫ßn frontend\ninstall nodejs:\nsudo yum install -y gcc-c++ make curl -sL https://rpm.nodesource.com/setup_13.x | sudo -E bash - sudo yum install -y nodejs node -v npm -v s·ª≠a c√°i localhost b·∫±ng Public IP c·ªßa EC2 (gi·∫£ s·ª≠ l√† ip 52.90.234.33)\nnano ~/k8s-mastery/sa-frontend/src/App.js analyzeSentence() { fetch('http://52.90.234.33:8080/sentiment', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({sentence: this.textField.getValue()}) }) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; this.setState(data)); } R·ªìi verify n·∫øu mu·ªën\ncd ~/k8s-mastery/sa-frontend npm install npm start Verify xong, gi·ªù th√¨ build\nnpm run build Install nginx\nsudo yum install nginx S·ª≠a file config nginx, c·ª• th·ªÉ l√† s·ª≠a c√°i path ƒë·∫øn folder ch·ª©a html c·ªßa app v√† add th√™m c√°i ph·∫ßn location ƒë·∫øn path /sentiment\nsudo nano /etc/nginx/nginx.conf server { listen 80 default_server; listen [::]:80 default_server; server_name localhost; #root /usr/share/nginx/html; root /home/ec2-user/k8s-mastery/sa-frontend/build; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { } location /sentiment { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://localhost:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } Start nginx ƒë·ªÉ apply thay ƒë·ªïi\nchmod 755 /home/ec2-user/ sudo /etc/init.d/nginx restart Xong ph·∫ßn frontend, d√πng browser ƒë·ªÉ v√†o Public IP c·ªßa EC2 ƒë·ªÉ check frontend:\nhttp://52.90.234.33\nSang ph·∫ßn backend Java webapp\ninstall maven v√† java 8 theo link https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html\nInstall dependencies\ncd ~/k8s-mastery/sa-webapp mvn install Build ph·∫ßn webapp\ncd ~/k8s-mastery/sa-webapp/target java -jar sentiment-analysis-web-0.0.1-SNAPSHOT.jar --sa.logic.api.url=http://localhost:5000 Sang ph·∫ßn backend Python logic\nInstall dependencies\ncd ~/k8s-mastery/sa-logic/sa sudo python -m pip install -r requirements.txt python -m textblob.download_corpora Build ph·∫ßn backend logic\ncd ~/k8s-mastery/sa-logic/sa python sentiment_analysis.py Gi·ªù v√†o browser test app ch·∫°y ok l√† ƒëc\nXong ph·∫ßn n√†y\n1.b. Ubuntu Launch EC2 Ubuntu 18.04\ncd ~ git clone https://github.com/rinormaloku/k8s-mastery Ph·∫ßn frontend\ns·ª≠a c√°i ip localhost b·∫±ng public ip c·ªßa EC2\nnano ~/k8s-mastery/sa-frontend/src/App.js analyzeSentence() { fetch('http://52.90.234.33:8080/sentiment', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({sentence: this.textField.getValue()}) }) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; this.setState(data)); } Install nodejs, nh·ªõ l√† b·∫°n ƒëang ·ªü user ubuntu, ko ph·∫£i root\ncurl -sL https://deb.nodesource.com/setup_10.x | sudo bash - sudo apt-get install -y nodejs R·ªìi verify n·∫øu mu·ªën\ncd ~/k8s-mastery/sa-frontend npm install npm start Verify xong, gi·ªù th√¨ build\nnpm run build Install nginx\nsudo apt install nginx S·ª≠a config c·ªßa nginx\nsudo nano /etc/nginx/sites-available/default # Default server configuration # server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration # # listen 443 ssl default_server; # listen [::]:443 ssl default_server; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # # include snippets/snakeoil.conf; #root /var/www/html; root /home/ubuntu/k8s-mastery/sa-frontend/build; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } location /sentiment { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://localhost:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } Gi·ªù restart nginx\nchmod 755 /home/ubuntu/ sudo /etc/init.d/nginx restart Xong ph·∫ßn frontend, d√πng browser ƒë·ªÉ v√†o Public IP c·ªßa EC2 ƒë·ªÉ check frontend:\nhttp://52.90.234.33\nSang ph·∫ßn backend Java webapp\nC·∫ßn install maven https://www.hostinger.com/tutorials/how-to-install-maven-on-ubuntu-18-04/\nsudo apt-get -y install maven install dependencies\ncd ~/k8s-mastery/sa-webapp mvn install build ph·∫ßn webapp\ncd ~/k8s-mastery/sa-webapp/target java -jar sentiment-analysis-web-0.0.1-SNAPSHOT.jar --sa.logic.api.url=http://localhost:5000 C·ª© ƒë·ªÉ c·ª≠a s·ªï terminal ƒë√≥, b·∫≠t th√™m c√°i n·ªØa ƒë·ªÉ ch·∫°y ph·∫ßn backend Python logic\nSang ph·∫ßn backend Python logic\nC·∫ßn install pip n·∫øu ch∆∞a c√≥\nsudo apt install python3-pip Install dependencies\ncd ~/k8s-mastery/sa-logic/sa sudo python3 -m pip install -r requirements.txt python3 -m textblob.download_corpora Build ph·∫ßn backend\ncd ~/k8s-mastery/sa-logic/sa python3 sentiment_analysis.py C·ª© ƒë·ªÉ c·ª≠a s·ªï terminal ƒë√≥, gi·ªù v√†o browser (http://52.90.234.33) test app ch·∫°y ok t·ª´ frontend ƒë·∫øn backend l√† ƒëc\nXong ph·∫ßn n√†y\n2. Deploy 1 Micro-service app tr√™n AWS EC2 b·∫±ng Docker Tr∆∞·ªõc ti√™n c·∫ßn install docker, c√°i n√†y kh√° d·ªÖ n√™n s·∫Ω vi·∫øt sau\nSau ƒë√≥ ƒëƒÉng k√Ω 1 account Docker Hub\nSet c√°c bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ d√πng v·ªÅ sau\nexport DOCKER_USERNAME=AAAABBBB export DOCKER_PASSWORD=CCCCDDDD export DOCKER_USER_ID=AAAABBBB 2.a. Linux Install docker (bonus c·∫£ docker compose)\nsudo yum install git -y sudo yum update -y sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user sudo -i curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose exit #Sau ƒë√≥ log out and log in tr·ªü l·∫°i EC2 ƒë·ªÉ EC2 apply docker user group Login v√†o Docker Hub v·ªõi account v√† password ƒë√£ ƒëƒÉng k√Ω\ndocker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Dockerize ph·∫ßn frontend\nAdd c√°c line sau v√†o ƒë·ªÉ khi build docker s·∫Ω ignore nh·ªØng c√°i n√†y\ncd ~/k8s-mastery/sa-frontend nano .dockerignore node_modules src public Build Docker image\ndocker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend . Push docker image ƒë√≥ l√™n Docker Hub\ndocker push $DOCKER_USER_ID/sentiment-analysis-frontend l√∫c n√†y docker image m√¨nh build ra ƒë√£ dc push l√™n Docker Hub, ai c≈©ng c√≥ th·ªÉ d√πng dc\nRun container\ndocker pull $DOCKER_USER_ID/sentiment-analysis-frontend docker run -d -p 80:80 $DOCKER_USER_ID/sentiment-analysis-frontend n·∫øu b·ªã l·ªói tcp 0.0.0.0:80: bind: address already in use\nth√¨ kill process root nginx theo b√†i n√†y: https://github.com/hadim/docker-omero/issues/9\nsudo netstat -tulpn | grep :80 ps aux | grep nginx sudo kill 1027 Test v√†o Frontend b√¨nh th∆∞·ªùng l√† ok\nDockerize ph·∫ßn backend Python logic\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-logic docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-logic . docker push $DOCKER_USER_ID/sentiment-analysis-logic Run container\ndocker pull $DOCKER_USER_ID/sentiment-analysis-logic docker run -d -p 5050:5000 $DOCKER_USER_ID/sentiment-analysis-logic Dockerize ph·∫ßn backend Java webapp\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-webapp docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-webapp . docker push $DOCKER_USER_ID/sentiment-analysis-webapp Run container\ndocker pull $DOCKER_USER_ID/sentiment-analysis-webapp L·∫•y container ip c·ªßa sa-logic ƒë·ªÉ d√πng b√™n d∆∞·ªõi (s·∫Ω th√™m h√¨nh ·∫£nh minh h·ªça sau)\ndocker container list docker inspect \u0026lt;container_id c·ªßa sa-logic\u0026gt; #ch·ªó n√†y c·∫ßn thay b·∫±ng container_id c·ªßa sa-logic nh√¨n th·∫•y ·ªü command \u0026#34;docker container list\u0026#34; Copy c√°i Containers IP address ·ªü NetworkSettings.IPAddress\nGi·∫£ s·ª≠ l√† 172.17.0.3\ndocker run -d -p 8080:8080 -e SA_LOGIC_API_URL=\u0026#39;http://172.17.0.3:5000\u0026#39; $DOCKER_USER_ID/sentiment-analysis-webapp Test app b√¨nh th∆∞·ªùng tr√™n port 8080\nXong ph·∫ßn Dockerize cho app\n2.b. Ubuntu *Tr∆∞·ªõc khi l√†m ph·∫ßn n√†y th√¨ h√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒë√£ l√†m ph·∫ßn 1.b. Ubuntu (ƒë·ªÉ hi·ªÉu 1 app ƒë·ªÉ build ch·∫°y ƒë∆∞·ª£c c·∫ßn l√†m nh·ªØng g√¨)\nInstall docker\nsudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install docker.io -y Login v√†o Docker Hub v·ªõi account v√† password ƒë√£ ƒëƒÉng k√Ω\nsudo docker login -u=\u0026#34;$DOCKER_USERNAME\u0026#34; -p=\u0026#34;$DOCKER_PASSWORD\u0026#34; Hi·ªán nh∆∞ n√†y l√† ƒë√£ login th√†nh c√¥ng:\nubuntu@ip-172-31-28-137:~$ sudo docker login -u=\u0026quot;$DOCKER_USERNAME\u0026quot; -p=\u0026quot;$DOCKER_PASSWORD\u0026quot; WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Dockerize ph·∫ßn frontend\nAdd c√°c line sau v√†o ƒë·ªÉ khi build docker s·∫Ω ignore nh·ªØng c√°i n√†y\ncd ~/k8s-mastery/sa-frontend nano .dockerignore node_modules src public Build Docker image\nsudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend . Push docker image ƒë√≥ l√™n Docker Hub\nsudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend L√∫c n√†y docker image m√¨nh build ra ƒë√£ dc push l√™n Docker Hub, ai c≈©ng c√≥ th·ªÉ d√πng dc\nRun container\nsudo docker pull $DOCKER_USER_ID/sentiment-analysis-frontend sudo docker run -d -p 80:80 $DOCKER_USER_ID/sentiment-analysis-frontend N·∫øu b·ªã l·ªói tcp 0.0.0.0:80: bind: address already in use\nth√¨ kill process root nginx theo b√†i n√†y: https://github.com/hadim/docker-omero/issues/9\nsudo netstat -tulpn | grep :80 ps aux | grep nginx sudo kill 1027 Test v√†o Frontend b√¨nh th∆∞·ªùng l√† ok\nDockerize ph·∫ßn backend Python logic\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-logic sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-logic . sudo docker push $DOCKER_USER_ID/sentiment-analysis-logic Run container\nsudo docker pull $DOCKER_USER_ID/sentiment-analysis-logic sudo docker run -d -p 5050:5000 $DOCKER_USER_ID/sentiment-analysis-logic Dockerize ph·∫ßn backend Java webapp\nBuild v√† push docker image\ncd ~/k8s-mastery/sa-webapp sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-webapp . sudo docker push $DOCKER_USER_ID/sentiment-analysis-webapp Run container\nsudo docker pull $DOCKER_USER_ID/sentiment-analysis-webapp l·∫•y container ip c·ªßa sa-logic ƒë·ªÉ d√πng b√™n d∆∞·ªõi (s·∫Ω th√™m h√¨nh ·∫£nh minh h·ªça sau)\nsudo docker container list sudo docker inspect \u0026lt;container_id c·ªßa sa-logic\u0026gt; #ch·ªó n√†y c·∫ßn thay b·∫±ng container_id c·ªßa sa-logic nh√¨n th·∫•y ·ªü command \u0026#34;docker container list\u0026#34; copy c√°i Containers IP address ·ªü NetworkSettings.IPAddress\nGi·∫£ s·ª≠ l√† 172.17.0.3\nsudo docker run -d -p 8080:8080 -e SA_LOGIC_API_URL=\u0026#39;http://172.17.0.3:5000\u0026#39; $DOCKER_USER_ID/sentiment-analysis-webapp Test app b√¨nh th∆∞·ªùng tr√™n port 8080\nxong ph·∫ßn Dockerize cho app\n3. Deploy 1 Micro-service app tr√™n AWS EC2 b·∫±ng Kubernetes (Ubuntu) Kh√¥ng th·ªÉ ch·∫°y minikube tr√™n Amazon EC2 Linux ƒë∆∞·ª£c\n(C√≥ 1 c√°ch ƒë·ªÉ d·ª±ng app k8s tr√™n Amazon EC2 Linux ƒë√≥ l√† d√πng eksctl, tuy nhi√™n c√°ch n√†y b·∫Øt bu·ªôc ph·∫£i ƒë·ªÉ eksctl d·ª±ng network, t·ªën kh√° nhi·ªÅu ti·ªÅn - s·∫Ω c√≥ 1 b√†i v·ªÅ d√πng c√°ch eksctl n√†y)\nN√™n n·∫øu mu·ªën d√πng minikube tr√™n Amazon EC2 th√¨ ph·∫£i ch·ªçn Ubuntu - 18.04 LTS - t2.medium\n*Tr∆∞·ªõc khi l√†m ph·∫ßn n√†y th√¨ h√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒë√£ c√≥ nh·ªØng docker images ri√™ng tr√™n Docker Hub (c√≥ th·ªÉ l√†m ph·∫ßn 2.b. Ubuntu ·ªü ph√≠a tr√™n ƒë·ªÉ t·∫°o Docker images ri√™ng)\nInstall kubectl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Install docker\nsudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install docker.io -y Install minikube\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ check minikube version:\nubuntu@ip-172-31-17-59:~$ minikube version minikube version: v1.5.2 commit: 792dbf92a1de583fcee76f8791cff12e0c9440ad-dirty Start minikube l√™n, t·ª´ ƒë√¢y tr·ªü xu·ªëng s·∫Ω switch sang user root\nsudo -i minikube start --vm-driver=none Check status c·ªßa minikube\nminikube status Hi·ªán nh∆∞ n√†y l√† ok:\nroot@ip-172-31-16-165:~# minikube status host: Running kubelet: Running apiserver: Running kubeconfig: Configured Check c√°c node c·ªßa kubectl\nkubectl get nodes Direct v√†o th∆∞ m·ª•c project:\ncd /home/ubuntu/k8s-mastery/resource-manifests Gi·ªù s·∫Ω t·∫°o 2 pods v√† 1 Service ƒë·ªÉ loadbalancing 2 pods ƒë√≥\nBest practice th√¨ c·∫ßn d√πng deployment template ƒë·ªÉ t·∫°o c√°c pods\nnano sa-frontend-deployment.yaml S·ª≠a ph·∫ßn \u0026ldquo;images\u0026rdquo; b·∫±ng Username tr√™n Docker Hub c·ªßa b·∫°n:\napiVersion: apps/v1 kind: Deployment # 1 metadata: name: sa-frontend spec: replicas: 2 # 2: ƒë√¢y l√† s·ªë l∆∞·ª£ng pods selector: # 3: Pods matching the selector will be taken under the management of this deployment. matchLabels: app: sa-frontend minReadySeconds: 15 strategy: type: RollingUpdate # 4 rollingUpdate: maxUnavailable: 1 # 5 maxSurge: 1 # 6 template: # 7 metadata: labels: app: sa-frontend # 8 the label to use for the pods created by this template. spec: containers: - image: hoangmnsd/sentiment-analysis-frontend imagePullPolicy: Always # 9 name: sa-frontend ports: - containerPort: 80 Apply c√°i deployment template m√† m√¨nh v·ª´a t·∫°o\nkubectl apply -f sa-frontend-deployment.yaml Get c√°c pods hi·ªán ƒëang ch·∫°y\nkubectl get pods N·∫øu mu·ªën x√≥a pods\nkubectl delete pod \u0026lt;pod-name\u0026gt; T·∫°o file service load balancer config\nnano service-sa-frontend-lb.yaml apiVersion: v1 kind: Service # 1 metadata: name: sa-frontend-lb spec: type: LoadBalancer # 2 Specification type, we choose LoadBalancer because we want to balance the load between the pods. ports: - port: 80 # 3 Specifies the port in which the service gets requests. protocol: TCP # 4 targetPort: 80 # 5 The port at which incomming requests are forwarded. selector: # 6 app: sa-frontend # 7 app Defines which pods to target, only pods that are labeled with ‚Äúapp: sa-frontend‚Äù T·∫°o service t·ª´ file config ƒë√£ t·∫°o\nkubectl create -f service-sa-frontend-lb.yaml Check c√°c service kubectl ƒëang ch·∫°y\nkubectl get svc V√†o browser g√µ http://\u0026lt;public-ip-ec2\u0026gt;:32613\nth√¨ s·∫Ω ra dc app frontend\nTr∆∞·ªõc ƒë√≥ th√¨ SG c·ªßa EC2 n√™n m·ªü port TCP 32613 v·ªõi source l√† 0.0.0.0/0 ƒë·ªÉ c√≥ th·ªÉ access v√†o dc t·ª´ everywhere\nN·∫øu mu·ªën delete services th√¨\nkubectl delete services sa-frontend-lb Gi·ªù gi·∫£ s·ª≠ b·∫°n mu·ªën update D√≤ng ch·ªØ \u0026ldquo;Sentiment Analyser\u0026rdquo; th√†nh \u0026ldquo;Sentiment Analyser Update\u0026rdquo;\nth√¨ b·∫°n s·∫Ω s·ª≠a code file /home/ubuntu/k8s-mastery/sa-frontend/src/App.js\nsau ƒë√≥ build l·∫°i frontend\ncd /home/ubuntu/k8s-mastery/sa-frontend npm run build sau ƒë√≥ build th√†nh image m·ªõi g·∫Øn lu√¥n tag l√† 19 ch·∫≥ng h·∫°n\nsudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:19 . list c√°c images ra\ndocker images N·∫øu mu·ªën ƒë√°nh l·∫°i tag cho image m√† m√¨nh v·ª´a t·∫°o t·ª´ :19 th√†nh :20 ch·∫≥ng h·∫°n\ndocker tag hoangmnsd/sentiment-analysis-frontend:19 hoangmnsd/sentiment-analysis-frontend:20 push image v·ª´a build xong l√™n Docker Hub\nsudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend:19 t·∫°o deployment config file m·ªõi, s·ª≠a c√°i image m√¨nh s·∫Ω d√πng l√† c√°i c√≥ tag:19\nnano sa-frontend-deployment-update.yaml apiVersion: apps/v1 kind: Deployment # 1 metadata: name: sa-frontend spec: replicas: 2 # 2 selector: matchLabels: app: sa-frontend minReadySeconds: 15 strategy: type: RollingUpdate # 3 rollingUpdate: maxUnavailable: 1 # 4 maxSurge: 1 # 5 template: # 6 metadata: labels: app: sa-frontend # 7 spec: containers: - image: hoangmnsd/sentiment-analysis-frontend:19 imagePullPolicy: Always # 8 name: sa-frontend ports: - containerPort: 80 sau ƒë√≥ d√πng kubectl ƒë·ªÉ apply l·∫°i file deployment yaml v·ªõi t√™n m·ªõi\nkubectl apply -f sa-frontend-deployment-update.yaml --record l·∫ßn l∆∞·ª£t 2 pods c≈© s·∫Ω b·ªã terminate:\ncheck status c·ªßa qu√° tr√¨nh rollout images\nkubectl rollout status deployment sa-frontend V√†o link http://\u0026lt;public-ip-ec2\u0026gt;:32613 ƒë·ªÉ confirm s·ª± thay ƒë·ªïi c·ªßa frontend: \u0026ldquo;Sentiment Analyser Update\u0026rdquo;\ncheck l·∫°i l·ªãch s·ª≠ c√°c revision ƒë√£ dc deploy\nkubectl rollout history deployment sa-frontend rollback l·∫°i b·∫£n deploy tr∆∞·ªõc , v√≠ d·ª• b·∫£n 14\nkubectl rollout undo deployment sa-frontend --to-revision=14 Gi·ªù s·∫Ω t·∫°o backend SA-Logic pods\nDeployment SA-Logic\nb·ªüi v√¨ trong folder resource-manifests ƒë√£ c√≥ s·∫µn file config k8s ƒë·ªÉ t·∫°o backend SA-logic r·ªìi nh∆∞ng c≈©ng n√™n s·ª≠a l·∫°i ƒë·ªÉ d√πng image c·ªßa m√¨nh t·∫°o ra:\nimage: hoangmnsd/sentiment-analysis-logic cd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f sa-logic-deployment.yaml --record R·ªìi t·∫°o backend SA-Logic service ƒë·ªÉ load balance traffic gi·ªØa c√°c pods c·ªßa n√≥\ndeploy Service SA-Logic\ncd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f service-sa-logic.yaml check c√°c pods v√† services ƒë√£ ƒë∆∞·ª£c t·∫°o ra b·∫±ng command kubectl get pods v√† kubectl get svc\nGi·ªù s·∫Ω t·∫°o backend SA-WebApp pods\nDeployment SA-WebApp\nb·ªüi v√¨ trong folder resource-manifests ƒë√£ c√≥ s·∫µn file config k8s ƒë·ªÉ t·∫°o backend SA-WebApp r·ªìi nh∆∞ng c≈©ng n√™n s·ª≠a l·∫°i ƒë·ªÉ d√πng image c·ªßa m√¨nh t·∫°o ra:\nimage: hoangmnsd/sentiment-analysis-webapp cd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f sa-web-app-deployment.yaml --record R·ªìi t·∫°o backend SA-WebApp service ƒë·ªÉ load balance traffic gi·ªØa c√°c pods c·ªßa n√≥\nService SA-WebApp\ncd /home/ubuntu/k8s-mastery/resource-manifests kubectl apply -f service-sa-web-app-lb.yaml list c√°c services c·ªßa minikube\nminikube service list th·∫•y l√† sa-web-app-lb ƒëang m·ªü port 30079 (c·ªßa m√¨nh l√† v·∫≠y, c·ªßa c√°c b·∫°n c√≥ th·ªÉ port kh√°c), n√™n frontend khi fetch request c·∫ßn s·ª≠a sang port ƒë√≥\nnano /home/ubuntu/k8s-mastery/sa-frontend/src/App.js  analyzeSentence() { // fetch('http://54.236.201.29:8080/sentiment', { fetch('http://54.236.201.29:30079/sentiment', { method: 'POST', Build l·∫°i source code, images v·ªõi tag t√πy b·∫°n (·ªü ƒë√¢y m√¨nh ch·ªçn 20) v√† push l√™n Docker Hub\ncd /home/ubuntu/k8s-mastery/sa-frontend npm run build sudo docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:20 . sudo docker push $DOCKER_USER_ID/sentiment-analysis-frontend:20 S·ª≠a l·∫°i file config sa-frontend-deployment-update.yaml b·∫±ng image c√≥ tag:20, r·ªìi apply n√≥\nimage: hoangmnsd/sentiment-analysis-frontend:20 kubectl apply -f sa-frontend-deployment-update.yaml --record B·∫±ng kubectl get pods b·∫°n s·∫Ω th·∫•y 2 pods c≈© s·∫Ω d·∫ßn d·∫ßn b·ªã terminate, thay b·∫±ng 2 pods m·ªõi ch·∫°y t·ª´ c√°i docker image:20\nNh·ªõ s·ª≠a Security Group m·ªü port cho port 30079\nV√†o l·∫°i frontend v·ªõi port c·ªßa frontend test app ho√†n ch·ªânh\nB√†i sau s·∫Ω n√≥i v·ªÅ Kubernetes Dashboard\nReferences:\nhttps://www.freecodecamp.org/news/learn-kubernetes-in-under-3-hours-a-detailed-guide-to-orchestrating-containers-114ff420e882/\nhttps://www.radishlogic.com/kubernetes/running-minikube-in-aws-ec2-ubuntu/\nhttps://blog.kornelondigital.com/2019/04/28/setting-up-a-kubernetes-sandbox-in-aws-ec2/\nhttps://stackoverflow.com/questions/51912879/is-it-possible-to-setup-a-local-kubernetes-development-environment-minikube-et\nhttps://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md\n","href":"/posts/k8s-i-app-micro-services-on-aws-w-minikube/","title":"K8S 1: App Micro Services on AWS with Minikube"},{"content":"C√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh h·ªçc \u0026mdash;P1\u0026mdash;  Q. Cty c·ªßa b·∫°n c√≥ 1 VPC v·ªõi v√†i ec2. App tr√™n ec2 l√†m vi·ªác v·ªõi IPv6. C·∫ßn ensure l√† ec2 n√†y c√≥ th·ªÉ init traffic ra Internet, nh∆∞ng ko cho connect t·ª´ internet v√†o instance. L√†m n√†o?\n D√πng egress-only Internet gateway.\nko d√πng NAT GW hay NAT instance v√¨ c√°i ƒë√≥ d√πng cho IP v4, IGW th√¨ hi·ªÉn nhi√™n ko d√πng r·ªìi.\negress-only IGW l√† stateful, n√≥ forward traffic t·ª´ instance ra Internet, v√† g·ª≠i respone l·∫°i cho instance\n Q. Cty b·∫°n c√≥ VPC v·ªõi v√†i Ec2. 1 instance m·ªõi launch s·∫Ω host app l√†m vi·ªác v·ªõi IPv6. VPC c·ªßa b·∫°n c·∫ßn c√°i g√¨ ƒë·ªÉ ensure c√°c Ec2 m·ªõi launch s·∫Ω c√≥ th·ªÉ communicate qua IPv6?\n C·∫ßn ensure l√† VPC ƒë∆∞·ª£c enable Dual Stack mode.\nvi·ªác attach 1 egress-only IGW c≈©ng ƒë√∫ng nh∆∞ng l√† ch∆∞a ƒë·ªß.\nC√°c step ƒë·ªÉ enable VPC s·ª≠ d·ª•ng dc IPv6:\n1-VPC v√† subnet c·∫ßn dc associate IPv6 CIDR block\n2-Update route table:\n_Public subnet th√¨ c·∫ßn t·∫°o route ƒë·ªÉ route traffic IPv6 t·ª´ subnet ƒë·∫øn IGW\n_Private subnet th√¨ c·∫ßn t·∫°o route ƒë·ªÉ route traffic Ipv6 t·ª´ subnet ƒë·∫øn egress-only IGW\n3-Update SG (v√† ACL n·∫øu c√≥), ƒë·ªÉ include IPv6 address v√†o SG\n4-X√°c ƒë·ªãnh xem instance type c√≥ support IPv6 ko, n·∫øu ko th√¨ ph·∫£i ƒë·ªïi (m3.large ko support ph·∫£i tƒÉng l√™n m4.large)\n5-Assign Ipv6 cho instance\n6-N·∫øu instance ch∆∞a dc configure DHCPv6 th√¨ c≈©ng ph·∫£i configure.\n Q. Cty b·∫°n c·∫ßn setup 1 hybric connection gi·ªØa on-premise v√† aws vpc. H·ªç s·∫Ω transfer l∆∞·ª£ng l·ªõn data t·ª´ on premise l√™n aws. C·∫ßn d√πng g√¨? VPN hay Direct Connect?\n AWS Direct Connect.\nV√¨ VPN ko th·ªÉ ƒë·∫£m b·∫£o high banwidth connection cho l∆∞·ª£ng l·ªõn data dc.\n Q. B·∫°n setup VPC v√† 1 subnet. T·∫°o 1 IGW attach v√†o VPC. Vpc d√£ cho ph√©p DNS resolution v√† hostname. B·∫°n launch 1 ec2 c√≥ public ip v√† ƒë√£ setting SG v√† NACL r·ªìi, nh∆∞ng v·∫´n ko access dc EC2 v√¨ sao? C√≥ ph·∫£i v√¨ ch∆∞a attach IGW v√†o public subnet? hay v√¨ ch∆∞a setup Route table?\n V√¨ ch∆∞a setup Route table\nIGW ch·ªâ c·∫ßn attach v√†o VPC l√† ƒë·ªß r·ªìi, ko c·∫ßn subnet\n Q. B·∫°n mu·ªën cho ph√©p 1 admin v√†o setup EC2, EC2 ƒë√≥ c·∫ßn quy·ªÅn access v√†o DynamoDB. Nh·ªØng policy permission g√¨ c·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o security?\n C·∫ßn trust policy cho ph√©p EC2 instance assume role\nC·∫ßn permission policy cho ph√©p User Pass role to EC2\nTheo AWS Documents c·∫ßn 3 y·∫øu t·ªë:\nFirst, IAM permission policy attach v√†o role, define nh·ªØng action v√† resource m√† role c√≥ th·ªÉ l√†m.\nv√≠ d·ª• define action read-only DynamoDB table.\nSecond, 1 trust policy cho role, cho ph√©p service assume role.\nv√≠ d·ª• sau l√† trust policy cho ph√©p EC2 service s·ª≠ d·ª•ng role v√† c√°c permission dc attach v√†o role.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Sid\u0026quot;: \u0026quot;TrustPolicyStatementThatAllowsEC2ServiceToAssumeTheAttachedRole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;ec2.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } } Third, 1 IAM permission policy attach v√†o IAM User, cho ph√©p user pass nh·ªØng role ƒë√£ dc aproved.\nv√≠ d·ª• sau l√† User ch·ªâ ƒë∆∞·ª£c pass c√°i role t√™n l√† EC2-role-for-XYZ-*\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:PassRole\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;account-id\u0026gt;:role/EC2-roles-for-XYZ-*\u0026quot; }] } Gi·ªù, User c√≥ th·ªÉ start EC2, app run tr√™n EC2 c√≥ th·ªÉ access temporary credential th√¥ng qua instance profile meta data. Instance c√≥ th·ªÉ l√†m nh·ªØng action ƒë√£ dc determine trong permission policies.\n1 V√≠ d·ª• kh√°c d·ªÖ hi·ªÉu h∆°n:\nIAM permission policy attach v√†o IAM User A nh∆∞ sau:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [{ \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[\u0026quot;ec2:*\u0026quot;], \u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:\u0026quot;iam:PassRole\u0026quot;, \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:iam::123456789012:role/S3Access\u0026quot; }] } User A s·∫Ω c√≥ full quy·ªÅn v·ªÅ EC2, v√† khi User A launch 1 EC2, Anh ta ch·ªâ c√≥ quy·ªÅn Pass (associate/g√°n) c√°i role t√™n l√† S3Access v√†o Instance th√¥i.\nT·∫•t nhi√™n tr∆∞·ªõc ƒë√≥, B·∫°n c≈©ng c·∫ßn t·∫°o c√°i role t√™n l√† S3Access v·ªõi trust policy attach v√†o role l√†:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Sid\u0026quot;: \u0026quot;TrustPolicyStatementThatAllowsEC2ServiceToAssumeTheAttachedRole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;ec2.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } } V√† Role S3Access c≈©ng c·∫ßn c√≥ permission policy attach v√†o role t∆∞∆°ng t·ª± nh∆∞ sau:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:ListAllMyBuckets\u0026quot;, \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Q. Nh·ªØng service sau c√°i n√†o by default ko cung c·∫•p kh·∫£ nƒÉng HA? EC2-RDS-DynamoDB-ELB?\n EC2 ko HA by default, mu·ªën HA th√¨ ph·∫£i t·ª± l√†m b·∫±ng scripts.\nRDS c√≥ feature Multi-AZ, DynamoDB th√¨ full mamanged by AWS, ELB th√¨ v·ªën ƒë√£ HA\n Q. B·∫°n mu·ªën c√≥ 1 list c√°c user trong AWS account c·ªßa b·∫°n, check xem c√°c user ƒë√≥ c√≥ d√πng MFA ko? l√†m n√†o?\n V√†o IAM, download credential report, nh·ªØng th√¥ng tin r·∫•t nhi·ªÅu li√™n quan ƒë·∫øn v√≠ d·ª• nh∆∞ password, access key, MFA\n Q. 1 bucket c√≥ th·ªÉ set policy ƒë·ªÉ deny ho·∫∑c allow 1 IP range n√†o ƒë√≥ b·∫±ng c√°ch:\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;S3PolicyId1\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;IPAllow\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::examplebucket/*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;IpAddress\u0026quot;: {\u0026quot;aws:SourceIp\u0026quot;: \u0026quot;54.240.143.0/24\u0026quot;}, \u0026quot;NotIpAddress\u0026quot;: {\u0026quot;aws:SourceIp\u0026quot;: \u0026quot;54.240.143.188/32\u0026quot;} } } ] }  Q. B·∫°n l√† sysops admin c·ªßa 1 cty IT. Sau khi ƒë√£ setup SSM tr√™n EC2 ·ªü c√°c region, b·∫°n c·∫ßn setup SSM cho 50 db server ·ªü data centre c·ªßa c√¥ng ty b·∫°n. Nh·ªØng action g√¨ l√† b·∫Øt bu·ªôc?\n T·∫°o 1 IAM role ƒë·ªÉ communicate SSM service.\nInstall TLS certificate tr√™n server c·ªßa Data centre.\nT·∫°o 1 managed-instance activation cho m√¥i tr∆∞·ªùng Hybrid. C√°i n√†y ƒë·ªÉ t·∫°o ra 1 c·∫∑p Code-ID d√πng cho vi·ªác install SSM agent sau n√†y.\nDownload v√† Install SSM agent tr√™n c√°c server c·ªßa Data centre.\n Q. Tu·∫ßn tr∆∞·ªõc c√≥ 1 v√†i s·ª± c·ªë cho m√¥i tr∆∞·ªùng Production khi 1 s·ªë ph·∫ßn m·ªÅm unauthorised c·ªßa third-party dc install tr√™n servers. Gi·ªù b·∫°n c·∫ßn duy tr√¨ 1 tr·∫°ng th√°i ƒë√£ dc defined tr∆∞·ªõc (predefined state) cho all Ec2 th√¨ c·∫ßn l√†m g√¨ tr√™n SSM?\n D√πng State manager v·ªõi Command Document hay Policy Document, hay Automation Document, hay Package Document, hay Session Document?\nCommand Document: Runcommand d√πng lo·∫°i n√†y ƒë·ªÉ run command. C√≤n State Manager d√πng lo·∫°i n√†y ƒë·ªÉ apply configuration. Maintenance Windows th√¨ d√πng lo·∫°i n√†y ƒë·ªÉ apply configuraition theo schedule.\nPolicy Document: State Manager d√πng lo·∫°i n√†y ƒë·ªÉ apply policy tr√™n EC2\nAutomation Document: D√πng ƒë·ªÉ th·ª±c hi·ªán nh·ªØng task automation\nPackage Document: D√πng ƒë·ªÉ t·∫°o zip folder ƒë·ªÉ install tr√™n target instance\nSession Document: Session Manager d√πng lo·∫°i n√†y ƒë·ªÉ start 1 session ki·ªÉu nh∆∞ SSH tunnel hay port forwarding\n Q. 1 cty mu·ªën transfer 1 l∆∞·ª£ng l·ªõn data l√™n s3. S·ªë l∆∞·ª£ng data ban ƒë·∫ßu kho·∫£ng 100TB. D√πng service g√¨? Direct Connect hay Snowball?\n Snowball.\nko th·∫•y ƒë·ªÅ c·∫≠p v·ªÅ vi·ªác transfer data on-going, th·∫ø n√™n setup Direct connect ch·ªâ ƒë·ªÉ cho 1 l·∫ßn transfer nh∆∞ n√†y th√¨ ko h·ª£p l√Ω.\n Q. B·∫°n ƒëang d√πng CodeDeploy ƒë·ªÉ update app v·ªõi 1 password m·ªõi cho production. Ngta lo ng·∫°i r·∫±ng vi·ªác store new password trong khi update app tr√™n 1 l∆∞·ª£ng l·ªõn instance s·∫Ω l√†m service b·ªã outage. L√†m n√†o?\n D√πng System manager Parameter Store ƒë·ªÉ store pw. N√≥ store v√† save config data theo encrypted format. Parameter Store tƒÉng c∆∞·ªùng secure v√¨ n√≥ store password t√°ch ri√™ng kh·ªèi configuration file\n Q. VPC c·ªßa b·∫°n c√≥ c·∫£ Public subnet v√† private subnet. Application dc deploy trong 1 Ec2 ·ªü Private subnet. EC2 ƒë√≥ c·∫ßn t∆∞∆°ng t√°c v·ªõi S3. Nh∆∞ng l·∫°i ko t∆∞∆°ng t√°c S3 ƒëc, m·∫∑c d√π IAM role ƒë√£ ƒë√∫ng r·ªìi, L√†m n√†o?\n VPC c·ªßa b·∫°n c·∫ßn dc attach 1 VPC gateway enpoint.\nHo·∫∑c b·∫°n c·∫ßn ƒë·∫∑t Ec2 ƒë√≥ ·ªü Public subnet thay v√¨ private subnet nh∆∞ hi·ªán t·∫°i.\nT√∫m l·∫°i, n·∫øu EC2 ƒë·ªÉ trong private subnet mu·ªën connect S3 th√¨ c·∫ßn c√≥ VPC gateway enpoint\n Q. B·∫°n l√† IT admin, C√¥ng ty c√≥ 1 set EC2 tr√™n AWS, v√† 1 set server onpremise. H·ªç mu·ªën monitor system level log c·ªßa both side server theo metric th√¨ l√†m n√†o?\n Install cloudwatch agent tr√™n both set of servers\nsetup metrics dashboard tr√™n Cloudwatch\n Q. C√¥ng ty mu·ªën d√πng aws v√† s·ª≠ d·ª•ng g√≥i support plan c·ªßa aws. Nhu c·∫ßu l√†: C√≥ operational reviews v√† c√≥ respone time \u0026lt; 30 minutes cho nh·ªØng critical issues. N√™n ch·ªçn c√°i n√†o? Developer, Basic, Business, Enterprise?\n D√πng Enterprise\nko c√≥ Basic plan\ngi·ªëng nhau gi·ªØa Business v√† Enterprise:\nsystem h∆∞ h·ªèng (support \u0026lt; 12h)\nsystem Production b·ªã h∆∞ h·ªèng (support \u0026lt; 4h)\nsystem Production b·ªã down (support \u0026lt; 1h)\nkh√°c nhau gi·ªØa Business v√† Enterprise:\nsystem critical-quan tr·ªçng b·ªã down (Business ko support, Enterprise support \u0026lt; 15 ph√∫t)\nEnterprise c√≥ review operation, review well architect, coordinate v·ªõi expert c·ªßa AWS, c√≥ th·ªÉ access self-paced lab, c√≥ team ƒë·∫øn t·∫≠n n∆°i support, gi√° \u0026gt; 15000 USD/th√°ng\nTrong khi Bussiness ch·ªâ kho·∫£ng \u0026gt; 100 USD/th√°ng\n Q. Cty b·∫°n c√≥ kho·∫£ng 500 instances, c·∫ßn ensure l√† SSH lu√¥n dc disable. L√†m n√†o?\n D√πng AWS Config Rules ƒë·ªÉ check Security Groups\nTh·ª±c ch·∫•t AWS Config rule l√† nh·ªØng h√†m lambda check li√™n t·ª•c nh·ªØng ƒëi·ªÅu ki·ªán nh∆∞: EC2 lu√¥n dc associate v·ªõi √≠t nh·∫•t 1 SG, port 22 ko ƒëc m·ªü trong production SG, c√°c resource c·∫ßn c√≥ required tags, EIP c·∫ßn dc attach v√†o instances, c√°c volume ƒëang dc encrypt\n Q. 1 Dev mu·ªën dc notify b·∫•t c·ª© khi n√†o c√≥ API activity dc call tr√™n S3 bucket c·ªßa anh ta. L√†m n√†o?\n Config a cloud trail v√† SNS\n Q. B·∫°n start c√°c EC2 instance, nh∆∞ng ch√∫ng chuy·ªÉn sang b·ªã terminate ngay sau tr·∫°ng th√°i pending, v√¨ sao?\n ƒë√¢y l√† l·ªói Instance Terminates Immediately, l√Ω do:\n_EBS volume b·ªã limit\n_EBS snapshot b·ªã h·ªèng (corrupt)\n_EBS root volume b·ªã encrypt v√† b·∫°n ko c√≥ quy·ªÅn access v√†o KMS ƒë·ªÉ decrypt key\n_AMI m√† b·∫°n d√πng ƒë·ªÉ launch instance b·ªã thi·∫øu file n√†o ƒë√≥.\n-\u0026gt; T√∫m l·∫°i ch·ªß y·∫øu l√† l·ªói v·ªÅ EBS v√† AMI, ko th·ªÉ l√† l·ªói v·ªÅ limit instances ƒë∆∞·ª£c\n Q. Team b·∫°n d√πng EC2 ƒë·ªÉ setup database, s·∫Ω r·∫•t n·∫∑ng v·ªÅ workload, n√™n d√πng lo·∫°i EBS n√†o?\n Provisioned IOPS\nko d√πng General purpose SSD v√¨ AWS Document ghi r√µ io1 d√πng ƒë·ªÉ host DB workload l·ªõn\n Q. 1 cty ƒëang d√πng EC2 ƒë·ªÉ host Memcached d√πng l√† Cache cho c√°c app. Th√¨ n√™n ch·ªçn lo·∫°i instance n√†o? Memory Optimized? hay Compute Optimized? hay Storage Optimized? hay General Purpose?\n D√πng Memory Optimized Instances\nTrong AWS Documents c√≥ vi·∫øt:\nCompute Optimized Instances: (c4, c5)\nBatch processing\nMedia transcoding\nHigh-performance web servers\nScientific modeling\nDedicated gaming servers and ad serving engines\nMachine learning\nGeneral Purpose Instances: (a1, m5, t2, t3)\nWeb/Game servers\nmicroservices\nCaching fleets\nSmall and medium databases\nCode repositories\nDevelopment, build, test, and staging environments\nMemory Optimized Instances: (r4, r5)\nHigh-performance Database Relational/NoSQL\nIn-memory caching (Memcached and Redis).\nIn-memory databases and analytics (for example, SAP HANA).\nReal-time processing (Hadoop/Spark clusters).\nStorage Optimized Instances: (d2, h1, i3)\ndata warehouse\nMapReduce and Hadoop distributed computing\nLog or data processing applications\nHigh frequency online transaction processing (OLTP) systems\n Q. Cty b·∫°n c√≥ 1 set EC2, c√°c EC2 n√†y run 24/7 trong su·ªët c·∫£ nƒÉm. V√† b·∫°n c·∫ßn upgrade instance type ƒë·ªÉ ƒë√°p ·ª©ng ƒëc workload trong 1 nƒÉm ƒë√≥ th√¨ n√™n ch·ªçn lo·∫°i n√†o? Convertible Reserved hay Standard Reserved Instance?\n Convertible Reserved Instances\n2 lo·∫°i Convertible v√† Standard kh√°c nhau ·ªü ch·ªó lo·∫°i Standard ko th·ªÉ change dc instance type during the term.\nLo·∫°i Standard th√¨ c√≥ th·ªÉ change instance size, tuy nhi√™n ko th·ªÉ exchange\nLo·∫°i Convertible th√¨ c√≥ th·ªÉ change dc instance family, type, size, platform, scope, tenacy.\n Q. B·∫°n c√≥ 3 VPC A, B, C, c·∫ßn ph·∫£i communicate v·ªõi nhau, l√†m sao ƒë·ªÉ ensure l√† all instance s·∫Ω communicate dc v·ªõi nhau?\n A peer v·ªõi B, B peer C, A peer C\nM·ªói Route table trong 1 vpc c·∫ßn ph·∫£i add CIDR block c·ªßa 2 c√°i c√≤n l·∫°i v√†o Destination\n Q. B·∫°n c√≥ 1 VPC, v√† 1 custom domain, b·∫°n mu·ªën ensure l√† custom domain ch·ªâ dc access b·ªüi c√°c instance trong VPC th√¥i, ko ph·∫£i public internet, l√†m n√†o?\n D√πng private hosted zone ·ªü Route53\n Q. NAT Gateway setup ·ªü trong public subnet hay private subnet? bandwidth restrictions l√† g√¨?\n Trong public subnet\nbandwidth restrictions l√† h·∫°n ch·∫ø v·ªÅ bƒÉng th√¥ng\n Q. B·∫°n ƒëang c·∫ßn connect on-premise l√™n AWS vpc. B·∫°n nh·∫≠n nhi·ªám v·ª• setup vpn connection ƒë·ªÉ setup hybrid architecture th√¨ c·∫ßn l√†m nh·ªØng step g√¨?\n T·∫°o customer gateway\nT·∫°o virtual private gateway attach v√†o vpc\nUpdate route table, s·ª≠a c√°i route propagation: add c√°i virtual private gateway v·ª´a t·∫°o v√†o\nUpdate SG: ph·∫ßn inbound c·∫ßn enable SSH, RDP, ICMP access\nT·∫°o VPN connection\n Q. B·∫°n ƒëang t·∫°o 1 Organization cho c√¥ng ty b·∫°n, nh·ªØng g√¨ l√† ƒë√∫ng v·ªõi Master account c·ªßa Organization?\n t·∫°o Organization v√† OU,\ninvite external account ƒë·ªÉ join v√†o org,\nthanh to√°n t·∫•t c·∫£ charge c·ªßa all account trong org (ko ch·ªâ OU),\nmaster account ko b·ªã ·∫£nh h∆∞·ªüng b·ªüi service control policies\n Q. 1 App ƒëc deploy tr√™n ec2, c√≥ d√πng ELB. B·∫°n c·∫ßn list c√°c Ip address truy c·∫≠p v√†o ELB, l√†m n√†o?\n enable access logging cho ELB,\nc·∫•p quy·ªÅn ƒë·ªÉ access logging truy c·∫≠p dc v√†o s3 bucket\nAWS Document gi·∫£i th√≠ch: ELB access logging l∆∞u tr·ªØ client ip, timestamp, latencies, request path, server respone. Tuy nhi√™n default n√≥ disable, N·∫øu mu·ªën th√¨ ph·∫£i enable l√™n. Sau khi enable th√¨ ELB s·∫Ω capture log v√† l∆∞u v√†o S3 bucket m√† b·∫°n ch·ªâ ƒë·ªãnh\nC·∫ßn ph·∫£i s·ª≠a Bucket policy ƒë·ªÉ ELB c√≥ quy·ªÅn ghi log v√†o bucket ƒë√≥\n Q. Web app server ƒëc deploy tr√™n ec2, C·∫ßn 1 ph∆∞∆°ng ph√°p ph√≤ng ch·ªëng security, b·∫°n c·∫ßn t·∫°o 1 process ƒë·ªÉ ƒë√°nh gi√° security sau khi deploy. C√°ch hi·ªáu qu·∫£ ƒë·ªÉ m·ªói khi c√≥ thay ƒë·ªïi v·ªÅ Security Group d·∫´n ƒë·∫øn s·ª± ti·∫øp x√∫c v·ªõi Internet (assess network exposure) b·∫°n s·∫Ω dc bi·∫øt l√† g√¨?\n S·ª≠ d·ª•ng Amazon Inspector, n√≥ s·∫Ω c√≥ c√°i Network Reachability package ƒë·ªÉ evaluate Network config c√≥ ƒëang risk ko. C√°i package n√†y ko y√™u c·∫ßu install agent Inspector tr√™n ec2.\nB·∫°n c√≥ th·ªÉ t·∫°o 1 Cloudwatch event ƒë·ªÉ monitor s·ª± thay ƒë·ªïi c·ªßa SG or VPC config, khi ƒë√≥ n√≥ s·∫Ω trigger c√°i Network Assessment\nNetwork Reachability package s·∫Ω t·ª± ƒë·ªông monitor AWS network c·ªßa b·∫°n, xem c√≥ c√°i n√†o misconfig ko. N√≥ ko y√™u c·∫ßu install agent Inspector, tuy nhi√™n n·∫øu install s·∫Ω dc cung c·∫•p ch√≠nh x√°c c√°c process ƒëang listen on ports.\npackage n√†y ko support Clasic EC2\n Q. 1 cty setup 1 website host tr√™n s3, mu·ªën user c√≥ seamless user experience (tr·∫£i nghi·ªám li·ªÅn m·∫°ch) th√¨ c·∫ßn g√¨?\n D√πng cloudfront\n Q. Ph√¢n bi·ªát 3 c√°ch Server side encrpytion s3: (SSE-S3, SSE-KMS, SSE-C)\n  Q. 1 Cty c·∫ßn store historical data. C√≥ th·ªÉ s·∫Ω d√πng Business Intelligence solution ƒë·ªÉ query ƒë·ªëng data ƒë√≥. Th√¨ n√™n d√πng lo·∫°i DB n√†o? EMR, DynamoDB, Redshift?\n D√πng Redshift.\nDynamoDB th√¨ ko th·ªÉ l∆∞u tr·ªØ historical data\nEMR d√πng ƒë·ªÉ l√†m data transformation, v√† load data ƒë√£ processed ƒë·∫øn Redshift th√¥i\nC√≤n Redshift th√¨ full managed, petabyte scale data warehouse service, l√† gi·∫£i ph√°p v·ªÅ historical data storage, c√≥ th·ªÉ query nh∆∞ SQL\n Q. B·∫°n ƒëang c√≥ AWS RDS MySQL database, ƒëang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ performance, v√¨ l√† sysadmin b·∫°n c·∫ßn xem c√≥ c√°ch n√†o c·∫£i thi·ªán performance ko. D√πng c√°i g√¨? (AWS Trust Advisor, AWS Inspector, AWS Performance Insights, AWS Config?)\n D√πng AWS Performance Insights\nN√≥ l√† 1 ch·ª©c nƒÉng c√≥ trong RDS/Aurora, support cho c√°c lo·∫°i DB nh·∫•t ƒë·ªãnh, gi√∫p monitor, ph√¢n t√≠ch issue, trouble shooting performance problem v√† visualize data load.\nInspector d√πng ƒë·ªÉ ph√¢n t√≠ch EC2 vulnerability\nAWS Config ƒë·ªÉ manage configuration\nTrust Advisor ch·ªâ ƒë∆∞a ra c√°c best practice ch·ª© ko ph√¢n t√≠ch s√¢u c√°c issue trong DB c·ªßa b·∫°n\n Q. B·∫°n c√≥ 1 set c√°c EC2, c·∫ßn 1 dashboard metric c·ªßa CPU utilization trong interval 1 ph√∫t. C·∫ßn l√†m n√†o?\n Enable detailed monitoring for EC2 (Console-\u0026gt;Action-\u0026gt;Cloudwatch monitoring-\u0026gt;Enable Detailed monitoring)\nT·∫°o 1 dashboard trong Cloudwatch\nB·ªüi v√¨ n·∫øu ƒë·ªÉ ch·∫ø ƒë·ªô Basic monitoring c·ªßa EC2 th√¨ interval send metric to Cloudwatch l√† 5 ph√∫t\n Q. C√¥ng ty b·∫°n ƒëang setup database tr√™n EC2, C·∫ßn ph·∫£i config fault tolenrance v√¨ ƒë√¢y s·∫Ω host nh·ªØng data quan tr·ªçng. D·ª± ƒë·ªãnh implement RAID configuration. Th√¨ n√™n ch·ªçn lo·∫°i n√†o? RAID 0, RAID 1, RAID 5, RAID 6?\n D√πng RAID 1\nRAID 0 d√πng ch·ªß y·∫øu cho m·ª•c ƒë√≠ch I/O performance. B·∫°n add th√™m volume th√¨ b·∫°n s·∫Ω c√≥ th√™m throughput. N√≥ k·∫øt h·ª£p c√°c volume l·∫°i th√†nh 1 stripe. Tuy nhi√™n performance c·ªßa stripe b·ªã gi·ªõi h·∫°n ·ªü c√°i performance c·ªßa c√°i volume k√©m nh·∫•t. V√† n·∫øu m·∫•t 1 volume c≈©ng s·∫Ω m·∫•t to√†n b·ªô data\nRAID 1 d√πng ch·ªß y·∫øu cho m·ª•c ƒë√≠ch fault tolerance (c√°c ·ª©ng d·ª•ng critical). Data s·∫Ω ·ªïn ƒë·ªãnh h∆°n. Nh∆∞ng v√¨ data dc ghi v√†o nhi·ªÅu volume c√πng 1 l√∫c, n√™n c·∫ßn bƒÉng th√¥ng l·ªõn. V√† n√≥ ko c·∫£i thi·ªán performance WRITE dc.\nRAID 5,6 ko dc recommend cho Amazon EBS, v√¨ n√≥ l√†m gi·∫£m IOPS c·ªßa EBS xu·ªëng 20-30% so v·ªõi RAID 0, trong khi gi√° tƒÉng g·∫•p 2. V√≠ d·ª•: 1 array g·ªìm 2 volume RAID 0 s·∫Ω performance t·ªët h∆°n 1 array 4 volume RAID 6 m√† c√≥ gi√° g·∫•p ƒë√¥i.\nN√≥i chung c·ª© nh·ªõ, RAID 0 stripe, v√¨ n√≥ stripe nhi·ªÅu volume l·∫°i ƒë·ªÉ tƒÉng performance. T·ª´ ƒë√≥ suy ra RAID 1 s·∫Ω fault tolerance.\n Q. B·∫°n c·∫ßn s·ª≠ d·ª•ng AWS QuickSight v√† c√≥ g√¨ c·∫ßn ch√∫ √Ω gi·ªØa 2 phi√™n b·∫£n Enterpise v√† Standard?\n Enterprise h∆°n Standard ·ªü ch·ªó cho ph√©p encrpyt data rest v√† t∆∞∆°ng th√≠ch v·ªõi Microsoft Active Directory (AD)\nB·∫£n Standard th√¨ b·∫°n c√≥ th·ªÉ invite IAM user v√†o ho·∫∑c t·∫°o cho h·ªç 1 user QuickSight only\nB·∫£n Enterprise th√¨ d√πng MS AD ƒë·ªÉ user access QuickSight\n Q. 1 Cty mu·ªën c√°c EC2 ƒë∆∞·ª£c patch v·ªõi latest security patches?\n AWS System Management\n Q. B·∫°n c√≥ 1 VPC v√† c√°c subnets. Sau khi launch EC2 th√¨ n√≥ ko c√≥ public DNS name, c·∫ßn l√†m g√¨?\n Check 2 attribute c·ªßa VPC: enableDnsHostnames, enableDnsSupport\nN·∫øu c·∫£ 2 ƒëang dc set = true th√¨ EC2 m·ªõi c√≥ public DNS hostname\n Q. B·∫°n ƒëang setup AWS Direct Connect gi·ªØa AWS VPC v√† on-premise data center. Th√¨ Network c·∫ßn ph·∫£i nh∆∞ n√†o?\n Network c·ªßa b·∫°n ph·∫£i ƒëang ƒë·∫∑t c√πng ch·ªó v·ªõi AWS Direct Connect location (C·∫ßn t√¨m hi·ªÉu v·ªÅ location hi·ªán c√≥ c·ªßa Direct Connect)\nNetwork c·ªßa b·∫°n ph·∫£i l√†m vi·ªác v·ªõi AWS Direct Connect Partner-ƒë·ªëi t√°c s·∫Ω h·ªó tr·ª£ kh√°ch h√†ng (l√† b·∫°n) connect v√†o Direct Connect\nV·ªÅ k·ªπ thu·∫≠t:\nNetwork c·ªßa b·∫°n ph·∫£i d√πng single-mode fiber\nAuto negotiation for port ph·∫£i disabled\n802.1Q VLAN ph·∫£i dc support\nDevice ph·∫£i support BGP (Border Gateway Protocol) v√† BGP MD5\n Q. B·∫°n ƒëang qu·∫£n l√Ω vi·ªác migrate 1 app on-premise l√™n AWS. Dev ƒëang launch EC2 ƒë·ªÉ test app. B·∫°n concern v·ªÅ s·ªë l∆∞·ª£ng ec2, service n√†o d√πng ƒë·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng ec2 c√≥ ƒëang trong limit hay ko? AWS Config, AWS Trust Advisor, SSM, Cloudwatch?\n AWS Trust Advisor\nAWS Trust Advisor dashboard c√≥ 1 feature t√™n l√† Service limit, d√πng ƒë·ªÉ li√™n t·ª•c monitor usaged resource ƒë·ªÉ b·∫°n c√≥ th·ªÉ request tƒÉng limit ho·∫∑c shut down b·ªõt resource tr∆∞·ªõc khi limit b·ªã reached\n Q. B·∫°n t·∫°o EC2 nh∆∞ng n√≥ ko auto c√≥ public Ip address, l√†m n√†o?\n S·ª≠a Subnet, modify auto-asign public IP = true\nNh·ªõ l√† S·ª≠a subnet ch·ª© ko ph·∫£i VPC\n Q. B·∫°n ƒëang setup Test env. B·∫°n b·∫≠t VPC flow log ƒë·ªÉ capture IP traffic cho sau n√†y c√≤n audit. VPC flow log publish nhi·ªÅu log file v√†o 1 single s3 bucket. V√†i flow log ƒë·∫ßu th√¨ dc publish v√†o encrypted bucket th√†nh c√¥ng, nh∆∞ng sau ƒë√≥ th√¨ b·ªã l·ªói LogDestinationPermissionIssueException. V√¨ sao?\n V√¨ S3 bucket policy gi·ªõi h·∫°n 20KB th√¥i, M·ªói l·∫ßn t·∫°o 1 flow log m·ªõi, aws s·∫Ω add th√™m ARN c·ªßa folder m·ªõi v√†o Resource element c·ªßa bucket policy. C√†ng nhi·ªÅu th√¨ s·∫Ω c√†ng c√≥ nguy c∆° v∆∞·ª£t qu√° limit 20KB\nGi·∫£i ph√°p: x√≥a h·∫øt c√°c entries c≈© ko d√πng trong policy. Ho·∫∑c ph√¢n quy·ªÅn cho to√†n b·ªô bucket policy l√† arn:aws:s3:::bucket_name/*\nC√≤n n·∫øu g·∫∑p l·ªói LogDestinationNotFoundException: th√¨ c√≥ th·ªÉ ARN c·ªßa s3 bucket ƒëang ch·ªâ ƒë·ªãnh sai, ho·∫∑c bucket ƒë√≥ ko c√≤n t·ªìn t·∫°i\nC√ín n·∫øu g·∫∑p l·ªói t·∫°o Flow log r·ªìi nh∆∞ng ko nh√¨n th·∫•y log ·ªü trong s3 bucket hay Cloudwatch, th√¨ l√† do ch∆∞a c√≥ traffic sinh ra, ho·∫∑c ph·∫£i ch·ªù 10 ph√∫t\n\u0026mdash;P2\u0026mdash;  Q. B·∫°n c√≥ 1 VPC, ƒë√£ setup SG, network ACL, nh∆∞ng traffic ko reach ƒë·∫øn instance, l√†m sao ƒë·ªÉ diagnose issue? d√πng Cloudtrail hay VPC Flow logs ?\n D√πng VPC Flow logs, n√≥ gi√∫p monitor traffic reach instances\nch·ª© Cloudtrail ƒë·ªÉ monitoring API th√¥i\n Q. 2 VPC A(10.0.0.0/16) v√† VPC B(20.0.0.0/16), ƒë√£ t·∫°o 1 VPC peering c√≥ id l√†: pcx-1a2b1a2b. Th√¨ route table c·ªßa 2 VPC c·∫ßn config Destination v√† Target l√† g√¨?\n Destination l√† CIDR block c·ªßa VPC kia, c√≤n Target l√† id c·ªßa VPC peering\n Q. B·∫°n ƒë√£ t·∫°o 1 RDS Instance. C√¥ng ty mu·ªën ko c√≥ downtime khi 1 snapshot dc t·∫°o ra. L√†m n√†o?\n enable multi-AZ\nv√¨ single-AZ g√¢y downtime, n√™n n·∫øu multi-AZ th√¨ vi·ªác t·∫°o snapshot s·∫Ω ch·ªâ di·ªÖn ra tr√™n con RDS standby\n Q. 1 Consultant mu·ªën view contents c·ªßa 1 S3 bucket trong AWS Account. Content ƒë√≥ s·∫Ω available trong 1 kho·∫£ng time. Th√¨ n√™n d√πng c√°ch g√¨?\n T·∫°o 1 IAM Role v·ªõi session duration parameter\nKo th·ªÉ specify time limit trong bucket ACL ƒë∆∞·ª£c\nC≈©ng ko th·ªÉ t·∫°o IAM User v·ªõi session duration parameter ƒë∆∞·ª£c\nb·∫Øt bu·ªôc d√πng ROLE\n Q. Cty b·∫°n host web tr√™n S3. User s·∫Ω truy c·∫≠p v√†o web b·∫±ng uRL: http://companya.com, v·∫≠y khi ƒë·∫∑t t√™n cho s3 bucket c·∫ßn ƒë·∫∑t t√™n g√¨?\n companya.com\nk·∫øt h·ª£p v·ªõi Route53 ƒë·ªÉ setting c√°i domain c·ªßa ri√™ng c√¥ng ty b·∫°n\n Q. 1 flow log record c√≥ format nh∆∞ sau:\n10 123456789010 eni-abc123de 172.31.41.189 172.8.51.117 39751 3389 6 20 3279 1218430010 1218430070 REJECT OK\nnghƒ©a l√† g√¨?\n ip 172.31.41.189 t·ª´ port 39751 ƒëang c·ªë connect ƒë·∫øn ip 172.8.51.117 ·ªü port RDP 3389, giao th·ª©c TCP, v√† b·ªã REJECT, ƒë√£ l∆∞u v√†o log\ns·ªë packages g·ª≠i ƒë·∫øn l√† 20, n·∫∑ng 3279 byte, timestamp l√∫c start l√† 1218430010 v√† end l√† 1218430070\n\u0026lt;version 10\u0026gt; \u0026lt;account-id\u0026gt; \u0026lt;interface-id\u0026gt; \u0026lt;source address\u0026gt; \u0026lt;dest address\u0026gt; \u0026lt;srcport\u0026gt; \u0026lt;dstport\u0026gt; \u0026lt;protocol\u0026gt; \u0026lt;packets\u0026gt; \u0026lt;bytes\u0026gt; \u0026lt;start\u0026gt; \u0026lt;end\u0026gt; \u0026lt;action\u0026gt; \u0026lt;log-status\u0026gt;\n Q. 1 Cty host 1 web app v√† c√≥ DynamoDB l√†m backend. V·∫•n ƒë·ªÅ l√† khi peak loads c√≥ th·ªÉ c√°c request ƒë·∫øn DB s·∫Ω b·ªã throttle. L√†m n√†o ƒë·ªÉ ƒë√°p ·ª©ng dc l∆∞·ª£ng l·ªõn c√°c request ƒë√≥?\n Enable auto scaling cho DynamoDB, n√≥ s·∫Ω tƒÉng read/write capacity ƒë·ªÉ DB ko b·ªã throttle\nC√°c DB nh∆∞ RDS, Aurora, DynamoDB ƒë·ªÅu c√≥ th·ªÉ enable auto scaling\nTr∆∞·ªõc ƒë√¢y SAA Notes c√≥ 1 c√¢u: DB b·ªã tƒÉng s·ªë l∆∞·ª£ng write, ko th·ªÉ handle ƒëc load. L√†m n√†o ƒë·ªÉ write ko b·ªã m·∫•t c√°i n√†o? - D√πng SQS FIFO ƒë·ªÉ l∆∞u c√°c write request pending\n-\u0026gt; th√¨ ƒë√≥ l√† do c√¢u h·ªèi ch·ªâ mu·ªën ƒë·ªÉ write request ko b·ªã m·∫•t c√°i n√†o th√¥i, ch·ª© SQS ko ƒë√°p ·ª©ng dc vi·ªác high throughput c·ªßa DynamoDB\n Q. B·∫°n restore 1 volume t·ª´ 1 snapshot ƒë·ªÉ attach v√†o EC2. B·∫°n nh·∫≠n ra c√≥ ƒë·ªô tr·ªÖ v·ªÅ I/O tr√™n c√°i New volume ƒë√≥ khi app ƒëc launch l·∫ßn ƒë·∫ßu. L√†m sao ƒë·ªÉ c√°c volume sau n√†y dc restore t·ª´ snapshot ko b·ªã issue ƒë√≥ n·ªØa?\n Ensure l√† all block tr√™n c√°i volume m·ªõi ƒë√≥ ph·∫£i dc access √≠t nh·∫•t 1 l·∫ßn\n1 New EBS volume th√¨ s·∫Ω lu√¥n maximum performance ngay t·ª´ ƒë·∫ßu m√† ko c·∫ßn initialization (hay c√≤n g·ªçi l√† pre-warming)\nNh∆∞ng c√°c storage blocks c·ªßa volume dc restore t·ª´ snapshot th√¨ ph·∫£i dc initialization, n·∫øu ko s·∫Ω c√≥ issue v·ªÅ performance trong l·∫ßn ƒë·∫ßu access\n Q. 1 Cty c√≥ set c√°c EC2 trong VPC. C·∫ßn scan xem c√≥ risk v·ªÅ security ko, check c·∫ßn ph·∫£i l√†m v·ªõi ‚ÄúCenter for Internet Security (CIS) Benchmarks‚Äù. N√™n d√πng service n√†o? Trusted Advisor, Inspector, Config, GuardDuty?\n Inspector\nHi·ªán gi·ªù Inspector ƒëang support nh·ªØng rule sau:\n¬∑ Common Vulnerabilities and Exposures\n¬∑ Center for Internet Security (CIS) Benchmarks\n¬∑ Security Best Practices\n¬∑ Runtime Behavior Analysis\nTrusted Advisor ƒë·ªÉ ƒë∆∞a ra recommendations,\nConfig ƒë·ªÉ config,\nGuardDuty ƒë·ªÉ qu·∫£n l√Ω threat detections\n Q. B·∫°n c√≥ 1 set c√°c EC2, ƒë·ªÉ gi·∫£m cost, th√¨ mu·ªën shutdown c√°c EC2 ko ƒëc s·ª≠ d·ª•ng, l√†m n√†o?\n t·∫°o Cloudwatch alarm, base on c√°c metric c·ª• th·ªÉ ƒë·ªÉ shutdown instance\n Q. Cty b·∫°n t·∫°o AWS Organization ƒë·ªÉ qu·∫£n l√Ω nhi·ªÅu account. V·ªõi nh·ªØng service m√† li√™n quan ƒë·∫øn critical v·ªÅ security th√¨ h·ªç mu·ªën ch√∫ng dc nottify cho Security Team t·ª´ nh·ªØng account trong org ƒë√≥. L√†m n√†o?\n D√πng AWS Personal Health Dashboard v√† trong m·ªói account s·∫Ω t·∫°o 1 CF template ƒë·ªÉ run Cloudwatch rule c·ªßa Org, Rule ƒë√≥ s·∫Ω notify Security Team m·ªói khi c√≥ critical alarm ƒë∆∞·ª£c generate ra.\n Q. B·∫°n c√≥ 1 set c√°c app tr√™n EC2 trong private subnet. C√°c app n√†y c·∫ßn d√πng Kinesis stream. L√†m n√†o ƒë·ªÉ ensure l√† c√°c app c√≥ th·ªÉ s·ª≠ d·ª•ng Kinesis stream?\n D√πng 1 VPC Endpoint interface\nVPC Endpoint interface dc cung c·∫•p b·ªüi Private link, ƒë·∫£m b·∫£o c√°c traffic ko ƒëi ra kh·ªèi Amazon network\n Q. C√¥ng ty b·∫°n ƒëang ch·ªçn g√≥i support c·ªßa AWS, c·∫ßn:\n‚ñ† 24x7 access v√†o customer service\n‚ñ† trong gi·ªù h√†nh ch√≠nh access v√†o Cloud Support Associate\n Ch·ªçn Developer l√† ƒë·ªß  Q. B·∫°n d√πng public key hay private key ƒë·ªÉ access v√†o EC2? key ƒë·ªÉ trong EC2 l√† public hay private?\n D√πng private key ƒë·ªÉ access v√†o EC2,\nKey trong EC2 l√† public key\n Q. B·∫°n ƒëang d√πng Storage Gateway ƒë·ªÉ extend data storage c·ªßa b·∫°n, y√™u c·∫ßu l√† all data c·∫ßn encrypt at rest th√¨ d√πng g√¨?\n KMS, ho·∫∑c S3 SSE\n Q. B·∫°n ƒëang d√πng Redshift ƒë·ªÉ manage data warehouse. Y√™u c·∫ßu l√† all data c·∫ßn transfer ƒë·∫øn Redshift qua 1 VPC. L√†m n√†o?\n Enable Amazon Redshift Enhanced VPC Routing\nkhi l√†m v·∫≠y, Redshift s·∫Ω force all COPY v√† UNLOAD traffic th√¥ng qua VPC.\n Q. Cty b·∫°n mu·ªën d√πng S3 d·ªÉ l∆∞u file, v√† l√†m ch√∫ng available nh∆∞ NFS file share ƒë·∫øn On-premise server. L√†m n√†o?\n D√πng File gateway\nAWS Storage Gateway cung c·∫•p, File-based, Volume-based, Tape-based, th√¨ c√°i File Gateway ch√≠nh l√† File-based.\nN√≥ cung c·∫•p access v√†o s3 object nh∆∞ l√† files/share mount point B·∫°n c√≥ th·ªÉ l∆∞u v√† retrieve S3 object s·ª≠ d·ª•ng NFS ho·∫∑c SMB (server message block)\n Q. Cty b·∫°n d√πng ElastiCache, b·∫°n ƒë√£ setup Memcahed cho h·ªç, Sau khi d√πng th√¨ h·ªç alert r·∫±ng s·∫Ω c√≥ th·ªÉ b·ªã high CPU utilization cho cluster. L√†m sao ƒë·ªÉ resolve issue ƒë√≥?\n scale up cluster b·∫±ng c√°ch d√πng 1 larger node type,\nho·∫∑c scale out cluster b·∫±ng c√°ch add th√™m nodes\nAWS Document vi·∫øt:\nC√≥ nhi·ªÅu lo·∫°i Cloudwatch metric monitor performance c·ªßa ElastiCache: CPUUtilization, SwapUsage, Evictions, CurrConnections.\n‚ñ† CPUUtilization \u0026gt; 90% th√¨ n√™n l√†m nh∆∞ tr√™n\n‚ñ† SwapUsage \u0026gt; 50MB th√¨ n√™n tƒÉng value c·ªßa parameter ConnectionOverhead\n‚ñ† Evictions l√† lo·∫°i engine metric. N·∫øu g·∫∑p th√¨ n√™n scale up cluster b·∫±ng c√°ch d√πng 1 larger node type, ho·∫∑c scale out cluster b·∫±ng c√°ch add th√™m nodes\n‚ñ† CurrConnections, n·∫øu g·∫∑p th√¨ n√™n xem l·∫°i app c·ªßa b·∫°n\n Q. C√¥ng ty b·∫°n c√≥ 1 set c√°c EC2 v√† c·∫£ c√°c server tr√™n on-premise env. B·∫°n c·∫ßn collect th√¥ng tin r·∫±ng c√°c server n√†y ƒë√£ install software g√¨? th√¨ l√†m n√†o? D√πng System Manager ƒë·ªÉ get software inventory, hay d√πng AWS Config?\n B·∫°n c√≥ th·ªÉ d√πng SSM ƒë·ªÉ query metadata ƒë·ªÉ bi·∫øt nh·ªØng software n√†o ƒë√£ dc install, c√°i server n√†o c·∫ßn update, ..\nD√πng AWS Config ch·ªâ c√≥ th·ªÉ check s·ª± thay ƒë·ªïi v·ªÅ configuration c·ªßa server, ch·ª© ko th·ªÉ bi·∫øt server ƒë√£ install g√¨.\n Q. 1 Cty mu·ªën d√πng Active Directory c√≥ s·∫µn ƒë·ªÉ grant quy·ªÅn cho user access v√†o AWS Console th√¨ chu·ªói action s·∫Ω l√† g√¨? (nghƒ©a l√† d√πng IAM Federated sign-in using AD)\n 1- User access v√†o portal sign-in page ƒë∆∞·ª£c cung c·∫•p b·ªüi AD FS (Active Directory Federation Service), r·ªìi login\n2- AD FS s·∫Ω authenticate user ƒë·ªëi v·ªõi AD\n3- AD tr·∫£ v·ªÅ User info, trong ƒë√≥ bao g·ªìm AD group membership\n4- AD FS d√πng group membership info ƒë·ªÉ create SAML redirect link ƒë·∫øn AWS STS\n5- AWS STS d√πng AssumeRoleWithSAML ƒë·ªÉ grant temporary credential cho user (session t·ª´ 15m ƒë·∫øn max 12h, default 1h) v√† tr·∫£ v·ªÅ\n6- User ƒë∆∞·ª£c authenticated v√† access v√†o AWS console\nKh√¥ng d√πng AssumeRoleWithWebIdentity v√¨ m√¨nh d√πng AD n√™n ph·∫£i ƒëi v·ªõi SAML\n Q. 1 Cty c√≥ DynamoDB. H·ªç mu·ªën ensure l√† c√≥ th·ªÉ recover dc trong TH accidental write or delete operation. L√†m n√†o? c√≥ d√πng Multi-AZ ko?\n D√πng Point-in-time recovery feature, b·∫°n c√≥ th·ªÉ recover v·ªÅ any point trong kho·∫£ng 35 ng√†y (backup retention period (th·ªùi gian l∆∞u backup) max 35 days)\nMulti-AZ l√† feature c·ªßa RDS th√¥i, ko ph·∫£i c·ªßa DynamoDB, DynamoDB c√≥ c√°i g·ªçi l√† global table\n Q. So s√°nh gi·ªØa Multi-AZ v√† Read Replicas c·ªßa RDS:\n Multi-AZ l√† Synchronous (ReadRep l√† async)\nMulti-AZ ch·ªâ primary instance l√† active (ReadRep th√¨ m·∫•y c√°i replica c√≥ th·ªÉ read)\nMulti-AZ th√¨ auto backup tr√™n con standby (ReadRep th√¨ default ko config)\nMulti-AZ th√¨ lu√¥n span 2 AZ trong region (ReadRep th√¨ c√≥ th·ªÉ 1 AZ, ho·∫∑c Cross-AZ, Cross-Region)\nMulti-AZ th√¨ DB engine upgrade tr√™n con primary (ReadRep th√¨ ko ph·ª• thu·ªôc)\nMulti-AZ th√¨ auto failover khi c√≥ v·∫•n ƒë·ªÅ (ReadRep th√¨ manual promote t·ª´ replica th√†nh 1 con db instance standalone)\n Q. B·∫°n c√≥ 1 set c√°c DynamoDB table. C·∫ßn ph·∫£i monitoring report v·ªÅ Read/Write Capacity c·ªßa DynamoDB ƒë·ªÉ utilized. L√†m n√†o?\n D√πng Cloudwatch metrics ƒë·ªÉ xem Read/Write capacity dc utilized\nCloudwatch cung c·∫•p c√°c metric ƒë·ªÉ monitor DynamoDB:\nN·∫øu mu·ªën monitor rate of TTL deletion on table? - monitor c√°i TimeToLiveDeletedItemCount\nN·∫øu mu·ªën xem dynamo table c·ªßa m√¨nh ƒëang c√≥ provisioned throughput l√† bao nhi√™u? - monitor c√°i ConsumedReadCapacityUnits ho·∫∑c ConsumedWriteCapacityUnits\nN·∫øu mu·ªën x√°c ƒë·ªãnh request n√†o v∆∞·ª£t qu√° limit c·ªßa throughput? - monitor c√°i ThrottledRequest, ReadThrottleEvents, WriteThrottleEvents\nN·∫øu mu·ªën x√°c ƒë·ªãnh c√≥ l·ªói system err ko? - monitor c√°i SystemErrors xem c√≥ l·ªói HTTP 500 (Server error) hay ko\n Q. Cty b·∫°n c√≥ 1 S3 bucket tr√™n EU region. Gi·ªù c√≥ y√™u c·∫ßu replicate c√°c object sang 1 region kh√°c, c·∫ßn ensurre ƒëi·ªÅu g√¨?\n C·∫£ source v√† destination bucket ƒë·ªÅu ph·∫£i enable versioning\nv√† t·∫•t nhi√™n l√† ch√∫ng ph·∫£i kh√°c AWS region\n Q. Cty b·∫°n c√≥ 2 account AWS, 1 c√°i cho Dev, 1 c√°i cho Prod, gi·ªù Dev c·∫ßn access v√†o Prod account ƒë·ªÉ update application th√¨ l√†m n√†o cho secure?\n T·∫°o 1 cross account role, r·ªìi share ARN cho Role\nTrong AWS Document c√≥ ghi:\n1- Trong Prod account, admin t·∫°o 1 role UpdateApp, trong trust policy c·ªßa role s·∫Ω ch·ªâ ƒë·ªãnh AWS account id c·ªßa Dev Account. Trong permission c·ªßa role th√¨ define nh·ªØng c√°i quy·ªÅn c·ª• th·ªÉ nh∆∞ write s3 bucket\u0026hellip;\n2- Trong Dev account, admin s·∫Ω grant quy·ªÅn cho group developer c√≥ quy·ªÅn AssumeRole, ƒë·ªÉ t·ª´ ƒë√≥ nh∆∞ng IAM trong group developer s·∫Ω c√≥ th·ªÉ switch role sang.\n3- User s·∫Ω switch Role tr√™n console, ho·∫∑c CLI\n4- STS s·∫Ω verify v√† return credential t·∫°m th·ªùi,\n5- T·ª´ ƒë√≥ User s·∫Ω t·ª± ƒë·ªông dc redirect sang console c·ªßa account Prod\n Q. Cty c√≥ 1 on-premise web, mu·ªën healthcheck li√™n t·ª•c, n·∫øu health b·ªã degrade th√¨ s·∫Ω switch sang 1 static website tr√™n aws, l√†m n√†o?\n D√πng Route53, health check to failover\n Q. B·∫°n c√≥ set c√°c EC2 trong private subnet (10.0.1.0/24), nh·ªØng EC2 n√†y c·∫ßn download update t·ª´ internet HTTPS. B·∫°n ƒë√£ setup NAT instance trong public subnet. Gi·ªù c·∫ßn setup SG cho NAT instance nh∆∞ n√†o?\n Inbound rule: cho ph√©p traffic t·ª´ private subnet IP qua port 443\nTheo AWS Document:\nSG c·ªßa NAT instance n√™n setting l√†:\n‚ñ† Inbound cho ph√©p traffic t·ª´ private subnet IP qua port 443/80\n‚ñ† Outbound cho ph√©p traffic t·ª´ port 80/443 ra 0.0.0.0/0 (ƒë·ªÉ cho NAT access internet)\nGi·∫£i th√≠ch th√™m: B·∫£n th√¢n con NAT ƒë√£ n·∫±m ·ªü public subnet r·ªìi, n√≥ ƒë√£ nh·∫≠n dc traffic t·ª´ World r·ªìi, ch·ªâ c·∫ßn config th√™m cho n√≥ dc nh·∫≠n traffic t·ª´ private subnet n·ªØa th√¥i\n Q. B·∫°n c√≥ 1 app host tr√™n aws. B·∫°n mu·ªën setup instance type hi·ªáu qu·∫£, s·∫Ω flexible khi spikes usage. N√™n ch·ªçn c√°i g√¨?\n T2.micro, Config T2 unlimited option\nV√≠ d·ª•: b·∫°n c√≥ 1 ec2 t3.large, baseline CPU c·ªßa n√≥ l√† 30%, n·∫øu n√≥ ch·∫°y trung b√¨nh ·ªü 30% ho·∫∑c √≠t h∆°n trong kho·∫£ng 24h, th√¨ s·∫Ω ko ph√°t sinh th√™m chi ph√≠. Nh∆∞ng n·∫øu run trung b√¨nh ·ªü m·ª©c 40% trong kho·∫£ng 24h ƒë√≥ th√¨ s·∫Ω m·∫•t th√™m ti·ªÅn 10% ƒë√≥.\n Q. B·∫°n plan host 1 batch process app tr√™n ec2. N√™n Instances lo·∫°i n√†o? General Purpose, Memory Optimized, Storage Optimized, Compute Optimized\n Compute Optimized\n Q. VPC c√≥ 2 subnet l√† public v√† private. Public subnet c·∫ßn 1 NAT gateway. Th√¨ config tr√™n main route table v√† custom route table c·∫ßn nh∆∞ n√†o? c√°c route table ·ªü trong private hay public subnet?\n main route table ph·∫£i ·ªü trong private subnet, target ƒë·∫øn NAT:\n0.0.0.0/0 target ƒë·∫øn NAT gateway\ncustom route table ph·∫£i ·ªü trong public subnet, target ƒë·∫øn IGW:\n0.0.0.0/0 target ƒë·∫øn IGW\n Q. 1 Cty l∆∞u document tr√™n s3. H·ªç b·ªã audit raise v·∫•n ƒë·ªÅ v·ªÅ non-maintaining access logs. B·∫°n d·ª± ƒë·ªãnh enable s3 server access log cho all buckets. C·∫ßn ch√∫ √Ω g√¨?\n Source v√† target bucket ph·∫£i l√† 2 bucket ri√™ng v√† c√πng trong 1 region\nAWS Document vi·∫øt:\nServer access logging ƒë·ªÉ track request v√†o bucket. B·∫°n ko t·ªën ti·ªÅn cho vi·ªác d√πng feature n√†y. T·∫•t nhi√™n v·∫´n tr·∫£ ph√≠ l∆∞u tr·ªØ, ph√≠ access v√†o xem log nh∆∞ c√°c request th√¥ng th∆∞·ªùng.\nsource bucket l√† bucket b·∫°n mu·ªën config ƒë·ªÉ track access log c·ªßa n√≥, (s·∫Ω config ph·∫ßn logging)\ntarget bucket l√† bucket l∆∞u access log c·ªßa source\ndefault encrpytion tr√™n target bucket l√† AES256 (S3-SSE), v√† ko th·ªÉ thay ƒë·ªïi sang c√°i kh√°c nh∆∞ SSE-KMS dc.\n Q. Cty b·∫°n c√≥ 1 Redshift cluster tr√™n EU region. H·ªç c·∫ßn ph·∫£i ensure l√† cluster s·∫Ω available tr√™n other region ƒë·ªÅ ph√≤ng TH primary cluster b·ªã isue do region. L√†m n√†o? C√≥ th·ªÉ Multi-AZ, Cross Region Read Replica, Global table hay ko?\n Readshift ko c√≥ m·∫•y t√≠nh nƒÉng tr√™n ƒë√¢u m√† ch·ªâ c√≥ th·ªÉ Config Redshift automatic/manual copy snapshot to secondary region.\nko th·ªÉ Multi-AZ hay Read replica nh∆∞ RDS dc\nc≈©ng ko th·ªÉ global table nh∆∞ DynamoDB dc\n Q. B·∫°n mu·ªën block public access v√†o all s3 buckets. L√†m n√†o? apply tr√™n bucket level hay object level?\napply theo region hay to√†n b·ªô region? apply cho current bucket hay future bucket?\n block public access s·∫Ω apply tr√™n bucket level or account level, tr√™n to√†n b·ªô region (globally), cho c·∫£ current v√† future bucket\n Q. B·∫°n ƒëang deploy 1 v√†i Cloudformation templates. Khi deploy g·∫∑p l·ªói sau:\nSender\nThrottling\nRate exceeded\nL√†m n√†o?\n add 1 exponential backoff gi·ªØa c√°c l·∫ßn call API createStack\nL·ªói x·∫£y ra do createStack API t·∫°o qu√° nhi·ªÅu resource c√πng 1 th·ªùi ƒëi·ªÉm. C·∫ßn ph·∫£i c√≥ delay gi·ªØa c√°c request ƒë√≥\n(B·∫°n ko th·ªÉ add pause trong CF template m√† ch·ªâ c√≥ th·ªÉ d√πng wait ho·∫∑c depend on condition th√¥i)\n Q. 1 c√¥ng ty Audit ƒëang audit your infra. H·ªç mu·ªën bi·∫øt lo·∫°i security n√†o ƒëang dc implement ·ªü data centre m√† host c√°c AWS resource c·ªßa b·∫°n? l√†m n√†o?\n D√πng document cung c·∫•p ·ªü AWS Artifact service\nAWS Artifact service l√† service cung c·∫•p c√°c security \u0026amp; compliance documents gi·ªëng nh∆∞ ISO cert, PCI, SOC reports\n Q. App c·ªßa b·∫°n d√πng RDS host database. Sau kho·∫£ng v√†i ng√†y, 1 table b·ªã drop accidently v√† c·∫ßn ph·∫£i retrieve ngay l·∫≠p t·ª©c trong v√≤ng 5 ph√∫t sau khi b·ªã drop. D√πng t√≠nh nƒÉng n√†o c·ªßa RDS? Multi-AZ hay Read replica hay Automated backup?\n RDS Automated backup\nRDS t·ª± ƒë·ªông backup DB instance theo period m√† b·∫°n ch·ªâ ƒë·ªãnh, v√† b·∫°n c√≥ th·ªÉ recover db v·ªÅ b·∫•t k·ª≥ th·ªùi ƒëi·ªÉm n√†o trong backup retention period ƒë√≥\nAWS Document vi·∫øt:\nKhi b·∫°n delete DB instance, n·∫øu ko ch·ªçn gi·ªØ l·∫°i backup th√¨ m·∫∑c ƒë·ªãnh s·∫Ω x√≥a h·∫øt c√°c b·∫£n backup lu√¥n\nBackup retention period, n·∫øu b·∫°n ko setup th√¨ m·∫∑c ƒë·ªãnh l√† 1 ng√†y n·∫øu b·∫°n t·∫°o DB instance qua CLI, API. L√† 7 ng√†y n·∫øu t·∫°o DB instance qua Console.\nAutomated backup ch·ªâ di·ªÖn ra v·ªõi DB trong tr·∫°ng th√°i Avalable.\nAutomated backup di·ªÖn ra h√†ng ng√†y. N·∫øu ko ch·ªâ ƒë·ªãnh th√¨ default l√† 30 ph√∫t backup windows, ch·ªçn ng·∫´u nhi√™n trong block 8h t√πy theo region (v√≠ d·ª•: virginia: 3-11h, tokyo: 13-21h)\n Q. B·∫°n mu·ªën setup EC2 trong VPC. Y√™u c·∫ßu l√† support higher bandwidth v√† higher packet per second (PPS) performance. L√†m n√†o? C√≥ d√πng EBS Optimized Instance type dc ko?\n D√πng Instance type lo·∫°i m√† support Enhanced Networking\nko d√πng EBS Optimized Instance type v√¨ n√≥ ch·ªâ d√πng ƒë·ªÉ tƒÉng performance cho EBS volumes\nAWS Document vi·∫øt:\nENA (Elastic Network Adapter) support tƒÉng t·ªëc network 100 Gbps: a1, c5, m5, t3, r5, p2, p3\nVF (Intel Virtual function) support tƒÉng t·ªëc network 10 Gbps: c3, c4, d2, i2, m4, r3\n Q. B·∫°n c√≥ 1 ASG trong ƒë√≥ c√≥ 1 s·ªë ec2, gi·ªù app ƒëang c√≥ issue v√† c·∫ßn debug th√¨ l√†m n√†o? c√≥ th·ªÉ d√πng AWS Config ƒë·ªÉ l·∫•y config snapshot c·ªßa ec2 ƒë·ªÉ ƒëi·ªÅu tra ko?\n C·∫ßn suspend 1 ho·∫∑c nhi·ªÅu scaling process ƒë·ªÉ ƒëi·ªÅu tra ec2\nko th·ªÉ d√πng AWS config v√¨ ƒëi·ªÅu ƒë√≥ l√† ko th·ªÉ\nAWS Document vi·∫øt:\nC√≥ 1 s·ªë scaling process trong ASG c√≥ th·ªÉ suspend nh∆∞:\nLaunch: add ec2 v√†o ASG,\nTerminate: remove ec2 kh·ªèi ASG,\nAddToLoadBalancer: add ec2 v√†o LB or target group,\nAlarmNotification: ch·∫•p nh·∫≠n th√¥ng b√°o c·ªßa Cloudwatch,\nAZRebalance: c√¢n b·∫±ng s·ªë l∆∞·ª£ng ec2 gi·ªØa c√°c AZ,\nHealthCheck: check health of ec2,\nReplaceUnhealthy: terminate ec2 v√† create c√°i m·ªõi,\nScheduledActions: th·ª±c hi·ªán scheduled scaling action\n Q. 1 c√¥ng ty c√≥ c√°c file m√† n·∫øu ƒë√£ dc verify th√¨ s·∫Ω ko c·∫ßn trong t∆∞∆°ng lai tr·ª´ TH c√≥ issue. N√™n l∆∞u ·ªü ƒë√¢u cho ti·∫øt ki·ªám? Glacier hay S3 RRS hay S3 IA\n Glacier v√¨ n√≥ r·∫ª h∆°n h·∫≥n\n‚ñ† D√πng S3-RRS (Reduced Redundancy Storage) n·∫øu b·∫°n mu·ªën l∆∞u nh·ªØng data ko quan tr·ªçng v√† n·∫øu m·∫•t th√¨ c≈©ng d·ªÖ t√°i t·∫°o l·∫°i ‚ñ† D√πng IA n·∫øu b·∫°n mu·ªën 1 ch·ªó l∆∞u long-term nh∆∞ng c√≥ th·ªÉ retrieve ngay l·∫≠p t·ª©c\n‚ñ† D√πng Glacier n·∫øu b·∫°n mu·ªën 1 ch·ªó l∆∞u long-term nh∆∞ng c√≥ th·ªÉ retrieve trong v√≤ng 3-5 gi·ªù\n Q. B·∫°n c·∫ßn capture all traffic flowing gi·ªØa client v√† server, b·∫°n plan to enable Access log tr√™n Application Load Balancer. C√≥ lo·∫°i traffic n√†o m√† Access log ALB ko capture l·∫°i?\n health check request t·ª´ ALB ƒë·∫øn target group s·∫Ω ko dc ALB access log capture l·∫°i\n Q. 1 Cty mu·ªën implement qu√° tr√¨nh deployment automatically t·∫°o LAMP stack, download latest PHP t·ª´ s3, setup ELB. N√™n d√πng service n√†o ƒë·ªÉ t·∫°o 1 orderly deployment of software?\nBeanstalk hay Opswork?\n AWS Elastic Beanstalk\n Q. Cty b·∫°n plan setup 1 performance-sensitive workload tr√™n aws. B·∫°n c·∫ßn setup ec2 cho hosting this workload. C·∫ßn ch√∫ √Ω g√¨ v·ªÅ config?\n Ch·ªçn EBS Optimized Instance\nHo·∫∑c Ch·ªçn Instance type with 10 Gigabit network connectivity\nkhi b·∫°n plan config EBS volume cho app, 2 c√°i tr√™n r·∫•t quan tr·ªçng khi b·∫°n mu·ªën stripe nhi·ªÅu volume l·∫°i trong RAID configuration\nC√≥ 1 list c√°c lo·∫°i instance type dc support EBS Optimized, ƒëa s·ªë dc enable by default, 1 s·ªë c·∫ßn ph·∫£i enable b·∫±ng tay.\nC√≥ 1 list c√°c instance type dc support Enhanced Networking l√™n ƒë·∫øn 10 Gbps bandwidth\n\u0026mdash;P3\u0026mdash;  Q. B·∫°n ƒëang setup DynamoDB, c√≥ 40 device ghi data m·ªói 10s, m·ªói data 1KB. V·∫≠y Write throughput set cho table l√† bao nhi√™u?\n 4WCU\n Q. Cty c·ªßa b·∫°n c√≥ VPN connection gi·ªØa AWS VPC v√† on-premise data centre. H·ªç c·∫ßn c√°c ec2 s·ª≠ d·ª•ng DNS server c·ªßa On-premise cho vi·ªác resolve DNS, l√†m n√†o?\n T·∫°o 1 DHCP Option set r·ªìi asign n√≥ v√†o VPC\nCh√∫ √Ω sau khi t·∫°o DHCP Option set r·ªìi th√¨ b·∫°n ko th·ªÉ modify n√≥. Mu·ªën th√¨ ph·∫£i t·∫°o c√°i m·ªõi.\n Q. Cty ƒëang qu·∫£n l√Ω MySQL DB tr√™n on-premise. H·ªç mu·ªën migrate l√™n AWS, h·ªç ko mu·ªën m·∫•t c√¥ng managed hay scaling DB th√¨ n√™n d√πng RDS hay Aurora?\n Aurora, v√¨ RDS v·∫´n c·∫ßn 1 ph·∫ßn qu·∫£n l√Ω manage v√† scale\nC√≤n Aurora l√† full-managed by AWS\n Q. C√¥ng ty b·∫°n mu·ªën c·∫Øt gi·∫£m cost th√¨ c√≥ n√™n consider vi·ªác ko s·ª≠ d·ª•ng Autoscaling group v√¨ n√≥ g√¢y cost ko?\n ko, v√¨ ASG ko ph√°t sinh cost\n Q. B·∫°n c·∫ßn ph·∫£i t·∫°o c√°c metric cloudwatch, c√°i n√†o sau ƒë√¢y l√† c√°i b·∫°n c·∫ßn ph·∫£i t·∫°o custom metric?\n CPU Utilization.\nNetwork Throughput.\nS·ªë bytes read/writte v√†o volume.\nDung l∆∞·ª£ng Disk storage c√≤n l·∫°i trong volume.\n????\nc√°i c·∫ßn t·∫°o custom metric l√† Dung l∆∞·ª£ng Disk storage c√≤n l·∫°i trong volume,\nnh·ªØng c√°i kia ƒë·ªÅu l√† default metric, ko c·∫ßn custom, ngay c·∫£ s·ªë l∆∞·ª£ng byte read/write v√†o volume c≈©ng default\n Q. Limit v·ªÅ ELB v√† DynamoDB table?\n ELB limit 20 LB m·ªói region, c√≥ th·ªÉ request ƒë·ªÉ tƒÉng\nDynamoDB limit 256 tables per account, c√≥ th·ªÉ request ƒë·ªÉ tƒÉng\n Q. C√¥ng ty b·∫°n ƒëang audit c·∫£ nh·ªØng AWS resource, h·ªç y√™u c·∫ßu compliance c·ªßa aws resources th√¨ l·∫•y ƒë√¢u?\n V√†o compliance portion c·ªßa AWS website ƒë·ªÉ l·∫•y all:\nhttps://aws.amazon.com/compliance/resources/\n Q. B·∫°n access dc v√†o web server qua bastion nh∆∞ng ko th·ªÉ access v√†o qua ELB, c·∫ßn ensure g√¨?\n ensure l√† ELB c√≥ attach v√†o public subnet ensure SG ho·∫∑c ACL c·ªßa ELB c√≥ allow inbound traffic\n Q. C√¥ng ty b·∫°n mu·ªën keep control cost, v√† mu·ªën nh·∫≠n alert cho billing th√¨ c·∫ßn enable g√¨ tr∆∞·ªõc khi t·∫°o alarm for estimated metric? C√≤n n·∫øu mu·ªën t·∫°o billng alarm th√¨ t·∫°o ·ªü ƒë√¢u?\n Enable billing alert ·ªü m·ª•c Account Preferences\nN·∫øu mu·ªën t·∫°o billing alarm th√¨ t·∫°o ·ªü Cloudwatch\n Q. B·∫°n c√≥ 1 web app static content c√≥ nhi·ªÅu truy c·∫≠p g√¢y b·ªã ch·∫≠m, n√™n l√†m n√†o? host tr√™n s3 hay d√πng read replica hay elastiCache?\n host tr√™n s3\napp b·ªã ch·∫≠m v√¨ b·ªã truy c·∫≠p nhi·ªÅu, app respone ko ph·∫£i v√¨ dynamic content dc fetch t·ª´ DB n√™n d√πng elastiCache ko h·ª£p l√Ω.\nread replica c≈©ng ch·ªâ d√πng ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ c·ªßa dynamic data th√¥i.\n1 static website g·ªìm c√°c static content, c√≥ th·ªÉ ch·ª©a client-side script.\n1 dynamic website th√¨ d·ª±a v√†o server-side process, script nh∆∞ PHP, JSP, .NET. S3 ko h·ªó tr·ª£ server-side scripting.\n Q. Team member c·ªßa b·∫°n launch 1 ec2 thu·ªôc lo·∫°i EBS-backed instance, xong v√†i tu·∫ßn sau th√¨ m·∫•t private key, ko th·ªÉ login v√†o dc n·ªØa. Gi·ªù l√†m n√†o?\n ch√∫ √Ω c√°c step sau ch·ªâ d√†nh cho EBS-backed instance, ko d√†nh cho instance store-backed instance:\n0- ƒë·∫ßu ti√™n, t·∫°o 1 keypair m·ªõi,\n1- launch 1 temp instance c≈©ng t·ª´ ami ƒë√≥,\n2- stop origin instance,\n3- detach root volume c·ªßa origin ra, attach v√†o temp instance d∆∞·ªõi d·∫°ng data volume,\n4- connect v√†o temp insntace,\n5- mount volume m·ªõi v√†o 1 folder t·∫°m ƒë·ªÉ modify file system c·ªßa volume ƒë√≥,\n5- modify authorized_keys: copy authorized_keys t·ª´ temp instance sang origin volume v·ª´a mount,\n6- move c√°i volume ƒë√≥ tr·ªü l·∫°i origin instance,\n7- restart instance.\n Q. B·∫°n l∆∞u billing report tr√™n 1 s3 bucket theo d·∫°ng csv. Gi·ªù c·∫ßn d√πng Athena ƒë·ªÉ query nh·ªØng report ƒë√≥. Th√¨ trong Athena ch·ªâ specify s3 bucket theo path n√†o?\n s3://bucketname/prefix/\nhay\ns3://bucketname/prefix/*\nPh·∫£i d√πng: s3://bucketname/prefix/\n Q. Team b·∫°n c√≥ 1 ASG v√† 1 ELB, trong ƒë√≥ c√≥ 1 set ec2. Sau khi cho ng d√πng th·ª≠ th√¨ th·∫•y l√† traffic ko distribute ƒë·ªÅu qua c√°c ec2. L√Ω do g√¨?\n sticky session ƒë√£ dc enable cho LB\nall subnets c√≥ ec2 ko dc register v·ªõi LB\nAWS Document vi·∫øt: sticky session s·∫Ω ensure l√† all request c·ªßa user trong su·ªët session s·∫Ω g·ª≠i d·∫øn c√πng 1 ec2\n Q. Kh√°c nhau gi·ªØa Dedicated Host v√† Dedicated Instance?\n EC2 - Dedicated Host:\nAWS cho b·∫°n to√†n quy·ªÅn tr√™n 1 con server v·∫≠t l√Ω lu√¥n, ko chia s·∫ª v·ªõi ai, cho b·∫°n to√†n quy·ªÅn control s·ªë l∆∞·ª£ng core, socket‚Ä¶, cho b·∫°n quy·ªÅn s·ª≠ d·ª•ng licence c√≥ s·∫µn c·ªßa b·∫°n (BYOL-bring your own licene), ko m·∫•t ti·ªÅn mua license c·ªßa AWS n·ªØa. B·∫°n c√≥ th·ªÉ run nhi·ªÅu instances tr√™n c√°i host ƒë√≥. C√°c instance trong host s·∫Ω c√≥ network performance cao.\nEC2 - Dedicated Instance:\nT·∫°o Ec2 tr√™n 1 con server v·∫≠t l√Ω, ƒë·∫£m b·∫£o duy nh·∫•t cho b·∫°n (tu√¢n th·ªß v·ªÅ m·∫∑t compliance)\n Q. B·∫°n c·∫ßn migrate Oracle 10g Release 2 v√† MySQL 5.6 l√™n AWS RDS read replica. C·∫ßn ch√∫ √Ω g√¨?\n RDS ko support Oracle version d∆∞·ªõi 12.1\n Q. B·∫°n d√πng RDS MySQL. Th√¨ ·ªü Maintenance Window s·∫Ω th·ª±c hi·ªán nh·ªØng activities v·ªÅ RDS?\n Update DB instance hardware,\nUpdate OS,\nUpdate DB engine version,\nPatching server.\n Q. Cty ƒëang d√πng RDS MySQL, gi·ªù ƒëang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ performance v·ªõi read request, l√†m n√†o?\nc√≥ n√™n enable Multi-AZ ko? add Read Replica? tƒÉng instance type c·ªßa DB instance?\n Add Read Replica (l√† scaling theo chi·ªÅu ngang).\nTƒÉng instance type c·ªßa DB instance (l√† scaling theo chi·ªÅu d·ªçc)\nVi·ªác enable multi-AZ ch·ªâ ƒë·ªÉ l√†m cho RDS c√≥ HA (high-avalability) th√¥i\n Q. S·ªë l∆∞·ª£ng Read Replica c·ªßa c√°c lo·∫°i DB: RDS, Aurora, DynamoDB?\n RDS c√≥ th·ªÉ c√≥ max 5 Read Replica,\nAurora c√≥ th·ªÉ c√≥ max 15 Read Replica.\nDynamoDB th√¨ ko c√≥ kh√°i ni·ªám Read Replica (ch·ªâ c√≥ global table: l√† 1 Read/Write Replica),\nDynamoDB DAX th√¨ c√≥ max 9 Read Replica\n Q. Team b·∫°n setup ASG ƒë·ªÉ launch ec2, nh∆∞ng ec2 ko launch n√™n b·∫°n c·∫ßn debug. L√Ω do c√≥ th·ªÉ l√† g√¨?\n _AZ ko c√≤n support\n_instance type ko c√≤n available\n_keypair c·ªßa ec2 ko t·ªìn t·∫°i\n_Security group (SG) ko t·ªìn t·∫°i\n_ko t√¨m th·∫•y ASG\n_b·∫°n ko subcribed c√°i service ƒë√≥\n_EBS ko support instance-store AMI\n_placement groups ko l√†m dc v·ªõi ec2 m1.large\n_client error\n Q. Cty b·∫°n host 1 app tr√™n ec2, app sau 1 ELB. B·∫°n c·∫ßn config Route53 v·ªõi company DNS ELB. S·ª≠ d·ª•ng lo·∫°i record n√†o c·ªßa R53? A, MX, ALIAS, AAAA record?\n D√πng ALIAS\ntheo AWS documents th√¨ ALIAS record ƒë·ªÉ d√πng cho c√°c service sau:\nCloudfront, Elastic Beanstalk, ELB, S3 bucket, another R53 record\nc√≤n c√°c l·ª±a ch·ªçn kh√°c:\n‚ñ† A record d√πng ƒë·ªÉ point 1 resource qua Ipv4 record\n‚ñ† MX record d√πng cho Mail Server record\n‚ñ† AAAA record d√πng ƒë·ªÉ point 1 resource qua Ipv6 record\n Q. 1 Cty c√≥ 1 critical app tr√™n VPC. Gi·ªù b·∫°n c·∫ßn 1 list c√°c connection dc t·∫°o ra tr√™n SSH port. L√†m n√†o ƒë·ªÉ r·∫ª v√† ti·ªán?\n D√πng Athena query S3 bucket n∆°i ch∆∞a VPC flow logs ƒë√£ save\nc√°ch 2 t·ªën c√¥ng h∆°n l√†: Export VPC flow logs ra CSV format r·ªìi filter port SSH\n Q. Cty mu·ªën launch c√°c ec2, ch√∫ng c·∫ßn ƒë·ªô tr·ªÖ th·∫•p, v√† network performance cao th√¨ l√†m n√†o?\n t·∫°o 1 placement group trong single AZ\nch·ªçn instance type lo·∫°i Enhanced Networking\n Q. 1 cty d√πng Aurora MySQL, h·ªç c·∫ßn d√πng Read Replica ƒë·ªÉ c·∫£i thi·ªán read performance, nh∆∞ng h·ªç mu·ªën Read Replica ·ªü tr√™n 1 region kh√°c (cross-region read replica) th√¨ c·∫ßn ch√∫ √Ω g√¨?\n Ch√∫ √Ω l√† b·∫°n c√≥ th·ªÉ t·∫°o 5 cross-region Read Replica (c·∫£ encrypted v√† unencrypted)\nAurora ko t·ª± ƒë·ªông t·∫°o cross-region replica, b·∫°n ph·∫£i t·∫°o manual\n Q. Cty b·∫°n c√≥ 1 set c√°c resource tr√™n AWS. Gi·ªù mu·ªën l√† t·∫•t c·∫£ nh·ªØng thay ƒë·ªïi c·ªßa resource ƒë·ªÅu c·∫ßn monitor l·∫°i. N√™n d√πng service g√¨? Cloudwatch? cloudtrail? Config?\n Cloudtrail v√† Config\nkh√¥ng d√πng Cloudwatch v√¨ n√≥ ko cung c·∫•p detail v·ªÅ nh·ªØng config thay ƒë·ªïi\nAWS config gi√∫p b·∫°n review nh·ªØng config thay ƒë·ªïi c·ªßa resource\nCloudtrail th√¨ cung c·∫•p monitor c√°c account activity tr√™n Aws acocunt b·∫°n\n Q. Cty b·∫°n c√≥ infra tr√™n aws. Gi·ªù h·ªç mu·ªën share project data store v∆°i 1 c√¥ng ty kh√°c (A), user c·ªßa c√¥ng ty A l√†m vi·ªác tr√™n linux v√† s·∫Ω access v√†o data ƒë√≥. C√°ch n√†o ƒë·ªÉ most secure v√† kh·∫£ thi nh·∫•t?\n D√πng EFS Mount Target, v·ªõi 1 Security Group m·ªü inbound NFS port cho c√¥ng ty A access qua AWS Direct Connect/ho·∫∑c VPN.\nEFS MOunt Target ch·ªâ dc access v·ªõi ƒëi·ªÅu ki·ªán:\n‚ñ† EC2 trong AWS VPC,\n‚ñ† EC2 trong VPC perring with other VPC,\n‚ñ† On-premise server c√≥ AWS Direct Connect ho·∫∑c AWS VPN to VPC\nCh√∫ √Ω l√† SG g·∫Øn v√†o Mount Target ph·∫£i m·ªü port NFS\n Q. Cty b·∫°n deploy 1 web app tr√™n US-east region. C√≥ 1 ALB config ƒë·ªÉ HA. C√≥ d√πng Cloudfront @edge. B·ªô ph·∫≠n Security y√™u c·∫ßu block 1 black list IP spam t·∫°i ƒëi·ªÉm farthest point from cloud infa. L√†m n√†o?\n D√πng AWS WAF, t·∫°o 1 Web ACL ƒë·ªÉ block IP ·ªü Global Region, v√† apply t·∫°i edge level Cloudfront\nAWS Documents c√≥ vi·∫øt:\nWAF c√≥ th·ªÉ d√πng ƒë·ªÉ block IP, pattern in HTTP header\u0026amp;body, SQL injection, URL String, cross-site scripting, geographical location.\nWAF c√≥ th·ªÉ l√†m ƒëi·ªÅu tr√™n ·ªü t·∫ßng Cloudfront edge at Global level, ho·∫∑c t·∫ßng Applicaion at regional level\nch√∫ √Ω v·ªÅ ACL WAF c√≥ kh√°i ni·ªám Web ACL, c√≤n VPC c√≥ kh√°i ni·ªám Network ACL\n Q. Cty b·∫°n ƒëang d√πng RDS Database. B·∫°n c·∫ßn d·ª± to√°n cost. C√°i n√†o b·∫°n c·∫ßn consider v·ªÅ cost khi d√πng RDS?\n S·ªë gi·ªù DB running, lo·∫°i DB instance type (small hay large\u0026hellip;)\nStorage m·ªói th√°ng\ns·ªë l∆∞·ª£ng I/O request m·ªói th√°ng\ns·ªë l∆∞·ª£ng data transfer khi sang Region kh√°c (ch·ª© trong c√πng region th√¨ data transfer l√† free)\n Q. B·∫°n c√≥ 1 set Ec2, y√™u c·∫ßu l√† t·∫•t c·∫£ EBS volume (root/data) c·∫ßn preserve khi instance b·ªã terminate, l√†m n√†o?\n set gi√° tr·ªã DeleteOnTerminationc·ªßa volume = False\nth√¨ khi ec2 b·ªã terminate th√¨ volume s·∫Ω ko b·ªã x√≥a theo, c√≤n n·∫øu ko set, default l√† c√°i ·ªï root dc set l√† true n√™n khi terminate ec2 n√≥ s·∫Ω b·ªã x√≥a theo.\n Q. Dev team ƒëang l√†m 1 web intranet, data content t·ª´ s3. B·∫°n c·∫ßn l√†m 1 access policy ƒë·ªÉ cho least quy·ªÅn v√†o s3 bucket. S·∫Ω c·∫ßn nh·ªØng g√¨?\n D√πng StringLike v√† aws:Referer ƒë·ªÉ allow s3:getObject action, Deny all other actions t·ª´ site intranet ƒë√≥.\nV·ªõi StringLike v√† aws:Referer, ch·ªâ nh·ªØng request dc kh·ªüi t·∫°o t·ª´ website ƒë√≥ (intranet) m·ªõi c√≥ th·ªÉ get object ƒëc.\nN·∫øu d√πng c√°i IPAddress v√† aws:SourceIp th√¨ nh·ªØng ip range ƒë√≥ v·∫´n c√≥ th·ªÉ get object tr·ª±c ti·∫øp t·ª´ s3 bucket\n{ \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Id\u0026quot;:\u0026quot;http referer policy example\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Sid\u0026quot;:\u0026quot;Allow get requests originating from www.example.com and example.com.\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;:\u0026quot;*\u0026quot;, \u0026quot;Action\u0026quot;:\u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::examplebucket/*\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;StringLike\u0026quot;:{\u0026quot;aws:Referer\u0026quot;:[\u0026quot;http://www.example.com/*\u0026quot;,\u0026quot;http://example.com/*\u0026quot;]} } } ] } R·ªìi ph·∫£i ensure l√† browser b·∫°n d√πng c√≥ include http referer header ·ªü trong request\n Q. App c·ªßa b·∫°n ƒëang mu·ªën d√πng SQS, y√™u c·∫ßu l√† c·∫ßn c√≥ 1 queue ri√™ng ƒë·ªÉ keep nh·ªØng msg m√† ko dc process success. D√πng feature g√¨?\n Dead-letter queue (DLQ)\n Q. Cty b·∫°n d√πng 1 s·ªë EC2, y√™u c·∫ßu l√† c√°c file log t·ª´ ec2 c·∫ßn ship l√™n s3 bucket ƒë·ªÉ sau n√†y analysis. D√πng service n√†o?\n AWS DataPipeline\nService n√†y gi√∫p b·∫°n automate qu√° tr√¨nh movement v√† transform data. V√≠ d·ª• log t·ª´ ec2 -\u0026gt; S3 -\u0026gt; EMR ƒë·ªÉ analisis\n Q. Cty b·∫°n d√πng S3 bucket l∆∞u docs. B·∫°n c·∫ßn t·∫°o policy apply cho nhi·ªÅu nh√¢n vi√™n ƒë·ªÉ h·ªç c√≥ quy·ªÅn get/put/delete object tr√™n folder c√° nh√¢n c·ªßa h·ªç tr√™n s3, l√†m n√†o?\n { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::test2019bucket/${aws:userid}*\u0026quot; } ] }  Q. C√¥ng ty b·∫°n ƒë√£ setup 1 dual tunnel VPN connection t·ª´ On-premise ƒë·∫øn AWS. Gi·ªù c·∫ßn l√†m 1 k·∫øt n·ªëi d·ª± ph√≤ng (redundant connection) trong TH VPN b·ªã fail. Th√¨ l√†m n√†o?\n T·∫°o th√™m 1 secondary VPN connection\nM·∫∑c d√π 1 VPN connection ƒë√£ c√≥ 2 tunnel ƒë·ªÉ ƒë·∫£m b·∫£o connection trong TH 1 c√°i b·ªã fail, th√¨ b·∫°n v·∫´n c·∫ßn t·∫°o th√™m 1 secondary VPN connection th√¨ m·ªõi ƒë·ªÅ ph√≤ng dc Tr∆∞·ªùng h·ª£p customer gateway b·ªã fail\n Q. B·∫°n ƒëang run 1 ec2 v√† gi·ªù mu·ªën tƒÉng type c·ªßa ec2, l√†m n√†o?\n stop, change type, start\n Q. B·∫°n c·∫ßn publish metric l√™n Cloudwatch, y√™u c·∫ßu l√† interval c·ªßa metric l√† 1s, l√†m n√†o, d√πng CLI hay Console?\n publish metric high resolution, via CLI (ch·ª© ƒë√¢y l√† custom metric th√¨ kh√¥ng th·ªÉ publish qua Console dc, ph·∫£i d√πng CLI)\n Q. T·∫°o billing alarm t·ª´ ƒë√¢u? Cloudwatch? Cost Explorer? T·∫°o billng alert th√¨ sao?\n T·∫°o billing alarm t·ª´ Cloudwatch\nT·∫°o billing alert t·ª´ Account Preferences\n Q. B·∫°n t·∫°o 1 SAML indentity provider, nh·ªØng attribute n√†o m√† SAML c·∫ßn cung c·∫•p cho AWS STS?\n Role, RoleSessionName, Audience, NameID\n Q. Cty b·∫°n c√≥ 1 set CMK keys ƒëc define trong KMS. B·∫°n mu·ªën control quy·ªÅn access c√°c key ƒë√≥ trong KMS th√¨ d√πng c√°i g√¨? KMS policies hay Key policies?\n Key policies, b·ªüi v√¨ ko c√≥ kh√°i ni·ªám KMS policies\n Q. B·∫°n ƒëang d√πng Cloudtrail log. B·∫°n mu·ªën ensure l√† c√°c file log n√†y ko b·ªã gia m·∫°o, l√†m n√†o?\n Enable t√≠nh nƒÉng log file integrity cho log files\n Q. C√¥ng ty b·∫°n c√≥ 1 vendor mu·ªën access v√†o AWS account c·ªßa cty b·∫°n. B·∫°n t·∫°o cho h·ªç 1 IAM user, gi·ªù mu·ªën t·∫°o 1 policy cho user ƒë√≥ th√¥i th√¨ d√πng policy n√†o? AWS Managed policy hay Inline policy?\n Inline policy th√¥i, khi b·∫°n t·∫°o inline policy th√¨ c√°i policy ƒë√≥ s·∫Ω ko dc s·ª≠ d·ª•ng cho user kh√°c, v√† khi b·∫°n x√≥a user th√¨ c√°i policy ƒë√≥ c≈©ng b·ªã x√≥a theo\n\u0026mdash;P4\u0026mdash;  Q. 1 cty c√≥ 1 set S3 bucket, v√† h·ªç lo s·ª£ data b·ªã stolen ho·∫∑c b·ªã virus ransomeware t·∫•n c√¥ng, l√†m n√†o? N√™n d√πng t√≠nh nƒÉng n√†o enable s3 versioning, hay enable s3 encryption?\n enable s3 versioning\nch·ª© c√°i s3 encryption th√¨ ko gi√∫p ch·∫∑n virus ransomeware dc.\n Q. B·∫°n mu·ªën b·∫≠t enable MFA delete s3 bucket versioning th√¨ c·∫ßn l√†m g√¨?\n D√πng root account credential v√† l√†m b·∫±ng CLI\nch·ª© c√°i MFA delete n√†y ko th·ªÉ d√πng IAM account v√† Console ƒë∆∞·ª£c\nNh·ªØng task c·∫ßn d√πng root credential: t·∫°o Cloudfront keypair, S3 MFA delete, change aws support plan (c√≥ nhi·ªÅu nh∆∞ng ch·ªâ n√™u 3 c√°i ƒë·∫∑c bi·ªát th·∫ø th√¥i)\n Q. B·∫°n upload kho·∫£ng 70 TB l√™n S3 Glacier nh∆∞ng khi upload th√¨ file name b·ªã thay ƒë·ªïi khi ƒë∆∞a l√™n Glacier, l√†m n√†o?\n 1_D√πng AWS Snowball upload l√™n s3 r·ªìi t·ª´ s3 move sang Galicer d√πng Lifecycle rule\n2_Upload l√™n S3 IA r·ªìi move sang Galicer d√πng Lifecycle rule\nKh√¥ng n√™n up tr·ª±c ti·∫øp l√™n S3 Glacier\nKh√¥ng n√™n d√πng Snowball Edge v√¨ t·ªën k√©m\n Q. Memory utilization c√≥ ph·∫£i metric built-in c·ªßa Cloudwatch ko?\n Kh√¥ng, Memory l√† custom metric\n Q. Cty mu·ªën t·∫•t c·∫£ nh·ªØng API call ƒë·∫øn EC2 dc log l·∫°i, v√† n·∫øu c√≥ c√°i Termination api dc call th√¨ s·∫Ω notify b·∫°n, l√†m n√†o?\n T·∫°o 1 trail trong Cloudtrail, trail s·∫Ω deliver log v√†o s3 bucket\nt·∫°o 1 lambda dc trigger khi s3 c√≥ object d·∫°ng terminate dc put v√†o, d√πng SNS ƒë·ªÉ notify\n Q. B·∫°n d√πng Cloudwatch metric DiskReadBytes cho EC2, nh∆∞ng sau v√†i ng√†y v√†o xem th·∫•y ch·ªâ hi·ªÉn th·ªã 0 bytes m·∫∑c d√π app v·∫´n ƒë·ªçc data ƒë·ªÅu, v√¨ sao?\n ‚ñ† C√≥ nh·ªØng lo·∫°i metric ch·ªâ d√πng cho instance store volumes, ko ph·∫£i EBS-backed volumes:\nDiskReadOps\nDiskWriteOps\nDiskReadBytes\nDiskWriteBytes\n‚ñ† C√≥ nh·ªØng lo·∫°i metric ch·ªâ c√≥ trong Basic monitoring:\nNetworkPacketsIn\nNetworkPacketsOut\n Q. 1 Cty c√≥ service qu·∫£n l√Ω asymmetric key, mu·ªën integrate v·ªõi aws th√¨ l√†m n√†o?\n D√πng CloudHSM cho asymmetric key ƒë·ªÉ integrate v·ªõi on-premise service\nCh·ª© KMS l√† d·ªãch v·ª• multi-tenant key storage \u0026amp; ch·ªâ support symetric key th√¥i.\ntr∆∞·ªõc ƒë√¢y t·ª´ng vi·∫øt:\nCloudHSM, b·∫°n t·ª± gen ra encryption key v√† qu·∫£n l√Ω key ƒë√≥\nEphemeral backup key (EBK) d√πng ƒë·ªÉ encrpyt data, Persistent backup key (PBK) ƒë·ªÉ encrpyt EBK sau ƒë√≥ save to S3 in same region v·ªõi CloudHSM.\n Q. Cy b·∫°n s·∫Ω t·∫°o v√†i RDS instance, b·∫°n mu·ªën t·∫°o alarm ƒë·ªÉ notify n·∫øu c√≥ 75% instance limit dc launch th√¨ l√†m n√†o?\n Enable Bussiness/Enterprise support plan v√† d√πng Trust advisor ƒë·ªÉ check limit r·ªìi alarm cloudwatch\nch√∫ √Ω l√† Developer support plan ko th·ªÉ l√†m dc\n Q. Cty b·∫°n mu·ªën l·∫•y trung b√¨nh CPU Utilization c·ªßa all EC2 c·ªßa nhi·ªÅu region, l√†m n√†o?\n Ko th·ªÉ t·∫°o aggregate c·ªßa cross region dc. Ph·∫£i l√† t·ª´ng region.\nV√† mu·ªën l√†m th√¨ ph·∫£i enable detailed monitoring\n Q. B·∫°n chu·∫©n b·ªã deploy 1 static web l√™n s3. D√πng Route53 b·∫°n ƒë√£ register domain v√† sub-domain r·ªìi. V·∫≠y s3 bucket c√≥ c·∫ßn ph·∫£i tr√πng t√™n v·ªõi domain hay sub-domain ko?\n C√≥, c·∫£ sub-domain v√† domain\n Q. B·∫°n b·ªã DDos attack l·ªõn, th√¨ n√™n d√πng c√°i g√¨ ƒë·ªÉ protect website?\n AWS shield advanced\nho·∫∑c WAF nh∆∞ng c√°i n√†y ko ch·ªëng dc DDos l·ªõn\n Q. Cty hi·ªán ƒëang setup Direct connect connection t·ª´ Onpremise data centre ƒë·∫øn 1 VPC us-west. Gi·ªù h·ªç l·∫°i c·∫ßn connect data centre ƒë·∫øn 1 VPC trong us-east. ƒê·∫£m b·∫£o cost-effective th√¨ d√πng g√¨? D√πng AWS Direct connect Gateway hay AWS Direct Connect?\n D√πng AWS Direct connect Gateway (tuy nhi√™n ko support connect vpc trong account kh√°c, ko support China, ko connect tr·ª±c ti·∫øp vpc v·ªõi vpc)\nAWS Direct Connect th√¨ ƒë·∫Øt h∆°n\n Q. N·∫øu ph·∫£i l·ª±a ch·ªçn CNAME v√† ALIAS record trong Route53?\n Lu√¥n ch·ªçn alias, Ch·ªçn Alias ch·ª© ko ch·ªçn CNAME record v√¨ Alias th√¨ c√≥ th·ªÉ t·∫°o cho c·∫£ root domain v√† sub domain, trong khi CNAME th√¨ ch·ªâ c√≥ th·ªÉ t·∫°o cho sub domain.\n Q. Cty b·∫°n c√≥ 1 app d√πng ƒë·ªÉ distribute patches. User s·∫Ω download patches update v·ªÅ. H·ªç complain v·ªÅ respone ch·∫≠m, l√†m n√†o? D√πng Cloudfront, ALB, CLB, Route53?\n Cloudfront, n√≥ s·∫Ω speed up c√°i t·ªëc ƒë·ªô respone\n Q. B·∫°n c√≥ 1 EC2 d√πng volume General Purpose SSD, gi·ªù mu·ªën tƒÉng IOPS c·ªßa volume th√¨ l√†m n√†o?\n gp2 max IOPS ch·ªâ 16000, io1 (provisioned IOPS th√¨ max 32000)\nho·∫∑c l√† ƒë·∫∑t volume trong RAID 0 Configuration (c√≥ th·ªÉ stripe c√°c volume l·∫°i tƒÉng th√™m IOPS)\n Q. B·∫°n d√πng EC2 v√† nh·∫≠n th·∫•y ƒë√¥i khi CPU credit b·ªã go down = 0. N√™n l√†m g√¨?\n change instance type cao l√™n\nho·∫∑c d√πng t2.unlimited, t3.unlimited\n Q. 1 ASG c√≥ c√°c EC2, b·∫°n nh·∫≠n th·∫•y c√°c ec2 m·ªõi launch ko dc li·ªát k√™ l√† 1 ph·∫ßn c·ªßa aggregated metric. L√†m n√†o?\n v√¨ c√°c instance m·ªõi launch v·∫´n c√≤n trong giai ƒëo·∫°n warming, giai ƒëo·∫°n ƒë√≥ s·∫Ω ph·∫£i expired xong m·ªõi dc t√≠nh v√†o metric trung b√¨nh\n Q. C√¥ng ty b·∫°n s·∫Øp dc audit th√¨ c·∫ßn c·∫•p quy·ªÅn access cho auditor nh∆∞ n√†o?\n t·∫°o IAM user c√≥ quy·ªÅn read log t·∫°o b·ªüi Cloudtrail trong s3\n Q. C√¥ng ty b·∫°n dev app mobile, user c·ªßa app ƒë√≥ c·∫ßn access v√†o resource trong aws. D√πng service n√†o? c√≥ d√πng dc AWS Federated Access ko?\n Cognito, IAM role\nko d√πng AWS Federated Access\n Q. Khi b·∫°n launch EC2 g·∫∑p c√°c l·ªói sau nghƒ©a l√† g√¨? InstanceLimitExceeded, InsufficientInstanceCapacity, Instance Terminates Immediately\n AWS Document m√¥ t·∫£:\n‚ñ† InsufficientInstanceCapacity: khi launch/restart EC2 th√¨ c√≥ th·ªÉ do AWS ƒëang ko ƒë·ªß available On-demand capacity cho b·∫°n\nGi·∫£i ph√°p: ch·ªù v√†i ph√∫t, ho·∫∑c chuy·ªÉn AZ kh√°c, instance type kh√°c, gi·∫£m l∆∞·ª£ng EC2 mu·ªën start\u0026hellip;\n‚ñ† InstanceLimitExceeded: v∆∞·ª£t qu√° limit s·ªë ec2 c√≥ th·ªÉ run trong 1 region, c·∫ßn request AWS\n‚ñ† Instance Terminates Immediately:\n_EBS volume limit has been reached.\n_EBS snapshot c·ªßa c√°i instance ƒë√≥ b·ªã corrupt (h·ªèng).\n_EBS root volume b·ªã encrypt v√† b·∫°n ko c√≥ quy·ªÅn access v√†o KMS ƒë·ªÉ decrypt key.\n_AMI m√† b·∫°n d√πng ƒë·ªÉ launch instance b·ªã thi·∫øu file n√†o ƒë√≥.\n Q. B·∫°n v·ª´a t·∫°o 1 RDS instance, app c·ªßa b·∫°n s·∫Ω config ƒë·ªÉ connect RDS. Nh∆∞ng t·ª´ app ƒëang b·ªã l·ªói ‚ÄúError connecting to database‚Äù V√¨ sao?\n Sai port\nSecurity groups sai\nRDS v·∫´n ƒëang t·∫°o v√† ch∆∞a available (c√≥ th·ªÉ ph·∫£i ch·ªù 20 ph√∫t)\n Q. C√¥ng ty b·∫°n c√≥ EC2, S3, PostgreSQL, EFS, gi·ªù mu·ªën encrpyt data at rest, nh·ªØng c√°i n√†o c√≥ th·ªÉ encrypt m√† ko l√†m gi√°n ƒëo·∫°n user d√πng?\n S3 th√¥i\nNh·ªØng c√°i c√≤n l·∫°i ƒë·ªÅu ph·∫£i encrpyt t·ª´ ƒë·∫ßu khi t·∫°o, gi·ªù mu·ªën th√¨ ph·∫£i t·∫°o l·∫°i\n Q. EFS c√≥ kh·∫£ nƒÉng scale l√™n peta byte ko?\n c√≥\n Q. B·∫°n c√≥ web server ·ªü 2 region us-east-1 v√† us-west-1, config routing policy l·∫ßn l∆∞·ª£t l√† latency-based v√† weighted-based. Khi test failover th√¨ n·∫øu all ec2 ·ªü us-west-1 b·ªã down th√¨ traffic ko shift sang us-east-1 v√¨ sao?\n Do thi·∫øu config Evaluate Target Health check = yes\nH√¨nh tr√™n n·∫øu Evaluate Target Health check = NO th√¨ Route53 s·∫Ω ko th·ªÉ quay l·∫°i c√°i ch·ªó branch cha ƒë·ªÉ n√≥ switch sang region kh√°c ƒë∆∞·ª£c\n Q. B·∫°n c√≥ AWS env v·ªõi EC2 cho web app, CLB, RDS, b·∫°n th·∫•y gi√° tr·ªã high-value SpilloverCount ·ªü trong metric c·ªßa Cloudwatch, b·∫°n n√™n monitor c√°i g√¨ ƒë·ªÉ ko c√≤n c√°i gi√° tr·ªã n√†y ?\n SpilloverCount l√† t·ªïng s·ªë request b·ªã reject do surge queue is full.\nGi·∫£i th√≠ch v·ªÅ 4 lo·∫°i metric c·ªßa LB ƒë√¢y:\nSurgeQueueLength: t·ªïng s·ªë request (HTTP listener) ho·∫∑c connection (TCP listener) ƒëang pending. Max c·ªßa queue l√† 1024. N·∫øu queue b·ªã full th√¨ request s·∫Ω b·ªã reject. S·ªë request b·ªã reject ƒë√≥ s·∫Ω hi·ªÉn th·ªã ·ªü metric SpilloverCount\nRequestCount: l√† s·ªë request completed trong 1 kho·∫£ng time nh·∫•t ƒë·ªãnh (1 or 5 ph√∫t)\nBackendConnectionErrors: s·ªë l∆∞·ª£ng connection fail khi connect gi·ªØa LB v√† Instances\n Q. Ph√¢n bi·ªát c√°c lo·∫°i ELB: CLB, NLB, ALB (c√°i n√†o c≈©ng c√≥ t√≠nh scalability):\n Classic Load Balancer: cung c·∫•p (HTTP, HTTPS or TCP listeners) cho only 1 backend port. Ch·∫°y b√™n ngo√†i VPC. ( N·∫øu app c·ªßa b·∫°n ch·∫°y ngo√†i VPC). Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\nNetwork Load Balancer: Layer 4, cung c·∫•p only TCP. D√πng EIP/static IP. Nhi·ªÅu ports. D√πng v·ªõi nh·ªØng app m√† ch·ªâ c·∫ßn s·ª≠ d·ª•ng TCP. V√† mu·ªën qu·∫£n l√Ω b·∫±ng whilelist IP. C√≥ th·ªÉ scale ƒë·ªÉ handle workload l√™n ƒë·∫øn h√†ng tri·ªáu request 1 gi√¢y (millions of requests per second)\nApplication Load Balancer: Layer7, cung c·∫•p c√¢n b·∫±ng t·∫£i cho only HTTP/HTTPS. Nhi·ªÅu ports. flexible h∆°n CLB. Nhi·ªÅu ng d√πng. Cung c·∫•p t√≠nh nƒÉng n√¢ng cao v·ªÅ routing ƒë·∫øn target, d√πng cho c√°c app hi·ªán ƒë·∫°i, micro-services, containers. Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\n Q. B·∫°n c·∫ßn t·∫°o stack Cloudformation, v√† mu·ªën ch·∫Øc ch·∫Øn b·∫°n c√≥ th·ªÉ s·ª≠a l·ªói b·∫±ng manually n·∫øu c√≥ th√¨ l√†m n√†o?\n Set gi√° tr·ªã OnFailure = DO_NOTHING khi create stack, ƒë·ªÉ khi x·∫£y ra l·ªói stack ko t·ª± ƒë·ªông rollback\n Q. B·∫°n d√πng cloudformation t·∫°o stack g·ªìm c√≥ c√°c resource trong ƒë√≥ c√≥ RDS, b·∫°n mu·ªën RDS dc gi·ªØ l·∫°i n·∫øu stack b·ªã delete, l√†m n√†o?\n D√πng DeletionPolicy attribute cho c√°c resource trong stack ·ªü trong file template (json/yml)\n Q. B·∫°n mu·ªën h·ªá th·ªëng c·ªßa b·∫°n tr√™n AWS khi m√† c√≥ issue v·ªõi hardware host AWS resource (nh·ªØng ph·∫ßn c·ª©ng host AWS resource c·ªßa b·∫°n) th√¨ b·∫°n dc notify, l√†m n√†o?\n Personal Health Dashboard\n Q. B·∫°n ƒëang d√πng RDS, mu·ªën encrpyt data in transit l√†m n√†o?\n D√πng SSL certificate m√† RDS cung c·∫•p\nEdit Parameter Group c·ªßa RDS instance (rds.force_ssl = true)\ncu·ªëi c√πng ph·∫£i Reboot RDS instance\nTrong AWS Document vi·∫øt: C√≥ 2 c√°ch d√πng SSL ƒë·ªÉ encrypt data in transit RDS l√†:\n‚ñ† Force SSL cho all connection (Client s·∫Ω ko c·∫ßn l√†m g√¨) C√°i n√†y s·∫Ω c·∫ßn s·ª≠a parameter group (rds.force_ssl = true)\n‚ñ† Encrpyt nh·ªØng connection c·ª• th·ªÉ (setup SSL tr√™n client computer)\n Q. B·∫°n d√πng S3, tr∆∞·ªõc ƒë√≥ l√† Cloudfront, b·∫°n g·∫∑p 1 s·ªë l·ªói 4xx error, th√¨ ƒë√≥ l√† v√¨ sao? l·ªói 5xx th√¨ sao?\n ‚ñ† l·ªói 4xx li√™n quan ƒë·∫øn client\n404-Not Found\n405-Method Not Allowed\n414-Request-URI Too Large\n‚ñ† l·ªói 5xx li√™n quan ƒë·∫øn server\n500-Internal Server Error\n501-Not Implemented\n502-Bad Gateway\n503-Service Unavailable\n504-Gateway Time-out\n Q. B·∫°n d√πng AWS CloudHSM cho SSL/TLS processing. C√≥ incident l√† c√≥ user x√≥a key c·ªßa CloudHSM l√†m ·∫£nh h∆∞·ªüng traffic, gi·ªù b·∫°n c·∫ßn check log c·ªßa CloudHSM ƒë·ªÉ x√°c ƒë·ªãnh user n√†o, l√†m sao? check field n√†o trong c√°c field sau: Time, Reboot Counter, Sequence No, Command Type, Opcode, Session Handle, Response, Log Type\n Opcode (n√≥ ch·ªâ ra action c·ªßa command nh∆∞ CN_CREATE_USER, CN_GENERATE_KEY, CN_DESTROY_OBJECT, CN_CHANGE_PSWD)\nNh·ªØng c√°i kh√°c th√¨:\nTime: th·ªùi gian in UTC,\nReboot Counter: chu·ªói 32-bit,\nSequence No: chu·ªói 64-bit,\nCommand Type: 2 category c·ªßa command (CN_MGMT_CMD, CN_CERT_AUTH_CMD),\nOpcode: action c·ªßa cmd,\nSession Handle: chu·ªói k√≠ t·ª±,\nResponse: SUCCESS ho·∫∑c ERROR,\nLog Type: 4 lo·∫°i MINIMAL_LOG_ENTRY, MGMT_KEY_DETAILS_LOG, MGMT_USER_DETAILS_LOG, GENERIC_LOG\n Q. Khi b·∫°n setup VPC peering gi·ªØa 2 vpc, th√¨ vi·ªác config routing CIDR gi·ªØa 2 VPC dc l√†m th·∫ø n√†o? t·ª± ƒë·ªông route table update khi VPC connection dc accept, hay ph·∫£i l√†m b·∫±ng tay?\n Ph·∫£i l√†m b·∫±ng tay (manually) ch·ª© AWS ko t·ª± ƒë·ªông config route table cho b·∫°n\n Q. B·∫°n ƒëang d√πng DynamoDB, gi·ªù mu·ªën gi·∫£m ƒë·ªô tr·ªÖ c·ªßa request g·ª≠i v√†o DynamoDB th√¨ n√™n d√πng g√¨? (Global table, secondary index, DAX, dynamodb stream?)\n N√™n d√πng DAX (c√≥ cache cho DynamoDB, HA)\nngo√†i ra nh·ªØng c√°i kh√°c th√¨:\nGlobal table (multi-region master db, c≈©ng l√† 1 c√°ch gi·∫£m ƒë·ªô tr·ªÖ)\nSecondary Indexes (tƒÉng performance khi query)\nDynamoDB stream (get notify n·∫øu c√≥ s·ª± thay ƒë·ªïi trong DynamoDB table)\n\u0026mdash;P5\u0026mdash;  Q. Cty b·∫°n d·ª± ƒë·ªãnh move MySQL l√™n RDS. H·ªç mu·ªën bi·∫øt l√† vi·ªác d√πng AWS RDS th√¨ s·∫Ω gi√∫p gi·∫£m c√¥ng t√°c qu·∫£n l√Ω nh∆∞ th·∫ø n√†o cho h·ªç?\n T·∫°o v√† maintain backup ƒë·ªÉ b·∫°n c√≥ th·ªÉ restore v·ªÅ any point in time (max 35 ng√†y tr∆∞·ªõc) v√¨ RDS n√≥ upload transaction log l√™n s3 m·ªói 5 ph√∫t\nT·ª± ƒë·ªông install critical security patch (b·∫£n v√° b·∫£o m·∫≠t cho db)\n Q. V·ªÅ VPC peering qua Ipv6 th√¨ c·∫ßn ch√∫ √Ω g√¨?\n VPC Peering qua IPv6 ch·ªâ support intra-region (trong 1 region), ch√∫ √Ω l√† ko dc overlaping IPv4 (ho·∫∑c ko c·∫ßn IPv4 ch·ªâ c√≥ IPv6 th√¨ ok)\nVPC Peering qua IPv6 kh√¥ng support inter-region (c√≤n g·ªçi l√† cross-region)\n Q. Web app c·ªßa b·∫°n d√πng EC2, SQS. Gi·ªù Server tr√™n on-premise c≈©ng c·∫ßn connect SQS. M√† b·∫°n c·∫ßn gi·∫£i ph√°p ƒë·ªÉ k·∫øt n·ªëi on-premise v·ªõi AWS m√† bandwitch l·ªõn, traffic secure th√¨ l√†m n√†o?\n T·∫°o interface endpoint trong private subnet, d√πng AWS Direct Connect ƒë·ªÉ n·ªëi v·ªõi On-premise server, r·ªìi t·ª´ ƒë√≥ interface endpoint (PrivateLink) s·∫Ω gi√∫p connect v·ªõi SQS 1 c√°ch secure\n(v√¨ interface endpoint s·∫Ω gi√∫p k·∫øt n·ªëi c√°c resource trong AWS v·ªõi nhau ko c·∫ßn internet)\nCh√∫ √Ω l√† interface endpoint ko support AWS VPN (th·∫ø n√™n d√πng AWS Direct Connect)\n Q. VPC A peering v·ªõi VPC B \u0026amp; C, B v√† C th√¨ c√≥ CIDR block gi·ªëng nhau (10.0.0.0/16). EC2 trong VPC B send traffic qua pcx-aaabbb ƒë·∫øn VPC A, VPC A tr·∫£ v·ªÅ respone ko v·ªÅ EC2 trong VPC B, m√† v·ªÅ 1 EC2 tr√πng IP trong VPC C, l√†m sao ngƒÉn ch·∫∑n vi·ªác n√†y?\n S·ª≠a c√°i Route table trong VPC A, th√™m c√°i routing ƒë·∫øn c·ª• th·ªÉ ip EC2 n√†o c·ªßa VPC B (more specific routing):\nho·∫∑c fix th√†nh:\n Q. Cty mu·ªën th·ª±c hi·ªán backup c√°c EBS volumes, v√¨ trong ƒë√≥ ch·ª©a data quan tr·ªçng, n√™n t·∫°o RAID 1 config of multi volume hay d√πng regular EBS snapshot?\n D√πng regular EBS snapshot\nCh·ª© RAID 1 Config ch·ªâ c√≥ t√°c d·ª•ng mirroring data (d√πng cho HA)\n Q. 1 Cty l∆∞u document trong S3. Mu·ªën add th√™m 1 layer security khi m√† User mu·ªën delete document S3, l√†m n√†o? c√≥ enable MFA Delete tr√™n S3 bucket dc ko?\n S·ª≠a bucket policy, d√πng c√°i condition key aws:MultiFactorAuthPresent, ƒë·ªÉ ch·ªâ nh·ªØng user ƒë√£ authen v√†o aws b·∫±ng MFA m·ªõi delete dc docs\nko th·ªÉ enable MFA Delete S3 bucket dc, v√¨ n√≥ ch·ªâ d√†nh cho root account\n Q. Cty s·∫Ω deploy 1 s·ªë ec2, workload c·ªßa c√°c ec2 ko ƒë·ªïi v·ªÅ CPU Utilization v√† thi tho·∫£ng s·∫Ω burstable performance. N√™n d√πng lo·∫°i ec2 type n√†o? gi·ªØa c√°c lo·∫°i T2 T3 M5 C5 R5\n ch·ªçn T2, T3, T3a (Ch√∫ng cung c·∫•p kh·∫£ nƒÉng Burstable Performance)\nT Unlimited th√¨ cung c·∫•p high CPU Performance\nM5 C5 R5 th√¨ cung c·∫•p Fixed Performance\n Q. 1 Cty mu·ªën t·∫°o connection hybrid gi·ªØa On-premise v√† AWS, m√† cost r·∫ª nh·∫•t v√† implement nhanh, th√¨ n√™n ch·ªçn c√°i g√¨? AWS VPN hay Direct Connect, hay Direct Connect Gateway?\n D√πng AWS VPN,\nv√¨ n√≥ r·∫ª h∆°n Direct Connect\nDirect Connect ch·ªâ n√™n d√πng khi mu·ªën low latency v√† b·∫°n gi√†u vl\n Q. Cty d√πng AWS Organization ƒë·ªÉ qu·∫£n l√Ω nhi·ªÅu account. Mu·ªën deny access v√†o S3 c·ªßa root user, ch·ªâ cho ph√©p IAM user th√¥i th√¨ l√†m n√†o?\n D√πng SCP t·ª´ t·∫ßng Master account r·ªìi deny access s3 c·ªßa root user\n Q. N·∫øu SCP (Service control policy) c·ªßa AWS Organizations cho ph√©p User access EC2, S3. Nh∆∞ng IAM Policy cho ph√©p User access EC2, S3, ELB th√¥i. Th√¨ User s·∫Ω d√πng dc nh·ªØng service n√†o?\n Th√¨ user s·∫Ω ch·ªâ c√≥ quy·ªÅn access service chung gi·ªØa 2 policy ƒë√≥ l√† EC2, S3\n Q. B·∫°n c·∫ßn setup EC2, EC2 s·∫Ω c√≥ nhi·ªÅu read/write activity. V√† metric cho volume c·∫ßn update m·ªói 2 ph√∫t.\nV·∫≠y th√¨ c·∫ßn ch·ªçn volume Provisioned IOPS\nhay Enable Detailed Monitoring c·ªßa EBS volume?\n Ch·ªçn Provisioned IOPS l√† ƒë·ªß, lo·∫°i io1 n√†y s·∫Ω t·ª± ƒë·ªông send metric 1 ph√∫t update ƒë·∫øn cloudwatch m√† ko c·∫ßn enable Detailed monitoring\n Q. B·∫°n d√πng VPN cho cty, b·∫°n ƒë√£ t·∫°o VGW v√† Customer Gateway, routing attach v√†o VPC. Nh∆∞ng Tunnel v·∫´n ƒëang b·ªã down. L√†m sao ƒë·ªÉ active VPN tunnel gi·ªØa VGW v√† Customer Gateway?\n mu·ªën active Tunnel th√¨ ph·∫£i init traffic t·ª´ Customer side ·ªü on-premise location t·ªõi server trong AWS VPC.\n Q. B·∫°n c√≥ 1 app setup tr√™n ec2 ELB. Mu·ªën ensure vi·ªác HA v√† traffic s·∫Ω dc distribute across c√°c backend node th√¨ l√†m n√†o?\n launch ec2 tr√™n MultiAZ\nenable cross zone load balancing cho ELB\nAWS Document vi·∫øt: Khi b·∫°n enable 1 AZ cho ELB. ELB s·∫Ω t·∫°o 1 load balancer node trong AZ ƒë√≥. M·∫∑c ƒë·ªãnh l√† m·ªói load balancer node ch·ªâ distribute traffic ƒë·∫øn c√°c target trong AZ c·ªßa n√≥ th√¥i. N·∫øu b·∫°n enable cross-zone load balancing th√¨ m·ªói load balancer node s·∫Ω distribute traffic across c√°c target tr√™n t·∫•t c·∫£ AZ ƒë√£ dc enable.\n Q. Cty b·∫°n thi·∫øt l·∫≠p AWS VPN gi·ªØa on-premise network v√† VPC. ƒê√£ t·∫°o Virtual Private Gateway, Customer Gateway, VPN connection. ƒê√£ config router ·ªü ph√≠a Customer side, v√† connection ·ªü Console ƒë√£ show UP. C√≤n thi·∫øu g√¨ n·ªØa?\n config router ·ªü ph√≠a Virtual Private Gateway\n Q. N·∫øu RDS th·ª±c hi·ªán patching OS th√¨ report v·ªÅ process vi·ªác patching ƒë√≥ s·∫Ω c√≥ ·ªü ƒë√¢u?\n RDS Console\n Q. B·∫°n th·∫•y c√≥ issue v·ªõi ASG, khi Launch Configuration update, ASG s·∫Ω l√†m v√†i node b·ªã offline l√†m ·∫£nh h∆∞·ªüng customer. Team d√πng Cloudformation ƒë·ªÉ update app b·∫±ng c√°ch change parameter. L√†m n√†o ƒë·ªÉ h·∫°n ch·∫ø ·∫£nh h∆∞·ªüng ƒë·∫øn customer?\n Trong CF template, add c√°i AutoScalingRollingUpdate trong UpdatePolicy attribute, enable WaitOnResourceSignals, append 1 c√°i healthcheck ·ªü cu·ªëi user data script ƒë·ªÉ signal cho ASG bi·∫øt l√† vi·ªác update instance ƒë√£ success.\n Q. B·∫°n mu·ªën Cloudformation template lu√¥n l·∫•y c√°i Windows AMI latest khi launch th√¨ l√†m n√†o?\n _C√°ch 1: subcribe Windows AMI noti cho SNS v√† trigger Lambda update template v·ªõi new AMI\n_C√°ch 2: trong Cf template th√¨ ch·ªó Parameter AMI ƒë·ªÉ ch·ªâ ƒë·ªãnh c√°i Parameter trong SSM Parameter Store (n∆°i ch·ª©a c√°c public parameter d√πng cho to√†n b·ªô AWS resource).\nTr∆∞·ªõc ƒë√≥ th√¨ ph·∫£i t·∫°o 1 Parameter trong SSM Parameter Store, value ki·ªÉu nh∆∞ sau: /aws/service/ami-windows-latest/Windows_Server-2016-English-Core-Containers\n Q. Cty b·∫°n c√≥ 1 bucket ƒë√£ versioning enable. Gi·ªù mu·ªën x√≥a delete bucket th√¨ l√†m n√†o?\n X√≥a b·∫±ng AWS SDK (nh∆∞ boto3)\nX√≥a b·∫±ng bucket lifecycle config\nX√≥a b·∫±ng Console\nC√≤n c√°c command n√†y th√¨:\naws s3 rb s3://example-bucket-july --force: c√°i n√†y ko x√≥a dc bucket ƒë√£ versioning\naws s3 rm s3:// example-bucket-july --recursive: c√°i n√†y ko x√≥a dc bucket ƒë√£ versioning\n Q. S3 c√≥ nh·ªØng file hi·∫øm khi access sau 45 days. C√°ch n√†o ƒë·ªÉ reduce cost m√† c√≥ th·ªÉ cung c·∫•p access v√†o file cho user t·ª´ 1-5 ph√∫t?\n D√πng Glacier Expedited Retrieval (1-5 min)\nS3 IA ƒë·∫Øt h∆°n Glacier n√™n ko ch·ªçn\n Q. Cty migrate 500 TB l√™n S3, ƒëang concern v·ªÅ cost, time, performance. D√πng g√¨? Storage Gateway hay Snowball?\n n√™n d√πng Snowball\nSnowball: cost 1562 $\nStorage Gateway: 11500 $\n Q. B·∫°n d√πng AWS Opswork m√† c·∫ßn monitor log c·ªßa app dc deploy tr√™n instance c·ªßa stack trong Opswork l√†m n√†o? c√≥ Opswork log ko?\n D√πng Cloudwatch logs\nko c√≥ c√°i g·ªçi l√† Opswork logs\n Q. B·∫°n c·∫ßn delete Customer Master Key th√¨ c·∫ßn check nh·ªØng g√¨?\n Check CMK permission trong KMS ƒë·ªÉ xem ai ho·∫∑c c√°i g√¨ ƒëang d√πng CMK ƒë√≥\nCheck Cloudtrail logs, ƒë·ªÉ xem l·ªãch s·ª≠ s·ª≠ d·ª•ng CMK ƒë√≥, xem c√°i CMK ƒë√≥ c√≥ c·∫ßn thi·∫øt n·ªØa ko\n Q. B·∫°n ƒëang d√πng Redshift th√¨ c√≥ nh·ªØng lo·∫°i log n√†o?\n 3 lo·∫°i:\n‚ñ† Connection log: nh·ªØng log authen, connect, disconnect\n‚ñ† User log: log m√† l√†m li√™n quan ƒë·∫øn db user nh∆∞: Create User, Drop User, Alter User\n‚ñ† User activity log: Log nh·ªØng query tr∆∞·ªõc khi n√≥ run tr√™n DB\nTuy nhi√™n th√¨ logging c·ªßa Redshift default disable. Mu·ªën logging th√¨ ph·∫£i enable l√™n.\nCh√∫ √Ω c√°i User activity log mu·ªën enable th√¨ ph·∫£i set enable_user_activity_logging=True n·ªØa\nko c√≥ Transaction Logs nh√©\n Q. Cty b·∫°n d√πng RDS MySQL, gi·ªù mu·ªën disable backup c·ªßa RDS b·∫±ng c√°ch set retention period=0 nh∆∞ng b·ªã l·ªói, v√¨ sao?\n V√¨ b·∫°n ƒëang setup Read Replica th√¨ ko set retention period = 0 ƒë∆∞·ª£c\nAWS Documents vi·∫øt:\n‚ñ† N·∫øu b·∫°n lock yourself ·ªü ngo√†i c√°i db_owner c·ªßa SQL DB. B·∫°n c√≥ th·ªÉ reset DB instance master password ƒë·ªÉ l·∫•y l·∫°i quy·ªÅn\n‚ñ† DB instance reboot v√† h·ªèi b·∫°n c√≥ Apply Immediately? khi b·∫°n:\n_thay ƒë·ªïi rentention period t·ª´ 0 ho·∫∑c t·ªõi 0,\n_change DB instance class.\n‚ñ† DB instance reboot ngay l·∫≠p t·ª©c khi b·∫°n:\n_change storage type (SSD, Standard)\n‚ñ† DB instance reboot theo ki·ªÉu b·∫Øt b·∫°n t·ª± l√†m manually khi:\n_change static param trong DB parameter group\n(ch·ª© n·∫øu change dynamic param th√¨ ko c·∫ßn reboot s·∫Ω take effect ngay)\n‚ñ† DB instance h·∫øt storage space, b·∫°n c·∫ßn modify tƒÉng storage l√™n\n‚ñ† N·∫øu b·ªã l·ªói InsufficientDBInstanceCapacity khi t·∫°o/s·ª≠a db, restore db t·ª´ snapsshot th√¨ n√™n:\n_th·ª≠ l·∫°i v·ªõi db instance class kh√°c\n_th·ª≠ l·∫°i v·ªõi AZ kh√°c, ho·∫∑c th·ª≠ l·∫°i m√† ko ch·ªâ ƒë·ªãnh AZ\n_1 s·ªë DB instance c·∫ßn ·ªü trong VPC\n‚ñ† MySQL version \u0026lt; 5.6 c√≥ th·ªÉ b·ªã l·ªói khi query: n√™n upgrade l√™n version MySQL \u0026gt; 5.6\n‚ñ† PostgreSQL l·ªói connection timeout th√¨ n√™n:\n_check hostname l√† DB instance enpoint ch∆∞a\n_check port number ƒë√∫ng ch∆∞a\n_check SG ƒë√∫ng rule ch∆∞a\n Q. Cty ƒëang d√πng Kinesis mu·ªën encrpyt data at rest th√¨ n√™n d√πng server-side hay client-side encryption?\n N√™n d√πng server-side v√¨ ƒë·ª° t·ªën effort h∆°n\n\u0026mdash;P:monitoring-reporting\u0026mdash;  Q. B·∫°n ƒëang setting AWS Config qua CLI, b·∫°n ƒë√£ t·∫°o S3 bucket t√™n 12345. B·∫°n ko nh·∫≠n dc th√¥ng b√°o v·ªÅ change configuration c·ªßa bucket ƒë√≥. V√¨ sao?\n C√°i s3 bucket policy attach v√†o bucket c·∫ßn allow AWS Config quy·ªÅn record l·∫°i nh·ªØng thay ƒë·ªïi\nC√°i IAM Role assign cho AWS Config CLI c·∫ßn c√≥ quy·ªÅn AWSConfigRole\n Q. Cty b·∫°n l√† global company. B·∫°n mu·ªën check xem RDS DB instance n√†o tr√™n kh·∫Øp c√°c region kh√¥ng dc enable multi-AZ, d√πng c√°i g√¨?\n AWS Config ho·∫∑c Trusted Advisor\n Q. Cty B·∫°n c√≥ 1 EC2 run app, b√™n audit y√™u c·∫ßu xem report nh·ªØng c√°i config change li√™n quan ƒë·∫øn con server ƒë√≥. L√†m n√†o?\n D√πng AWS Config Periodic rule 1h, 3h, 6h, 12h, 24h\nch√∫ √Ω l√† ko c√≥ 4h\n Q. Cty b·∫°n c√≥ infras tr√™n nhi·ªÅu regions. V√† c≈©ng c√≥ nhi·ªÅu AWS account. Gi·ªù mu·ªën t·ªïng h·ª£p report v·ªÅ resource count v√† compliance c·ªßa all account all regions th√¨ l√†m dc ko?\n D√πng AWS Config Aggregated view\n Q. AWS Config pricing nh∆∞ n√†o, tr·∫£ ti·ªÅn cho nh·ªØng c√°i g√¨?\n B·∫°n tr·∫£ ti·ªÅn cho t·ªïng s·ªë Config rule ƒëang active, S·ªë l∆∞·ª£ng Config items\n Q. C√°c member ƒëang t·∫°o 1 s·ªë l∆∞·ª£ng l·ªõn EC2. B·∫°n mu·ªën evaluate xem c√≥ bao nhi√™u m4.large EC2 n√™n b·∫°n ƒë√£ t·∫°o AWS Config custom rule + Lambda function. Rule t·∫°o ok, nh∆∞ng ·ªü section Compliance b·∫°n th·∫•y l·ªói No resources in scope V√¨ sao?\n C·∫ßn verify l·∫°i xem c√°i Custom rule b·∫°n t·∫°o c√≥ ƒëang record EC2 hay ko, ko th√¨ ph·∫£i add th√™m v√†o\nCh·ª© n·∫øu member t·∫°o to√†n t2.micro instance th√¨ s·∫Ω b√°o t·ªïng s·ªë Non-compliance count ch·ª© ko ph·∫£i l·ªói kia\n Q. S3 Glacier ƒë√£ ƒë∆∞·ª£c support b·ªüi AWS Config Managed Rule ch∆∞a? mu·ªën record S3 Glacier vault l√†m n√†o?\n Ch∆∞a, gi·∫£i ph√°p l√†:\n1_T·∫°o 1 lambda function ƒë·ªÉ evaluate S3 Glacier vault,\n2_R·ªìi t·∫°o 1 AWS Config Custom Rule v√† assign c√°i Lambda t·∫°o b√™n tr√™n cho rule n√†y\n Q. B·∫°n c√≥ 1 S3 bucket c·∫ßn monitoring v√† t·ª± correct n·∫øu c√≥ b·∫•t k·ª≥ policy violation th√¨ l√†m n√†o?\n Enable AWS Config ƒë·ªÉ monitor bucket ACL v√† policy violation\nN·∫øu detect c√≥ violation th√¨ s·∫Ω trigger Cloudwatch Event, r·ªìi trigger ti·∫øp Lambda ƒëi correct l·∫°i bucket ACL (ho·∫∑c set bucket ACL to private)\n\u0026mdash;P:storage-management\u0026mdash;  Q. B·∫°n c√≥ 1 l∆∞·ª£ng l·ªõn document tr√™n s3 bucket dc upload t·ª´ kho·∫£ng 10 regions tr√™n TG. ƒê·ªÉ analyse th√¨ b·∫°n d√πng AWS Athena + Quicksight, nh∆∞ng sau 1 th√°ng th·∫•y performance th·∫•p, cost nhi·ªÅu, gi·ªù l√†m n√†o?\n Khi CREATE TABLE, d√πng PARTITIONED BY ƒë·ªÉ partition base on time/date. B·∫°n s·∫Ω h·∫°n ch·∫ø dc l∆∞·ª£ng data dc scan b·ªüi query v√† gi·∫£m cost.\nKhi ch·ªçn storage format th√¨ ch·ªçn Apache Parquet v√† ORC - ƒë√¢y l√† d·∫°ng columnar storage format (thay v√¨ d√πng JSON,CSV) ƒë·ªÉ Athena retrieve data nhanh nh·∫•t\n Q. Cty c·∫ßn analyse data nhanh v√† ko c√≥ budget ƒë·ªÉ d·ª±ng server th√¨ d√πng g√¨?\n Athena + QuickSight\n Q. B·∫°n mu·ªën file dc encrpyt tr∆∞·ªõc khi ƒë∆∞a l√™n s3, t√¨m 1 gi·∫£i ph√°p full managed v√† cost-effective?\n KMS client-side\nKhi enable client-side encryption th√¨ b·∫°n c·∫ßn cung c·∫•p CMK ID cho KMS\nKhi mu·ªën upload 1 object to s3, b·∫°n d√πng CMK ID ƒë·ªÉ request KMS tr·∫£ v·ªÅ: plain-text key ƒë·ªÉ encrypt object, v√† cipher blob ƒë·ªÉ l√†m object metadata\n Q. 1 Cty c·∫ßn 1 n∆°i l∆∞u tr·ªØ key ki·ªÉu single tenant key storage v√† support asymetric key integration. D√πng g√¨? KMS hay AWS CloudHSM?\n D√πng CloudHSM\nCh·ª© KMS l√† d·ªãch v·ª• multi-tenant key storage \u0026amp; ch·ªâ support symetric key th√¥i.\n Q. B·∫°n t·∫°o 1 vault trong S3 Glacier ƒë·ªÉ l∆∞u data l√¢u d√†i, c·∫ßn ensure l√† ko c√≥ thay ƒë·ªïi ho·∫∑c deletion x·∫£y ra trong ƒë√≥. Nh∆∞ng v·∫´n c·∫ßn v√†o ƒë·ªçc nhi·ªÅu l·∫ßn. D√πng policy g√¨?\n Vault Lock Policy\n Q. B·∫°n c·∫ßn l∆∞u document 1 ch·ªó kho·∫£ng 5 nƒÉm m√† s·∫Ω ko cho b·∫•t c·ª© ai s·ª≠a. B·∫°n d√πng vault trong s3 glacier. B·∫°n c·∫ßn setup Vault Lock th√¨ c·∫ßn ch√∫ √Ω g√¨?\n Init Vault lock v·ªõi Vault Lock policy r·ªìi ƒëi v√†o tr·∫°ng th√°i in-progress\nPh·∫£i ho√†n th√†nh Vault Lock trong v√≤ng 24h sau khi Init\nSau khi vault ƒë√£ locked th√¨ ko th·ªÉ unlock\nN·∫øu ko ho√†n th√†nh Vault lock trong 24h th√¨ vault lock policy s·∫Ω b·ªã remove v√† s·∫Ω ph·∫£i l√†m l·∫°i\n Q. Trong c√°i GET URL sau s3.amazonaws.com/mybucket/photos/2014/08/puppy.jpg?x-user=johndoe th√¨ x-user=johndoe nghƒ©a l√† g√¨?\n S3 s·∫Ω ignore nh·ªØng query b·∫Øt ƒë·∫ßu b·∫±ng x-, tuy nhi√™n s·∫Ω l∆∞u l·∫°i trong access log record.\n Q. Ph√¢n bi·ªát aws gateway cached volume v√† aws gateway stored volume?\n Aws gateway cached volume: l∆∞u data tr√™n s3, 1 c·ª•c c·∫ßn access th∆∞·ªùng xuy√™n s·∫Ω l∆∞u locally. d·ªÖ scale, ƒë·ªô tr·ªÖ th·∫•p\nAws gateway stored volume: data s·∫Ω l∆∞u ho√†n to√†n ·ªü locally, r·ªìi snapshot s·∫Ω dc async l√™n s3\n\u0026mdash;P1-udemy\u0026mdash;  Q. Team member c·ªßa b·∫°n upload Django project l√™n Beanstalk. Nh∆∞ng g·∫∑p l·ªói The instance profile aws-elasticbeanstalk-ec2-role associated with the environment does not exist. V√¨ sao?\n Beanstalk CLI ch∆∞a th·ªÉ t·∫°o instance profile role n·∫øu IAM role c·ªßa n√≥ ko c√≥ quy·ªÅn t·∫°o role\nHo·∫∑c l√† instance profile role ƒë√£ t·∫°o nh∆∞ng b·ªã insufficient ho·∫∑c outdate v·ªõi nh·ªØng g√¨ m√† Beanstalk c·∫ßn\n Q. B·∫°n c·∫ßn d·ª±ng 1 web app m√† web server c·∫ßn connect to internet v√† Db server th√¨ ko, v·∫≠y n√™n d·ª±ng ELB nh∆∞ n√†o?\n internet-facing load balancer cho web server\ninternal load balancer cho db server\nN·∫øu c√≥ th√™m y√™u c·∫ßu db server c·∫ßn init Ipv4 traffic ra Internet v√† ch·∫∑n vi·ªác nh·∫≠n traffic t·ª´ internet th√¨ m·ªõi c·∫ßn NAT Gateway (or NAT instance),\nnh∆∞ng s·∫Ω c·∫ßn NAT ·ªü trong public subnet\n Q. Cty b·∫°n t·∫°o 1 VPC v·ªõi 1 subnet, s·∫Ω c√≥ 20 instance b√™n trong th√¨ CIDR block s·∫Ω ph·∫£i l√† 172.0.0.0/x, x s·∫Ω l√† bao nhi√™u 27 28 29 30?\n c√°ch t√≠nh l√† 2 step: Gi·∫£ s·ª≠ x = 27\n‚òÖ 32 - 27 = 5\n‚òÖ 2 m≈© 5 = 32 (suy ra VPC c·ªßa b·∫°n c√≥ 32 ip available)\n‚òÖ Tuy nhi√™n trong AWS, 4 ip ƒë·∫ßu ti√™n v√† 1 ip cu·ªëi c√πng trong m·ªói subnet b·∫°n s·∫Ω ko d√πng dc:\n-\u0026gt; 32 - 4 - 1 = 27 (s·ªë ip b·∫°n c√≥ th·ªÉ assign cho instance)\n‚òÖ Ch√∫ √Ω n·ªØa l√† block size dc cho ph√©p ch·ªâ l√† gi·ªØa /28 v√† /16 th√¥i\nTh·∫ø n√™n /29 v√† /30 l√† lo·∫°i ngay\n Q. B·∫°n c·∫ßn migrate data 80 TB l√™n s3 th√¨ n√™n d√πng g√¨ cho r·∫ª v√† nhanh? Snowball th∆∞·ªùng, Snowball Edge, Storage Gateway?\n Snowball th∆∞·ªùng th√¨ 1 c√°i appliance ch·ªâ usable 72 TB data th√¥i, nghƒ©a l√† ph·∫£i d√πng 2 c√°i appliances -\u0026gt; th√†nh ra gi√° ƒë·∫Øt h∆°n Snowball Edge\nSnowball Edge usable 80 TB data/100 TB n√™n ƒë·ªß -\u0026gt; n√™n d√πng\nStorage Gateway th√¨ m·ª•c ƒë√≠ch ch√≠nh c·ªßa n√≥ ko ph·∫£i ƒë·ªÉ migration, v·ªõi l·∫°i n√≥ s·∫Ω d√πng Internet ƒë·ªÉ upload n√™n ch·∫≠m\n Q. B·∫°n c·∫ßn monitor dashboard cho EC2 v√† on-premise server th√¨ c·∫ßn l√†m g√¨?\n IAM Role v√† User ƒë·ªÉ d√πng cho Cloudwatch Agent\nInstall CW agent tr√™n c·∫£ EC2 v√† On-premise server\nT·∫°o CW agent configure file\n Q. S3 bucket store data. B·∫°n c·∫ßn generate 1 report tr√™n replication v√† encryption status c·ªßa object. D√πng g√¨?\n S3 Inventory ƒë·ªÉ gen ra report\n Q. Ph√¢n bi·ªát v·ªÅ 2 policy: Latency routing v√† Geolocation routing c·ªßa Route53?\n Latency routing ch·ªâ ƒë·∫£m b·∫£o v·ªÅ term \u0026ldquo;latency\u0026rdquo; ch·ª© ko ƒë·∫£m b·∫£o traffic dc route ƒë·∫øn resource g·∫ßn nh·∫•t.\nTh·∫ø n√™n 1 s·ªë tr∆∞·ªùng h·ª£p V√≠ d·ª• nh∆∞ b·∫°n c√≥ 2 server ·ªü UK, US,\ny√™u c·∫ßu c·ªßa Compliance l√† data c·ªßa User ·ªü UK c·∫ßn ƒë·∫∑t ·ªü UK, data user ·ªü US c·∫ßn ƒë·∫∑t ·ªü US,\nth·∫ø th√¨ ph·∫£i d√πng Geolocation ƒë·ªÉ ƒë·∫£m b·∫£o user ·ªü ƒë√¢u th√¨ h·ªç s·∫Ω dc route ƒë·∫øn server ƒë·∫∑t data c·ªßa h·ªç ·ªü ƒë√≥\nNgo√†i ra th√¨ n·∫øu User ·ªü 1 ch·ªó kh√°c UK v√† US th√¨ b·∫°n c·∫ßn t·∫°o 1 record default ch·ª© n·∫øu ko R53 s·∫Ω respone \u0026ldquo;no answer\u0026rdquo;\nC√≤n c√°i Geoproximity th√¨ d√πng khi b·∫°n c√≥ 1 c√°i instance size r·∫•t l·ªõn ·ªü 1 region v√† mu·ªën route user ƒë·∫øn c√°i server ƒë√≥\nNgo√†i ra:\nMultivalue answer routing policy: route traffic 1 c√°ch ng·∫´u nhi√™n ƒë·∫øn max l√† 8 record healthy.\nSimple routing policy: D√πng 1 resource cho 1 domain.\nLatency routing policy: N·∫øu c√≥ nhi·ªÅu resource th√¨ s·∫Ω route traffic ƒë·∫øn location c√≥ best latency nh·∫•t.\nWeighted routing policy: route traffic ƒë·∫øn nhi·ªÅu resoure theo t·ªâ l·ªá b·∫°n defined.\nGeoproximity routing policy: route traffic d·ª±a tr√™n location c·ªßa resource c·ªßa b·∫°n.\nGeolocation routing policy: route traffic d·ª±a tr√™n location c·ªßa user.\nFailover routing policy: d√πng ƒë·ªÉ configure Active/passive failover.\n Q. B·∫°n c·∫ßn terminate ec2 trong ASG m√† kh√¥ng update size c·ªßa ASG th√¨ d√πng command n√†o?\n terminate-instance-in-auto-scaling-group --instance-id \u0026lt;value\u0026gt; --no-should-decrement-desired-capacity\nnh·ªõ l√† --no-should-decrement-desired-capacity ch·ª© ko ph·∫£i --should-decrement-desired-capacity\n Q. B·∫°n c·∫ßn host Oracle DB tr√™n Ec2 m√† data c≈©ng access ko th∆∞·ªùng xuy√™n l·∫Øm, ch·ªçn lo·∫°i EBS n√†o cho cost-effective?\n Cold HDD\nCh·ª© General purpose SSD m·ª•c ƒë√≠ch ch√≠nh ko ph·∫£i host db, t·ªën ti·ªÅn h∆°n\nProvisioned IOPS HDD ƒë·ªÉ host db nh∆∞ng t·ªën ti·ªÅn h∆°n\nThroughput Optimized HDD th√¨ cost c≈©ng th·∫•p nh∆∞ng ko th·∫•p b·∫±ng Cold HDD, v√† data ƒë√≥ l√† cho access th∆∞·ªùng xuy√™n\n Q. B·∫°n c·∫ßn setup Disaster Recovery infra, h·ªç c·∫ßn 1 gi·∫£i ph√°p quick recovery v√† cost effective, so s√°nh warm standby, backup \u0026amp; restore, pilot light?\n D√πng pilot light\nSo s√°nh:\nv·ªÅ gi√° th√¨ backup \u0026amp; restore \u0026lt; pilot light \u0026lt; warm standby\nv·ªÅ recovery time th√¨ c√†ng ƒë·∫Øt c√†ng recovery nhanh\npilot light th√¨ ch·ªâ nh·ªØng core element dc run all the time (db server replica ch·∫≥ng h·∫°n)\nwarm standby th√¨ gi·ªëng nh∆∞ h·ªá th·ªëng th·∫≠t nh∆∞ng l√† version b·ªã scale down v√† run all the time\nhttps://tutorialsdojo.com/aws-cheat-sheet-backup-and-restore-vs-pilot-light-vs-warm-standby-vs-multi-site/\n Q. B·∫°n c·∫ßn install v√† config app tr√™n 1 ec2 m√† b·∫°n s·∫Ω deploy b·∫±ng CF, s·∫Ω d√πng policy g√¨ trong c√°i CF template? UpdatePolicy, UpdateReplacePolicy, CreationPolicy ?\n D√πng CreationPolicy\nch·ª© d√πng UpdatePolicy khi b·∫°n mu·ªën update resource c·ªßa stack\nc√≤n d√πng UpdateReplacePolicy khi b·∫°n mu·ªën backup c√°i ec2 hi·ªán t·∫°i khi n√≥ s·∫Øp b·ªã replace b·ªüi update stack\n Q. N·∫øu khi launch EC2 b·ªã l·ªói EC2 instance \u0026lt;instance ID\u0026gt; is in VPC. Updating load balancer configuration failed nghƒ©a l√† sao?\n LB ƒëang thu·ªôc lo·∫°i EC2-Classic, ASG th√¨ ·ªü trong VPC\n Q. Kinesis v√† Redshift c∆° b·∫£n l√† l√†m g√¨?\n Kinesis l√† service ƒë·ªÉ collect data real-time\nc√≤n Redshift l√† service ƒë·ªÉ storage peta-byte data cho OLAP app (online analysis processing)\n Q. So s√°nh Snowball v√† Snowmobile?\n Snowmobile d√πng cho dataset t·∫ßm 10 PB (petabyte) tr·ªü l√™n.\nC√≤n nh·ªè h∆°n 10 PB th√¨ d√πng Snowball th∆∞·ªùng th√¥i\n Q. C·∫ßn 1 service ƒë·ªÉ ph√¢n t√≠ch behavior c·ªßa AWS resource t·ª´ ƒë√≥ x√°c ƒë·ªãnh nh·ªØng issue v·ªÅ security ti·ªÅm t√†ng th√¨ d√πng g√¨?\n AWS Inspector\n Q. AWS Trusted Advisor v√† AWS Performance Insights th√¨ c√°i n√†o gi√∫p b·∫°n ph√¢n t√≠ch issue v·ªÅ Database?\n AWS Performance Insights\n Q. B·∫°n c√≥ 1 volume Provisioned IOPS size l√† 10 GB, v·∫≠y th√¨ max IOPS c√≥ th·ªÉ set l√† bao nhi√™u?\n c√¥ng th·ª©c l√† IOPS:VolumeSize = 50:1\n=\u0026gt; 10 GB th√¨ max IOPS = 500 (input/output per second)\n Q. Cty c√≥ 1 VPC c·∫ßn connect v·ªõi on-premise network, th√¨ AWS resource c·∫ßn cung c·∫•p nh·ªØng g√¨?\n Internet Gateway attach to VPC\nVirtual Private Gateway\nvi·ªác t·∫°o EIP hay public IP, hay additional ENI (network interface) l√† ko c·∫ßn thi·∫øt\n Q. Route53 c√≥ th·ªÉ th·ª±c hi·ªán bao nhi√™u lo·∫°i healhcheck?\n 3 lo·∫°i:\nhealthcheck m√† monitor 1 enpoint\nhealthcheck m√† monitor nh·ªØng healthcheck kh√°c\nhealthcheck m√† monitor Cloudwatch alarm\n Q. B·∫°n c·∫ßn investigate h·ªá th·ªëng n√™n c·∫ßn suspend ASG scaling process t√™n l√† Terminate, khi ƒë√≥ th√¨ n√≥ s·∫Ω ·∫£nh h∆∞·ªüng nh∆∞ n√†o ƒë·∫øn AZ rebalance process c·ªßa ASG?\n ASG s·∫Ω c√≥ th·ªÉ tƒÉng l√™n max l√† th√™m 10% so v·ªõi maximum size\nC√≤n n·∫øu suspend AZRebalance process, khi ƒë√≥ n·∫øu scale-out th√¨ n√≥ v·∫´n c·ªë g·∫Øng rebalance AZ v·ªõi b·∫±ng c√°ch launch th√™m 1 s·ªë √≠t nh·∫•t EC2\nC√ín n·∫øu suspend Launch process, AZRebalance s·∫Ω ko x√≥a hay t·∫°o th√™m EC2\n Q. B·∫°n c√≥ 1 bucket S3 c·∫ßn delete. Bucket ƒë√≥ ƒë√£ dc enable versioning v√† MFA delete. B·∫°n c·∫ßn l√†m g√¨ ƒë·ªÉ x√≥a?\n C√°ch 1_Remove c√°i bucket policy m√† y√™u c·∫ßu MFA delete, R·ªìi d√πng SDK ƒë·ªÉ x√≥a bucket\u0026rsquo;s markers v√† version, R·ªìi x√≥a l·∫°i b·∫±ng CLI command\nC√°ch 2_D√πng root account ƒë·ªÉ suspend MFA v√† versioning feature ƒëi. R·ªìi Config Lifecycle ƒë·ªÉ remove version, x√≥a object v√† markers, r·ªìi delete bucket again\n\u0026mdash;P2-udemy\u0026mdash;  Q. ƒê·ªÉ 1 EC2 c√≥ th·ªÉ communicate ra Internet qua IPv6 th√¨ VPC c·∫ßn config nh∆∞ n√†o?\n _associate /56 CIDR block cho VPC\n_associate /64 CIDR block cho subnet\n_t·∫°o custom route table r·ªìi associate cho subnet\nVi·ªác setup egress-only gateway ch·ªâ ƒë·ªÉ t√°c d·ª•ng l√† VPC ipv6 ch·∫∑n connect t·ª´ internet v√†o trong\n Q. B·∫°n c√≥ 1 S3 bucket m√† d·∫°o n√†y user complain l√† to√†n l·ªói 503. V√¨ sao?\n C√≥ object n√†o ƒë√≥ c√≥ h√†ng tri·ªáu version, khi ƒë√≥ s3 s·∫Ω throttle c√°c request ƒë·∫øn bucket\nGi·∫£i ph√°p l√† d√πng S3 inventory tool ƒë·ªÉ x√°c ƒë·ªãnh object n√†o\n Q. B·∫°n c√≥ 3 VPC v√† mu·ªën ensure l√† c√°c ec2 trong ƒë√≥ c√≥ th·ªÉ communicate v·ªõi nhau th√¨ l√†m n√†o?\n Setup VPC Peering full mesh config cho c·∫£ 3 VPC (full mesh nghƒ©a l√† 3 c√°i VPC s·∫Ω peering t·ª´ng c·∫∑p v·ªõi nhau)\nEnsure l√† all route table c·ªßa m·ªói VPC dc setup\nENsure l√† VPC ko b·ªã overlapping IPv4 CIDR block\n Q. B·∫°n d√πng PostgreSQL run tr√™n EC2, n√≥ s·∫Ω dc d√πng b·ªüi nhi·ªÅu internal app trong VPC c·ªßa b·∫°n. B·∫°n mu·ªën ƒë∆°n gi·∫£n vi·ªác naming convention b·∫±ng c√°ch ƒë·∫∑t 1 custom domain name cho c√°i DB server, l√†m n√†o?\n setup private hosted zone trong Route53.\nT·∫°o A ho·∫∑c AAAA record,\nspecify IP c·ªßa DB server v√†o record ƒë√≥\n Q. B·∫°n c·∫ßn 1 tool ƒë·ªÉ c√≥ th·ªÉ coordinate nhi·ªÅu AWS resources v·ªõi nhau, v√†o 1 serverless workflow, d√πng c√°i g√¨? SWF hay Step Function?\n D√πng Step Functions\nCh·ª© SWF l√† service ƒë·ªÉ track state v√† task, n√≥ c≈©ng ko ph·∫£i l√† serverless orchestration cho nhi·ªÅu resource\n Q. B·∫°n mu·ªën enable Cloudwatch Detailed monitoring cho Auto Scaling Group th√¨ c√≥ ƒë∆∞·ª£c ko?\n c√≥, nh∆∞ng s·∫Ω b·ªã t√≠nh ph√≠\nN·∫øu Basic monitoring cho ASG th√¨ ko t√≠nh th√™m ph√≠\nCloudwatch monitoring support cho h·∫ßu h·∫øt c√°c AWS Services\n Q. Cty b·∫°n c√≥ nhi·ªÅu thi·∫øt b·ªã ƒëo nhi·ªát ƒë·ªô mu·ªën g·ª≠i data custom metric l√™n AWS Cloudwatch ƒë·ªÉ xem tr√™n AWS 1 c√°ch graph visually, th√¨ c√≥ dc ko?\n D√πng CLI or API ƒë·ªÉ upload data metric l√™n Cloudwatch\n Q. Cty b·∫°n d√πng heavily used RDS, gi·ªù mu·ªën ch·∫Øc ch·∫Øn r·∫±ng ko c√≥ outage khi th·ª±c hi·ªán snapshot cho DB th√¨ l√†m n√†o? Multi-AZ hay Read replica?\n Design DB ki·ªÉu Multi-AZ v√† Snapshot s·∫Ω th·ª±c hi·ªán tr√™n con Standby\nc√≤n n·∫øu Read Replica th√¨ m·∫∑c d√π c≈©ng ko c√≥ outage nh∆∞ng m√† do replica lag n√™n c√≥ th·ªÉ c√≥ nhi·ªÅu transaction ch∆∞a k·ªãp ƒë·ªìng b·ªô t·ª´ con master sang con replica t·∫°i th·ªùi ƒëi·ªÉm ƒë√≥ (ƒë√¢y l√† heavily used RDS v√† ko quan t√¢m cost)\n Q. B·∫°n c√≥ 2 subnet trong VPC, EC2 trong 1 subnet ko ping dc EC2 c√≤n l·∫°i v√¨ sao?\n C·∫ßn check l·∫°i config c·ªßa SG\nv√† check Network ACL c√≥ cho ph√©p ICMP traffic ko\n Q. B·∫°n th·∫•y website c·ªßa b·∫°n c√≥ CPU usage tr√™n EC2 instance 90% v√† CPU usage DB ch·ªâ 20% n√™n l√†m g√¨ ƒë·ªÉ c·∫£i thi·ªán respone time?\n ƒê·ªïi lo·∫°i EC2 instance type kh√°c,\nConfig cho ASG c·ªßa EC2 s·∫Ω base tr√™n CPU load ƒë·ªÉ scale out\n Q. S3 bucket Access control list (ACL) c√≥ nh·ªØng lo·∫°i quy·ªÅn g√¨?\n READ\nWRITE\nREAD_ACP\nWRITE_ACP\nFULL_CONTROL\n Q. Khi b·∫°n c·∫ßn apply c√°i s3 server side encryption cho bucket c·ªßa b·∫°n, v√† b·∫°n s·∫Ω s·ª≠ d·ª•ng c√°i key c·ªßa b·∫°n th√¨ trong header c·ªßa reuqest c·∫ßn c√≥ g√¨?\n 1_ x-amz-server-side‚Äã-encryption‚Äã-customer-algorithm (n·∫øu d√πng c√°i n√†y th√¨ value ph·∫£i l√† AES256)\n2_ x-amz-server-side‚Äã-encryption‚Äã-customer-key\n3_ x-amz-server-side‚Äã-encryption‚Äã-customer-key-MD5\n Q. network packet sniffing thu·ªôc tr√°ch nhi·ªám c·ªßa Customer hay AWS?\n AWS ch·ªãu tr√°ch nhi·ªám protect ng∆∞·ªùi d√πng kh·ªèi vi·ªác ƒë√°nh c·∫Øp c√°c g√≥i tin network\n Q. Cty b·∫°n c√≥ 1 app host tr√™n EC2 v√† ALB, C√≥ 1 member thay ƒë·ªïi ALB d·∫´n ƒë·∫øn app b·ªã down, c·∫ßn monitoring changes dc t·∫°o ra tr√™n ALB, th√¨ c√°i ELB healthchecks c√≥ ph·∫£i l√† service gi√∫p monitor info c·ªßa ELB ko?\n ko, ELB healthchecks ch·ªâ check info c·ªßa EC2 c√≥ health hay ko th√¥i\n Q. Cloudfront c√≥ offer t√≠nh nƒÉng monitoring ko?\n C√≥, monitor, report, access log ƒë·ªÉ tracking activity\n Q. B·∫°n mu·ªën S3 t·ª± ph√¢n t√≠ch storage v√† khi m√† l∆∞·ª£ng access v√†o content √≠t th√¨ s·∫Ω transit ch√∫ng v√†o S3-IA, sau ƒë√≥ t·∫°o lifecycle policy ƒë·ªÉ transit ti·∫øp v√†o Glacier, d√πng g√¨?\n S3 analytics\n Q. B·∫°n c√≥ nh·ªØng RDS db instances, trong 1 VPC. C·∫ßn l√†m n√†o ƒë·ªÉ make maximum security cho db v√† db connection? C√≥ n√™n d√πng Db security group ko?\n t·∫°o snapshot c·ªßa db instance r·ªìi start 1 db m·ªõi dc encrypted t·ª´ b·∫£n snapshot ƒë√≥\nd√πng SSL cho RDS ƒë·ªÉ encrypt traffic\nt·∫°o ri√™ng VPC SG cho EC2 v√† VPC SG cho DB\n*ko n√™n d√πng DB SG v√¨ ƒë√≥ l√† EC2-clasic platform, ko ph·∫£i VPC platform\n Q. Auto Scaling Group ASG gi√∫p b·∫°n t·ª± ƒë·ªông scale c√°c lo·∫°i resource n√†o? c√≥ th·ªÉ scaling ra 1 region kh√°c ko?\n EC2,\nEC2 spot fleet,\nECS,\nDynamoDB,\nv√† Aurora\n*ko th·ªÉ l√†m v·ªõi resource ·ªü region kh√°c\n Q. S·ª± kh√°c bi·ªát gi·ªØa HTTP, TCP?\n ‚òÖ Tr∆∞·ªõc khi b·∫°n d√πng ELB, b·∫°n c·∫ßn config 1 or nhi·ªÅu listener cho CLB c·ªßa b·∫°n, listener c√≥ nhi·ªám v·ª• check c√°c request connection. Listener dc config v·ªõi 1 protocol v√† port cho LB c·ªßa front-end, 1 protocol v√† port cho LB c·ªßa back-end\n‚òÖ ng ta d√πng SSL protocol ƒë·ªÉ secure cho HTTP layer (khi ƒë√≥ g·ªçi l√† HTTPS protocol)\n‚òÖ c≈©ng c√≥ th·ªÉ d√πng SSL protocol d√πng ƒë·ªÉ secure cho TCP layer\n‚òÖ N·∫øu frontend d√πng TCP ho·∫∑c SSL th√¨ backend c≈©ng c√≥ th·ªÉ d√πng TCP ho·∫∑c SSL\n‚òÖ N·∫øu frontend d√πng HTTP ho·∫∑c HTTPS th√¨ backend c≈©ng c√≥ th·ªÉ d√πng HTTP ho·∫∑c HTTPS\n‚òÖ Khi d√πng TCP (layer 4) cho c·∫£ frontend v√† backend th√¨: LB s·∫Ω forward request ƒë·∫øn backend m√† kh√¥ng modify headers\n‚òÖ Khi d√πng HTTP (layer 7) cho c·∫£ frontend v√† backend th√¨: LB s·∫Ω modify (ho·∫∑c g·ªçi l√† parse) headers c·ªßa request, r·ªìi terminate connection, r·ªìi m·ªõi forward request ƒë·∫øn backend\n Q. Kh√°ch h√†ng ƒëang s·ª≠ d·ª•ng ELB, ƒë·ªÉ enhance security n√™n b·∫°n mu·ªën config ELB v·ªõi SSL d√πng 1 policy ƒë∆∞·ª£c define tr∆∞·ªõc qu√° tr√¨nh negotiate SSL connection gi·ªØa ELB v√† client. D√πng c√°i g√¨ ƒë·ªÉ ensure l√† ELB s·∫Ω quy·∫øt ƒë·ªãnh cipher n√†o s·∫Ω ƒë∆∞·ª£c d√πng cho SSL connection, (ch·ª© ko ph·∫£i tu√¢n theo th·ª© t·ª± cipher t·ª´ ph√≠a client) ?\n Server Order Preference\nDefault th√¨ c√°i cipher ƒë·∫ßu ti√™n trong client\u0026rsquo;s list m√† match v·ªõi 1 cipher n√†o ƒë√≥ c·ªßa ELB\u0026rsquo;s list s·∫Ω ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ d√πng cho SSL connection.\nNh∆∞ng n·∫øu ELB ƒë∆∞·ª£c config v·ªõi Server Order Preference th√¨ c√°i cipher ƒë·∫ßu ti√™n c·ªßa ELB\u0026rsquo;s list m√† match v·ªõi 1 cipher n√†o ƒë√≥ trong client\u0026rsquo;s list s·∫Ω ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ d√πng cho SSL connection. Config n√†y ƒë·∫£m b·∫£o ELB s·∫Ω ƒë∆∞·ª£c quy·∫øt ƒë·ªãnh cipher n√†o d√πng cho SSL\n Q. B·∫°n c√≥ th·ªÉ t·∫°o 1 ELB v·ªõi nh·ªØng feature n√†o v·ªÅ security? SSL Server Certificates, SSL Negotiation, Back-End Server Authentication, Front-End Server Authentication\n ko th·ªÉ c√≥ Front-End Server Authentication, ch·ªâ c√≤n 3 c√°i kia\n Q. B·∫°n mu·ªën cho ph√©p 1 admin v√†o setup EC2, EC2 ƒë√≥ c·∫ßn quy·ªÅn access v√†o DynamoDB. Nh·ªØng policy permission g√¨ c·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o security? C·∫ßn trust policy t√°c d·ª•ng g√¨?\nIAM permission policy t√°c d·ª•ng g√¨?\n Trust policy cho ph√©p EC2 Instance assume role\nIAM permission policy cho ph√©p User Pass role to EC2\nN√≥i chung ch·ªâ c·∫ßn nh·ªõ User Pass Role l√† ƒë∆∞·ª£c\n Q. C√≥ th·ªÉ l·∫•y ·ªü ƒë√¢u c√°i Credential Report c·ªßa List c√°c AWS User g·ªìm current status, access key age, MFA or not?\n AWS IAM Console -\u0026gt; download Credential Report\n Q. B·∫°n c√≥ 1 VPC with a CIDR block of 10.0.0.0/16, trong ƒë√≥ c√≥ 1 subnet c√≥ CIDR gi·ªëng v·ªõi VPC. B·∫°n ƒë·ªãnh t·∫°o 1 subnet th·ª© 2 (CIDR 10.0.1.0/24) nh∆∞ng b·ªã l·ªói, l√†m th·∫ø n√†o? C√≥ n√™n s·ª≠a c√°i CIDR c·ªßa subnet th·ª© 1 ko?\n B·∫°n ph·∫£i associate 1 secondary IPv4 CIDR block v√†o VPC ƒë·ªÉ tƒÉng size cho VPC ƒë√≥. Th√¨ sau ƒë√≥ m·ªõi add th√™m c√°i subnet th·ª© 2 dc\n Q. B·∫°n c√≥ 1 VPC v·ªõi primary IPv4 CIDR block of 10.0.0.0/24, v√† 4 secondary IPv4 CIDR block n·ªØa. G·∫ßn h·∫øt c√°c IP ƒë√£ ƒëc d√πng v√† b·∫°n ph·∫£i tƒÉng size ƒë·ªÉ c√≥ th·ªÉ add th√™m EC2. L√†m n√†o? l·∫°i add th√™m secondary IPv4 CIDR block √†?\n Ch·ªâ c√≥ th·ªÉ t·∫°o t·ªëi ƒëa 4 secondary IPv4 CIDR block th√¥i.\nTh·∫ø n√™n ph·∫£i x√≥a c√°i subnet n√†o ko s·ª≠ d·ª•ng ƒëi, ƒë·ªÉ c√≥ th·ªÉ t·∫°o subnet m·ªõi\n Q. B·∫°n d√πng API Gateway v√† Lambda, b·∫°n mu·ªën trace v√† analyze c√°c user request g·ª≠i ƒë·∫øn API Gateway th√¨ d√πng service n√†o c·ªßa AWS? VPC Flow Log hay Cloudtrail hay X-Ray?\n D√πng AWS X-Ray, n√≥ cho b·∫°n kh·∫£ nƒÉng debug v√† analyze vi·ªác tracing user request ƒë·ªÉ tim root cause v·ªÅ performances\nCloudtrail ch·ªâ d√πng cho API logging (ch·ª© ko ph·∫£i tracing),\nVPC Flow Log th√¨ ko t·ªët b·∫±ng X-Ray\n Q. B·∫°n ƒëang serve content t·ª´ S3 v√† d√πng CloudFront ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô deliver content ƒë·∫øn user to√†n c·∫ßu. B·∫°n mu·ªën improve service c·ªßa b·∫°n n√™n c·∫ßn l·∫•y report v·ªÅ h·ªá th·ªëng hi·ªán t·∫°i t·ª´ CloudFront th√¨ Cloudfront cung c·∫•p nh·ªØng lo·∫°i report nh∆∞ n√†o?\nCache Statistics Reports, Top Referrers Reports, Usage Reports, Popular Objects Report, Viewers Reports?\n Top Referrers Reports: List ra 25 website domain m√† t·∫°o ra HTTP/HTTPS request ƒë·∫øn object c·ªßa b·∫°n\nUsage Reports: S·ªë l∆∞·ª£ng v·ªÅ c√°c request HTTP/HTTPS m√† Cloudfront ph·∫£n h·ªìi t·ª´ edge location\nPopular Objects Report: Nh·ªØng object n√†o ƒë∆∞·ª£c access th∆∞·ªùng xuy√™n nh·∫•t\nViewers Reports: Xem location c·ªßa nh·ªØng user access v√†o xem content nhi·ªÅu nh·∫•t, v√† h·ªç ƒëang d√πng lo·∫°i browser n√†o\nCache Statistics Reports: Get d·ªØ li·ªáu c·ªßa c√°c request ƒë∆∞·ª£c nh√≥m l·∫°i b·ªüi HTTP status code\n Q. B·∫°n ƒëang d√πng RDS v√† mu·ªën monitoring v·ªÅ CPU v√† Memory c·ªßa c√°c process ch·∫°y trong RDS Instance. N√™n l√†m g√¨?\n Enable Enhanced Monitoring trong RDS\nch·ª© ko d√πng Cloudwatch, v√¨ Cloudwatch ch·ªâ l·∫•y metric v·ªÅ CPU utilization c·ªßa DB Instance th√¥i, c√≤n RDS Enhanced Monitoring th√¨ l·∫•y metric t·ª´ c√°i agent trong DB ƒë√≥\n\u0026mdash;P3-udemy\u0026mdash;  Q. Cty b·∫°n c√≥ 1 website v√† c√≥ 2 m√¥i tr∆∞·ªùng UAT v√† dev. B·∫°n deploy app l√™n 1 EC2 m1.small. Khi test nh·∫≠n th·∫•y l√† performance gi·∫£m xu·ªëng khi tƒÉng workload ·ªü m√¥i tr∆∞·ªùng UAT. L√†m n√†o? C√≥ n√™n enable Enhanced Networking ko?\n D√πng EC2 size to h∆°n m1.small, ch·ª© Enhanced Networking ko support EC2 m1.small (ch·ªâ support m4,m5,c5..etc th√¥i)\n Q. 1 cty ƒëang d√πng MongoDB v√† NGINX server. H·ªç mu·ªën 1 service c√≥ th·ªÉ cung c·∫•p 200,000 IOPS trong 4000 random I/O Read cho Database. Th√¨ n√™n d√πng lo·∫°i Volume n√†o? (io1? st1? sc1\u0026hellip;)\n Storage Optimized Instances\nStorage Optimized Instances ƒë∆∞·ª£c design cho workload l·ªõn, ƒë·ªçc v√† ghi (Read write) li√™n t·ª•c v√†o Dataset c·ª±c l·ªõn. V√≠ d·ª• i3.4xlarge c√≥ th·ªÉ cung c·∫•p 825,000 Random Read IOPS v√† 360,000 Write IOPS.\nch·ª© c√°c instance nh∆∞ Provisioned IOPS SSD (io1) EBS Volumes max c√≥ 64k IOPS th√¥i\n Q. B·∫°n ƒëang c√≥ 1 portal 20 EC2, chia 4 regions. 1 ALB dc setup tr√™n m·ªói region ƒë·ªÉ distribute traffic. L√†m n√†o ƒë·ªÉ duy tr√¨ availability n·∫øu 1 trong 4 regions b·ªã disconnect?\n Set Evaluate Target Health check = true\nv√† setup Latency Based Routing cho Route53, t·∫°o record set m√† s·∫Ω resolve ƒë·∫øn ALB tr√™n m·ªói regions\nV√≠ d·ª• nh∆∞ n√†y: B·∫°n c√≥ ELB ·ªü Oregon v√† Singapore, b·∫°n t·∫°o 1 latency record cho m·ªói ELB ƒë√≥. Khi 1 user ·ªü London truy c·∫≠p v√†o domain c·ªßa b·∫°n th√¨:\n1-DNS route c√°i query ƒë·∫øn R53\n2-R53 refer v·ªÅ ƒë·ªô tr·ªÖ gi·ªØa London ‚áî Singapore v√† London ‚áî Oregon\n3-N·∫øu ƒë·ªô tr·ªÖ gi·ªØa London ‚áî Oregon l√† th·∫•p h∆°n, th√¨ R53 s·∫Ω reponse c√°i IP c·ªßa ELB ·ªü Oregon\n Q. B·∫°n c√≥ v√†i S3 bucket gi·ªØ c√°c th√¥ng tin quan tr·ªçng. B·∫°n c√≥ 1 app (·ªü private subnet) c·∫ßn edit file trong S3 r·ªìi t·∫°o report g·ª≠i ƒë·∫øn c√¥ng ty partner qua internet public. B·∫°n ƒë√£ t·∫°o S3 VPC Enpoint r·ªìi c·∫ßn l√†m g√¨ n·ªØa?\n Update Route table c·ªßa Private subnet ƒë·ªÉ connect tr·ª±c ti·∫øp v·ªõi S3 VPC enpoint c√≤n send outbound traffic th√¨ qua NAT Gateway\n Q. Nh·ªØng service n√†o sau ƒë√¢y cung c·∫•p kh·∫£ nƒÉng backup automatic v√† rotation backup m√† user c√≥ th·ªÉ config? S3, EC2, RDS, Redshift, EFS?\n RDS v√† Redshift (Redshift default ƒë√£ b·∫≠t t√≠nh nƒÉng automated backup r·ªìi, v·ªõi 1 retention day. RDS th√¨ v·ªën ƒë√£ c√≥)\nS3 ko c√≥ backup policy, v·ªën n√≥ ƒë√£ r·∫•t durable v√† ko c·∫ßn option backup\nEC2 th√¨ mu·ªën backup th√¨ c·∫ßn l√†m tay (ho·∫∑c gi·ªù ƒë√£ c√≥ service AWS Backup g√¨ ƒë√≥, nh∆∞ng ƒëang gi·∫£ s·ª≠ ch∆∞a c√≥)\nEFS th√¨ v·ªën ƒë√£ HA v√† durable r·ªìi, ko c√≥ option backup\n Q. V·ªÅ Route53 health check. N·∫øu \u0026gt;18% Health checker report l√† healthy th√¨ R53 s·∫Ω consider n√≥ l√† healthy. V·∫≠y nh·ªØng HTTP/HTTPS status codes n√†o tr·∫£ v·ªÅ th√¨ R53 s·∫Ω nh·∫≠n l√† healthy?\n 2XX, 3XX\n Q. 1 Cty v·ªÅ data analytics s·∫Øp m·ªü 1 s·ªë service. B·∫°n c·∫ßn t·∫°o 1 Cloudformation template s·∫Ω init 1 s·ªë service quan tr·ªçng tr∆∞·ªõc, trong CF template ƒë√≥ c·∫ßn t·∫°o nh·ªØng g√¨?\n EC2, Redshift, EMR configuration\n(ch√∫ √Ω l√† c√¥ng ty v·ªÅ data analytics n√™n c·∫ßn Redshift, EMR\u0026hellip;)\n Q. B·∫°n l√† Database engineer, ƒëang c·∫ßn setup storage cho EC2 Server. M·ªói server c·∫ßn handle l∆∞·ª£ng l·ªõn DB workload. H·ªç c≈©ng c·∫ßn ph·∫£i secure v√† seprate ƒë·ªÉ keep c√°c file backup v√† logs. N√™n d√πng g√¨ l√†m root volume, d√πng g√¨ l√†m file system ƒë·ªÉ cost-effective?\n D√πng EBS-Provisioned IOPS SSD volume ƒë·ªÉ l√†m root volume (v√¨ n√≥ ƒë∆∞·ª£c optimized ƒë·ªÉ high disk read/write performance)\nD√πng Cold HDD volume ƒë·ªÉ l√†m file system (v√¨ n√≥ r·∫•t r·∫ª v√† great cho data access ko th∆∞·ªùng xuy√™n infrequently)\nKo d√πng EFS v√¨ ƒë·∫Øt, ko d√πng Throughput Optimized HDD v√¨ n√≥ t·∫≠p trung v√†o throughput h∆°n l√† IOPS n√™n c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn database performance\n Q. B·∫°n c√≥ subnet A c√≥ 2 EC2, gi·ªù t·∫°o th√™m subnet B. N·∫øu x√≥a subnet A th√¨ sao?\n S·∫Ω ko x√≥a dc, v√¨ ph·∫£i x√≥a h·∫øt EC2 trong ·∫•y ƒëi ƒë√£. C√≤n vi·ªác x√≥a route c·ªßa n√≥ th√¨ s·∫Ω t·ª± ƒë·ªông b·ªã x√≥a khi b·∫°n x√≥a th√†nh c√¥ng subnet\n Q. 1 Cty ƒëang c√≥ 1 platform host tr√™n EC2 sau ASG, v√† 1 CLB v·ªõi HTTPS listener. H·ªç y√™u c·∫ßu thay th·∫ø c√°i certificate hi·ªán t·∫°i trong load balancer b·∫±ng c√°i cert m·ªõi m√† ƒë√£ upload l√™n IAM. V√† c·∫ßn l√†m b·∫±ng CLI, l√†m n√†o?\n 1-aws iam get-server-certificate ƒë·ªÉ get ARN of the certificate.\n2-aws elb set-load-balancer-listener-ssl-certificate ƒë·ªÉ add cert m·ªõi v√†o LB\n Q. B·∫°n c√≥ 1 VPC v·ªõi CIDR block 10.0.0.0/16. B·∫°n c·∫ßn t·∫°o connect t·ª´ VPC t·ªõi data center. ƒê√£ c√≥ 1 public subnet CIDR (10.0.0.0/24) v√† 1 VPN-only subnet (10.0.1.0/24) c√πng v·ªõi VPN Gateway (vgw-51898). ƒê·ªÉ cho ph√©p traffic ra ngo√†i internet, b·∫°n ƒë√£ setup NAT instance (i-918273). Data center c√≥ CIDR 172.16.0.0/12. V·∫≠y config c·ªßa Main Route table v√† Custom Route table n√™n nh∆∞ n√†o?\n Custom Route table associate v·ªõi Public Subnet:    Destination Target     10.0.0.0/16 local   0.0.0.0/0 igw-id    Main Route table associate v·ªõi VPN-only Subnet:    Destination Target     10.0.0.0/16 local   0.0.0.0/0 vgw-id    Ch√∫ √Ω Destination l√† CIDR c·ªßa VPC ch·ª© ko ph·∫£i c·ªßa Subnet\n Q. V·ªÅ Cloudformation th√¨ c√≥ nh·ªØng c√°i anatomy n√†o l√† b·∫Øt bu·ªôc, c√°i n√†o l√† optional?\n Ch·ªâ c√≥ Resource l√† b·∫Øt bu·ªôc ph·∫£i c√≥. Ngay c·∫£ Format Version nh∆∞ AWSTemplateFormatVersion c≈©ng ch·ªâ l√† optional\n Q. Ph√≠a AWS ch·ªãu tr√°ch nhi·ªám g√¨ trong h·ªá th·ªëng infra c·ªßa b·∫°n?\n Ph√≠a AWS ch·ªãu tr√°ch nhi·ªám:\nQu·∫£n l√Ω Global infra c√°i run all your service\nPatch v√† update OS\nB·∫£o v·ªá Network traffic t·∫ßng abstract service nh∆∞ S3, Dynamo\nC√≤n ph√≠a b·∫°n c·∫ßn c√≥ tr√°ch nhi·ªám:\nClient side data encryption v√† data integrity authentication.\nServer side encryption (file system and data)\nB·∫£o v·ªá Network traffic c·ªßa b·∫°n\n Q. 1 Cty c√≥ nhi·ªÅu VPC tr√™n nhi·ªÅu regions. ƒê·ªÉ monitor h·ªá th·ªëng b·∫°n c·∫ßn l·∫•y aggregate CPU Utilization c·ªßa c√°c EC2 running tr√™n all VPC. L√†m n√†o?\n Setup Cloudwatch Dashboard, add 1 widget m·ªói region, l·∫•y aggregate CPU Utilization all EC2 trong m·ªói regions. (Ch·ª© ko th·ªÉ setup all regions dc)\nB·∫≠t Detailed Monitoring cho c√°c EC2 (B·∫Øt bu·ªôc ph·∫£i b·∫≠t c√°i n√†y v√¨ Cloudwatch ch·ªâ l·∫•y aggregate dc c·ªßa c√°c EC2 ƒë√£ b·∫≠t c√°i n√†y th√¥i)\n Q. 1 App mu·ªën user c·ªßa h·ªç c√≥ th·ªÉ send pic, videos l√™n. Login b·∫±ng Social media account v√† app s·∫Ω l∆∞u detail v√†o DynamoDB. C·∫ßn d√πng nh·ªØng service g√¨?\n D√πng AWS Cognito v√† AWS IAM Roles\nVi·ªác d√πng Federated Access l√† sai\n Q. B·∫°n c√≥ 1 h·ªá th·ªëng ASG c√°c EC2 ch·∫°y tr√™n c√°c AZ, v√† 1 CLB. B·∫°n nh·∫≠n th·∫•y c√≥ nhi·ªÅu l·ªói 4xx. Gi·ªù n√™n enable VPC Flow Logs hay Load Balancer access log ƒë·ªÉ l·∫•y Client IP v√† request path?\n N√™n d√πng enable access log c·ªßa Load Balancer, ch·ª© ko c·∫ßn l·∫•y IP address c·ªßa to√†n b·ªô traffic ra v√†o VPC\n Q. N·∫øu Instance b·ªã l·ªói Insufficient Instance Capacity th√¨ c√≥ th·ªÉ root cause l√† g√¨?\n Do hi·ªán t·∫°i AWS Ko c√≥ ƒë·ªß available capacity v·ªÅ s·ªë l∆∞·ª£ng instance On-demands\n Q. B·∫°n c√≥ 1 S3 bucket l∆∞u c√°c th√¥ng tin c·ªßa user. Bucket c·∫ßn dc monitor ƒë·ªÉ secure. Nhi·ªám v·ª• c·ªßa b·∫°n l√† automate the assessment of your resource configurations and resource changes, th√¨ c·∫ßn l√†m g√¨? Config bucket policy d√πng IAM role? Hay d√πng policy deny unauthored User? hay AWS Config?\n D√πng AWS Config, v√¨ y√™u c·∫ßu l√† automate vi·ªác ƒë√°nh gi√°/theo d√µi resource thay ƒë·ªïi\n Q. Gi·ªØa AWS Shield Advanced v√† AWS WAF kh√°c nhau nh∆∞ n√†o?\n AWS Shield Advanced l√† c√°i ƒë·∫Øt ti·ªÅn t√°c d·ª•ng ch·ªëng DDoS attacks\nch·ª© WAF ch·ªâ c√≥ th·ªÉ ch·∫∑n c√°c common attack nh∆∞ SQL injection, cross-site scripting th√¥i, ko ch·ªëng dc DDoS attack\n Q. B·∫°n ƒë∆∞·ª£c y√™u c·∫ßu assess (theo d√µi) behaivor c·ªßa c√°c resource nh∆∞ EC2, ƒë·ªÉ x√°c ƒë·ªãnh nh∆∞ng nguy hi·ªÉm v·ªÅ security. B·∫°n c·∫ßn b·∫£n report v·ªÅ c√°c activity di·ªÖn ra trong EC2 v√≠ d·ª• nh∆∞ detail v·ªÅ s·ª± giao ti·∫øp v·ªõi c√°c service kh√°c. Service n√†o s·∫Ω gi√∫p b·∫°n l√†m ƒëi·ªÅu n√†y? Trusted Advisor hay Inspector?\n AWS Inspector\nN√≥i chung c·ª© Security Assessment, Network Assessment th√¨ ph·∫£i l√† Inspector\n Q. 1 website cung c·∫•p livescore, tin t·ª©c, h√¨nh ·∫£nh th·ªÉ thao. ƒêang ƒëc d·ª±ng tr√™n h·ªá th·ªëng ASG multi-AZ, d√πng ALB (Application Load Balancer). G·∫ßn ƒë√¢y v√¨ Worldcup n√™n traffic tƒÉng l√™n, ng∆∞·ªùi d√πng b√°o c√°o l√† load ch·∫≠m. Gi·∫£i ph√°p n√†o ƒë·ªÉ scale v√† c·∫£i thi·ªán load time c·ªßa website? Cloudfront? hay NLB? hay b·∫≠t Enhaced Networking?\n D√πng Cloudfront v√¨ n√≥ cung c·∫•p kh·∫£ nƒÉng deliver content ƒë·∫øn user 1 c√°ch low latency, t·ªëc ƒë·ªô transfer cao.\nKo ph·∫£i NLB v√¨ n√≥ ch·ªß y·∫øu d√πng ƒë·ªÉ load balancing cho TCP traffic, handle h√†ng tri·ªáu request m·ªói gi√¢y v·ªõi ƒë·ªô tr·ªÖ c·ª±c th·∫•p, tuy c√≥ th·ªÉ tƒÉng performance nh∆∞ng n√≥ ko ph·∫£i gi·∫£i ph√°p ƒë·ªÉ tƒÉng th·ªùi gian load time c·ªßa website tr√™n to√†n th·∫ø gi·ªõi (respone time around globe)\nKo ph·∫£i Enhanced Networking v√¨ n√≥ ch·ªß y·∫øu ƒë·ªÉ cho ph√©p high-performance networking ƒë·ªëi v·ªõi 1 s·ªë lo·∫°i EC2 c·ª• th·ªÉ m·ªõi support\n Q. So s√°nh 4 c√°i sau: Cost Optimization Monitor? TCO calculator? Simple Monthly Calculator? AWS Pricing page?\n 1-Cost Optimization Monitor: report cho v·ªÅ chi ph√≠ operation chi ti·∫øt. Generate report v√† cung c·∫•p insight v·ªÅ service usage v√† cost\n2-TCO calculator: d√πng ƒë·ªÉ compare cost c·ªßa on-premise ho·∫∑c m√¥i tr∆∞·ªùng hosting truy·ªÅn th·ªëng so v·ªõi AWS\n3-Simple Monthly Calculator: d√πng ƒë·ªÉ estimate billing h√†ng th√°ng. Nh∆∞ng ko generate report 1 c√°ch chi ti·∫øt nh∆∞ c√°i Cost Optimization Monitor. V√† n√≥ c≈©ng ko monitor cost hi·ªán t·∫°i m√¨nh ƒëang ch·ªãu\n4-AWS Pricing page: ch·ªâ show ra th√¥i, ch·ª© ko gen ra report cho b·∫°n.\n Q. Nh·ªØng task m√† b·∫°n c√≥ th·ªÉ perform b·∫±ng s·ª± t·ª± ƒë·ªông c·ªßa System Manager cho EC2 c·ªßa b·∫°n?\n Build Automation workflow ƒë·ªÉ config v√† qu·∫£n l√Ω EC2\nT·∫°o custom workflow ho·∫∑c s·ª≠ d·ª•ng c√°i workflow m√† AWS ƒë√£ defined s·∫µn\nNh·∫≠n noti v·ªÅ Automation Workflow/Task b·∫±ng Cloudwatch Event\nMonitor qu√° tr√¨nh Automation v√† chi ti·∫øt v·ªÅ qu√° tr√¨nh execute\nN√≥i chung c√≥ ch·ªØ Automation ho·∫∑c workflow th√¨ l√† task c·ªßa SSM\n Q. B·∫°n mu·ªën monitor system infra. B·∫°n ƒë√£ c√≥ 1 custom shell script ƒë·ªÉ check memory ultilization c·ªßa EC2 r·ªìi. Gi·ªù b·∫°n mu·ªën dc notified m·ªói khi memory breach threshold th√¨ l√†m n√†o?\n C√†i Cloudwatch agent tr√™n EC2 n√≥ s·∫Ω ƒë·ªçc c√°i custom metric script m√† b·∫°n ƒë√£ chu·∫©n b·ªã\nSetup 1 c√°i CW alarm base tr√™n c√°i custom metric\nset SNS topic ƒë·ªÉ nh·∫≠n noti\n Q. CloudWatch alarms c√≥ th·ªÉ c√≥ nh·ªØng tr·∫°ng th√°i n√†o?\n OK\nALARM (metric outside kh·ªèi threshold ƒë√£ defined)\nINSUFFICIENT_DATA (alarm v·ª´a start, ho·∫∑c metric not available, ho·∫∑c ko ƒë·ªß data cho metric) ko c√≥ tr·∫°ng th√°i Pending ƒë√¢u nh√©\n Q. V·ªÅ AWS Risk and Compliance Whitepaper?\n Kh√°ch h√†ng dc t√πy ch·ªçn data c·ªßa h·ªç ·ªü trong region v·∫≠t l√Ω n√†o, AWS s·∫Ω ko di chuy·ªÉn m√† ko th√¥ng b√°o v·ªõi user, tr·ª´ khi c√≥ y√™u c·∫ßu c·ªßa ch√≠nh quy·ªÅn\nAWS ngƒÉn ch·∫∑n vi·ªác x√¢m nh·∫≠p v√†o host v·∫≠t l√Ω ho·∫∑c ·∫£o h√≥a m√† user ko c√≥ quy·ªÅn s·ªü h·ªØu, ko dc assign\nKh√°ch h√†ng retain quy·ªÅn v√† s·ª± s·ªü h·ªØu v·ªÅ data c·ªßa h·ªç (AWS ko can thi·ªáp)\nCh√∫ √Ω l√† AWS ko cho b√™n third-party n√†o deliver service c·ªßa AWS ƒë·∫øn KH\nAWS ko cho ph√©p c√°c auditor l√†m 1 tour v√†o data center ƒë·ªÉ validate h·ªá th·ªëng ƒë√¢u nh√©\nAWS ko notify khi h·ªá th·ªëng c·ªßa h·ªç c·∫ßn bring offline, b·ªüi v√¨ vi·ªác maintain v√† patch h·ªá th·ªëng s·∫Ω ko ·∫£nh h∆∞·ªüng ƒë·∫øn KH\n Q. 1 cty ƒëang host web app e-commerce v√† blog fetch data t·ª´ database. 1 s·ªë b√†i vi·∫øt (static web page) c√≥ nhi·ªÅu truy c·∫≠p l√†m app ho·∫°t ƒë·ªông ch·∫≠m. Ph·∫£i l√†m sao ƒë·ªÉ gi·∫£m load time nh∆∞ v·∫≠y? ElastiCache? hay S3? hay Read replica cho DB?\n D√πng S3 ƒë·ªÉ host c√°c web pages\nVi·ªác d√πng ElastiCache ch·ªâ ƒë·ªÉ caching content dc fetch th∆∞·ªùng xuy√™n t·ª´ DB\nRead replica ch·ªâ ƒë·ªÉ scaling nh·ªØng web m√† n·∫∑ng v·ªÅ ƒë·ªçc\n Q. B·∫°n qu·∫£n l√Ω 1 h·ªá th·ªëng batch ch·∫°y tr√™n EC2 v√† CLB. EC2 deploy tr√™n 3 AZ us-west-2a, 2b, 2c. B·∫°n nh·∫≠n th·∫•y EC2 trong 2b th·ª±c hi·ªán c√°c request r·∫•t ch·∫≠m. C·∫ßn monitor metric n√†o ƒë·ªÉ bi·∫øt dc t·ªïng s·ªë request ho·∫∑c connection ƒëang pending tr∆∞·ªõc khi dc route t·ªõi 1 EC2 kh·ªèe m·∫°nh healthy? SurgeQueueLength hay RequestCount hay BackendConnectionErrors hay SpilloverCount ?\n d√πng SurgeQueueLength: t·ªïng s·ªë request (HTTP listener) ho·∫∑c connection (TCP listener) ƒëang pending. Max c·ªßa queue l√† 1024. N·∫øu queue b·ªã full th√¨ request s·∫Ω b·ªã reject. S·ªë request b·ªã reject ƒë√≥ s·∫Ω hi·ªÉn th·ªã ·ªü metric SpilloverCount\nRequestCount: l√† s·ªë request completed trong 1 kho·∫£ng time nh·∫•t ƒë·ªãnh (1 or 5 ph√∫t)\nBackendConnectionErrors: s·ªë l∆∞·ª£ng connection fail khi connect gi·ªØa LB v√† Instances\n Q. B·∫°n ping 1 EC2 t·ª´ m√°y c·ªßa b·∫°n, nh∆∞ng ko nh·∫≠n dc respone, b·∫°n check VPC flow log th√¨ nh∆∞ sau:\n2 123456789010 eni-1235b8ca 110.237.99.166 172.31.17.140 0 0 1 4 336 1432917027 1432917142 ACCEPT OK\n2 123456789010 eni-1235b8ca 172.31.17.140 110.237.99.166 0 0 1 4 336 1432917094 1432917142 REJECT OK V√¨ sao?\n V√¨ Network ACL ƒëang ko cho ph√©p outbound ICMP traffic (v√¨ NACL l√† stateless)\nch·ª© c√≤n n·∫øu l√† SG th√¨ b·ªüi SG l√† stateful n√™n n·∫øu inbound traffic dc cho ph√©p th√¨ outbound c≈©ng s·∫Ω ƒë∆∞·ª£c cho ph√©p (ngay c·∫£ khi rule trong SG c·ªßa b·∫°n ch·∫∑n outbound traffic) üòÇ\n\u0026mdash;P4-udemy\u0026mdash;  Q. App c·ªßa b·∫°n l√† app share ·∫£nh, deploy tr√™n 1 group c√°c EC2. Team dev mu·ªën nhanh ch√≥ng push 1 b·∫£n update cho app, v√† b·∫£n update ƒë√≥ s·∫Ω ƒë∆∞·ª£c download b·ªüi user tr√™n global t·ª´ b√™n trong mobile app. L√†m n√†o ƒë·ªÉ ng d√πng c√≥ tr·∫£i nghi·ªám t·ªët.\n L∆∞u file tr√™n S3, Config sao cho Cloudfront s·ª≠ d·ª•ng S3 bucket nh∆∞ l√† 1 Origin.\nkh√¥ng th·ªÉ upload th·∫≥ng file update ƒë√≥ l√™n Cloudfront\n Q. App c·ªßa b·∫°n c√≥ nhi·ªÅu user, User tƒÉng l√™n n√™n c·∫ßn tƒÉng Storage l√™n. B·∫£n Backup c·∫ßn ƒë·ªÉ ·ªü ch·ªó n√†o secure v√† durable. B·∫°n mu·ªën d√πng AWS v√¨ n√≥ unlimited. Tuy nhi√™n b·∫°n v·∫´n mu·ªën gi·ªØ dc s·ª± access th∆∞·ªùng xuy√™n v√†o data. L√†m n√†o?\n D√πng Storage Gateway v√¨ n√≥ cho b·∫°n tr·∫£i nghi·ªám li·ªÅn m·∫°ch (seamlessly) trong vi·ªác integrate app v√† AWS block object storage\n Q. L√†m sao ƒë·ªÉ d√πng 1 Cloudformation Template ƒë·ªÉ t·∫°o stack tr√™n nhi·ªÅu AWS account?\n D√πng Cloudformation StackSet\n Q. Khi b·∫°n enable MFA Delete cho 1 S3 bucket, th√¨ s·∫Ω nh∆∞ n√†o?\n MFA Delete c·∫ßn enable c√πng v·ªõi Bucket Versioning\nPh·∫£i enable b·∫±ng CLI, b·∫±ng root account\nFeature n√†y ch·ªâ d√†nh cho root account. Ch·ªâ root account m·ªõi c√≥ th·ªÉ x√≥a Object trong S3 bucket ƒë√≥\n1 IAM User n·∫øu c√≥ cung c·∫•p MFA th√¨ c≈©ng ko th·ªÉ x√≥a dc object trong bucket ƒë√≥.\nhttps://www.cloudmantra.net/blog/how-to-enable-mfa-delete-for-s3-bucket/\n Q. Command n√†o ƒë·ªÉ put custom metric l√™n Cloudwatch?\n put-metric-data command CLI\n Q. C√°i helper script n√†o c·ªßa Cloudformation gi√∫p b·∫°n retrieve v√† interpret metadata, install package, t·∫°o file, start service?\n cfn-init\nko ph·∫£i cfn-signal v√¨ n√≥ ch·ªâ gi√∫p b√°o WaitCondition ch·ªù sync v·ªõi c√°c resource kh√°c c·∫£u stack khi app ready\nko ph·∫£i cfn-get-metadata v√¨ n√≥ ch·ªâ d√πng ƒë·ªÉ get metadata th√¥i\n Q. B·∫°n t·∫°o 1 Chef Server trong AWS Opswork, v·ªõi 3 node running. B·∫°n c·∫ßn ph·∫£i add nhi·ªÅu h∆°n, nh∆∞ng th·∫•y l√† m·ªói l·∫ßn add m·∫•t th·ªùi gian qu√°. C√≥ c√°ch n√†o t·ª± ƒë·ªông t·∫°o EC2 Node v√†o Chef Server ko?\n C√≥, d√πng Chef Client Cookbook\n Q. B·∫°n mu·ªën s·ª≠ d·ª•ng Pupet Enterpise, AWS c√≥ service n√†o l√†m c√°c nhi·ªám v·ª• nh∆∞ operation, backup, restore, upgrade software cho Pupet Enterprise ko?\n C√≥, AWS Opswork\n Q. B·∫°n mu·ªën secure cho data c·ªßa m√¨nh, 1 service c·ªßa AWS m√† d√πng FIPS 140-2 compliant, v√† n·∫±m trong VPC c·ªßa b·∫°n th√¨ d√πng c√°i g√¨? CloudHSM hay KMS ?\n CloudHSM v√¨ n√≥ n·∫±m trong VPC c·ªßa b·∫°n, do b·∫°n qu·∫£n l√Ω, b·∫°n t·ª± generate ra key v√† d√πng key ƒë√≥ trong AWS\nch·ª© KMS l√† Service n·∫±m ngo√†i VPC c·ªßa b·∫°n\n Q. V·ªÅ RDS th√¨ Cloudwatch c√≥ nh·ªØng metric nh∆∞ sau, ch√∫ng n√≥ √Ω nghƒ©a g√¨? FreeStorageSpace, BinLogDiskUsage, FreeableMemory, DiskQueueDepth?\n FreeStorageSpace: monitor s·ªë space c√≤n th·ª´a trong RDS instance,\nBinLogDiskUsage: monitor dung l∆∞·ª£ng ƒë√£ d√πng (ch·ªâ apply cho MySQL read replicas),\nFreeableMemory: s·ªë RAM c√≤n tr·ªëng,\nDiskQueueDepth: s·ªë I/O (read/write) request ƒëang ch·ªù access v√†o disk\n Q. B·∫°n c·∫ßn quick update RDS, launch th√™m v√†i EC2, new ALB v√†o c√°i Cloudformation Stack hi·ªán t·∫°i. B·∫°n mu·ªën c√≥ th·ªÉ preview nh·ªØng c√°i thay ƒë·ªïi trong stack m·ªõi, ch·∫Øc ch·∫Øn l√† new infra s·∫Ω dc provision ƒë√∫ng th√¨ l√†m n√†o?\n Cloudformation Change Sets (n√≥ s·∫Ω list ra nh·ªØng thay ƒë·ªïi cho b·∫°n review tr∆∞·ªõc khi th·ª±c s·ª± execute)\n Q. B·∫°n mu·ªën t·ª± ƒë·ªông x√≥a nh·ªØng EBS snapshot khi ƒë·ªß maximum snapshot th√¨ l√†m n√†o?\n D√πng Amazon Data Lifecycle Manager\n Q. Cross-zone Load balancing l√† nh∆∞ n√†o?\n xem h√¨nh sau:\nN·∫øu disable Cross-zone Load balancing th√¨ traffic t·ª´ client v√†o s·∫Ω chia ƒë·ªÅu 2 LB, m·ªói LB 50% (chia theo Route53 define), d·∫´n ƒë·∫øn m·ªói EC2 trong Zone A s·∫Ω l√† 25%, c√≤n m·ªói EC2 trong Zone B l√† 50/8 (%) -\u0026gt; ko ƒë·ªÅu.\nN·∫øu enable Cross-zone Load balancing th√¨ traffic s·∫Ω chia ƒë·ªÅu cho c√°c EC2, m·ªói c√°i 10%\n Q. B·∫°n mu·ªën m·ªói khi snapshot ƒëc t·∫°o ra t·ª´ EC2, ch√∫ng s·∫Ω dc copy qua region kh√°c ƒë·ªÉ ƒë·∫£m b·∫£o durability, l√†m n√†o?\n Cloudwatch Event cho EBS, r·ªìi trigger Lambda ƒë·ªÉ copy qua region kh√°c\n Q. RDS Multi-AZ ho·∫°t ƒë·ªông nh∆∞ n√†o?\n Con Primary Instance c·ªßa b·∫°n s·∫Ω dc synchonize replicate data sang con DB Instance standby ·ªü Zone kh√°c. 2 con n√†y ch·ªâ sync v·ªÅ Data, c√≤n infra c·ªßa ch√∫ng ƒë·ªôc l·∫≠p v·ªõi nhau.\n Q. V·ªÅ Cloudfront, h·ªá th·ªëng c·ªßa b·∫°n cung c·∫•p ·∫£nh, tin t·ª©c. C√≥ writer post 1 b·ª©c ·∫£nh fake v√† ƒë√£ b·ªã cache l·∫°i tr√™n Cloudfront, v√† c·∫ßn ph·∫£i ƒëc x√≥a ngay l·∫≠p t·ª©c (ko th·ªÉ ƒë·ª£i cache expire, m√† ph·∫£i lu√¥n v√† ngay), l√†m n√†o?\n ‚ñ† C√°ch 1: Invalidate file ƒë√≥ ·ªü edge cache b·∫±ng CLI command aws cloudfront create-invalidation, th√¨ t·ª´ l·∫ßn sau viewer v√†o request, Cloudfront s·∫Ω g·ª≠i ƒë·∫øn origin ƒë·ªÉ fetch file m·ªõi nh·∫•t\n‚ñ† C√°ch 2: D√πng file versioning ƒë·ªÉ serve 1 version kh√°c c·ªßa file ƒë√≥\nCh√∫ √Ω c√°ch invalidate file th√¨ ko th·ªÉ invalidate file media ƒë·ªãnh d·∫°ng Microsoft Smooth Streaming, ko th·ªÉ invalidate 1 file ƒë√£ dc serve b·ªüi RTMP (Real-Time Messaging Protocol)\n Q. B·∫°n c√≥ S3 bucket v√† mu·ªën secure data. Ch·ªâ nh·ªØng user c√≥ MFA th√¨ m·ªõi ƒëc v√†o access content bucket ƒë√≥. L√†m n√†o?\n COnfig bucket policy ch·ªâ cho ph√©p access n·∫øu User ƒë√£ authen b·∫±ng MFA\nEnsure MFA dc b·∫≠t cho c√°c User\n Q. 1 Cty d√πng DynamoDB, b·∫°n c·∫ßn ensure l√† backup c·∫ßn c√≥ s·∫µn available, l√†m n√†o?\n B·∫≠t ch·ª©c nƒÉng On-demands backup for DynamoDB tables (N√≥ cho ph√©p b·∫°n backup for long-term, b·∫°n c√≥ th·ªÉ backup v√† restore b·∫•t c·ª© l√∫c n√†o ch·ªâ v·ªõi v√†i click chu·ªôt, vi·ªác backup v√† restore s·∫Ω ko ·∫£nh h∆∞·ªüng ƒë·∫øn perfomance hay t√≠nh availability c·ªßa table)\n Q. 1 Cty mu·ªën b·∫°n manage h·ªá th·ªëng infa c·ªßa h·ªç tr√™n c·∫£ cloud v√† on-premise. H·ªç mu·ªën h·ªá th·ªëng backup v√† archive ·ªïn ƒë·ªãnh tr√™n c·∫£ on-premise v√† cloud cho Document c·ªßa h·ªç. Nh·ªØng document ph·∫£i lu√¥n ƒëc accessible ngay l·∫≠p t·ª©c trong 6 th√°ng, v√† sau 6 th√°ng th√¨ s·∫Ω dc archive cho 10 nƒÉm. D√πng g√¨?\n Storage Gateway v·ªõi File gateway connect ƒë·∫øn onpremise data center. D√πng lifecycle ƒë·ªÉ move sang Glacier\nKo d√πng Tape gateway v√¨ n√≥ l∆∞u data tr√™n Glacier n√™n ko th·ªÉ retrive data ngay l·∫≠p t·ª©c ƒë∆∞·ª£c nh∆∞ File gateway\nAWS Storage Gateway cung c·∫•p, File-based, Volume-based, Tape-based, th√¨:\nFile Gateway ch√≠nh l√† File-based,\nTape Gateway ch√≠nh l√† Tape-based.\n\u0026mdash;P5-udemy\u0026mdash;  Q. 1 Cty c√≥ nhi·ªÅu AWS account trong 1 Org. B·∫°n c·∫ßn ensure l√† c√°c tag c·∫ßn dc apply nh·∫•t qu√°n khi t·∫°o resource trong c√°c account. L√†m n√†o?\n D√πng Cloudformation Resource Tags\nD√πng AWS Service Catalogs\n Q. Cty b·∫°n qu·∫£n l√Ω nhi·ªÅu AWS resource, gi·ªù mu·ªën optimize cost c·ªßa c√°c resource th√¨ c·∫ßn d√πng c√°i g√¨? AWS Inspector? hay AWS Cost \u0026amp; Usage report?\n D√πng AWS Cost \u0026amp; Usage report\nV√¨ Inspector ch·ªâ ƒë·ªÉ check xem server c√≥ b·ªã vulnerabilities hay ko th√¥i\n Q. Nh·ªØng limit c·ªßa RDS encrpyted Database Instance l√† g√¨?\n Ch·ªâ c√≥ th·ªÉ enable encryption ngay khi t·∫°o DB, ch·ª© ko th·ªÉ sau khi DB ƒë√£ ƒë∆∞·ª£c t·∫°o\nDB ƒë√£ b·∫≠t encryption th√¨ ko th·ªÉ disable ƒëi ƒë∆∞·ª£c\nB·∫°n ko th·ªÉ c√≥ 1 b·∫£n Read Replica unencrypted c·ªßa 1 DB encrypted\nB·∫°n ko th·ªÉ c√≥ 1 b·∫£n Read Replica encrypted c·ªßa 1 DB unencrypted\nRead Replica v·ªõi DB g·ªëc c·∫ßn dc encrypt b·∫±ng c√πng 1 key\nB·∫°n ko th·ªÉ restore 1 b·∫£n backup/snapshot unencrypted v√†o 1 DB Instance ƒë√£ ƒë∆∞·ª£c encrypted\nKhi copy encrpyted snapshot t·ª´ 1 region sang regions kh√°c, b·∫°n c·∫ßn ch·ªâ ƒë·ªãnh key KMS v√¨ KMS ra specific cho m·ªói region b·∫°n t·∫°o DB\n Q. 1 Cty c√≥ 1 app host tr√™n EC2. Gi·ªù c·∫ßn 1 high throughput data storage cho qu·∫£n l√Ω content v√† c√≥ th·ªÉ cung c·∫•p parallel share access v√†o server c·ªßa h·ªç, th√¨ s·∫Ω d√πng g√¨? EBS Volume Throughput Optimize hay EFS hay S3?\n D√πng EFS\nV√¨ EBS ch·ªß y·∫øu d√πng cho local block level storage, m·∫∑c d√π c√≥ th·ªÉ cung c·∫•p high throughput nh∆∞ng ko th·ªÉ cung c·∫•p kh·∫£ nƒÉng parallel access v√†o multiple instances ƒë∆∞·ª£c\n Q. Cty b·∫°n c·∫ßn migrate app l√™n AWS. C·∫ßn infa c√≥ th·ªÉ HA v√† ch·ªëng dc cross-site scrpting, SQL injection, HTTP flood attacks. Th√¨ d√πng WAF hay Shield Advanced? d√πng Network Load Balancer hay Application Load Balancer?\n WAF v√† ALB (v√¨ WAF l√† ƒë·ªß ƒë·ªÉ ch·ªëng nh·ªØng c√°i tr√™n r·ªìi, bao gi·ªù c·∫ßn ch·ªëng DDos th√¨ m·ªõi c·∫ßn Shield. WAF th√¨ n√≥ ko th·ªÉ integrate v·ªõi NLB ƒë∆∞·ª£c, n√™n m·ªõi ch·ªçn ALB)\n Q. B·∫°n c√≥ 1 s·ªë EC2 trong ASG v√† 1 v√†i h√†m Lambda. M·ªói khi b·∫°n release 1 version code m·ªõi, x·∫£y ra s·ª± ko ·ªïn ƒë·ªãnh v√¨ b·∫°n ko th·ªÉ manually update all resource li√™n quan ƒë√∫ng c√°ch. V√¨ s·ªë l∆∞·ª£ng l·ªõn resource, b·∫°n c·∫ßn 1 c√°ch ƒë·ªÉ nh√≥m ch√∫ng l·∫°i v√† deploy new version c·ªßa code 1 c√°ch ·ªïn ƒë·ªãnh cho c√°i nh√≥m ƒë√≥ v·ªõi downtime b√© th√¥i. L√†m n√†o?\n D√πng deployment group trong AWS Code Deploy ƒë·ªÉ automate vi·ªác deploy code\n Q. Cloudwatch agent c√≥ th·ªÉ cung c·∫•p th√¥ng tin v·ªÅ vulnerabilities v√† errors trong system c·ªßa b·∫°n ko?\n KO, mu·ªën c√°i ƒë√≥ th√¨ d√πng Inspector\n Q. Data Lifecycle Manager cung c·∫•p 1 c√°ch ƒë∆°n gi·∫£n, t·ª± ƒë·ªông ƒë·ªÉ backup data tr√™n EBS volume c·ªßa b·∫°n. V·∫≠y n√≥ c√≥ kh·∫£ nƒÉng t·∫°o snapshot r·ªìi t·ª± ƒë·ªông copy sang region kh√°c hay ko?\n Kh√¥ng nh√©. B·∫°n ph·∫£i t·ª± l√†m vi·ªác move sang region kh√°c ƒë√≥\n Q. App c·ªßa b·∫°n d√πng ELB Clasic Load Balancer. B·∫°n mu·ªën enable ticky session ƒë·ªÉ CLB s·∫Ω bind c√°i user session v·ªõi 1 instance c·ª• th·ªÉ. Khi ƒë√≥ t√™n c·ªßa cookie m√† CLB s·∫Ω t·∫°o ƒë·ªÉ map v·ªõi user session l√† g√¨?\n AWSELB\nN·∫øu App c·ªßa b·∫°n c√≥ session cookie s·∫µn r·ªìi, b·∫°n c√≥ th·ªÉ config ELB ƒë·ªÉ c√°i session cookie c·ªßa ELB follow c√°i duration c·ªßa App\u0026rsquo;s session cookie.\nN·∫øu App c·ªßa b·∫°n ko c√≥ session cookie, b·∫°n c√≥ th·ªÉ config ELB ƒë·ªÉ n√≥ t·∫°o ra session cookie v√† ch·ªâ ƒë·ªãnh duration b·∫°n mu·ªën. Khi ELB t·∫°o cookie s·∫Ω c√≥ t√™n l√† AWSELB d√πng ƒë·ªÉ map c√°i session v√†o Instance\n Q. Khi b·∫°n delete CMK (Customer Master Key) c·ªßa KMS, th√¨ c·∫ßn ch√∫ √Ω g√¨? C√≥ th·ªÉ x√≥a ngay l·∫≠p t·ª©c ko? th·ªùi gian default?\n B·∫°n kh√¥ng th·ªÉ x√≥a CMK ngay l·∫≠p t·ª©c, b·∫°n ph·∫£i l√™n schedule cho n√≥\nTh·ªùi gian schedule ƒë·ªÉ x√≥a CMK default l√† 30 ng√†y (30 days)\nKMS s·∫Ω ko th·ª±c hi·ªán rotate v·ªõi nh∆∞ng CMK ƒëang tr·ªçng tr·∫°ng th√°i pending deletion\n1 CMK pending deletion ko th·ªÉ s·ª≠ d·ª•ng tron b·∫•t c·ª© ho·∫°t ƒë·ªông cryptographic n√†o\n Q. App c·ªßa b·∫°n d·ª±ng tr√™n EC2-Clastic Instance v√† trong 1 public subnet. Gi·ªù mu·ªën list c√°c IP truy c·∫≠p v√†o EC2 n√™n l√†m n√†o?\n D√πng VPC Flow log, nh∆∞ng tr∆∞·ªõc h·∫øt ph·∫£i migrate App sang EC2-VPC, ch·ª© lo·∫°i EC2-Clasic ko d√πng VPC Flow log ƒë∆∞·ª£c\n Q. Cloudformation c√≥ c√°i DeletionPolicy, c√°i n√†y c√≥ nh·ªØng option g√¨?\n Retain: gi·ªØ l·∫°i resource khi Stack b·ªã x√≥a\nDelete: delete all (default)\nSnapshot: t·∫°o snapshot tr∆∞·ªõc khi x√≥a\n Q. Default khi t·∫°o VPC th√¨ qua CLI/Console/VPC Wizard, nh·ªØng c√°i sau dc set true hay false: enableDnsHostnames v√† enableDnsSupport?\n Khi t·∫°o b·∫±ng VPC Wizard: c·∫£ 2 set true\nKhi t·∫°o b·∫±ng Console, CLI: ch·ªâ c√≥ enableDnsSupport=true\n Q. B·∫°n c·∫ßn setup 1 infra ƒë·ªÉ c√≥ th·ªÉ t·ª± ƒë·ªông provision ra h√†ng ng√†n EC2 at any scale, d√πng service g√¨?\n EMR (Elastic MapReduce), n√≥ gi√∫p b·∫°n d·ªÖ d√†ng tƒÉng gi·∫£m s·ªë l∆∞·ª£ng ec2 c√≥ th·ªÉ access v√† qu·∫£n l√Ω.\n Q. B·∫°n c√≥ 1 bucket S3, mu·ªën setting ƒë·ªÉ ch·ªâ c√≥ th·ªÉ ƒë·ªçc v√† vi·∫øt ch·ª© ko th·ªÉ x√≥a object th√¨ l√†m n√†o? MFA Delete hay attach S3 bucket policy?\n Attach 1 S3 bucket policy\nko ch·ªçn MFA delete v√¨ n√≥ d√πng ƒë·ªÉ ch·∫∑n tr∆∞·ªùng h·ª£p x√≥a 1 c√°ch v√¥ √Ω th√¥i, n·∫øu c√≥ quy·ªÅn v√† MFA code v·∫´n c√≥ th·ªÉ x√≥a dc\n Q. 1 Cty ƒëang consider d√πng RDS, khi d√πng RDS th√¨ nh·ªØng activities g√¨ s·∫Ω do AWS perform, ng d√πng ko c·∫ßn quan t√¢m?\n ‚ñ† create v√† maintain backup v·ªõi point in time recovery 5 minutes\n‚ñ† install c√°c b·∫£n v√° b·∫£o m·∫≠t (security patch)\nCh·ª© c√≤n nh·ªØng th∆∞ nh∆∞ config Multi-AZ t·ª± ƒë·ªông, t·∫°o Read Replica t·ª± ƒë·ªông khi utilization cao th√¨ AWS ko perform, m√† ph√≠a ng∆∞·ªùi d√πng ph·∫£i config/enable\n Q. 1 Game online ƒëang host tr√™n 1 EC2. H·ªç chu·∫©n b·ªã release 1 b·∫£n game m·ªõi, c·∫ßn ch·∫Øc ch·∫Øn l√† website c·ªßa h·ªç ƒë·ªß kh·∫£ nƒÉng support cho new players. C·∫ßn l√†m g√¨?\n T·∫°o 1 ASG t·ª´ c√°i EC2 ƒëang ch·∫°y\nC√≥ 2 ki·ªÉu t·∫°o ASG Launch Configuration t·ª´ scratch, v√† t·∫°o ASG Launch configuration t·ª´ exist EC2\n Q. B·∫°n c·∫ßn report v·ªÅ t√¨nh tr·∫°ng OS patching process c·ªßa RDS, xem c√≥ c√°i g√¨ vulnerability ko, th√¨ c√≥ c·∫ßn d√πng AWS Inspector ko?\n Kh√¥ng. Ch·ªâ c·∫ßn d√πng RDS Console l√† ƒë·ªß r·ªìi\n Q. B·∫°n c·∫ßn ensure l√† all object S3 bucket ƒë∆∞·ª£c encrypt at rest. C·∫ßn config S3 bucket ƒë·ªÉ n√≥ deny nh·ªØng request m√† ko c√≥ header l√† g√¨?\n x-amz-server-side-encryption\n Q. B·∫°n c√≥ 1 RDS database, gi·ªù mu·ªën chuy·ªÉn sang DynamoDB, n√™n b·∫°n c·∫ßn disable backup c·ªßa RDS v√¨ ko d√πng n·ªØa. B·∫°n c√≥ th·ªÉ set retention period xu·ªëng 1, nh∆∞ng khi set xu·ªëng 0 b·ªã l·ªói. V√¨ sao?\n V√¨ RDS c·ªßa b·∫°n ƒëang setting c√≥ Read Replica\n Q. B·∫°n migrate 1 online accounting app d√πng TCP protocol l√™n AWS. C·∫ßn ƒë·∫£m b·∫£o t√≠nh scalability v√† availability, ƒë·ªìng th·ªùi ph·∫£i record dc IP c·ªßa client ƒë·ªÉ tracking n·ªØa. D√πng c√°i g√¨ 2 c√°ch sau?\n1: ELB + 1 TCP Listener + enable Proxy Protocol\n2: ELB + 1 TCP Listener + enable Cross-Zone Load Balancing\n D√πng c√°ch 1, v√¨ enable Cross-Zone Load Balancing s·∫Ω ko th·ªÉ record dc IP c·ªßa User d√πng app\n Q. 1 Cty d√πng Kinesis ƒë·ªÉ li√™n t·ª•c collect data c·ªßa user. V√¨ data nh·∫°y c·∫£m n√™n c·∫ßn secure at rest. N√™n config Kinesis s·ª≠ d·ª•ng server-side encryption hay client-side encryption?\n D√πng Kinesis server-side encryption (ko n√™n d√πng client-side encrpytion v√¨ kh√≥ setup h∆°n nhi·ªÅu)\nC√¢u h·ªèi l√† secure at rest, n√™n d√πng server-side n√≥ s·∫Ω encrpyt khi n√≥ ch∆∞a ghi v√†o DB, v√† decrypt sau khi l·∫•y ra t·ª´ DB\nN·∫øu c·∫ßn secure in transit, th√¨ n√™n d√πng client-side, n√≥ s·∫Ω encrpyt khi n√≥ ƒëi ra kh·ªèi client, v√† ƒë·∫øn l√∫c v√†o DB th√¨ n√≥ ƒë√£ dc encrypt r·ªìi (qu√£ng ƒë∆∞·ªùng t·ª´ client ƒë·∫øn server th√¨ data ƒë√£ dc encrpyt n√™n ch√≠nh l√† in transit)\nC√°c c√¢u h·ªèi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh thi  Q. EFS encrpytion th√¨ c·∫ßn d√πng c√°i g√¨? CloudHSM hay Server-side Encryption c√≥ s·∫µn c·ªßa EFS\n  Q. Khi t·∫°o Billing alert th√¨ d√πng service n√†o? AWS budget? hay Trusted Advisor? hay Cost Explorer? hay Cost and Usage?\n  Q. Account A t·∫°o AWS Catalog portfolio r·ªìi share cho Account B, v·∫≠y th√¨ B c√≥ th·ªÉ l√†m g√¨ v·ªõi portfolio ƒë√≥? Create/Update/Delete ƒë∆∞·ª£c ko? hay ch·ªâ t·∫°o ƒë∆∞·ª£c ·ªü local?\n  Q. NAT Gateway n·∫±m trong public subnet hay private subnet?\n  Q. Cloudformation, sau khi ch·∫°y stack l·ªói, b·∫°n s·ª≠a l·∫°i template xong th√¨ x√≥a t·∫°o l·∫°i stack hay update stack ƒë√≥? hay t·∫°o ChangeSet?\n  Q. B·∫°n ƒë√£ issued 1 certificate, gi·ªù mu·ªën t·∫°o key trong 1 secure environment, c√≥ th·ªÉ l√†m c√°c crpytographic operation th√¨ d√πng service n√†o? CloudHSM hay KMS\n  Q. Cty b·∫°n ƒë√£ centralized t·∫•t c·∫£ log v√†o 1 log group. C·∫ßn ph·∫£i xem c√°i log ƒë·ªÉ notify cho c√°c team li√™n quan. Th√¨ c√≥ c√°ch n√†o most effective? C√≥ n√™n chia ra nhi·ªÅu log group r·ªìi subcribe m·ªói team 1 log group?\n  Q. Khi d√πng AWS Aurora th√¨ resposibility c·ªßa b·∫°n l√† g√¨, provisioning storage DB? hay Schedule patch, maintain, update OS? hay Excute patch, maintain, update OS?\n  Q. Lambda ·ªü account A push object l√™n s3 bucket ·ªü account B, nh∆∞ng B ko x√≥a dc object ƒë√≥ th√¨ n√™n l√†m n√†o? Lambda c·∫ßn put c·∫£ objectACl n·ªØa, define owner c√≥ quy·ªÅn full access\n  Q. Cloudtrail c√≥ kh√°i ni·ªám data event log v√† management event log ko? hay c√≥ c√°i n√†o trong 2 c√°i ƒë√≥?\n  Q. AWS Orgnization c·ªßa b·∫°n mu·ªën xem ai make request create new account th√¨ xem ·ªü ƒë√¢u? Cloudtrail log hay Organization access log? C√≥ c√°i g·ªçi l√† Orgnization access log ko?\n  Q. Khi t·∫°o S3 bucket policy, c√≥ c√°i resource arn ki·ªÉu ...bucket/* ko?\n  Q. 1 VPC c√≥ 2 public subnet, 2 private subnet, 1 igw, 1 natgw, c√≥ potential nguy hi·ªÉm ti·ªÅm t√†ng g√¨? Single natgw ko redundant?, Single igw ko redundant?, VPC default route table ch·ªâ associate dc v·ªõi 1 AZ?\n  Q. V·ªÅ cloudtrail intergrity, ƒë·ªÉ prevent modify/delete c√°i trail log, n√™n b·∫≠t mfa delele cho bucket ƒë√≥? Hay edit policy ƒë·ªÉ DENY ALL\n  Q. C√≥ 2 VPC, VPC 1 l√† ch√≠nh ·ªü 1 region, VPC 2 l√† c√°i d·ª± ph√≤ng ƒëang ƒë·ªÉ ·ªü region kh√°c, gi·ªù mu·ªën sync data t·ª´ VPC 1 sang 2, m√† traffic ko b·ªã expose qua internet th√¨ n√™n l√†m n√†o? t·∫°o 1 inter-vpc peering √†? hay t·∫°o 1 EC2 d√πng l√†m VPN?\n Done!\nCh√∫c c√°c b·∫°n th√†nh c√¥ng, m√¨nh passed r·ªìi üéâ üéâ üéâ\n","href":"/bk/encrypt-aws-certified-sysops-administrator-associate-note-soa/","title":"Aws Certified Sysops Administrator Associate Note (SOA)"},{"content":"C√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh h·ªçc \u0026mdash;P1\u0026mdash;  Q. Cty c·ªßa b·∫°n c√≥ 1 VPC v·ªõi v√†i ec2. App tr√™n ec2 l√†m vi·ªác v·ªõi IPv6. C·∫ßn ensure l√† ec2 n√†y c√≥ th·ªÉ init traffic ra Internet, nh∆∞ng ko cho connect t·ª´ internet v√†o instance. L√†m n√†o?\n D√πng egress-only Internet gateway.\nko d√πng NAT GW hay NAT instance v√¨ c√°i ƒë√≥ d√πng cho IP v4, IGW th√¨ hi·ªÉn nhi√™n ko d√πng r·ªìi.\negress-only IGW l√† stateful, n√≥ forward traffic t·ª´ instance ra Internet, v√† g·ª≠i respone l·∫°i cho instance\n Q. Cty b·∫°n c√≥ VPC v·ªõi v√†i Ec2. 1 instance m·ªõi launch s·∫Ω host app l√†m vi·ªác v·ªõi IPv6. VPC c·ªßa b·∫°n c·∫ßn c√°i g√¨ ƒë·ªÉ ensure c√°c Ec2 m·ªõi launch s·∫Ω c√≥ th·ªÉ communicate qua IPv6?\n C·∫ßn ensure l√† VPC ƒë∆∞·ª£c enable Dual Stack mode.\nvi·ªác attach 1 egress-only IGW c≈©ng ƒë√∫ng nh∆∞ng l√† ch∆∞a ƒë·ªß.\nC√°c step ƒë·ªÉ enable VPC s·ª≠ d·ª•ng dc IPv6:\n1-VPC v√† subnet c·∫ßn dc associate IPv6 CIDR block\n2-Update route table:\n_Public subnet th√¨ c·∫ßn t·∫°o route ƒë·ªÉ route traffic IPv6 t·ª´ subnet ƒë·∫øn IGW\n_Private subnet th√¨ c·∫ßn t·∫°o route ƒë·ªÉ route traffic Ipv6 t·ª´ subnet ƒë·∫øn egress-only IGW\n3-Update SG (v√† ACL n·∫øu c√≥), ƒë·ªÉ include IPv6 address v√†o SG\n4-X√°c ƒë·ªãnh xem instance type c√≥ support IPv6 ko, n·∫øu ko th√¨ ph·∫£i ƒë·ªïi (m3.large ko support ph·∫£i tƒÉng l√™n m4.large)\n5-Assign Ipv6 cho instance\n6-N·∫øu instance ch∆∞a dc configure DHCPv6 th√¨ c≈©ng ph·∫£i configure.\n Q. Cty b·∫°n c·∫ßn setup 1 hybric connection gi·ªØa on-premise v√† aws vpc. H·ªç s·∫Ω transfer l∆∞·ª£ng l·ªõn data t·ª´ on premise l√™n aws. C·∫ßn d√πng g√¨? VPN hay Direct Connect?\n AWS Direct Connect.\nV√¨ VPN ko th·ªÉ ƒë·∫£m b·∫£o high banwidth connection cho l∆∞·ª£ng l·ªõn data dc.\n Q. B·∫°n setup VPC v√† 1 subnet. T·∫°o 1 IGW attach v√†o VPC. Vpc d√£ cho ph√©p DNS resolution v√† hostname. B·∫°n launch 1 ec2 c√≥ public ip v√† ƒë√£ setting SG v√† NACL r·ªìi, nh∆∞ng v·∫´n ko access dc EC2 v√¨ sao? C√≥ ph·∫£i v√¨ ch∆∞a attach IGW v√†o public subnet? hay v√¨ ch∆∞a setup Route table?\n V√¨ ch∆∞a setup Route table\nIGW ch·ªâ c·∫ßn attach v√†o VPC l√† ƒë·ªß r·ªìi, ko c·∫ßn subnet\n Q. B·∫°n mu·ªën cho ph√©p 1 admin v√†o setup EC2, EC2 ƒë√≥ c·∫ßn quy·ªÅn access v√†o DynamoDB. Nh·ªØng policy permission g√¨ c·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o security?\n C·∫ßn trust policy cho ph√©p EC2 instance assume role\nC·∫ßn permission policy cho ph√©p User Pass role to EC2\nTheo AWS Documents c·∫ßn 3 y·∫øu t·ªë:\nFirst, IAM permission policy attach v√†o role, define nh·ªØng action v√† resource m√† role c√≥ th·ªÉ l√†m.\nv√≠ d·ª• define action read-only DynamoDB table.\nSecond, 1 trust policy cho role, cho ph√©p service assume role.\nv√≠ d·ª• sau l√† trust policy cho ph√©p EC2 service s·ª≠ d·ª•ng role v√† c√°c permission dc attach v√†o role.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Sid\u0026quot;: \u0026quot;TrustPolicyStatementThatAllowsEC2ServiceToAssumeTheAttachedRole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;ec2.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } } Third, 1 IAM permission policy attach v√†o IAM User, cho ph√©p user pass nh·ªØng role ƒë√£ dc aproved.\nv√≠ d·ª• sau l√† User ch·ªâ ƒë∆∞·ª£c pass c√°i role t√™n l√† EC2-role-for-XYZ-*\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:PassRole\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;account-id\u0026gt;:role/EC2-roles-for-XYZ-*\u0026quot; }] } Gi·ªù, User c√≥ th·ªÉ start EC2, app run tr√™n EC2 c√≥ th·ªÉ access temporary credential th√¥ng qua instance profile meta data. Instance c√≥ th·ªÉ l√†m nh·ªØng action ƒë√£ dc determine trong permission policies.\n1 V√≠ d·ª• kh√°c d·ªÖ hi·ªÉu h∆°n:\nIAM permission policy attach v√†o IAM User A nh∆∞ sau:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [{ \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[\u0026quot;ec2:*\u0026quot;], \u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:\u0026quot;iam:PassRole\u0026quot;, \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:iam::123456789012:role/S3Access\u0026quot; }] } User A s·∫Ω c√≥ full quy·ªÅn v·ªÅ EC2, v√† khi User A launch 1 EC2, Anh ta ch·ªâ c√≥ quy·ªÅn Pass (associate/g√°n) c√°i role t√™n l√† S3Access v√†o Instance th√¥i.\nT·∫•t nhi√™n tr∆∞·ªõc ƒë√≥, B·∫°n c≈©ng c·∫ßn t·∫°o c√°i role t√™n l√† S3Access v·ªõi trust policy attach v√†o role l√†:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Sid\u0026quot;: \u0026quot;TrustPolicyStatementThatAllowsEC2ServiceToAssumeTheAttachedRole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;ec2.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } } V√† Role S3Access c≈©ng c·∫ßn c√≥ permission policy attach v√†o role t∆∞∆°ng t·ª± nh∆∞ sau:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:ListAllMyBuckets\u0026quot;, \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Q. Nh·ªØng service sau c√°i n√†o by default ko cung c·∫•p kh·∫£ nƒÉng HA? EC2-RDS-DynamoDB-ELB?\n EC2 ko HA by default, mu·ªën HA th√¨ ph·∫£i t·ª± l√†m b·∫±ng scripts.\nRDS c√≥ feature Multi-AZ, DynamoDB th√¨ full mamanged by AWS, ELB th√¨ v·ªën ƒë√£ HA\n Q. B·∫°n mu·ªën c√≥ 1 list c√°c user trong AWS account c·ªßa b·∫°n, check xem c√°c user ƒë√≥ c√≥ d√πng MFA ko? l√†m n√†o?\n V√†o IAM, download credential report, nh·ªØng th√¥ng tin r·∫•t nhi·ªÅu li√™n quan ƒë·∫øn v√≠ d·ª• nh∆∞ password, access key, MFA\n Q. 1 bucket c√≥ th·ªÉ set policy ƒë·ªÉ deny ho·∫∑c allow 1 IP range n√†o ƒë√≥ b·∫±ng c√°ch:\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;S3PolicyId1\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;IPAllow\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::examplebucket/*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;IpAddress\u0026quot;: {\u0026quot;aws:SourceIp\u0026quot;: \u0026quot;54.240.143.0/24\u0026quot;}, \u0026quot;NotIpAddress\u0026quot;: {\u0026quot;aws:SourceIp\u0026quot;: \u0026quot;54.240.143.188/32\u0026quot;} } } ] }  Q. B·∫°n l√† sysops admin c·ªßa 1 cty IT. Sau khi ƒë√£ setup SSM tr√™n EC2 ·ªü c√°c region, b·∫°n c·∫ßn setup SSM cho 50 db server ·ªü data centre c·ªßa c√¥ng ty b·∫°n. Nh·ªØng action g√¨ l√† b·∫Øt bu·ªôc?\n T·∫°o 1 IAM role ƒë·ªÉ communicate SSM service.\nInstall TLS certificate tr√™n server c·ªßa Data centre.\nT·∫°o 1 managed-instance activation cho m√¥i tr∆∞·ªùng Hybrid. C√°i n√†y ƒë·ªÉ t·∫°o ra 1 c·∫∑p Code-ID d√πng cho vi·ªác install SSM agent sau n√†y.\nDownload v√† Install SSM agent tr√™n c√°c server c·ªßa Data centre.\n Q. Tu·∫ßn tr∆∞·ªõc c√≥ 1 v√†i s·ª± c·ªë cho m√¥i tr∆∞·ªùng Production khi 1 s·ªë ph·∫ßn m·ªÅm unauthorised c·ªßa third-party dc install tr√™n servers. Gi·ªù b·∫°n c·∫ßn duy tr√¨ 1 tr·∫°ng th√°i ƒë√£ dc defined tr∆∞·ªõc (predefined state) cho all Ec2 th√¨ c·∫ßn l√†m g√¨ tr√™n SSM?\n D√πng State manager v·ªõi Command Document hay Policy Document, hay Automation Document, hay Package Document, hay Session Document?\nCommand Document: Runcommand d√πng lo·∫°i n√†y ƒë·ªÉ run command. C√≤n State Manager d√πng lo·∫°i n√†y ƒë·ªÉ apply configuration. Maintenance Windows th√¨ d√πng lo·∫°i n√†y ƒë·ªÉ apply configuraition theo schedule.\nPolicy Document: State Manager d√πng lo·∫°i n√†y ƒë·ªÉ apply policy tr√™n EC2\nAutomation Document: D√πng ƒë·ªÉ th·ª±c hi·ªán nh·ªØng task automation\nPackage Document: D√πng ƒë·ªÉ t·∫°o zip folder ƒë·ªÉ install tr√™n target instance\nSession Document: Session Manager d√πng lo·∫°i n√†y ƒë·ªÉ start 1 session ki·ªÉu nh∆∞ SSH tunnel hay port forwarding\n Q. 1 cty mu·ªën transfer 1 l∆∞·ª£ng l·ªõn data l√™n s3. S·ªë l∆∞·ª£ng data ban ƒë·∫ßu kho·∫£ng 100TB. D√πng service g√¨? Direct Connect hay Snowball?\n Snowball.\nko th·∫•y ƒë·ªÅ c·∫≠p v·ªÅ vi·ªác transfer data on-going, th·∫ø n√™n setup Direct connect ch·ªâ ƒë·ªÉ cho 1 l·∫ßn transfer nh∆∞ n√†y th√¨ ko h·ª£p l√Ω.\n Q. B·∫°n ƒëang d√πng CodeDeploy ƒë·ªÉ update app v·ªõi 1 password m·ªõi cho production. Ngta lo ng·∫°i r·∫±ng vi·ªác store new password trong khi update app tr√™n 1 l∆∞·ª£ng l·ªõn instance s·∫Ω l√†m service b·ªã outage. L√†m n√†o?\n D√πng System manager Parameter Store ƒë·ªÉ store pw. N√≥ store v√† save config data theo encrypted format. Parameter Store tƒÉng c∆∞·ªùng secure v√¨ n√≥ store password t√°ch ri√™ng kh·ªèi configuration file\n Q. VPC c·ªßa b·∫°n c√≥ c·∫£ Public subnet v√† private subnet. Application dc deploy trong 1 Ec2 ·ªü Private subnet. EC2 ƒë√≥ c·∫ßn t∆∞∆°ng t√°c v·ªõi S3. Nh∆∞ng l·∫°i ko t∆∞∆°ng t√°c S3 ƒëc, m·∫∑c d√π IAM role ƒë√£ ƒë√∫ng r·ªìi, L√†m n√†o?\n VPC c·ªßa b·∫°n c·∫ßn dc attach 1 VPC gateway enpoint.\nHo·∫∑c b·∫°n c·∫ßn ƒë·∫∑t Ec2 ƒë√≥ ·ªü Public subnet thay v√¨ private subnet nh∆∞ hi·ªán t·∫°i.\nT√∫m l·∫°i, n·∫øu EC2 ƒë·ªÉ trong private subnet mu·ªën connect S3 th√¨ c·∫ßn c√≥ VPC gateway enpoint\n Q. B·∫°n l√† IT admin, C√¥ng ty c√≥ 1 set EC2 tr√™n AWS, v√† 1 set server onpremise. H·ªç mu·ªën monitor system level log c·ªßa both side server theo metric th√¨ l√†m n√†o?\n Install cloudwatch agent tr√™n both set of servers\nsetup metrics dashboard tr√™n Cloudwatch\n Q. C√¥ng ty mu·ªën d√πng aws v√† s·ª≠ d·ª•ng g√≥i support plan c·ªßa aws. Nhu c·∫ßu l√†: C√≥ operational reviews v√† c√≥ respone time \u0026lt; 30 minutes cho nh·ªØng critical issues. N√™n ch·ªçn c√°i n√†o? Developer, Basic, Business, Enterprise?\n D√πng Enterprise\nko c√≥ Basic plan\ngi·ªëng nhau gi·ªØa Business v√† Enterprise:\nsystem h∆∞ h·ªèng (support \u0026lt; 12h)\nsystem Production b·ªã h∆∞ h·ªèng (support \u0026lt; 4h)\nsystem Production b·ªã down (support \u0026lt; 1h)\nkh√°c nhau gi·ªØa Business v√† Enterprise:\nsystem critical-quan tr·ªçng b·ªã down (Business ko support, Enterprise support \u0026lt; 15 ph√∫t)\nEnterprise c√≥ review operation, review well architect, coordinate v·ªõi expert c·ªßa AWS, c√≥ th·ªÉ access self-paced lab, c√≥ team ƒë·∫øn t·∫≠n n∆°i support, gi√° \u0026gt; 15000 USD/th√°ng\nTrong khi Bussiness ch·ªâ kho·∫£ng \u0026gt; 100 USD/th√°ng\n Q. Cty b·∫°n c√≥ kho·∫£ng 500 instances, c·∫ßn ensure l√† SSH lu√¥n dc disable. L√†m n√†o?\n D√πng AWS Config Rules ƒë·ªÉ check Security Groups\nTh·ª±c ch·∫•t AWS Config rule l√† nh·ªØng h√†m lambda check li√™n t·ª•c nh·ªØng ƒëi·ªÅu ki·ªán nh∆∞: EC2 lu√¥n dc associate v·ªõi √≠t nh·∫•t 1 SG, port 22 ko ƒëc m·ªü trong production SG, c√°c resource c·∫ßn c√≥ required tags, EIP c·∫ßn dc attach v√†o instances, c√°c volume ƒëang dc encrypt\n Q. 1 Dev mu·ªën dc notify b·∫•t c·ª© khi n√†o c√≥ API activity dc call tr√™n S3 bucket c·ªßa anh ta. L√†m n√†o?\n Config a cloud trail v√† SNS\n Q. B·∫°n start c√°c EC2 instance, nh∆∞ng ch√∫ng chuy·ªÉn sang b·ªã terminate ngay sau tr·∫°ng th√°i pending, v√¨ sao?\n ƒë√¢y l√† l·ªói Instance Terminates Immediately, l√Ω do:\n_EBS volume b·ªã limit\n_EBS snapshot b·ªã h·ªèng (corrupt)\n_EBS root volume b·ªã encrypt v√† b·∫°n ko c√≥ quy·ªÅn access v√†o KMS ƒë·ªÉ decrypt key\n_AMI m√† b·∫°n d√πng ƒë·ªÉ launch instance b·ªã thi·∫øu file n√†o ƒë√≥.\n-\u0026gt; T√∫m l·∫°i ch·ªß y·∫øu l√† l·ªói v·ªÅ EBS v√† AMI, ko th·ªÉ l√† l·ªói v·ªÅ limit instances ƒë∆∞·ª£c\n Q. Team b·∫°n d√πng EC2 ƒë·ªÉ setup database, s·∫Ω r·∫•t n·∫∑ng v·ªÅ workload, n√™n d√πng lo·∫°i EBS n√†o?\n Provisioned IOPS\nko d√πng General purpose SSD v√¨ AWS Document ghi r√µ io1 d√πng ƒë·ªÉ host DB workload l·ªõn\n Q. 1 cty ƒëang d√πng EC2 ƒë·ªÉ host Memcached d√πng l√† Cache cho c√°c app. Th√¨ n√™n ch·ªçn lo·∫°i instance n√†o? Memory Optimized? hay Compute Optimized? hay Storage Optimized? hay General Purpose?\n D√πng Memory Optimized Instances\nTrong AWS Documents c√≥ vi·∫øt:\nCompute Optimized Instances: (c4, c5)\nBatch processing\nMedia transcoding\nHigh-performance web servers\nScientific modeling\nDedicated gaming servers and ad serving engines\nMachine learning\nGeneral Purpose Instances: (a1, m5, t2, t3)\nWeb/Game servers\nmicroservices\nCaching fleets\nSmall and medium databases\nCode repositories\nDevelopment, build, test, and staging environments\nMemory Optimized Instances: (r4, r5)\nHigh-performance Database Relational/NoSQL\nIn-memory caching (Memcached and Redis).\nIn-memory databases and analytics (for example, SAP HANA).\nReal-time processing (Hadoop/Spark clusters).\nStorage Optimized Instances: (d2, h1, i3)\ndata warehouse\nMapReduce and Hadoop distributed computing\nLog or data processing applications\nHigh frequency online transaction processing (OLTP) systems\n Q. Cty b·∫°n c√≥ 1 set EC2, c√°c EC2 n√†y run 24/7 trong su·ªët c·∫£ nƒÉm. V√† b·∫°n c·∫ßn upgrade instance type ƒë·ªÉ ƒë√°p ·ª©ng ƒëc workload trong 1 nƒÉm ƒë√≥ th√¨ n√™n ch·ªçn lo·∫°i n√†o? Convertible Reserved hay Standard Reserved Instance?\n Convertible Reserved Instances\n2 lo·∫°i Convertible v√† Standard kh√°c nhau ·ªü ch·ªó lo·∫°i Standard ko th·ªÉ change dc instance type during the term.\nLo·∫°i Standard th√¨ c√≥ th·ªÉ change instance size, tuy nhi√™n ko th·ªÉ exchange\nLo·∫°i Convertible th√¨ c√≥ th·ªÉ change dc instance family, type, size, platform, scope, tenacy.\n Q. B·∫°n c√≥ 3 VPC A, B, C, c·∫ßn ph·∫£i communicate v·ªõi nhau, l√†m sao ƒë·ªÉ ensure l√† all instance s·∫Ω communicate dc v·ªõi nhau?\n A peer v·ªõi B, B peer C, A peer C\nM·ªói Route table trong 1 vpc c·∫ßn ph·∫£i add CIDR block c·ªßa 2 c√°i c√≤n l·∫°i v√†o Destination\n Q. B·∫°n c√≥ 1 VPC, v√† 1 custom domain, b·∫°n mu·ªën ensure l√† custom domain ch·ªâ dc access b·ªüi c√°c instance trong VPC th√¥i, ko ph·∫£i public internet, l√†m n√†o?\n D√πng private hosted zone ·ªü Route53\n Q. NAT Gateway setup ·ªü trong public subnet hay private subnet? bandwidth restrictions l√† g√¨?\n Trong public subnet\nbandwidth restrictions l√† h·∫°n ch·∫ø v·ªÅ bƒÉng th√¥ng\n Q. B·∫°n ƒëang c·∫ßn connect on-premise l√™n AWS vpc. B·∫°n nh·∫≠n nhi·ªám v·ª• setup vpn connection ƒë·ªÉ setup hybrid architecture th√¨ c·∫ßn l√†m nh·ªØng step g√¨?\n T·∫°o customer gateway\nT·∫°o virtual private gateway attach v√†o vpc\nUpdate route table, s·ª≠a c√°i route propagation: add c√°i virtual private gateway v·ª´a t·∫°o v√†o\nUpdate SG: ph·∫ßn inbound c·∫ßn enable SSH, RDP, ICMP access\nT·∫°o VPN connection\n Q. B·∫°n ƒëang t·∫°o 1 Organization cho c√¥ng ty b·∫°n, nh·ªØng g√¨ l√† ƒë√∫ng v·ªõi Master account c·ªßa Organization?\n t·∫°o Organization v√† OU,\ninvite external account ƒë·ªÉ join v√†o org,\nthanh to√°n t·∫•t c·∫£ charge c·ªßa all account trong org (ko ch·ªâ OU),\nmaster account ko b·ªã ·∫£nh h∆∞·ªüng b·ªüi service control policies\n Q. 1 App ƒëc deploy tr√™n ec2, c√≥ d√πng ELB. B·∫°n c·∫ßn list c√°c Ip address truy c·∫≠p v√†o ELB, l√†m n√†o?\n enable access logging cho ELB,\nc·∫•p quy·ªÅn ƒë·ªÉ access logging truy c·∫≠p dc v√†o s3 bucket\nAWS Document gi·∫£i th√≠ch: ELB access logging l∆∞u tr·ªØ client ip, timestamp, latencies, request path, server respone. Tuy nhi√™n default n√≥ disable, N·∫øu mu·ªën th√¨ ph·∫£i enable l√™n. Sau khi enable th√¨ ELB s·∫Ω capture log v√† l∆∞u v√†o S3 bucket m√† b·∫°n ch·ªâ ƒë·ªãnh\nC·∫ßn ph·∫£i s·ª≠a Bucket policy ƒë·ªÉ ELB c√≥ quy·ªÅn ghi log v√†o bucket ƒë√≥\n Q. Web app server ƒëc deploy tr√™n ec2, C·∫ßn 1 ph∆∞∆°ng ph√°p ph√≤ng ch·ªëng security, b·∫°n c·∫ßn t·∫°o 1 process ƒë·ªÉ ƒë√°nh gi√° security sau khi deploy. C√°ch hi·ªáu qu·∫£ ƒë·ªÉ m·ªói khi c√≥ thay ƒë·ªïi v·ªÅ Security Group d·∫´n ƒë·∫øn s·ª± ti·∫øp x√∫c v·ªõi Internet (assess network exposure) b·∫°n s·∫Ω dc bi·∫øt l√† g√¨?\n S·ª≠ d·ª•ng Amazon Inspector, n√≥ s·∫Ω c√≥ c√°i Network Reachability package ƒë·ªÉ evaluate Network config c√≥ ƒëang risk ko. C√°i package n√†y ko y√™u c·∫ßu install agent Inspector tr√™n ec2.\nB·∫°n c√≥ th·ªÉ t·∫°o 1 Cloudwatch event ƒë·ªÉ monitor s·ª± thay ƒë·ªïi c·ªßa SG or VPC config, khi ƒë√≥ n√≥ s·∫Ω trigger c√°i Network Assessment\nNetwork Reachability package s·∫Ω t·ª± ƒë·ªông monitor AWS network c·ªßa b·∫°n, xem c√≥ c√°i n√†o misconfig ko. N√≥ ko y√™u c·∫ßu install agent Inspector, tuy nhi√™n n·∫øu install s·∫Ω dc cung c·∫•p ch√≠nh x√°c c√°c process ƒëang listen on ports.\npackage n√†y ko support Clasic EC2\n Q. 1 cty setup 1 website host tr√™n s3, mu·ªën user c√≥ seamless user experience (tr·∫£i nghi·ªám li·ªÅn m·∫°ch) th√¨ c·∫ßn g√¨?\n D√πng cloudfront\n Q. Ph√¢n bi·ªát 3 c√°ch Server side encrpytion s3: (SSE-S3, SSE-KMS, SSE-C)\n  Q. 1 Cty c·∫ßn store historical data. C√≥ th·ªÉ s·∫Ω d√πng Business Intelligence solution ƒë·ªÉ query ƒë·ªëng data ƒë√≥. Th√¨ n√™n d√πng lo·∫°i DB n√†o? EMR, DynamoDB, Redshift?\n D√πng Redshift.\nDynamoDB th√¨ ko th·ªÉ l∆∞u tr·ªØ historical data\nEMR d√πng ƒë·ªÉ l√†m data transformation, v√† load data ƒë√£ processed ƒë·∫øn Redshift th√¥i\nC√≤n Redshift th√¨ full managed, petabyte scale data warehouse service, l√† gi·∫£i ph√°p v·ªÅ historical data storage, c√≥ th·ªÉ query nh∆∞ SQL\n Q. B·∫°n ƒëang c√≥ AWS RDS MySQL database, ƒëang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ performance, v√¨ l√† sysadmin b·∫°n c·∫ßn xem c√≥ c√°ch n√†o c·∫£i thi·ªán performance ko. D√πng c√°i g√¨? (AWS Trust Advisor, AWS Inspector, AWS Performance Insights, AWS Config?)\n D√πng AWS Performance Insights\nN√≥ l√† 1 ch·ª©c nƒÉng c√≥ trong RDS/Aurora, support cho c√°c lo·∫°i DB nh·∫•t ƒë·ªãnh, gi√∫p monitor, ph√¢n t√≠ch issue, trouble shooting performance problem v√† visualize data load.\nInspector d√πng ƒë·ªÉ ph√¢n t√≠ch EC2 vulnerability\nAWS Config ƒë·ªÉ manage configuration\nTrust Advisor ch·ªâ ƒë∆∞a ra c√°c best practice ch·ª© ko ph√¢n t√≠ch s√¢u c√°c issue trong DB c·ªßa b·∫°n\n Q. B·∫°n c√≥ 1 set c√°c EC2, c·∫ßn 1 dashboard metric c·ªßa CPU utilization trong interval 1 ph√∫t. C·∫ßn l√†m n√†o?\n Enable detailed monitoring for EC2 (Console-\u0026gt;Action-\u0026gt;Cloudwatch monitoring-\u0026gt;Enable Detailed monitoring)\nT·∫°o 1 dashboard trong Cloudwatch\nB·ªüi v√¨ n·∫øu ƒë·ªÉ ch·∫ø ƒë·ªô Basic monitoring c·ªßa EC2 th√¨ interval send metric to Cloudwatch l√† 5 ph√∫t\n Q. C√¥ng ty b·∫°n ƒëang setup database tr√™n EC2, C·∫ßn ph·∫£i config fault tolenrance v√¨ ƒë√¢y s·∫Ω host nh·ªØng data quan tr·ªçng. D·ª± ƒë·ªãnh implement RAID configuration. Th√¨ n√™n ch·ªçn lo·∫°i n√†o? RAID 0, RAID 1, RAID 5, RAID 6?\n D√πng RAID 1\nRAID 0 d√πng ch·ªß y·∫øu cho m·ª•c ƒë√≠ch I/O performance. B·∫°n add th√™m volume th√¨ b·∫°n s·∫Ω c√≥ th√™m throughput. N√≥ k·∫øt h·ª£p c√°c volume l·∫°i th√†nh 1 stripe. Tuy nhi√™n performance c·ªßa stripe b·ªã gi·ªõi h·∫°n ·ªü c√°i performance c·ªßa c√°i volume k√©m nh·∫•t. V√† n·∫øu m·∫•t 1 volume c≈©ng s·∫Ω m·∫•t to√†n b·ªô data\nRAID 1 d√πng ch·ªß y·∫øu cho m·ª•c ƒë√≠ch fault tolerance (c√°c ·ª©ng d·ª•ng critical). Data s·∫Ω ·ªïn ƒë·ªãnh h∆°n. Nh∆∞ng v√¨ data dc ghi v√†o nhi·ªÅu volume c√πng 1 l√∫c, n√™n c·∫ßn bƒÉng th√¥ng l·ªõn. V√† n√≥ ko c·∫£i thi·ªán performance WRITE dc.\nRAID 5,6 ko dc recommend cho Amazon EBS, v√¨ n√≥ l√†m gi·∫£m IOPS c·ªßa EBS xu·ªëng 20-30% so v·ªõi RAID 0, trong khi gi√° tƒÉng g·∫•p 2. V√≠ d·ª•: 1 array g·ªìm 2 volume RAID 0 s·∫Ω performance t·ªët h∆°n 1 array 4 volume RAID 6 m√† c√≥ gi√° g·∫•p ƒë√¥i.\nN√≥i chung c·ª© nh·ªõ, RAID 0 stripe, v√¨ n√≥ stripe nhi·ªÅu volume l·∫°i ƒë·ªÉ tƒÉng performance. T·ª´ ƒë√≥ suy ra RAID 1 s·∫Ω fault tolerance.\n Q. B·∫°n c·∫ßn s·ª≠ d·ª•ng AWS QuickSight v√† c√≥ g√¨ c·∫ßn ch√∫ √Ω gi·ªØa 2 phi√™n b·∫£n Enterpise v√† Standard?\n Enterprise h∆°n Standard ·ªü ch·ªó cho ph√©p encrpyt data rest v√† t∆∞∆°ng th√≠ch v·ªõi Microsoft Active Directory (AD)\nB·∫£n Standard th√¨ b·∫°n c√≥ th·ªÉ invite IAM user v√†o ho·∫∑c t·∫°o cho h·ªç 1 user QuickSight only\nB·∫£n Enterprise th√¨ d√πng MS AD ƒë·ªÉ user access QuickSight\n Q. 1 Cty mu·ªën c√°c EC2 ƒë∆∞·ª£c patch v·ªõi latest security patches?\n AWS System Management\n Q. B·∫°n c√≥ 1 VPC v√† c√°c subnets. Sau khi launch EC2 th√¨ n√≥ ko c√≥ public DNS name, c·∫ßn l√†m g√¨?\n Check 2 attribute c·ªßa VPC: enableDnsHostnames, enableDnsSupport\nN·∫øu c·∫£ 2 ƒëang dc set = true th√¨ EC2 m·ªõi c√≥ public DNS hostname\n Q. B·∫°n ƒëang setup AWS Direct Connect gi·ªØa AWS VPC v√† on-premise data center. Th√¨ Network c·∫ßn ph·∫£i nh∆∞ n√†o?\n Network c·ªßa b·∫°n ph·∫£i ƒëang ƒë·∫∑t c√πng ch·ªó v·ªõi AWS Direct Connect location (C·∫ßn t√¨m hi·ªÉu v·ªÅ location hi·ªán c√≥ c·ªßa Direct Connect)\nNetwork c·ªßa b·∫°n ph·∫£i l√†m vi·ªác v·ªõi AWS Direct Connect Partner-ƒë·ªëi t√°c s·∫Ω h·ªó tr·ª£ kh√°ch h√†ng (l√† b·∫°n) connect v√†o Direct Connect\nV·ªÅ k·ªπ thu·∫≠t:\nNetwork c·ªßa b·∫°n ph·∫£i d√πng single-mode fiber\nAuto negotiation for port ph·∫£i disabled\n802.1Q VLAN ph·∫£i dc support\nDevice ph·∫£i support BGP (Border Gateway Protocol) v√† BGP MD5\n Q. B·∫°n ƒëang qu·∫£n l√Ω vi·ªác migrate 1 app on-premise l√™n AWS. Dev ƒëang launch EC2 ƒë·ªÉ test app. B·∫°n concern v·ªÅ s·ªë l∆∞·ª£ng ec2, service n√†o d√πng ƒë·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng ec2 c√≥ ƒëang trong limit hay ko? AWS Config, AWS Trust Advisor, SSM, Cloudwatch?\n AWS Trust Advisor\nAWS Trust Advisor dashboard c√≥ 1 feature t√™n l√† Service limit, d√πng ƒë·ªÉ li√™n t·ª•c monitor usaged resource ƒë·ªÉ b·∫°n c√≥ th·ªÉ request tƒÉng limit ho·∫∑c shut down b·ªõt resource tr∆∞·ªõc khi limit b·ªã reached\n Q. B·∫°n t·∫°o EC2 nh∆∞ng n√≥ ko auto c√≥ public Ip address, l√†m n√†o?\n S·ª≠a Subnet, modify auto-asign public IP = true\nNh·ªõ l√† S·ª≠a subnet ch·ª© ko ph·∫£i VPC\n Q. B·∫°n ƒëang setup Test env. B·∫°n b·∫≠t VPC flow log ƒë·ªÉ capture IP traffic cho sau n√†y c√≤n audit. VPC flow log publish nhi·ªÅu log file v√†o 1 single s3 bucket. V√†i flow log ƒë·∫ßu th√¨ dc publish v√†o encrypted bucket th√†nh c√¥ng, nh∆∞ng sau ƒë√≥ th√¨ b·ªã l·ªói LogDestinationPermissionIssueException. V√¨ sao?\n V√¨ S3 bucket policy gi·ªõi h·∫°n 20KB th√¥i, M·ªói l·∫ßn t·∫°o 1 flow log m·ªõi, aws s·∫Ω add th√™m ARN c·ªßa folder m·ªõi v√†o Resource element c·ªßa bucket policy. C√†ng nhi·ªÅu th√¨ s·∫Ω c√†ng c√≥ nguy c∆° v∆∞·ª£t qu√° limit 20KB\nGi·∫£i ph√°p: x√≥a h·∫øt c√°c entries c≈© ko d√πng trong policy. Ho·∫∑c ph√¢n quy·ªÅn cho to√†n b·ªô bucket policy l√† arn:aws:s3:::bucket_name/*\nC√≤n n·∫øu g·∫∑p l·ªói LogDestinationNotFoundException: th√¨ c√≥ th·ªÉ ARN c·ªßa s3 bucket ƒëang ch·ªâ ƒë·ªãnh sai, ho·∫∑c bucket ƒë√≥ ko c√≤n t·ªìn t·∫°i\nC√ín n·∫øu g·∫∑p l·ªói t·∫°o Flow log r·ªìi nh∆∞ng ko nh√¨n th·∫•y log ·ªü trong s3 bucket hay Cloudwatch, th√¨ l√† do ch∆∞a c√≥ traffic sinh ra, ho·∫∑c ph·∫£i ch·ªù 10 ph√∫t\n\u0026mdash;P2\u0026mdash;  Q. B·∫°n c√≥ 1 VPC, ƒë√£ setup SG, network ACL, nh∆∞ng traffic ko reach ƒë·∫øn instance, l√†m sao ƒë·ªÉ diagnose issue? d√πng Cloudtrail hay VPC Flow logs ?\n D√πng VPC Flow logs, n√≥ gi√∫p monitor traffic reach instances\nch·ª© Cloudtrail ƒë·ªÉ monitoring API th√¥i\n Q. 2 VPC A(10.0.0.0/16) v√† VPC B(20.0.0.0/16), ƒë√£ t·∫°o 1 VPC peering c√≥ id l√†: pcx-1a2b1a2b. Th√¨ route table c·ªßa 2 VPC c·∫ßn config Destination v√† Target l√† g√¨?\n Destination l√† CIDR block c·ªßa VPC kia, c√≤n Target l√† id c·ªßa VPC peering\n Q. B·∫°n ƒë√£ t·∫°o 1 RDS Instance. C√¥ng ty mu·ªën ko c√≥ downtime khi 1 snapshot dc t·∫°o ra. L√†m n√†o?\n enable multi-AZ\nv√¨ single-AZ g√¢y downtime, n√™n n·∫øu multi-AZ th√¨ vi·ªác t·∫°o snapshot s·∫Ω ch·ªâ di·ªÖn ra tr√™n con RDS standby\n Q. 1 Consultant mu·ªën view contents c·ªßa 1 S3 bucket trong AWS Account. Content ƒë√≥ s·∫Ω available trong 1 kho·∫£ng time. Th√¨ n√™n d√πng c√°ch g√¨?\n T·∫°o 1 IAM Role v·ªõi session duration parameter\nKo th·ªÉ specify time limit trong bucket ACL ƒë∆∞·ª£c\nC≈©ng ko th·ªÉ t·∫°o IAM User v·ªõi session duration parameter ƒë∆∞·ª£c\nb·∫Øt bu·ªôc d√πng ROLE\n Q. Cty b·∫°n host web tr√™n S3. User s·∫Ω truy c·∫≠p v√†o web b·∫±ng uRL: http://companya.com, v·∫≠y khi ƒë·∫∑t t√™n cho s3 bucket c·∫ßn ƒë·∫∑t t√™n g√¨?\n companya.com\nk·∫øt h·ª£p v·ªõi Route53 ƒë·ªÉ setting c√°i domain c·ªßa ri√™ng c√¥ng ty b·∫°n\n Q. 1 flow log record c√≥ format nh∆∞ sau:\n10 123456789010 eni-abc123de 172.31.41.189 172.8.51.117 39751 3389 6 20 3279 1218430010 1218430070 REJECT OK\nnghƒ©a l√† g√¨?\n ip 172.31.41.189 t·ª´ port 39751 ƒëang c·ªë connect ƒë·∫øn ip 172.8.51.117 ·ªü port RDP 3389, giao th·ª©c TCP, v√† b·ªã REJECT, ƒë√£ l∆∞u v√†o log\ns·ªë packages g·ª≠i ƒë·∫øn l√† 20, n·∫∑ng 3279 byte, timestamp l√∫c start l√† 1218430010 v√† end l√† 1218430070\n\u0026lt;version 10\u0026gt; \u0026lt;account-id\u0026gt; \u0026lt;interface-id\u0026gt; \u0026lt;source address\u0026gt; \u0026lt;dest address\u0026gt; \u0026lt;srcport\u0026gt; \u0026lt;dstport\u0026gt; \u0026lt;protocol\u0026gt; \u0026lt;packets\u0026gt; \u0026lt;bytes\u0026gt; \u0026lt;start\u0026gt; \u0026lt;end\u0026gt; \u0026lt;action\u0026gt; \u0026lt;log-status\u0026gt;\n Q. 1 Cty host 1 web app v√† c√≥ DynamoDB l√†m backend. V·∫•n ƒë·ªÅ l√† khi peak loads c√≥ th·ªÉ c√°c request ƒë·∫øn DB s·∫Ω b·ªã throttle. L√†m n√†o ƒë·ªÉ ƒë√°p ·ª©ng dc l∆∞·ª£ng l·ªõn c√°c request ƒë√≥?\n Enable auto scaling cho DynamoDB, n√≥ s·∫Ω tƒÉng read/write capacity ƒë·ªÉ DB ko b·ªã throttle\nC√°c DB nh∆∞ RDS, Aurora, DynamoDB ƒë·ªÅu c√≥ th·ªÉ enable auto scaling\nTr∆∞·ªõc ƒë√¢y SAA Notes c√≥ 1 c√¢u: DB b·ªã tƒÉng s·ªë l∆∞·ª£ng write, ko th·ªÉ handle ƒëc load. L√†m n√†o ƒë·ªÉ write ko b·ªã m·∫•t c√°i n√†o? - D√πng SQS FIFO ƒë·ªÉ l∆∞u c√°c write request pending\n-\u0026gt; th√¨ ƒë√≥ l√† do c√¢u h·ªèi ch·ªâ mu·ªën ƒë·ªÉ write request ko b·ªã m·∫•t c√°i n√†o th√¥i, ch·ª© SQS ko ƒë√°p ·ª©ng dc vi·ªác high throughput c·ªßa DynamoDB\n Q. B·∫°n restore 1 volume t·ª´ 1 snapshot ƒë·ªÉ attach v√†o EC2. B·∫°n nh·∫≠n ra c√≥ ƒë·ªô tr·ªÖ v·ªÅ I/O tr√™n c√°i New volume ƒë√≥ khi app ƒëc launch l·∫ßn ƒë·∫ßu. L√†m sao ƒë·ªÉ c√°c volume sau n√†y dc restore t·ª´ snapshot ko b·ªã issue ƒë√≥ n·ªØa?\n Ensure l√† all block tr√™n c√°i volume m·ªõi ƒë√≥ ph·∫£i dc access √≠t nh·∫•t 1 l·∫ßn\n1 New EBS volume th√¨ s·∫Ω lu√¥n maximum performance ngay t·ª´ ƒë·∫ßu m√† ko c·∫ßn initialization (hay c√≤n g·ªçi l√† pre-warming)\nNh∆∞ng c√°c storage blocks c·ªßa volume dc restore t·ª´ snapshot th√¨ ph·∫£i dc initialization, n·∫øu ko s·∫Ω c√≥ issue v·ªÅ performance trong l·∫ßn ƒë·∫ßu access\n Q. 1 Cty c√≥ set c√°c EC2 trong VPC. C·∫ßn scan xem c√≥ risk v·ªÅ security ko, check c·∫ßn ph·∫£i l√†m v·ªõi ‚ÄúCenter for Internet Security (CIS) Benchmarks‚Äù. N√™n d√πng service n√†o? Trusted Advisor, Inspector, Config, GuardDuty?\n Inspector\nHi·ªán gi·ªù Inspector ƒëang support nh·ªØng rule sau:\n¬∑ Common Vulnerabilities and Exposures\n¬∑ Center for Internet Security (CIS) Benchmarks\n¬∑ Security Best Practices\n¬∑ Runtime Behavior Analysis\nTrusted Advisor ƒë·ªÉ ƒë∆∞a ra recommendations,\nConfig ƒë·ªÉ config,\nGuardDuty ƒë·ªÉ qu·∫£n l√Ω threat detections\n Q. B·∫°n c√≥ 1 set c√°c EC2, ƒë·ªÉ gi·∫£m cost, th√¨ mu·ªën shutdown c√°c EC2 ko ƒëc s·ª≠ d·ª•ng, l√†m n√†o?\n t·∫°o Cloudwatch alarm, base on c√°c metric c·ª• th·ªÉ ƒë·ªÉ shutdown instance\n Q. Cty b·∫°n t·∫°o AWS Organization ƒë·ªÉ qu·∫£n l√Ω nhi·ªÅu account. V·ªõi nh·ªØng service m√† li√™n quan ƒë·∫øn critical v·ªÅ security th√¨ h·ªç mu·ªën ch√∫ng dc nottify cho Security Team t·ª´ nh·ªØng account trong org ƒë√≥. L√†m n√†o?\n D√πng AWS Personal Health Dashboard v√† trong m·ªói account s·∫Ω t·∫°o 1 CF template ƒë·ªÉ run Cloudwatch rule c·ªßa Org, Rule ƒë√≥ s·∫Ω notify Security Team m·ªói khi c√≥ critical alarm ƒë∆∞·ª£c generate ra.\n Q. B·∫°n c√≥ 1 set c√°c app tr√™n EC2 trong private subnet. C√°c app n√†y c·∫ßn d√πng Kinesis stream. L√†m n√†o ƒë·ªÉ ensure l√† c√°c app c√≥ th·ªÉ s·ª≠ d·ª•ng Kinesis stream?\n D√πng 1 VPC Endpoint interface\nVPC Endpoint interface dc cung c·∫•p b·ªüi Private link, ƒë·∫£m b·∫£o c√°c traffic ko ƒëi ra kh·ªèi Amazon network\n Q. C√¥ng ty b·∫°n ƒëang ch·ªçn g√≥i support c·ªßa AWS, c·∫ßn:\n‚ñ† 24x7 access v√†o customer service\n‚ñ† trong gi·ªù h√†nh ch√≠nh access v√†o Cloud Support Associate\n Ch·ªçn Developer l√† ƒë·ªß  Q. B·∫°n d√πng public key hay private key ƒë·ªÉ access v√†o EC2? key ƒë·ªÉ trong EC2 l√† public hay private?\n D√πng private key ƒë·ªÉ access v√†o EC2,\nKey trong EC2 l√† public key\n Q. B·∫°n ƒëang d√πng Storage Gateway ƒë·ªÉ extend data storage c·ªßa b·∫°n, y√™u c·∫ßu l√† all data c·∫ßn encrypt at rest th√¨ d√πng g√¨?\n KMS, ho·∫∑c S3 SSE\n Q. B·∫°n ƒëang d√πng Redshift ƒë·ªÉ manage data warehouse. Y√™u c·∫ßu l√† all data c·∫ßn transfer ƒë·∫øn Redshift qua 1 VPC. L√†m n√†o?\n Enable Amazon Redshift Enhanced VPC Routing\nkhi l√†m v·∫≠y, Redshift s·∫Ω force all COPY v√† UNLOAD traffic th√¥ng qua VPC.\n Q. Cty b·∫°n mu·ªën d√πng S3 d·ªÉ l∆∞u file, v√† l√†m ch√∫ng available nh∆∞ NFS file share ƒë·∫øn On-premise server. L√†m n√†o?\n D√πng File gateway\nAWS Storage Gateway cung c·∫•p, File-based, Volume-based, Tape-based, th√¨ c√°i File Gateway ch√≠nh l√† File-based.\nN√≥ cung c·∫•p access v√†o s3 object nh∆∞ l√† files/share mount point B·∫°n c√≥ th·ªÉ l∆∞u v√† retrieve S3 object s·ª≠ d·ª•ng NFS ho·∫∑c SMB (server message block)\n Q. Cty b·∫°n d√πng ElastiCache, b·∫°n ƒë√£ setup Memcahed cho h·ªç, Sau khi d√πng th√¨ h·ªç alert r·∫±ng s·∫Ω c√≥ th·ªÉ b·ªã high CPU utilization cho cluster. L√†m sao ƒë·ªÉ resolve issue ƒë√≥?\n scale up cluster b·∫±ng c√°ch d√πng 1 larger node type,\nho·∫∑c scale out cluster b·∫±ng c√°ch add th√™m nodes\nAWS Document vi·∫øt:\nC√≥ nhi·ªÅu lo·∫°i Cloudwatch metric monitor performance c·ªßa ElastiCache: CPUUtilization, SwapUsage, Evictions, CurrConnections.\n‚ñ† CPUUtilization \u0026gt; 90% th√¨ n√™n l√†m nh∆∞ tr√™n\n‚ñ† SwapUsage \u0026gt; 50MB th√¨ n√™n tƒÉng value c·ªßa parameter ConnectionOverhead\n‚ñ† Evictions l√† lo·∫°i engine metric. N·∫øu g·∫∑p th√¨ n√™n scale up cluster b·∫±ng c√°ch d√πng 1 larger node type, ho·∫∑c scale out cluster b·∫±ng c√°ch add th√™m nodes\n‚ñ† CurrConnections, n·∫øu g·∫∑p th√¨ n√™n xem l·∫°i app c·ªßa b·∫°n\n Q. C√¥ng ty b·∫°n c√≥ 1 set c√°c EC2 v√† c·∫£ c√°c server tr√™n on-premise env. B·∫°n c·∫ßn collect th√¥ng tin r·∫±ng c√°c server n√†y ƒë√£ install software g√¨? th√¨ l√†m n√†o? D√πng System Manager ƒë·ªÉ get software inventory, hay d√πng AWS Config?\n B·∫°n c√≥ th·ªÉ d√πng SSM ƒë·ªÉ query metadata ƒë·ªÉ bi·∫øt nh·ªØng software n√†o ƒë√£ dc install, c√°i server n√†o c·∫ßn update, ..\nD√πng AWS Config ch·ªâ c√≥ th·ªÉ check s·ª± thay ƒë·ªïi v·ªÅ configuration c·ªßa server, ch·ª© ko th·ªÉ bi·∫øt server ƒë√£ install g√¨.\n Q. 1 Cty mu·ªën d√πng Active Directory c√≥ s·∫µn ƒë·ªÉ grant quy·ªÅn cho user access v√†o AWS Console th√¨ chu·ªói action s·∫Ω l√† g√¨? (nghƒ©a l√† d√πng IAM Federated sign-in using AD)\n 1- User access v√†o portal sign-in page ƒë∆∞·ª£c cung c·∫•p b·ªüi AD FS (Active Directory Federation Service), r·ªìi login\n2- AD FS s·∫Ω authenticate user ƒë·ªëi v·ªõi AD\n3- AD tr·∫£ v·ªÅ User info, trong ƒë√≥ bao g·ªìm AD group membership\n4- AD FS d√πng group membership info ƒë·ªÉ create SAML redirect link ƒë·∫øn AWS STS\n5- AWS STS d√πng AssumeRoleWithSAML ƒë·ªÉ grant temporary credential cho user (session t·ª´ 15m ƒë·∫øn max 12h, default 1h) v√† tr·∫£ v·ªÅ\n6- User ƒë∆∞·ª£c authenticated v√† access v√†o AWS console\nKh√¥ng d√πng AssumeRoleWithWebIdentity v√¨ m√¨nh d√πng AD n√™n ph·∫£i ƒëi v·ªõi SAML\n Q. 1 Cty c√≥ DynamoDB. H·ªç mu·ªën ensure l√† c√≥ th·ªÉ recover dc trong TH accidental write or delete operation. L√†m n√†o? c√≥ d√πng Multi-AZ ko?\n D√πng Point-in-time recovery feature, b·∫°n c√≥ th·ªÉ recover v·ªÅ any point trong kho·∫£ng 35 ng√†y (backup retention period (th·ªùi gian l∆∞u backup) max 35 days)\nMulti-AZ l√† feature c·ªßa RDS th√¥i, ko ph·∫£i c·ªßa DynamoDB, DynamoDB c√≥ c√°i g·ªçi l√† global table\n Q. So s√°nh gi·ªØa Multi-AZ v√† Read Replicas c·ªßa RDS:\n Multi-AZ l√† Synchronous (ReadRep l√† async)\nMulti-AZ ch·ªâ primary instance l√† active (ReadRep th√¨ m·∫•y c√°i replica c√≥ th·ªÉ read)\nMulti-AZ th√¨ auto backup tr√™n con standby (ReadRep th√¨ default ko config)\nMulti-AZ th√¨ lu√¥n span 2 AZ trong region (ReadRep th√¨ c√≥ th·ªÉ 1 AZ, ho·∫∑c Cross-AZ, Cross-Region)\nMulti-AZ th√¨ DB engine upgrade tr√™n con primary (ReadRep th√¨ ko ph·ª• thu·ªôc)\nMulti-AZ th√¨ auto failover khi c√≥ v·∫•n ƒë·ªÅ (ReadRep th√¨ manual promote t·ª´ replica th√†nh 1 con db instance standalone)\n Q. B·∫°n c√≥ 1 set c√°c DynamoDB table. C·∫ßn ph·∫£i monitoring report v·ªÅ Read/Write Capacity c·ªßa DynamoDB ƒë·ªÉ utilized. L√†m n√†o?\n D√πng Cloudwatch metrics ƒë·ªÉ xem Read/Write capacity dc utilized\nCloudwatch cung c·∫•p c√°c metric ƒë·ªÉ monitor DynamoDB:\nN·∫øu mu·ªën monitor rate of TTL deletion on table? - monitor c√°i TimeToLiveDeletedItemCount\nN·∫øu mu·ªën xem dynamo table c·ªßa m√¨nh ƒëang c√≥ provisioned throughput l√† bao nhi√™u? - monitor c√°i ConsumedReadCapacityUnits ho·∫∑c ConsumedWriteCapacityUnits\nN·∫øu mu·ªën x√°c ƒë·ªãnh request n√†o v∆∞·ª£t qu√° limit c·ªßa throughput? - monitor c√°i ThrottledRequest, ReadThrottleEvents, WriteThrottleEvents\nN·∫øu mu·ªën x√°c ƒë·ªãnh c√≥ l·ªói system err ko? - monitor c√°i SystemErrors xem c√≥ l·ªói HTTP 500 (Server error) hay ko\n Q. Cty b·∫°n c√≥ 1 S3 bucket tr√™n EU region. Gi·ªù c√≥ y√™u c·∫ßu replicate c√°c object sang 1 region kh√°c, c·∫ßn ensurre ƒëi·ªÅu g√¨?\n C·∫£ source v√† destination bucket ƒë·ªÅu ph·∫£i enable versioning\nv√† t·∫•t nhi√™n l√† ch√∫ng ph·∫£i kh√°c AWS region\n Q. Cty b·∫°n c√≥ 2 account AWS, 1 c√°i cho Dev, 1 c√°i cho Prod, gi·ªù Dev c·∫ßn access v√†o Prod account ƒë·ªÉ update application th√¨ l√†m n√†o cho secure?\n T·∫°o 1 cross account role, r·ªìi share ARN cho Role\nTrong AWS Document c√≥ ghi:\n1- Trong Prod account, admin t·∫°o 1 role UpdateApp, trong trust policy c·ªßa role s·∫Ω ch·ªâ ƒë·ªãnh AWS account id c·ªßa Dev Account. Trong permission c·ªßa role th√¨ define nh·ªØng c√°i quy·ªÅn c·ª• th·ªÉ nh∆∞ write s3 bucket\u0026hellip;\n2- Trong Dev account, admin s·∫Ω grant quy·ªÅn cho group developer c√≥ quy·ªÅn AssumeRole, ƒë·ªÉ t·ª´ ƒë√≥ nh∆∞ng IAM trong group developer s·∫Ω c√≥ th·ªÉ switch role sang.\n3- User s·∫Ω switch Role tr√™n console, ho·∫∑c CLI\n4- STS s·∫Ω verify v√† return credential t·∫°m th·ªùi,\n5- T·ª´ ƒë√≥ User s·∫Ω t·ª± ƒë·ªông dc redirect sang console c·ªßa account Prod\n Q. Cty c√≥ 1 on-premise web, mu·ªën healthcheck li√™n t·ª•c, n·∫øu health b·ªã degrade th√¨ s·∫Ω switch sang 1 static website tr√™n aws, l√†m n√†o?\n D√πng Route53, health check to failover\n Q. B·∫°n c√≥ set c√°c EC2 trong private subnet (10.0.1.0/24), nh·ªØng EC2 n√†y c·∫ßn download update t·ª´ internet HTTPS. B·∫°n ƒë√£ setup NAT instance trong public subnet. Gi·ªù c·∫ßn setup SG cho NAT instance nh∆∞ n√†o?\n Inbound rule: cho ph√©p traffic t·ª´ private subnet IP qua port 443\nTheo AWS Document:\nSG c·ªßa NAT instance n√™n setting l√†:\n‚ñ† Inbound cho ph√©p traffic t·ª´ private subnet IP qua port 443/80\n‚ñ† Outbound cho ph√©p traffic t·ª´ port 80/443 ra 0.0.0.0/0 (ƒë·ªÉ cho NAT access internet)\nGi·∫£i th√≠ch th√™m: B·∫£n th√¢n con NAT ƒë√£ n·∫±m ·ªü public subnet r·ªìi, n√≥ ƒë√£ nh·∫≠n dc traffic t·ª´ World r·ªìi, ch·ªâ c·∫ßn config th√™m cho n√≥ dc nh·∫≠n traffic t·ª´ private subnet n·ªØa th√¥i\n Q. B·∫°n c√≥ 1 app host tr√™n aws. B·∫°n mu·ªën setup instance type hi·ªáu qu·∫£, s·∫Ω flexible khi spikes usage. N√™n ch·ªçn c√°i g√¨?\n T2.micro, Config T2 unlimited option\nV√≠ d·ª•: b·∫°n c√≥ 1 ec2 t3.large, baseline CPU c·ªßa n√≥ l√† 30%, n·∫øu n√≥ ch·∫°y trung b√¨nh ·ªü 30% ho·∫∑c √≠t h∆°n trong kho·∫£ng 24h, th√¨ s·∫Ω ko ph√°t sinh th√™m chi ph√≠. Nh∆∞ng n·∫øu run trung b√¨nh ·ªü m·ª©c 40% trong kho·∫£ng 24h ƒë√≥ th√¨ s·∫Ω m·∫•t th√™m ti·ªÅn 10% ƒë√≥.\n Q. B·∫°n plan host 1 batch process app tr√™n ec2. N√™n Instances lo·∫°i n√†o? General Purpose, Memory Optimized, Storage Optimized, Compute Optimized\n Compute Optimized\n Q. VPC c√≥ 2 subnet l√† public v√† private. Public subnet c·∫ßn 1 NAT gateway. Th√¨ config tr√™n main route table v√† custom route table c·∫ßn nh∆∞ n√†o? c√°c route table ·ªü trong private hay public subnet?\n main route table ph·∫£i ·ªü trong private subnet, target ƒë·∫øn NAT:\n0.0.0.0/0 target ƒë·∫øn NAT gateway\ncustom route table ph·∫£i ·ªü trong public subnet, target ƒë·∫øn IGW:\n0.0.0.0/0 target ƒë·∫øn IGW\n Q. 1 Cty l∆∞u document tr√™n s3. H·ªç b·ªã audit raise v·∫•n ƒë·ªÅ v·ªÅ non-maintaining access logs. B·∫°n d·ª± ƒë·ªãnh enable s3 server access log cho all buckets. C·∫ßn ch√∫ √Ω g√¨?\n Source v√† target bucket ph·∫£i l√† 2 bucket ri√™ng v√† c√πng trong 1 region\nAWS Document vi·∫øt:\nServer access logging ƒë·ªÉ track request v√†o bucket. B·∫°n ko t·ªën ti·ªÅn cho vi·ªác d√πng feature n√†y. T·∫•t nhi√™n v·∫´n tr·∫£ ph√≠ l∆∞u tr·ªØ, ph√≠ access v√†o xem log nh∆∞ c√°c request th√¥ng th∆∞·ªùng.\nsource bucket l√† bucket b·∫°n mu·ªën config ƒë·ªÉ track access log c·ªßa n√≥, (s·∫Ω config ph·∫ßn logging)\ntarget bucket l√† bucket l∆∞u access log c·ªßa source\ndefault encrpytion tr√™n target bucket l√† AES256 (S3-SSE), v√† ko th·ªÉ thay ƒë·ªïi sang c√°i kh√°c nh∆∞ SSE-KMS dc.\n Q. Cty b·∫°n c√≥ 1 Redshift cluster tr√™n EU region. H·ªç c·∫ßn ph·∫£i ensure l√† cluster s·∫Ω available tr√™n other region ƒë·ªÅ ph√≤ng TH primary cluster b·ªã isue do region. L√†m n√†o? C√≥ th·ªÉ Multi-AZ, Cross Region Read Replica, Global table hay ko?\n Readshift ko c√≥ m·∫•y t√≠nh nƒÉng tr√™n ƒë√¢u m√† ch·ªâ c√≥ th·ªÉ Config Redshift automatic/manual copy snapshot to secondary region.\nko th·ªÉ Multi-AZ hay Read replica nh∆∞ RDS dc\nc≈©ng ko th·ªÉ global table nh∆∞ DynamoDB dc\n Q. B·∫°n mu·ªën block public access v√†o all s3 buckets. L√†m n√†o? apply tr√™n bucket level hay object level?\napply theo region hay to√†n b·ªô region? apply cho current bucket hay future bucket?\n block public access s·∫Ω apply tr√™n bucket level or account level, tr√™n to√†n b·ªô region (globally), cho c·∫£ current v√† future bucket\n Q. B·∫°n ƒëang deploy 1 v√†i Cloudformation templates. Khi deploy g·∫∑p l·ªói sau:\nSender\nThrottling\nRate exceeded\nL√†m n√†o?\n add 1 exponential backoff gi·ªØa c√°c l·∫ßn call API createStack\nL·ªói x·∫£y ra do createStack API t·∫°o qu√° nhi·ªÅu resource c√πng 1 th·ªùi ƒëi·ªÉm. C·∫ßn ph·∫£i c√≥ delay gi·ªØa c√°c request ƒë√≥\n(B·∫°n ko th·ªÉ add pause trong CF template m√† ch·ªâ c√≥ th·ªÉ d√πng wait ho·∫∑c depend on condition th√¥i)\n Q. 1 c√¥ng ty Audit ƒëang audit your infra. H·ªç mu·ªën bi·∫øt lo·∫°i security n√†o ƒëang dc implement ·ªü data centre m√† host c√°c AWS resource c·ªßa b·∫°n? l√†m n√†o?\n D√πng document cung c·∫•p ·ªü AWS Artifact service\nAWS Artifact service l√† service cung c·∫•p c√°c security \u0026amp; compliance documents gi·ªëng nh∆∞ ISO cert, PCI, SOC reports\n Q. App c·ªßa b·∫°n d√πng RDS host database. Sau kho·∫£ng v√†i ng√†y, 1 table b·ªã drop accidently v√† c·∫ßn ph·∫£i retrieve ngay l·∫≠p t·ª©c trong v√≤ng 5 ph√∫t sau khi b·ªã drop. D√πng t√≠nh nƒÉng n√†o c·ªßa RDS? Multi-AZ hay Read replica hay Automated backup?\n RDS Automated backup\nRDS t·ª± ƒë·ªông backup DB instance theo period m√† b·∫°n ch·ªâ ƒë·ªãnh, v√† b·∫°n c√≥ th·ªÉ recover db v·ªÅ b·∫•t k·ª≥ th·ªùi ƒëi·ªÉm n√†o trong backup retention period ƒë√≥\nAWS Document vi·∫øt:\nKhi b·∫°n delete DB instance, n·∫øu ko ch·ªçn gi·ªØ l·∫°i backup th√¨ m·∫∑c ƒë·ªãnh s·∫Ω x√≥a h·∫øt c√°c b·∫£n backup lu√¥n\nBackup retention period, n·∫øu b·∫°n ko setup th√¨ m·∫∑c ƒë·ªãnh l√† 1 ng√†y n·∫øu b·∫°n t·∫°o DB instance qua CLI, API. L√† 7 ng√†y n·∫øu t·∫°o DB instance qua Console.\nAutomated backup ch·ªâ di·ªÖn ra v·ªõi DB trong tr·∫°ng th√°i Avalable.\nAutomated backup di·ªÖn ra h√†ng ng√†y. N·∫øu ko ch·ªâ ƒë·ªãnh th√¨ default l√† 30 ph√∫t backup windows, ch·ªçn ng·∫´u nhi√™n trong block 8h t√πy theo region (v√≠ d·ª•: virginia: 3-11h, tokyo: 13-21h)\n Q. B·∫°n mu·ªën setup EC2 trong VPC. Y√™u c·∫ßu l√† support higher bandwidth v√† higher packet per second (PPS) performance. L√†m n√†o? C√≥ d√πng EBS Optimized Instance type dc ko?\n D√πng Instance type lo·∫°i m√† support Enhanced Networking\nko d√πng EBS Optimized Instance type v√¨ n√≥ ch·ªâ d√πng ƒë·ªÉ tƒÉng performance cho EBS volumes\nAWS Document vi·∫øt:\nENA (Elastic Network Adapter) support tƒÉng t·ªëc network 100 Gbps: a1, c5, m5, t3, r5, p2, p3\nVF (Intel Virtual function) support tƒÉng t·ªëc network 10 Gbps: c3, c4, d2, i2, m4, r3\n Q. B·∫°n c√≥ 1 ASG trong ƒë√≥ c√≥ 1 s·ªë ec2, gi·ªù app ƒëang c√≥ issue v√† c·∫ßn debug th√¨ l√†m n√†o? c√≥ th·ªÉ d√πng AWS Config ƒë·ªÉ l·∫•y config snapshot c·ªßa ec2 ƒë·ªÉ ƒëi·ªÅu tra ko?\n C·∫ßn suspend 1 ho·∫∑c nhi·ªÅu scaling process ƒë·ªÉ ƒëi·ªÅu tra ec2\nko th·ªÉ d√πng AWS config v√¨ ƒëi·ªÅu ƒë√≥ l√† ko th·ªÉ\nAWS Document vi·∫øt:\nC√≥ 1 s·ªë scaling process trong ASG c√≥ th·ªÉ suspend nh∆∞:\nLaunch: add ec2 v√†o ASG,\nTerminate: remove ec2 kh·ªèi ASG,\nAddToLoadBalancer: add ec2 v√†o LB or target group,\nAlarmNotification: ch·∫•p nh·∫≠n th√¥ng b√°o c·ªßa Cloudwatch,\nAZRebalance: c√¢n b·∫±ng s·ªë l∆∞·ª£ng ec2 gi·ªØa c√°c AZ,\nHealthCheck: check health of ec2,\nReplaceUnhealthy: terminate ec2 v√† create c√°i m·ªõi,\nScheduledActions: th·ª±c hi·ªán scheduled scaling action\n Q. 1 c√¥ng ty c√≥ c√°c file m√† n·∫øu ƒë√£ dc verify th√¨ s·∫Ω ko c·∫ßn trong t∆∞∆°ng lai tr·ª´ TH c√≥ issue. N√™n l∆∞u ·ªü ƒë√¢u cho ti·∫øt ki·ªám? Glacier hay S3 RRS hay S3 IA\n Glacier v√¨ n√≥ r·∫ª h∆°n h·∫≥n\n‚ñ† D√πng S3-RRS (Reduced Redundancy Storage) n·∫øu b·∫°n mu·ªën l∆∞u nh·ªØng data ko quan tr·ªçng v√† n·∫øu m·∫•t th√¨ c≈©ng d·ªÖ t√°i t·∫°o l·∫°i ‚ñ† D√πng IA n·∫øu b·∫°n mu·ªën 1 ch·ªó l∆∞u long-term nh∆∞ng c√≥ th·ªÉ retrieve ngay l·∫≠p t·ª©c\n‚ñ† D√πng Glacier n·∫øu b·∫°n mu·ªën 1 ch·ªó l∆∞u long-term nh∆∞ng c√≥ th·ªÉ retrieve trong v√≤ng 3-5 gi·ªù\n Q. B·∫°n c·∫ßn capture all traffic flowing gi·ªØa client v√† server, b·∫°n plan to enable Access log tr√™n Application Load Balancer. C√≥ lo·∫°i traffic n√†o m√† Access log ALB ko capture l·∫°i?\n health check request t·ª´ ALB ƒë·∫øn target group s·∫Ω ko dc ALB access log capture l·∫°i\n Q. 1 Cty mu·ªën implement qu√° tr√¨nh deployment automatically t·∫°o LAMP stack, download latest PHP t·ª´ s3, setup ELB. N√™n d√πng service n√†o ƒë·ªÉ t·∫°o 1 orderly deployment of software?\nBeanstalk hay Opswork?\n AWS Elastic Beanstalk\n Q. Cty b·∫°n plan setup 1 performance-sensitive workload tr√™n aws. B·∫°n c·∫ßn setup ec2 cho hosting this workload. C·∫ßn ch√∫ √Ω g√¨ v·ªÅ config?\n Ch·ªçn EBS Optimized Instance\nHo·∫∑c Ch·ªçn Instance type with 10 Gigabit network connectivity\nkhi b·∫°n plan config EBS volume cho app, 2 c√°i tr√™n r·∫•t quan tr·ªçng khi b·∫°n mu·ªën stripe nhi·ªÅu volume l·∫°i trong RAID configuration\nC√≥ 1 list c√°c lo·∫°i instance type dc support EBS Optimized, ƒëa s·ªë dc enable by default, 1 s·ªë c·∫ßn ph·∫£i enable b·∫±ng tay.\nC√≥ 1 list c√°c instance type dc support Enhanced Networking l√™n ƒë·∫øn 10 Gbps bandwidth\n\u0026mdash;P3\u0026mdash;  Q. B·∫°n ƒëang setup DynamoDB, c√≥ 40 device ghi data m·ªói 10s, m·ªói data 1KB. V·∫≠y Write throughput set cho table l√† bao nhi√™u?\n 4WCU\n Q. Cty c·ªßa b·∫°n c√≥ VPN connection gi·ªØa AWS VPC v√† on-premise data centre. H·ªç c·∫ßn c√°c ec2 s·ª≠ d·ª•ng DNS server c·ªßa On-premise cho vi·ªác resolve DNS, l√†m n√†o?\n T·∫°o 1 DHCP Option set r·ªìi asign n√≥ v√†o VPC\nCh√∫ √Ω sau khi t·∫°o DHCP Option set r·ªìi th√¨ b·∫°n ko th·ªÉ modify n√≥. Mu·ªën th√¨ ph·∫£i t·∫°o c√°i m·ªõi.\n Q. Cty ƒëang qu·∫£n l√Ω MySQL DB tr√™n on-premise. H·ªç mu·ªën migrate l√™n AWS, h·ªç ko mu·ªën m·∫•t c√¥ng managed hay scaling DB th√¨ n√™n d√πng RDS hay Aurora?\n Aurora, v√¨ RDS v·∫´n c·∫ßn 1 ph·∫ßn qu·∫£n l√Ω manage v√† scale\nC√≤n Aurora l√† full-managed by AWS\n Q. C√¥ng ty b·∫°n mu·ªën c·∫Øt gi·∫£m cost th√¨ c√≥ n√™n consider vi·ªác ko s·ª≠ d·ª•ng Autoscaling group v√¨ n√≥ g√¢y cost ko?\n ko, v√¨ ASG ko ph√°t sinh cost\n Q. B·∫°n c·∫ßn ph·∫£i t·∫°o c√°c metric cloudwatch, c√°i n√†o sau ƒë√¢y l√† c√°i b·∫°n c·∫ßn ph·∫£i t·∫°o custom metric?\n CPU Utilization.\nNetwork Throughput.\nS·ªë bytes read/writte v√†o volume.\nDung l∆∞·ª£ng Disk storage c√≤n l·∫°i trong volume.\n????\nc√°i c·∫ßn t·∫°o custom metric l√† Dung l∆∞·ª£ng Disk storage c√≤n l·∫°i trong volume,\nnh·ªØng c√°i kia ƒë·ªÅu l√† default metric, ko c·∫ßn custom, ngay c·∫£ s·ªë l∆∞·ª£ng byte read/write v√†o volume c≈©ng default\n Q. Limit v·ªÅ ELB v√† DynamoDB table?\n ELB limit 20 LB m·ªói region, c√≥ th·ªÉ request ƒë·ªÉ tƒÉng\nDynamoDB limit 256 tables per account, c√≥ th·ªÉ request ƒë·ªÉ tƒÉng\n Q. C√¥ng ty b·∫°n ƒëang audit c·∫£ nh·ªØng AWS resource, h·ªç y√™u c·∫ßu compliance c·ªßa aws resources th√¨ l·∫•y ƒë√¢u?\n V√†o compliance portion c·ªßa AWS website ƒë·ªÉ l·∫•y all:\nhttps://aws.amazon.com/compliance/resources/\n Q. B·∫°n access dc v√†o web server qua bastion nh∆∞ng ko th·ªÉ access v√†o qua ELB, c·∫ßn ensure g√¨?\n ensure l√† ELB c√≥ attach v√†o public subnet ensure SG ho·∫∑c ACL c·ªßa ELB c√≥ allow inbound traffic\n Q. C√¥ng ty b·∫°n mu·ªën keep control cost, v√† mu·ªën nh·∫≠n alert cho billing th√¨ c·∫ßn enable g√¨ tr∆∞·ªõc khi t·∫°o alarm for estimated metric? C√≤n n·∫øu mu·ªën t·∫°o billng alarm th√¨ t·∫°o ·ªü ƒë√¢u?\n Enable billing alert ·ªü m·ª•c Account Preferences\nN·∫øu mu·ªën t·∫°o billing alarm th√¨ t·∫°o ·ªü Cloudwatch\n Q. B·∫°n c√≥ 1 web app static content c√≥ nhi·ªÅu truy c·∫≠p g√¢y b·ªã ch·∫≠m, n√™n l√†m n√†o? host tr√™n s3 hay d√πng read replica hay elastiCache?\n host tr√™n s3\napp b·ªã ch·∫≠m v√¨ b·ªã truy c·∫≠p nhi·ªÅu, app respone ko ph·∫£i v√¨ dynamic content dc fetch t·ª´ DB n√™n d√πng elastiCache ko h·ª£p l√Ω.\nread replica c≈©ng ch·ªâ d√πng ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ c·ªßa dynamic data th√¥i.\n1 static website g·ªìm c√°c static content, c√≥ th·ªÉ ch·ª©a client-side script.\n1 dynamic website th√¨ d·ª±a v√†o server-side process, script nh∆∞ PHP, JSP, .NET. S3 ko h·ªó tr·ª£ server-side scripting.\n Q. Team member c·ªßa b·∫°n launch 1 ec2 thu·ªôc lo·∫°i EBS-backed instance, xong v√†i tu·∫ßn sau th√¨ m·∫•t private key, ko th·ªÉ login v√†o dc n·ªØa. Gi·ªù l√†m n√†o?\n ch√∫ √Ω c√°c step sau ch·ªâ d√†nh cho EBS-backed instance, ko d√†nh cho instance store-backed instance:\n0- ƒë·∫ßu ti√™n, t·∫°o 1 keypair m·ªõi,\n1- launch 1 temp instance c≈©ng t·ª´ ami ƒë√≥,\n2- stop origin instance,\n3- detach root volume c·ªßa origin ra, attach v√†o temp instance d∆∞·ªõi d·∫°ng data volume,\n4- connect v√†o temp insntace,\n5- mount volume m·ªõi v√†o 1 folder t·∫°m ƒë·ªÉ modify file system c·ªßa volume ƒë√≥,\n5- modify authorized_keys: copy authorized_keys t·ª´ temp instance sang origin volume v·ª´a mount,\n6- move c√°i volume ƒë√≥ tr·ªü l·∫°i origin instance,\n7- restart instance.\n Q. B·∫°n l∆∞u billing report tr√™n 1 s3 bucket theo d·∫°ng csv. Gi·ªù c·∫ßn d√πng Athena ƒë·ªÉ query nh·ªØng report ƒë√≥. Th√¨ trong Athena ch·ªâ specify s3 bucket theo path n√†o?\n s3://bucketname/prefix/\nhay\ns3://bucketname/prefix/*\nPh·∫£i d√πng: s3://bucketname/prefix/\n Q. Team b·∫°n c√≥ 1 ASG v√† 1 ELB, trong ƒë√≥ c√≥ 1 set ec2. Sau khi cho ng d√πng th·ª≠ th√¨ th·∫•y l√† traffic ko distribute ƒë·ªÅu qua c√°c ec2. L√Ω do g√¨?\n sticky session ƒë√£ dc enable cho LB\nall subnets c√≥ ec2 ko dc register v·ªõi LB\nAWS Document vi·∫øt: sticky session s·∫Ω ensure l√† all request c·ªßa user trong su·ªët session s·∫Ω g·ª≠i d·∫øn c√πng 1 ec2\n Q. Kh√°c nhau gi·ªØa Dedicated Host v√† Dedicated Instance?\n EC2 - Dedicated Host:\nAWS cho b·∫°n to√†n quy·ªÅn tr√™n 1 con server v·∫≠t l√Ω lu√¥n, ko chia s·∫ª v·ªõi ai, cho b·∫°n to√†n quy·ªÅn control s·ªë l∆∞·ª£ng core, socket‚Ä¶, cho b·∫°n quy·ªÅn s·ª≠ d·ª•ng licence c√≥ s·∫µn c·ªßa b·∫°n (BYOL-bring your own licene), ko m·∫•t ti·ªÅn mua license c·ªßa AWS n·ªØa. B·∫°n c√≥ th·ªÉ run nhi·ªÅu instances tr√™n c√°i host ƒë√≥. C√°c instance trong host s·∫Ω c√≥ network performance cao.\nEC2 - Dedicated Instance:\nT·∫°o Ec2 tr√™n 1 con server v·∫≠t l√Ω, ƒë·∫£m b·∫£o duy nh·∫•t cho b·∫°n (tu√¢n th·ªß v·ªÅ m·∫∑t compliance)\n Q. B·∫°n c·∫ßn migrate Oracle 10g Release 2 v√† MySQL 5.6 l√™n AWS RDS read replica. C·∫ßn ch√∫ √Ω g√¨?\n RDS ko support Oracle version d∆∞·ªõi 12.1\n Q. B·∫°n d√πng RDS MySQL. Th√¨ ·ªü Maintenance Window s·∫Ω th·ª±c hi·ªán nh·ªØng activities v·ªÅ RDS?\n Update DB instance hardware,\nUpdate OS,\nUpdate DB engine version,\nPatching server.\n Q. Cty ƒëang d√πng RDS MySQL, gi·ªù ƒëang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ performance v·ªõi read request, l√†m n√†o?\nc√≥ n√™n enable Multi-AZ ko? add Read Replica? tƒÉng instance type c·ªßa DB instance?\n Add Read Replica (l√† scaling theo chi·ªÅu ngang).\nTƒÉng instance type c·ªßa DB instance (l√† scaling theo chi·ªÅu d·ªçc)\nVi·ªác enable multi-AZ ch·ªâ ƒë·ªÉ l√†m cho RDS c√≥ HA (high-avalability) th√¥i\n Q. S·ªë l∆∞·ª£ng Read Replica c·ªßa c√°c lo·∫°i DB: RDS, Aurora, DynamoDB?\n RDS c√≥ th·ªÉ c√≥ max 5 Read Replica,\nAurora c√≥ th·ªÉ c√≥ max 15 Read Replica.\nDynamoDB th√¨ ko c√≥ kh√°i ni·ªám Read Replica (ch·ªâ c√≥ global table: l√† 1 Read/Write Replica),\nDynamoDB DAX th√¨ c√≥ max 9 Read Replica\n Q. Team b·∫°n setup ASG ƒë·ªÉ launch ec2, nh∆∞ng ec2 ko launch n√™n b·∫°n c·∫ßn debug. L√Ω do c√≥ th·ªÉ l√† g√¨?\n _AZ ko c√≤n support\n_instance type ko c√≤n available\n_keypair c·ªßa ec2 ko t·ªìn t·∫°i\n_Security group (SG) ko t·ªìn t·∫°i\n_ko t√¨m th·∫•y ASG\n_b·∫°n ko subcribed c√°i service ƒë√≥\n_EBS ko support instance-store AMI\n_placement groups ko l√†m dc v·ªõi ec2 m1.large\n_client error\n Q. Cty b·∫°n host 1 app tr√™n ec2, app sau 1 ELB. B·∫°n c·∫ßn config Route53 v·ªõi company DNS ELB. S·ª≠ d·ª•ng lo·∫°i record n√†o c·ªßa R53? A, MX, ALIAS, AAAA record?\n D√πng ALIAS\ntheo AWS documents th√¨ ALIAS record ƒë·ªÉ d√πng cho c√°c service sau:\nCloudfront, Elastic Beanstalk, ELB, S3 bucket, another R53 record\nc√≤n c√°c l·ª±a ch·ªçn kh√°c:\n‚ñ† A record d√πng ƒë·ªÉ point 1 resource qua Ipv4 record\n‚ñ† MX record d√πng cho Mail Server record\n‚ñ† AAAA record d√πng ƒë·ªÉ point 1 resource qua Ipv6 record\n Q. 1 Cty c√≥ 1 critical app tr√™n VPC. Gi·ªù b·∫°n c·∫ßn 1 list c√°c connection dc t·∫°o ra tr√™n SSH port. L√†m n√†o ƒë·ªÉ r·∫ª v√† ti·ªán?\n D√πng Athena query S3 bucket n∆°i ch∆∞a VPC flow logs ƒë√£ save\nc√°ch 2 t·ªën c√¥ng h∆°n l√†: Export VPC flow logs ra CSV format r·ªìi filter port SSH\n Q. Cty mu·ªën launch c√°c ec2, ch√∫ng c·∫ßn ƒë·ªô tr·ªÖ th·∫•p, v√† network performance cao th√¨ l√†m n√†o?\n t·∫°o 1 placement group trong single AZ\nch·ªçn instance type lo·∫°i Enhanced Networking\n Q. 1 cty d√πng Aurora MySQL, h·ªç c·∫ßn d√πng Read Replica ƒë·ªÉ c·∫£i thi·ªán read performance, nh∆∞ng h·ªç mu·ªën Read Replica ·ªü tr√™n 1 region kh√°c (cross-region read replica) th√¨ c·∫ßn ch√∫ √Ω g√¨?\n Ch√∫ √Ω l√† b·∫°n c√≥ th·ªÉ t·∫°o 5 cross-region Read Replica (c·∫£ encrypted v√† unencrypted)\nAurora ko t·ª± ƒë·ªông t·∫°o cross-region replica, b·∫°n ph·∫£i t·∫°o manual\n Q. Cty b·∫°n c√≥ 1 set c√°c resource tr√™n AWS. Gi·ªù mu·ªën l√† t·∫•t c·∫£ nh·ªØng thay ƒë·ªïi c·ªßa resource ƒë·ªÅu c·∫ßn monitor l·∫°i. N√™n d√πng service g√¨? Cloudwatch? cloudtrail? Config?\n Cloudtrail v√† Config\nkh√¥ng d√πng Cloudwatch v√¨ n√≥ ko cung c·∫•p detail v·ªÅ nh·ªØng config thay ƒë·ªïi\nAWS config gi√∫p b·∫°n review nh·ªØng config thay ƒë·ªïi c·ªßa resource\nCloudtrail th√¨ cung c·∫•p monitor c√°c account activity tr√™n Aws acocunt b·∫°n\n Q. Cty b·∫°n c√≥ infra tr√™n aws. Gi·ªù h·ªç mu·ªën share project data store v∆°i 1 c√¥ng ty kh√°c (A), user c·ªßa c√¥ng ty A l√†m vi·ªác tr√™n linux v√† s·∫Ω access v√†o data ƒë√≥. C√°ch n√†o ƒë·ªÉ most secure v√† kh·∫£ thi nh·∫•t?\n D√πng EFS Mount Target, v·ªõi 1 Security Group m·ªü inbound NFS port cho c√¥ng ty A access qua AWS Direct Connect/ho·∫∑c VPN.\nEFS MOunt Target ch·ªâ dc access v·ªõi ƒëi·ªÅu ki·ªán:\n‚ñ† EC2 trong AWS VPC,\n‚ñ† EC2 trong VPC perring with other VPC,\n‚ñ† On-premise server c√≥ AWS Direct Connect ho·∫∑c AWS VPN to VPC\nCh√∫ √Ω l√† SG g·∫Øn v√†o Mount Target ph·∫£i m·ªü port NFS\n Q. Cty b·∫°n deploy 1 web app tr√™n US-east region. C√≥ 1 ALB config ƒë·ªÉ HA. C√≥ d√πng Cloudfront @edge. B·ªô ph·∫≠n Security y√™u c·∫ßu block 1 black list IP spam t·∫°i ƒëi·ªÉm farthest point from cloud infa. L√†m n√†o?\n D√πng AWS WAF, t·∫°o 1 Web ACL ƒë·ªÉ block IP ·ªü Global Region, v√† apply t·∫°i edge level Cloudfront\nAWS Documents c√≥ vi·∫øt:\nWAF c√≥ th·ªÉ d√πng ƒë·ªÉ block IP, pattern in HTTP header\u0026amp;body, SQL injection, URL String, cross-site scripting, geographical location.\nWAF c√≥ th·ªÉ l√†m ƒëi·ªÅu tr√™n ·ªü t·∫ßng Cloudfront edge at Global level, ho·∫∑c t·∫ßng Applicaion at regional level\nch√∫ √Ω v·ªÅ ACL WAF c√≥ kh√°i ni·ªám Web ACL, c√≤n VPC c√≥ kh√°i ni·ªám Network ACL\n Q. Cty b·∫°n ƒëang d√πng RDS Database. B·∫°n c·∫ßn d·ª± to√°n cost. C√°i n√†o b·∫°n c·∫ßn consider v·ªÅ cost khi d√πng RDS?\n S·ªë gi·ªù DB running, lo·∫°i DB instance type (small hay large\u0026hellip;)\nStorage m·ªói th√°ng\ns·ªë l∆∞·ª£ng I/O request m·ªói th√°ng\ns·ªë l∆∞·ª£ng data transfer khi sang Region kh√°c (ch·ª© trong c√πng region th√¨ data transfer l√† free)\n Q. B·∫°n c√≥ 1 set Ec2, y√™u c·∫ßu l√† t·∫•t c·∫£ EBS volume (root/data) c·∫ßn preserve khi instance b·ªã terminate, l√†m n√†o?\n set gi√° tr·ªã DeleteOnTerminationc·ªßa volume = False\nth√¨ khi ec2 b·ªã terminate th√¨ volume s·∫Ω ko b·ªã x√≥a theo, c√≤n n·∫øu ko set, default l√† c√°i ·ªï root dc set l√† true n√™n khi terminate ec2 n√≥ s·∫Ω b·ªã x√≥a theo.\n Q. Dev team ƒëang l√†m 1 web intranet, data content t·ª´ s3. B·∫°n c·∫ßn l√†m 1 access policy ƒë·ªÉ cho least quy·ªÅn v√†o s3 bucket. S·∫Ω c·∫ßn nh·ªØng g√¨?\n D√πng StringLike v√† aws:Referer ƒë·ªÉ allow s3:getObject action, Deny all other actions t·ª´ site intranet ƒë√≥.\nV·ªõi StringLike v√† aws:Referer, ch·ªâ nh·ªØng request dc kh·ªüi t·∫°o t·ª´ website ƒë√≥ (intranet) m·ªõi c√≥ th·ªÉ get object ƒëc.\nN·∫øu d√πng c√°i IPAddress v√† aws:SourceIp th√¨ nh·ªØng ip range ƒë√≥ v·∫´n c√≥ th·ªÉ get object tr·ª±c ti·∫øp t·ª´ s3 bucket\n{ \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Id\u0026quot;:\u0026quot;http referer policy example\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Sid\u0026quot;:\u0026quot;Allow get requests originating from www.example.com and example.com.\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;:\u0026quot;*\u0026quot;, \u0026quot;Action\u0026quot;:\u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::examplebucket/*\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;StringLike\u0026quot;:{\u0026quot;aws:Referer\u0026quot;:[\u0026quot;http://www.example.com/*\u0026quot;,\u0026quot;http://example.com/*\u0026quot;]} } } ] } R·ªìi ph·∫£i ensure l√† browser b·∫°n d√πng c√≥ include http referer header ·ªü trong request\n Q. App c·ªßa b·∫°n ƒëang mu·ªën d√πng SQS, y√™u c·∫ßu l√† c·∫ßn c√≥ 1 queue ri√™ng ƒë·ªÉ keep nh·ªØng msg m√† ko dc process success. D√πng feature g√¨?\n Dead-letter queue (DLQ)\n Q. Cty b·∫°n d√πng 1 s·ªë EC2, y√™u c·∫ßu l√† c√°c file log t·ª´ ec2 c·∫ßn ship l√™n s3 bucket ƒë·ªÉ sau n√†y analysis. D√πng service n√†o?\n AWS DataPipeline\nService n√†y gi√∫p b·∫°n automate qu√° tr√¨nh movement v√† transform data. V√≠ d·ª• log t·ª´ ec2 -\u0026gt; S3 -\u0026gt; EMR ƒë·ªÉ analisis\n Q. Cty b·∫°n d√πng S3 bucket l∆∞u docs. B·∫°n c·∫ßn t·∫°o policy apply cho nhi·ªÅu nh√¢n vi√™n ƒë·ªÉ h·ªç c√≥ quy·ªÅn get/put/delete object tr√™n folder c√° nh√¢n c·ªßa h·ªç tr√™n s3, l√†m n√†o?\n { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::test2019bucket/${aws:userid}*\u0026quot; } ] }  Q. C√¥ng ty b·∫°n ƒë√£ setup 1 dual tunnel VPN connection t·ª´ On-premise ƒë·∫øn AWS. Gi·ªù c·∫ßn l√†m 1 k·∫øt n·ªëi d·ª± ph√≤ng (redundant connection) trong TH VPN b·ªã fail. Th√¨ l√†m n√†o?\n T·∫°o th√™m 1 secondary VPN connection\nM·∫∑c d√π 1 VPN connection ƒë√£ c√≥ 2 tunnel ƒë·ªÉ ƒë·∫£m b·∫£o connection trong TH 1 c√°i b·ªã fail, th√¨ b·∫°n v·∫´n c·∫ßn t·∫°o th√™m 1 secondary VPN connection th√¨ m·ªõi ƒë·ªÅ ph√≤ng dc Tr∆∞·ªùng h·ª£p customer gateway b·ªã fail\n Q. B·∫°n ƒëang run 1 ec2 v√† gi·ªù mu·ªën tƒÉng type c·ªßa ec2, l√†m n√†o?\n stop, change type, start\n Q. B·∫°n c·∫ßn publish metric l√™n Cloudwatch, y√™u c·∫ßu l√† interval c·ªßa metric l√† 1s, l√†m n√†o, d√πng CLI hay Console?\n publish metric high resolution, via CLI (ch·ª© ƒë√¢y l√† custom metric th√¨ kh√¥ng th·ªÉ publish qua Console dc, ph·∫£i d√πng CLI)\n Q. T·∫°o billing alarm t·ª´ ƒë√¢u? Cloudwatch? Cost Explorer? T·∫°o billng alert th√¨ sao?\n T·∫°o billing alarm t·ª´ Cloudwatch\nT·∫°o billing alert t·ª´ Account Preferences\n Q. B·∫°n t·∫°o 1 SAML indentity provider, nh·ªØng attribute n√†o m√† SAML c·∫ßn cung c·∫•p cho AWS STS?\n Role, RoleSessionName, Audience, NameID\n Q. Cty b·∫°n c√≥ 1 set CMK keys ƒëc define trong KMS. B·∫°n mu·ªën control quy·ªÅn access c√°c key ƒë√≥ trong KMS th√¨ d√πng c√°i g√¨? KMS policies hay Key policies?\n Key policies, b·ªüi v√¨ ko c√≥ kh√°i ni·ªám KMS policies\n Q. B·∫°n ƒëang d√πng Cloudtrail log. B·∫°n mu·ªën ensure l√† c√°c file log n√†y ko b·ªã gia m·∫°o, l√†m n√†o?\n Enable t√≠nh nƒÉng log file integrity cho log files\n Q. C√¥ng ty b·∫°n c√≥ 1 vendor mu·ªën access v√†o AWS account c·ªßa cty b·∫°n. B·∫°n t·∫°o cho h·ªç 1 IAM user, gi·ªù mu·ªën t·∫°o 1 policy cho user ƒë√≥ th√¥i th√¨ d√πng policy n√†o? AWS Managed policy hay Inline policy?\n Inline policy th√¥i, khi b·∫°n t·∫°o inline policy th√¨ c√°i policy ƒë√≥ s·∫Ω ko dc s·ª≠ d·ª•ng cho user kh√°c, v√† khi b·∫°n x√≥a user th√¨ c√°i policy ƒë√≥ c≈©ng b·ªã x√≥a theo\n\u0026mdash;P4\u0026mdash;  Q. 1 cty c√≥ 1 set S3 bucket, v√† h·ªç lo s·ª£ data b·ªã stolen ho·∫∑c b·ªã virus ransomeware t·∫•n c√¥ng, l√†m n√†o? N√™n d√πng t√≠nh nƒÉng n√†o enable s3 versioning, hay enable s3 encryption?\n enable s3 versioning\nch·ª© c√°i s3 encryption th√¨ ko gi√∫p ch·∫∑n virus ransomeware dc.\n Q. B·∫°n mu·ªën b·∫≠t enable MFA delete s3 bucket versioning th√¨ c·∫ßn l√†m g√¨?\n D√πng root account credential v√† l√†m b·∫±ng CLI\nch·ª© c√°i MFA delete n√†y ko th·ªÉ d√πng IAM account v√† Console ƒë∆∞·ª£c\nNh·ªØng task c·∫ßn d√πng root credential: t·∫°o Cloudfront keypair, S3 MFA delete, change aws support plan (c√≥ nhi·ªÅu nh∆∞ng ch·ªâ n√™u 3 c√°i ƒë·∫∑c bi·ªát th·∫ø th√¥i)\n Q. B·∫°n upload kho·∫£ng 70 TB l√™n S3 Glacier nh∆∞ng khi upload th√¨ file name b·ªã thay ƒë·ªïi khi ƒë∆∞a l√™n Glacier, l√†m n√†o?\n 1_D√πng AWS Snowball upload l√™n s3 r·ªìi t·ª´ s3 move sang Galicer d√πng Lifecycle rule\n2_Upload l√™n S3 IA r·ªìi move sang Galicer d√πng Lifecycle rule\nKh√¥ng n√™n up tr·ª±c ti·∫øp l√™n S3 Glacier\nKh√¥ng n√™n d√πng Snowball Edge v√¨ t·ªën k√©m\n Q. Memory utilization c√≥ ph·∫£i metric built-in c·ªßa Cloudwatch ko?\n Kh√¥ng, Memory l√† custom metric\n Q. Cty mu·ªën t·∫•t c·∫£ nh·ªØng API call ƒë·∫øn EC2 dc log l·∫°i, v√† n·∫øu c√≥ c√°i Termination api dc call th√¨ s·∫Ω notify b·∫°n, l√†m n√†o?\n T·∫°o 1 trail trong Cloudtrail, trail s·∫Ω deliver log v√†o s3 bucket\nt·∫°o 1 lambda dc trigger khi s3 c√≥ object d·∫°ng terminate dc put v√†o, d√πng SNS ƒë·ªÉ notify\n Q. B·∫°n d√πng Cloudwatch metric DiskReadBytes cho EC2, nh∆∞ng sau v√†i ng√†y v√†o xem th·∫•y ch·ªâ hi·ªÉn th·ªã 0 bytes m·∫∑c d√π app v·∫´n ƒë·ªçc data ƒë·ªÅu, v√¨ sao?\n ‚ñ† C√≥ nh·ªØng lo·∫°i metric ch·ªâ d√πng cho instance store volumes, ko ph·∫£i EBS-backed volumes:\nDiskReadOps\nDiskWriteOps\nDiskReadBytes\nDiskWriteBytes\n‚ñ† C√≥ nh·ªØng lo·∫°i metric ch·ªâ c√≥ trong Basic monitoring:\nNetworkPacketsIn\nNetworkPacketsOut\n Q. 1 Cty c√≥ service qu·∫£n l√Ω asymmetric key, mu·ªën integrate v·ªõi aws th√¨ l√†m n√†o?\n D√πng CloudHSM cho asymmetric key ƒë·ªÉ integrate v·ªõi on-premise service\nCh·ª© KMS l√† d·ªãch v·ª• multi-tenant key storage \u0026amp; ch·ªâ support symetric key th√¥i.\ntr∆∞·ªõc ƒë√¢y t·ª´ng vi·∫øt:\nCloudHSM, b·∫°n t·ª± gen ra encryption key v√† qu·∫£n l√Ω key ƒë√≥\nEphemeral backup key (EBK) d√πng ƒë·ªÉ encrpyt data, Persistent backup key (PBK) ƒë·ªÉ encrpyt EBK sau ƒë√≥ save to S3 in same region v·ªõi CloudHSM.\n Q. Cy b·∫°n s·∫Ω t·∫°o v√†i RDS instance, b·∫°n mu·ªën t·∫°o alarm ƒë·ªÉ notify n·∫øu c√≥ 75% instance limit dc launch th√¨ l√†m n√†o?\n Enable Bussiness/Enterprise support plan v√† d√πng Trust advisor ƒë·ªÉ check limit r·ªìi alarm cloudwatch\nch√∫ √Ω l√† Developer support plan ko th·ªÉ l√†m dc\n Q. Cty b·∫°n mu·ªën l·∫•y trung b√¨nh CPU Utilization c·ªßa all EC2 c·ªßa nhi·ªÅu region, l√†m n√†o?\n Ko th·ªÉ t·∫°o aggregate c·ªßa cross region dc. Ph·∫£i l√† t·ª´ng region.\nV√† mu·ªën l√†m th√¨ ph·∫£i enable detailed monitoring\n Q. B·∫°n chu·∫©n b·ªã deploy 1 static web l√™n s3. D√πng Route53 b·∫°n ƒë√£ register domain v√† sub-domain r·ªìi. V·∫≠y s3 bucket c√≥ c·∫ßn ph·∫£i tr√πng t√™n v·ªõi domain hay sub-domain ko?\n C√≥, c·∫£ sub-domain v√† domain\n Q. B·∫°n b·ªã DDos attack l·ªõn, th√¨ n√™n d√πng c√°i g√¨ ƒë·ªÉ protect website?\n AWS shield advanced\nho·∫∑c WAF nh∆∞ng c√°i n√†y ko ch·ªëng dc DDos l·ªõn\n Q. Cty hi·ªán ƒëang setup Direct connect connection t·ª´ Onpremise data centre ƒë·∫øn 1 VPC us-west. Gi·ªù h·ªç l·∫°i c·∫ßn connect data centre ƒë·∫øn 1 VPC trong us-east. ƒê·∫£m b·∫£o cost-effective th√¨ d√πng g√¨? D√πng AWS Direct connect Gateway hay AWS Direct Connect?\n D√πng AWS Direct connect Gateway (tuy nhi√™n ko support connect vpc trong account kh√°c, ko support China, ko connect tr·ª±c ti·∫øp vpc v·ªõi vpc)\nAWS Direct Connect th√¨ ƒë·∫Øt h∆°n\n Q. N·∫øu ph·∫£i l·ª±a ch·ªçn CNAME v√† ALIAS record trong Route53?\n Lu√¥n ch·ªçn alias, Ch·ªçn Alias ch·ª© ko ch·ªçn CNAME record v√¨ Alias th√¨ c√≥ th·ªÉ t·∫°o cho c·∫£ root domain v√† sub domain, trong khi CNAME th√¨ ch·ªâ c√≥ th·ªÉ t·∫°o cho sub domain.\n Q. Cty b·∫°n c√≥ 1 app d√πng ƒë·ªÉ distribute patches. User s·∫Ω download patches update v·ªÅ. H·ªç complain v·ªÅ respone ch·∫≠m, l√†m n√†o? D√πng Cloudfront, ALB, CLB, Route53?\n Cloudfront, n√≥ s·∫Ω speed up c√°i t·ªëc ƒë·ªô respone\n Q. B·∫°n c√≥ 1 EC2 d√πng volume General Purpose SSD, gi·ªù mu·ªën tƒÉng IOPS c·ªßa volume th√¨ l√†m n√†o?\n gp2 max IOPS ch·ªâ 16000, io1 (provisioned IOPS th√¨ max 32000)\nho·∫∑c l√† ƒë·∫∑t volume trong RAID 0 Configuration (c√≥ th·ªÉ stripe c√°c volume l·∫°i tƒÉng th√™m IOPS)\n Q. B·∫°n d√πng EC2 v√† nh·∫≠n th·∫•y ƒë√¥i khi CPU credit b·ªã go down = 0. N√™n l√†m g√¨?\n change instance type cao l√™n\nho·∫∑c d√πng t2.unlimited, t3.unlimited\n Q. 1 ASG c√≥ c√°c EC2, b·∫°n nh·∫≠n th·∫•y c√°c ec2 m·ªõi launch ko dc li·ªát k√™ l√† 1 ph·∫ßn c·ªßa aggregated metric. L√†m n√†o?\n v√¨ c√°c instance m·ªõi launch v·∫´n c√≤n trong giai ƒëo·∫°n warming, giai ƒëo·∫°n ƒë√≥ s·∫Ω ph·∫£i expired xong m·ªõi dc t√≠nh v√†o metric trung b√¨nh\n Q. C√¥ng ty b·∫°n s·∫Øp dc audit th√¨ c·∫ßn c·∫•p quy·ªÅn access cho auditor nh∆∞ n√†o?\n t·∫°o IAM user c√≥ quy·ªÅn read log t·∫°o b·ªüi Cloudtrail trong s3\n Q. C√¥ng ty b·∫°n dev app mobile, user c·ªßa app ƒë√≥ c·∫ßn access v√†o resource trong aws. D√πng service n√†o? c√≥ d√πng dc AWS Federated Access ko?\n Cognito, IAM role\nko d√πng AWS Federated Access\n Q. Khi b·∫°n launch EC2 g·∫∑p c√°c l·ªói sau nghƒ©a l√† g√¨? InstanceLimitExceeded, InsufficientInstanceCapacity, Instance Terminates Immediately\n AWS Document m√¥ t·∫£:\n‚ñ† InsufficientInstanceCapacity: khi launch/restart EC2 th√¨ c√≥ th·ªÉ do AWS ƒëang ko ƒë·ªß available On-demand capacity cho b·∫°n\nGi·∫£i ph√°p: ch·ªù v√†i ph√∫t, ho·∫∑c chuy·ªÉn AZ kh√°c, instance type kh√°c, gi·∫£m l∆∞·ª£ng EC2 mu·ªën start\u0026hellip;\n‚ñ† InstanceLimitExceeded: v∆∞·ª£t qu√° limit s·ªë ec2 c√≥ th·ªÉ run trong 1 region, c·∫ßn request AWS\n‚ñ† Instance Terminates Immediately:\n_EBS volume limit has been reached.\n_EBS snapshot c·ªßa c√°i instance ƒë√≥ b·ªã corrupt (h·ªèng).\n_EBS root volume b·ªã encrypt v√† b·∫°n ko c√≥ quy·ªÅn access v√†o KMS ƒë·ªÉ decrypt key.\n_AMI m√† b·∫°n d√πng ƒë·ªÉ launch instance b·ªã thi·∫øu file n√†o ƒë√≥.\n Q. B·∫°n v·ª´a t·∫°o 1 RDS instance, app c·ªßa b·∫°n s·∫Ω config ƒë·ªÉ connect RDS. Nh∆∞ng t·ª´ app ƒëang b·ªã l·ªói ‚ÄúError connecting to database‚Äù V√¨ sao?\n Sai port\nSecurity groups sai\nRDS v·∫´n ƒëang t·∫°o v√† ch∆∞a available (c√≥ th·ªÉ ph·∫£i ch·ªù 20 ph√∫t)\n Q. C√¥ng ty b·∫°n c√≥ EC2, S3, PostgreSQL, EFS, gi·ªù mu·ªën encrpyt data at rest, nh·ªØng c√°i n√†o c√≥ th·ªÉ encrypt m√† ko l√†m gi√°n ƒëo·∫°n user d√πng?\n S3 th√¥i\nNh·ªØng c√°i c√≤n l·∫°i ƒë·ªÅu ph·∫£i encrpyt t·ª´ ƒë·∫ßu khi t·∫°o, gi·ªù mu·ªën th√¨ ph·∫£i t·∫°o l·∫°i\n Q. EFS c√≥ kh·∫£ nƒÉng scale l√™n peta byte ko?\n c√≥\n Q. B·∫°n c√≥ web server ·ªü 2 region us-east-1 v√† us-west-1, config routing policy l·∫ßn l∆∞·ª£t l√† latency-based v√† weighted-based. Khi test failover th√¨ n·∫øu all ec2 ·ªü us-west-1 b·ªã down th√¨ traffic ko shift sang us-east-1 v√¨ sao?\n Do thi·∫øu config Evaluate Target Health check = yes\nH√¨nh tr√™n n·∫øu Evaluate Target Health check = NO th√¨ Route53 s·∫Ω ko th·ªÉ quay l·∫°i c√°i ch·ªó branch cha ƒë·ªÉ n√≥ switch sang region kh√°c ƒë∆∞·ª£c\n Q. B·∫°n c√≥ AWS env v·ªõi EC2 cho web app, CLB, RDS, b·∫°n th·∫•y gi√° tr·ªã high-value SpilloverCount ·ªü trong metric c·ªßa Cloudwatch, b·∫°n n√™n monitor c√°i g√¨ ƒë·ªÉ ko c√≤n c√°i gi√° tr·ªã n√†y ?\n SpilloverCount l√† t·ªïng s·ªë request b·ªã reject do surge queue is full.\nGi·∫£i th√≠ch v·ªÅ 4 lo·∫°i metric c·ªßa LB ƒë√¢y:\nSurgeQueueLength: t·ªïng s·ªë request (HTTP listener) ho·∫∑c connection (TCP listener) ƒëang pending. Max c·ªßa queue l√† 1024. N·∫øu queue b·ªã full th√¨ request s·∫Ω b·ªã reject. S·ªë request b·ªã reject ƒë√≥ s·∫Ω hi·ªÉn th·ªã ·ªü metric SpilloverCount\nRequestCount: l√† s·ªë request completed trong 1 kho·∫£ng time nh·∫•t ƒë·ªãnh (1 or 5 ph√∫t)\nBackendConnectionErrors: s·ªë l∆∞·ª£ng connection fail khi connect gi·ªØa LB v√† Instances\n Q. Ph√¢n bi·ªát c√°c lo·∫°i ELB: CLB, NLB, ALB (c√°i n√†o c≈©ng c√≥ t√≠nh scalability):\n Classic Load Balancer: cung c·∫•p (HTTP, HTTPS or TCP listeners) cho only 1 backend port. Ch·∫°y b√™n ngo√†i VPC. ( N·∫øu app c·ªßa b·∫°n ch·∫°y ngo√†i VPC). Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\nNetwork Load Balancer: Layer 4, cung c·∫•p only TCP. D√πng EIP/static IP. Nhi·ªÅu ports. D√πng v·ªõi nh·ªØng app m√† ch·ªâ c·∫ßn s·ª≠ d·ª•ng TCP. V√† mu·ªën qu·∫£n l√Ω b·∫±ng whilelist IP. C√≥ th·ªÉ scale ƒë·ªÉ handle workload l√™n ƒë·∫øn h√†ng tri·ªáu request 1 gi√¢y (millions of requests per second)\nApplication Load Balancer: Layer7, cung c·∫•p c√¢n b·∫±ng t·∫£i cho only HTTP/HTTPS. Nhi·ªÅu ports. flexible h∆°n CLB. Nhi·ªÅu ng d√πng. Cung c·∫•p t√≠nh nƒÉng n√¢ng cao v·ªÅ routing ƒë·∫øn target, d√πng cho c√°c app hi·ªán ƒë·∫°i, micro-services, containers. Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\n Q. B·∫°n c·∫ßn t·∫°o stack Cloudformation, v√† mu·ªën ch·∫Øc ch·∫Øn b·∫°n c√≥ th·ªÉ s·ª≠a l·ªói b·∫±ng manually n·∫øu c√≥ th√¨ l√†m n√†o?\n Set gi√° tr·ªã OnFailure = DO_NOTHING khi create stack, ƒë·ªÉ khi x·∫£y ra l·ªói stack ko t·ª± ƒë·ªông rollback\n Q. B·∫°n d√πng cloudformation t·∫°o stack g·ªìm c√≥ c√°c resource trong ƒë√≥ c√≥ RDS, b·∫°n mu·ªën RDS dc gi·ªØ l·∫°i n·∫øu stack b·ªã delete, l√†m n√†o?\n D√πng DeletionPolicy attribute cho c√°c resource trong stack ·ªü trong file template (json/yml)\n Q. B·∫°n mu·ªën h·ªá th·ªëng c·ªßa b·∫°n tr√™n AWS khi m√† c√≥ issue v·ªõi hardware host AWS resource (nh·ªØng ph·∫ßn c·ª©ng host AWS resource c·ªßa b·∫°n) th√¨ b·∫°n dc notify, l√†m n√†o?\n Personal Health Dashboard\n Q. B·∫°n ƒëang d√πng RDS, mu·ªën encrpyt data in transit l√†m n√†o?\n D√πng SSL certificate m√† RDS cung c·∫•p\nEdit Parameter Group c·ªßa RDS instance (rds.force_ssl = true)\ncu·ªëi c√πng ph·∫£i Reboot RDS instance\nTrong AWS Document vi·∫øt: C√≥ 2 c√°ch d√πng SSL ƒë·ªÉ encrypt data in transit RDS l√†:\n‚ñ† Force SSL cho all connection (Client s·∫Ω ko c·∫ßn l√†m g√¨) C√°i n√†y s·∫Ω c·∫ßn s·ª≠a parameter group (rds.force_ssl = true)\n‚ñ† Encrpyt nh·ªØng connection c·ª• th·ªÉ (setup SSL tr√™n client computer)\n Q. B·∫°n d√πng S3, tr∆∞·ªõc ƒë√≥ l√† Cloudfront, b·∫°n g·∫∑p 1 s·ªë l·ªói 4xx error, th√¨ ƒë√≥ l√† v√¨ sao? l·ªói 5xx th√¨ sao?\n ‚ñ† l·ªói 4xx li√™n quan ƒë·∫øn client\n404-Not Found\n405-Method Not Allowed\n414-Request-URI Too Large\n‚ñ† l·ªói 5xx li√™n quan ƒë·∫øn server\n500-Internal Server Error\n501-Not Implemented\n502-Bad Gateway\n503-Service Unavailable\n504-Gateway Time-out\n Q. B·∫°n d√πng AWS CloudHSM cho SSL/TLS processing. C√≥ incident l√† c√≥ user x√≥a key c·ªßa CloudHSM l√†m ·∫£nh h∆∞·ªüng traffic, gi·ªù b·∫°n c·∫ßn check log c·ªßa CloudHSM ƒë·ªÉ x√°c ƒë·ªãnh user n√†o, l√†m sao? check field n√†o trong c√°c field sau: Time, Reboot Counter, Sequence No, Command Type, Opcode, Session Handle, Response, Log Type\n Opcode (n√≥ ch·ªâ ra action c·ªßa command nh∆∞ CN_CREATE_USER, CN_GENERATE_KEY, CN_DESTROY_OBJECT, CN_CHANGE_PSWD)\nNh·ªØng c√°i kh√°c th√¨:\nTime: th·ªùi gian in UTC,\nReboot Counter: chu·ªói 32-bit,\nSequence No: chu·ªói 64-bit,\nCommand Type: 2 category c·ªßa command (CN_MGMT_CMD, CN_CERT_AUTH_CMD),\nOpcode: action c·ªßa cmd,\nSession Handle: chu·ªói k√≠ t·ª±,\nResponse: SUCCESS ho·∫∑c ERROR,\nLog Type: 4 lo·∫°i MINIMAL_LOG_ENTRY, MGMT_KEY_DETAILS_LOG, MGMT_USER_DETAILS_LOG, GENERIC_LOG\n Q. Khi b·∫°n setup VPC peering gi·ªØa 2 vpc, th√¨ vi·ªác config routing CIDR gi·ªØa 2 VPC dc l√†m th·∫ø n√†o? t·ª± ƒë·ªông route table update khi VPC connection dc accept, hay ph·∫£i l√†m b·∫±ng tay?\n Ph·∫£i l√†m b·∫±ng tay (manually) ch·ª© AWS ko t·ª± ƒë·ªông config route table cho b·∫°n\n Q. B·∫°n ƒëang d√πng DynamoDB, gi·ªù mu·ªën gi·∫£m ƒë·ªô tr·ªÖ c·ªßa request g·ª≠i v√†o DynamoDB th√¨ n√™n d√πng g√¨? (Global table, secondary index, DAX, dynamodb stream?)\n N√™n d√πng DAX (c√≥ cache cho DynamoDB, HA)\nngo√†i ra nh·ªØng c√°i kh√°c th√¨:\nGlobal table (multi-region master db, c≈©ng l√† 1 c√°ch gi·∫£m ƒë·ªô tr·ªÖ)\nSecondary Indexes (tƒÉng performance khi query)\nDynamoDB stream (get notify n·∫øu c√≥ s·ª± thay ƒë·ªïi trong DynamoDB table)\n\u0026mdash;P5\u0026mdash;  Q. Cty b·∫°n d·ª± ƒë·ªãnh move MySQL l√™n RDS. H·ªç mu·ªën bi·∫øt l√† vi·ªác d√πng AWS RDS th√¨ s·∫Ω gi√∫p gi·∫£m c√¥ng t√°c qu·∫£n l√Ω nh∆∞ th·∫ø n√†o cho h·ªç?\n T·∫°o v√† maintain backup ƒë·ªÉ b·∫°n c√≥ th·ªÉ restore v·ªÅ any point in time (max 35 ng√†y tr∆∞·ªõc) v√¨ RDS n√≥ upload transaction log l√™n s3 m·ªói 5 ph√∫t\nT·ª± ƒë·ªông install critical security patch (b·∫£n v√° b·∫£o m·∫≠t cho db)\n Q. V·ªÅ VPC peering qua Ipv6 th√¨ c·∫ßn ch√∫ √Ω g√¨?\n VPC Peering qua IPv6 ch·ªâ support intra-region (trong 1 region), ch√∫ √Ω l√† ko dc overlaping IPv4 (ho·∫∑c ko c·∫ßn IPv4 ch·ªâ c√≥ IPv6 th√¨ ok)\nVPC Peering qua IPv6 kh√¥ng support inter-region (c√≤n g·ªçi l√† cross-region)\n Q. Web app c·ªßa b·∫°n d√πng EC2, SQS. Gi·ªù Server tr√™n on-premise c≈©ng c·∫ßn connect SQS. M√† b·∫°n c·∫ßn gi·∫£i ph√°p ƒë·ªÉ k·∫øt n·ªëi on-premise v·ªõi AWS m√† bandwitch l·ªõn, traffic secure th√¨ l√†m n√†o?\n T·∫°o interface endpoint trong private subnet, d√πng AWS Direct Connect ƒë·ªÉ n·ªëi v·ªõi On-premise server, r·ªìi t·ª´ ƒë√≥ interface endpoint (PrivateLink) s·∫Ω gi√∫p connect v·ªõi SQS 1 c√°ch secure\n(v√¨ interface endpoint s·∫Ω gi√∫p k·∫øt n·ªëi c√°c resource trong AWS v·ªõi nhau ko c·∫ßn internet)\nCh√∫ √Ω l√† interface endpoint ko support AWS VPN (th·∫ø n√™n d√πng AWS Direct Connect)\n Q. VPC A peering v·ªõi VPC B \u0026amp; C, B v√† C th√¨ c√≥ CIDR block gi·ªëng nhau (10.0.0.0/16). EC2 trong VPC B send traffic qua pcx-aaabbb ƒë·∫øn VPC A, VPC A tr·∫£ v·ªÅ respone ko v·ªÅ EC2 trong VPC B, m√† v·ªÅ 1 EC2 tr√πng IP trong VPC C, l√†m sao ngƒÉn ch·∫∑n vi·ªác n√†y?\n S·ª≠a c√°i Route table trong VPC A, th√™m c√°i routing ƒë·∫øn c·ª• th·ªÉ ip EC2 n√†o c·ªßa VPC B (more specific routing):\nho·∫∑c fix th√†nh:\n Q. Cty mu·ªën th·ª±c hi·ªán backup c√°c EBS volumes, v√¨ trong ƒë√≥ ch·ª©a data quan tr·ªçng, n√™n t·∫°o RAID 1 config of multi volume hay d√πng regular EBS snapshot?\n D√πng regular EBS snapshot\nCh·ª© RAID 1 Config ch·ªâ c√≥ t√°c d·ª•ng mirroring data (d√πng cho HA)\n Q. 1 Cty l∆∞u document trong S3. Mu·ªën add th√™m 1 layer security khi m√† User mu·ªën delete document S3, l√†m n√†o? c√≥ enable MFA Delete tr√™n S3 bucket dc ko?\n S·ª≠a bucket policy, d√πng c√°i condition key aws:MultiFactorAuthPresent, ƒë·ªÉ ch·ªâ nh·ªØng user ƒë√£ authen v√†o aws b·∫±ng MFA m·ªõi delete dc docs\nko th·ªÉ enable MFA Delete S3 bucket dc, v√¨ n√≥ ch·ªâ d√†nh cho root account\n Q. Cty s·∫Ω deploy 1 s·ªë ec2, workload c·ªßa c√°c ec2 ko ƒë·ªïi v·ªÅ CPU Utilization v√† thi tho·∫£ng s·∫Ω burstable performance. N√™n d√πng lo·∫°i ec2 type n√†o? gi·ªØa c√°c lo·∫°i T2 T3 M5 C5 R5\n ch·ªçn T2, T3, T3a (Ch√∫ng cung c·∫•p kh·∫£ nƒÉng Burstable Performance)\nT Unlimited th√¨ cung c·∫•p high CPU Performance\nM5 C5 R5 th√¨ cung c·∫•p Fixed Performance\n Q. 1 Cty mu·ªën t·∫°o connection hybrid gi·ªØa On-premise v√† AWS, m√† cost r·∫ª nh·∫•t v√† implement nhanh, th√¨ n√™n ch·ªçn c√°i g√¨? AWS VPN hay Direct Connect, hay Direct Connect Gateway?\n D√πng AWS VPN,\nv√¨ n√≥ r·∫ª h∆°n Direct Connect\nDirect Connect ch·ªâ n√™n d√πng khi mu·ªën low latency v√† b·∫°n gi√†u vl\n Q. Cty d√πng AWS Organization ƒë·ªÉ qu·∫£n l√Ω nhi·ªÅu account. Mu·ªën deny access v√†o S3 c·ªßa root user, ch·ªâ cho ph√©p IAM user th√¥i th√¨ l√†m n√†o?\n D√πng SCP t·ª´ t·∫ßng Master account r·ªìi deny access s3 c·ªßa root user\n Q. N·∫øu SCP (Service control policy) c·ªßa AWS Organizations cho ph√©p User access EC2, S3. Nh∆∞ng IAM Policy cho ph√©p User access EC2, S3, ELB th√¥i. Th√¨ User s·∫Ω d√πng dc nh·ªØng service n√†o?\n Th√¨ user s·∫Ω ch·ªâ c√≥ quy·ªÅn access service chung gi·ªØa 2 policy ƒë√≥ l√† EC2, S3\n Q. B·∫°n c·∫ßn setup EC2, EC2 s·∫Ω c√≥ nhi·ªÅu read/write activity. V√† metric cho volume c·∫ßn update m·ªói 2 ph√∫t.\nV·∫≠y th√¨ c·∫ßn ch·ªçn volume Provisioned IOPS\nhay Enable Detailed Monitoring c·ªßa EBS volume?\n Ch·ªçn Provisioned IOPS l√† ƒë·ªß, lo·∫°i io1 n√†y s·∫Ω t·ª± ƒë·ªông send metric 1 ph√∫t update ƒë·∫øn cloudwatch m√† ko c·∫ßn enable Detailed monitoring\n Q. B·∫°n d√πng VPN cho cty, b·∫°n ƒë√£ t·∫°o VGW v√† Customer Gateway, routing attach v√†o VPC. Nh∆∞ng Tunnel v·∫´n ƒëang b·ªã down. L√†m sao ƒë·ªÉ active VPN tunnel gi·ªØa VGW v√† Customer Gateway?\n mu·ªën active Tunnel th√¨ ph·∫£i init traffic t·ª´ Customer side ·ªü on-premise location t·ªõi server trong AWS VPC.\n Q. B·∫°n c√≥ 1 app setup tr√™n ec2 ELB. Mu·ªën ensure vi·ªác HA v√† traffic s·∫Ω dc distribute across c√°c backend node th√¨ l√†m n√†o?\n launch ec2 tr√™n MultiAZ\nenable cross zone load balancing cho ELB\nAWS Document vi·∫øt: Khi b·∫°n enable 1 AZ cho ELB. ELB s·∫Ω t·∫°o 1 load balancer node trong AZ ƒë√≥. M·∫∑c ƒë·ªãnh l√† m·ªói load balancer node ch·ªâ distribute traffic ƒë·∫øn c√°c target trong AZ c·ªßa n√≥ th√¥i. N·∫øu b·∫°n enable cross-zone load balancing th√¨ m·ªói load balancer node s·∫Ω distribute traffic across c√°c target tr√™n t·∫•t c·∫£ AZ ƒë√£ dc enable.\n Q. Cty b·∫°n thi·∫øt l·∫≠p AWS VPN gi·ªØa on-premise network v√† VPC. ƒê√£ t·∫°o Virtual Private Gateway, Customer Gateway, VPN connection. ƒê√£ config router ·ªü ph√≠a Customer side, v√† connection ·ªü Console ƒë√£ show UP. C√≤n thi·∫øu g√¨ n·ªØa?\n config router ·ªü ph√≠a Virtual Private Gateway\n Q. N·∫øu RDS th·ª±c hi·ªán patching OS th√¨ report v·ªÅ process vi·ªác patching ƒë√≥ s·∫Ω c√≥ ·ªü ƒë√¢u?\n RDS Console\n Q. B·∫°n th·∫•y c√≥ issue v·ªõi ASG, khi Launch Configuration update, ASG s·∫Ω l√†m v√†i node b·ªã offline l√†m ·∫£nh h∆∞·ªüng customer. Team d√πng Cloudformation ƒë·ªÉ update app b·∫±ng c√°ch change parameter. L√†m n√†o ƒë·ªÉ h·∫°n ch·∫ø ·∫£nh h∆∞·ªüng ƒë·∫øn customer?\n Trong CF template, add c√°i AutoScalingRollingUpdate trong UpdatePolicy attribute, enable WaitOnResourceSignals, append 1 c√°i healthcheck ·ªü cu·ªëi user data script ƒë·ªÉ signal cho ASG bi·∫øt l√† vi·ªác update instance ƒë√£ success.\n Q. B·∫°n mu·ªën Cloudformation template lu√¥n l·∫•y c√°i Windows AMI latest khi launch th√¨ l√†m n√†o?\n _C√°ch 1: subcribe Windows AMI noti cho SNS v√† trigger Lambda update template v·ªõi new AMI\n_C√°ch 2: trong Cf template th√¨ ch·ªó Parameter AMI ƒë·ªÉ ch·ªâ ƒë·ªãnh c√°i Parameter trong SSM Parameter Store (n∆°i ch·ª©a c√°c public parameter d√πng cho to√†n b·ªô AWS resource).\nTr∆∞·ªõc ƒë√≥ th√¨ ph·∫£i t·∫°o 1 Parameter trong SSM Parameter Store, value ki·ªÉu nh∆∞ sau: /aws/service/ami-windows-latest/Windows_Server-2016-English-Core-Containers\n Q. Cty b·∫°n c√≥ 1 bucket ƒë√£ versioning enable. Gi·ªù mu·ªën x√≥a delete bucket th√¨ l√†m n√†o?\n X√≥a b·∫±ng AWS SDK (nh∆∞ boto3)\nX√≥a b·∫±ng bucket lifecycle config\nX√≥a b·∫±ng Console\nC√≤n c√°c command n√†y th√¨:\naws s3 rb s3://example-bucket-july --force: c√°i n√†y ko x√≥a dc bucket ƒë√£ versioning\naws s3 rm s3:// example-bucket-july --recursive: c√°i n√†y ko x√≥a dc bucket ƒë√£ versioning\n Q. S3 c√≥ nh·ªØng file hi·∫øm khi access sau 45 days. C√°ch n√†o ƒë·ªÉ reduce cost m√† c√≥ th·ªÉ cung c·∫•p access v√†o file cho user t·ª´ 1-5 ph√∫t?\n D√πng Glacier Expedited Retrieval (1-5 min)\nS3 IA ƒë·∫Øt h∆°n Glacier n√™n ko ch·ªçn\n Q. Cty migrate 500 TB l√™n S3, ƒëang concern v·ªÅ cost, time, performance. D√πng g√¨? Storage Gateway hay Snowball?\n n√™n d√πng Snowball\nSnowball: cost 1562 $\nStorage Gateway: 11500 $\n Q. B·∫°n d√πng AWS Opswork m√† c·∫ßn monitor log c·ªßa app dc deploy tr√™n instance c·ªßa stack trong Opswork l√†m n√†o? c√≥ Opswork log ko?\n D√πng Cloudwatch logs\nko c√≥ c√°i g·ªçi l√† Opswork logs\n Q. B·∫°n c·∫ßn delete Customer Master Key th√¨ c·∫ßn check nh·ªØng g√¨?\n Check CMK permission trong KMS ƒë·ªÉ xem ai ho·∫∑c c√°i g√¨ ƒëang d√πng CMK ƒë√≥\nCheck Cloudtrail logs, ƒë·ªÉ xem l·ªãch s·ª≠ s·ª≠ d·ª•ng CMK ƒë√≥, xem c√°i CMK ƒë√≥ c√≥ c·∫ßn thi·∫øt n·ªØa ko\n Q. B·∫°n ƒëang d√πng Redshift th√¨ c√≥ nh·ªØng lo·∫°i log n√†o?\n 3 lo·∫°i:\n‚ñ† Connection log: nh·ªØng log authen, connect, disconnect\n‚ñ† User log: log m√† l√†m li√™n quan ƒë·∫øn db user nh∆∞: Create User, Drop User, Alter User\n‚ñ† User activity log: Log nh·ªØng query tr∆∞·ªõc khi n√≥ run tr√™n DB\nTuy nhi√™n th√¨ logging c·ªßa Redshift default disable. Mu·ªën logging th√¨ ph·∫£i enable l√™n.\nCh√∫ √Ω c√°i User activity log mu·ªën enable th√¨ ph·∫£i set enable_user_activity_logging=True n·ªØa\nko c√≥ Transaction Logs nh√©\n Q. Cty b·∫°n d√πng RDS MySQL, gi·ªù mu·ªën disable backup c·ªßa RDS b·∫±ng c√°ch set retention period=0 nh∆∞ng b·ªã l·ªói, v√¨ sao?\n V√¨ b·∫°n ƒëang setup Read Replica th√¨ ko set retention period = 0 ƒë∆∞·ª£c\nAWS Documents vi·∫øt:\n‚ñ† N·∫øu b·∫°n lock yourself ·ªü ngo√†i c√°i db_owner c·ªßa SQL DB. B·∫°n c√≥ th·ªÉ reset DB instance master password ƒë·ªÉ l·∫•y l·∫°i quy·ªÅn\n‚ñ† DB instance reboot v√† h·ªèi b·∫°n c√≥ Apply Immediately? khi b·∫°n:\n_thay ƒë·ªïi rentention period t·ª´ 0 ho·∫∑c t·ªõi 0,\n_change DB instance class.\n‚ñ† DB instance reboot ngay l·∫≠p t·ª©c khi b·∫°n:\n_change storage type (SSD, Standard)\n‚ñ† DB instance reboot theo ki·ªÉu b·∫Øt b·∫°n t·ª± l√†m manually khi:\n_change static param trong DB parameter group\n(ch·ª© n·∫øu change dynamic param th√¨ ko c·∫ßn reboot s·∫Ω take effect ngay)\n‚ñ† DB instance h·∫øt storage space, b·∫°n c·∫ßn modify tƒÉng storage l√™n\n‚ñ† N·∫øu b·ªã l·ªói InsufficientDBInstanceCapacity khi t·∫°o/s·ª≠a db, restore db t·ª´ snapsshot th√¨ n√™n:\n_th·ª≠ l·∫°i v·ªõi db instance class kh√°c\n_th·ª≠ l·∫°i v·ªõi AZ kh√°c, ho·∫∑c th·ª≠ l·∫°i m√† ko ch·ªâ ƒë·ªãnh AZ\n_1 s·ªë DB instance c·∫ßn ·ªü trong VPC\n‚ñ† MySQL version \u0026lt; 5.6 c√≥ th·ªÉ b·ªã l·ªói khi query: n√™n upgrade l√™n version MySQL \u0026gt; 5.6\n‚ñ† PostgreSQL l·ªói connection timeout th√¨ n√™n:\n_check hostname l√† DB instance enpoint ch∆∞a\n_check port number ƒë√∫ng ch∆∞a\n_check SG ƒë√∫ng rule ch∆∞a\n Q. Cty ƒëang d√πng Kinesis mu·ªën encrpyt data at rest th√¨ n√™n d√πng server-side hay client-side encryption?\n N√™n d√πng server-side v√¨ ƒë·ª° t·ªën effort h∆°n\n\u0026mdash;P:monitoring-reporting\u0026mdash;  Q. B·∫°n ƒëang setting AWS Config qua CLI, b·∫°n ƒë√£ t·∫°o S3 bucket t√™n 12345. B·∫°n ko nh·∫≠n dc th√¥ng b√°o v·ªÅ change configuration c·ªßa bucket ƒë√≥. V√¨ sao?\n C√°i s3 bucket policy attach v√†o bucket c·∫ßn allow AWS Config quy·ªÅn record l·∫°i nh·ªØng thay ƒë·ªïi\nC√°i IAM Role assign cho AWS Config CLI c·∫ßn c√≥ quy·ªÅn AWSConfigRole\n Q. Cty b·∫°n l√† global company. B·∫°n mu·ªën check xem RDS DB instance n√†o tr√™n kh·∫Øp c√°c region kh√¥ng dc enable multi-AZ, d√πng c√°i g√¨?\n AWS Config ho·∫∑c Trusted Advisor\n Q. Cty B·∫°n c√≥ 1 EC2 run app, b√™n audit y√™u c·∫ßu xem report nh·ªØng c√°i config change li√™n quan ƒë·∫øn con server ƒë√≥. L√†m n√†o?\n D√πng AWS Config Periodic rule 1h, 3h, 6h, 12h, 24h\nch√∫ √Ω l√† ko c√≥ 4h\n Q. Cty b·∫°n c√≥ infras tr√™n nhi·ªÅu regions. V√† c≈©ng c√≥ nhi·ªÅu AWS account. Gi·ªù mu·ªën t·ªïng h·ª£p report v·ªÅ resource count v√† compliance c·ªßa all account all regions th√¨ l√†m dc ko?\n D√πng AWS Config Aggregated view\n Q. AWS Config pricing nh∆∞ n√†o, tr·∫£ ti·ªÅn cho nh·ªØng c√°i g√¨?\n B·∫°n tr·∫£ ti·ªÅn cho t·ªïng s·ªë Config rule ƒëang active, S·ªë l∆∞·ª£ng Config items\n Q. C√°c member ƒëang t·∫°o 1 s·ªë l∆∞·ª£ng l·ªõn EC2. B·∫°n mu·ªën evaluate xem c√≥ bao nhi√™u m4.large EC2 n√™n b·∫°n ƒë√£ t·∫°o AWS Config custom rule + Lambda function. Rule t·∫°o ok, nh∆∞ng ·ªü section Compliance b·∫°n th·∫•y l·ªói No resources in scope V√¨ sao?\n C·∫ßn verify l·∫°i xem c√°i Custom rule b·∫°n t·∫°o c√≥ ƒëang record EC2 hay ko, ko th√¨ ph·∫£i add th√™m v√†o\nCh·ª© n·∫øu member t·∫°o to√†n t2.micro instance th√¨ s·∫Ω b√°o t·ªïng s·ªë Non-compliance count ch·ª© ko ph·∫£i l·ªói kia\n Q. S3 Glacier ƒë√£ ƒë∆∞·ª£c support b·ªüi AWS Config Managed Rule ch∆∞a? mu·ªën record S3 Glacier vault l√†m n√†o?\n Ch∆∞a, gi·∫£i ph√°p l√†:\n1_T·∫°o 1 lambda function ƒë·ªÉ evaluate S3 Glacier vault,\n2_R·ªìi t·∫°o 1 AWS Config Custom Rule v√† assign c√°i Lambda t·∫°o b√™n tr√™n cho rule n√†y\n Q. B·∫°n c√≥ 1 S3 bucket c·∫ßn monitoring v√† t·ª± correct n·∫øu c√≥ b·∫•t k·ª≥ policy violation th√¨ l√†m n√†o?\n Enable AWS Config ƒë·ªÉ monitor bucket ACL v√† policy violation\nN·∫øu detect c√≥ violation th√¨ s·∫Ω trigger Cloudwatch Event, r·ªìi trigger ti·∫øp Lambda ƒëi correct l·∫°i bucket ACL (ho·∫∑c set bucket ACL to private)\n\u0026mdash;P:storage-management\u0026mdash;  Q. B·∫°n c√≥ 1 l∆∞·ª£ng l·ªõn document tr√™n s3 bucket dc upload t·ª´ kho·∫£ng 10 regions tr√™n TG. ƒê·ªÉ analyse th√¨ b·∫°n d√πng AWS Athena + Quicksight, nh∆∞ng sau 1 th√°ng th·∫•y performance th·∫•p, cost nhi·ªÅu, gi·ªù l√†m n√†o?\n Khi CREATE TABLE, d√πng PARTITIONED BY ƒë·ªÉ partition base on time/date. B·∫°n s·∫Ω h·∫°n ch·∫ø dc l∆∞·ª£ng data dc scan b·ªüi query v√† gi·∫£m cost.\nKhi ch·ªçn storage format th√¨ ch·ªçn Apache Parquet v√† ORC - ƒë√¢y l√† d·∫°ng columnar storage format (thay v√¨ d√πng JSON,CSV) ƒë·ªÉ Athena retrieve data nhanh nh·∫•t\n Q. Cty c·∫ßn analyse data nhanh v√† ko c√≥ budget ƒë·ªÉ d·ª±ng server th√¨ d√πng g√¨?\n Athena + QuickSight\n Q. B·∫°n mu·ªën file dc encrpyt tr∆∞·ªõc khi ƒë∆∞a l√™n s3, t√¨m 1 gi·∫£i ph√°p full managed v√† cost-effective?\n KMS client-side\nKhi enable client-side encryption th√¨ b·∫°n c·∫ßn cung c·∫•p CMK ID cho KMS\nKhi mu·ªën upload 1 object to s3, b·∫°n d√πng CMK ID ƒë·ªÉ request KMS tr·∫£ v·ªÅ: plain-text key ƒë·ªÉ encrypt object, v√† cipher blob ƒë·ªÉ l√†m object metadata\n Q. 1 Cty c·∫ßn 1 n∆°i l∆∞u tr·ªØ key ki·ªÉu single tenant key storage v√† support asymetric key integration. D√πng g√¨? KMS hay AWS CloudHSM?\n D√πng CloudHSM\nCh·ª© KMS l√† d·ªãch v·ª• multi-tenant key storage \u0026amp; ch·ªâ support symetric key th√¥i.\n Q. B·∫°n t·∫°o 1 vault trong S3 Glacier ƒë·ªÉ l∆∞u data l√¢u d√†i, c·∫ßn ensure l√† ko c√≥ thay ƒë·ªïi ho·∫∑c deletion x·∫£y ra trong ƒë√≥. Nh∆∞ng v·∫´n c·∫ßn v√†o ƒë·ªçc nhi·ªÅu l·∫ßn. D√πng policy g√¨?\n Vault Lock Policy\n Q. B·∫°n c·∫ßn l∆∞u document 1 ch·ªó kho·∫£ng 5 nƒÉm m√† s·∫Ω ko cho b·∫•t c·ª© ai s·ª≠a. B·∫°n d√πng vault trong s3 glacier. B·∫°n c·∫ßn setup Vault Lock th√¨ c·∫ßn ch√∫ √Ω g√¨?\n Init Vault lock v·ªõi Vault Lock policy r·ªìi ƒëi v√†o tr·∫°ng th√°i in-progress\nPh·∫£i ho√†n th√†nh Vault Lock trong v√≤ng 24h sau khi Init\nSau khi vault ƒë√£ locked th√¨ ko th·ªÉ unlock\nN·∫øu ko ho√†n th√†nh Vault lock trong 24h th√¨ vault lock policy s·∫Ω b·ªã remove v√† s·∫Ω ph·∫£i l√†m l·∫°i\n Q. Trong c√°i GET URL sau s3.amazonaws.com/mybucket/photos/2014/08/puppy.jpg?x-user=johndoe th√¨ x-user=johndoe nghƒ©a l√† g√¨?\n S3 s·∫Ω ignore nh·ªØng query b·∫Øt ƒë·∫ßu b·∫±ng x-, tuy nhi√™n s·∫Ω l∆∞u l·∫°i trong access log record.\n Q. Ph√¢n bi·ªát aws gateway cached volume v√† aws gateway stored volume?\n Aws gateway cached volume: l∆∞u data tr√™n s3, 1 c·ª•c c·∫ßn access th∆∞·ªùng xuy√™n s·∫Ω l∆∞u locally. d·ªÖ scale, ƒë·ªô tr·ªÖ th·∫•p\nAws gateway stored volume: data s·∫Ω l∆∞u ho√†n to√†n ·ªü locally, r·ªìi snapshot s·∫Ω dc async l√™n s3\n\u0026mdash;P1-udemy\u0026mdash;  Q. Team member c·ªßa b·∫°n upload Django project l√™n Beanstalk. Nh∆∞ng g·∫∑p l·ªói The instance profile aws-elasticbeanstalk-ec2-role associated with the environment does not exist. V√¨ sao?\n Beanstalk CLI ch∆∞a th·ªÉ t·∫°o instance profile role n·∫øu IAM role c·ªßa n√≥ ko c√≥ quy·ªÅn t·∫°o role\nHo·∫∑c l√† instance profile role ƒë√£ t·∫°o nh∆∞ng b·ªã insufficient ho·∫∑c outdate v·ªõi nh·ªØng g√¨ m√† Beanstalk c·∫ßn\n Q. B·∫°n c·∫ßn d·ª±ng 1 web app m√† web server c·∫ßn connect to internet v√† Db server th√¨ ko, v·∫≠y n√™n d·ª±ng ELB nh∆∞ n√†o?\n internet-facing load balancer cho web server\ninternal load balancer cho db server\nN·∫øu c√≥ th√™m y√™u c·∫ßu db server c·∫ßn init Ipv4 traffic ra Internet v√† ch·∫∑n vi·ªác nh·∫≠n traffic t·ª´ internet th√¨ m·ªõi c·∫ßn NAT Gateway (or NAT instance),\nnh∆∞ng s·∫Ω c·∫ßn NAT ·ªü trong public subnet\n Q. Cty b·∫°n t·∫°o 1 VPC v·ªõi 1 subnet, s·∫Ω c√≥ 20 instance b√™n trong th√¨ CIDR block s·∫Ω ph·∫£i l√† 172.0.0.0/x, x s·∫Ω l√† bao nhi√™u 27 28 29 30?\n c√°ch t√≠nh l√† 2 step: Gi·∫£ s·ª≠ x = 27\n‚òÖ 32 - 27 = 5\n‚òÖ 2 m≈© 5 = 32 (suy ra VPC c·ªßa b·∫°n c√≥ 32 ip available)\n‚òÖ Tuy nhi√™n trong AWS, 4 ip ƒë·∫ßu ti√™n v√† 1 ip cu·ªëi c√πng trong m·ªói subnet b·∫°n s·∫Ω ko d√πng dc:\n-\u0026gt; 32 - 4 - 1 = 27 (s·ªë ip b·∫°n c√≥ th·ªÉ assign cho instance)\n‚òÖ Ch√∫ √Ω n·ªØa l√† block size dc cho ph√©p ch·ªâ l√† gi·ªØa /28 v√† /16 th√¥i\nTh·∫ø n√™n /29 v√† /30 l√† lo·∫°i ngay\n Q. B·∫°n c·∫ßn migrate data 80 TB l√™n s3 th√¨ n√™n d√πng g√¨ cho r·∫ª v√† nhanh? Snowball th∆∞·ªùng, Snowball Edge, Storage Gateway?\n Snowball th∆∞·ªùng th√¨ 1 c√°i appliance ch·ªâ usable 72 TB data th√¥i, nghƒ©a l√† ph·∫£i d√πng 2 c√°i appliances -\u0026gt; th√†nh ra gi√° ƒë·∫Øt h∆°n Snowball Edge\nSnowball Edge usable 80 TB data/100 TB n√™n ƒë·ªß -\u0026gt; n√™n d√πng\nStorage Gateway th√¨ m·ª•c ƒë√≠ch ch√≠nh c·ªßa n√≥ ko ph·∫£i ƒë·ªÉ migration, v·ªõi l·∫°i n√≥ s·∫Ω d√πng Internet ƒë·ªÉ upload n√™n ch·∫≠m\n Q. B·∫°n c·∫ßn monitor dashboard cho EC2 v√† on-premise server th√¨ c·∫ßn l√†m g√¨?\n IAM Role v√† User ƒë·ªÉ d√πng cho Cloudwatch Agent\nInstall CW agent tr√™n c·∫£ EC2 v√† On-premise server\nT·∫°o CW agent configure file\n Q. S3 bucket store data. B·∫°n c·∫ßn generate 1 report tr√™n replication v√† encryption status c·ªßa object. D√πng g√¨?\n S3 Inventory ƒë·ªÉ gen ra report\n Q. Ph√¢n bi·ªát v·ªÅ 2 policy: Latency routing v√† Geolocation routing c·ªßa Route53?\n Latency routing ch·ªâ ƒë·∫£m b·∫£o v·ªÅ term \u0026ldquo;latency\u0026rdquo; ch·ª© ko ƒë·∫£m b·∫£o traffic dc route ƒë·∫øn resource g·∫ßn nh·∫•t.\nTh·∫ø n√™n 1 s·ªë tr∆∞·ªùng h·ª£p V√≠ d·ª• nh∆∞ b·∫°n c√≥ 2 server ·ªü UK, US,\ny√™u c·∫ßu c·ªßa Compliance l√† data c·ªßa User ·ªü UK c·∫ßn ƒë·∫∑t ·ªü UK, data user ·ªü US c·∫ßn ƒë·∫∑t ·ªü US,\nth·∫ø th√¨ ph·∫£i d√πng Geolocation ƒë·ªÉ ƒë·∫£m b·∫£o user ·ªü ƒë√¢u th√¨ h·ªç s·∫Ω dc route ƒë·∫øn server ƒë·∫∑t data c·ªßa h·ªç ·ªü ƒë√≥\nNgo√†i ra th√¨ n·∫øu User ·ªü 1 ch·ªó kh√°c UK v√† US th√¨ b·∫°n c·∫ßn t·∫°o 1 record default ch·ª© n·∫øu ko R53 s·∫Ω respone \u0026ldquo;no answer\u0026rdquo;\nC√≤n c√°i Geoproximity th√¨ d√πng khi b·∫°n c√≥ 1 c√°i instance size r·∫•t l·ªõn ·ªü 1 region v√† mu·ªën route user ƒë·∫øn c√°i server ƒë√≥\nNgo√†i ra:\nMultivalue answer routing policy: route traffic 1 c√°ch ng·∫´u nhi√™n ƒë·∫øn max l√† 8 record healthy.\nSimple routing policy: D√πng 1 resource cho 1 domain.\nLatency routing policy: N·∫øu c√≥ nhi·ªÅu resource th√¨ s·∫Ω route traffic ƒë·∫øn location c√≥ best latency nh·∫•t.\nWeighted routing policy: route traffic ƒë·∫øn nhi·ªÅu resoure theo t·ªâ l·ªá b·∫°n defined.\nGeoproximity routing policy: route traffic d·ª±a tr√™n location c·ªßa resource c·ªßa b·∫°n.\nGeolocation routing policy: route traffic d·ª±a tr√™n location c·ªßa user.\nFailover routing policy: d√πng ƒë·ªÉ configure Active/passive failover.\n Q. B·∫°n c·∫ßn terminate ec2 trong ASG m√† kh√¥ng update size c·ªßa ASG th√¨ d√πng command n√†o?\n terminate-instance-in-auto-scaling-group --instance-id \u0026lt;value\u0026gt; --no-should-decrement-desired-capacity\nnh·ªõ l√† --no-should-decrement-desired-capacity ch·ª© ko ph·∫£i --should-decrement-desired-capacity\n Q. B·∫°n c·∫ßn host Oracle DB tr√™n Ec2 m√† data c≈©ng access ko th∆∞·ªùng xuy√™n l·∫Øm, ch·ªçn lo·∫°i EBS n√†o cho cost-effective?\n Cold HDD\nCh·ª© General purpose SSD m·ª•c ƒë√≠ch ch√≠nh ko ph·∫£i host db, t·ªën ti·ªÅn h∆°n\nProvisioned IOPS HDD ƒë·ªÉ host db nh∆∞ng t·ªën ti·ªÅn h∆°n\nThroughput Optimized HDD th√¨ cost c≈©ng th·∫•p nh∆∞ng ko th·∫•p b·∫±ng Cold HDD, v√† data ƒë√≥ l√† cho access th∆∞·ªùng xuy√™n\n Q. B·∫°n c·∫ßn setup Disaster Recovery infra, h·ªç c·∫ßn 1 gi·∫£i ph√°p quick recovery v√† cost effective, so s√°nh warm standby, backup \u0026amp; restore, pilot light?\n D√πng pilot light\nSo s√°nh:\nv·ªÅ gi√° th√¨ backup \u0026amp; restore \u0026lt; pilot light \u0026lt; warm standby\nv·ªÅ recovery time th√¨ c√†ng ƒë·∫Øt c√†ng recovery nhanh\npilot light th√¨ ch·ªâ nh·ªØng core element dc run all the time (db server replica ch·∫≥ng h·∫°n)\nwarm standby th√¨ gi·ªëng nh∆∞ h·ªá th·ªëng th·∫≠t nh∆∞ng l√† version b·ªã scale down v√† run all the time\nhttps://tutorialsdojo.com/aws-cheat-sheet-backup-and-restore-vs-pilot-light-vs-warm-standby-vs-multi-site/\n Q. B·∫°n c·∫ßn install v√† config app tr√™n 1 ec2 m√† b·∫°n s·∫Ω deploy b·∫±ng CF, s·∫Ω d√πng policy g√¨ trong c√°i CF template? UpdatePolicy, UpdateReplacePolicy, CreationPolicy ?\n D√πng CreationPolicy\nch·ª© d√πng UpdatePolicy khi b·∫°n mu·ªën update resource c·ªßa stack\nc√≤n d√πng UpdateReplacePolicy khi b·∫°n mu·ªën backup c√°i ec2 hi·ªán t·∫°i khi n√≥ s·∫Øp b·ªã replace b·ªüi update stack\n Q. N·∫øu khi launch EC2 b·ªã l·ªói EC2 instance \u0026lt;instance ID\u0026gt; is in VPC. Updating load balancer configuration failed nghƒ©a l√† sao?\n LB ƒëang thu·ªôc lo·∫°i EC2-Classic, ASG th√¨ ·ªü trong VPC\n Q. Kinesis v√† Redshift c∆° b·∫£n l√† l√†m g√¨?\n Kinesis l√† service ƒë·ªÉ collect data real-time\nc√≤n Redshift l√† service ƒë·ªÉ storage peta-byte data cho OLAP app (online analysis processing)\n Q. So s√°nh Snowball v√† Snowmobile?\n Snowmobile d√πng cho dataset t·∫ßm 10 PB (petabyte) tr·ªü l√™n.\nC√≤n nh·ªè h∆°n 10 PB th√¨ d√πng Snowball th∆∞·ªùng th√¥i\n Q. C·∫ßn 1 service ƒë·ªÉ ph√¢n t√≠ch behavior c·ªßa AWS resource t·ª´ ƒë√≥ x√°c ƒë·ªãnh nh·ªØng issue v·ªÅ security ti·ªÅm t√†ng th√¨ d√πng g√¨?\n AWS Inspector\n Q. AWS Trusted Advisor v√† AWS Performance Insights th√¨ c√°i n√†o gi√∫p b·∫°n ph√¢n t√≠ch issue v·ªÅ Database?\n AWS Performance Insights\n Q. B·∫°n c√≥ 1 volume Provisioned IOPS size l√† 10 GB, v·∫≠y th√¨ max IOPS c√≥ th·ªÉ set l√† bao nhi√™u?\n c√¥ng th·ª©c l√† IOPS:VolumeSize = 50:1\n=\u0026gt; 10 GB th√¨ max IOPS = 500 (input/output per second)\n Q. Cty c√≥ 1 VPC c·∫ßn connect v·ªõi on-premise network, th√¨ AWS resource c·∫ßn cung c·∫•p nh·ªØng g√¨?\n Internet Gateway attach to VPC\nVirtual Private Gateway\nvi·ªác t·∫°o EIP hay public IP, hay additional ENI (network interface) l√† ko c·∫ßn thi·∫øt\n Q. Route53 c√≥ th·ªÉ th·ª±c hi·ªán bao nhi√™u lo·∫°i healhcheck?\n 3 lo·∫°i:\nhealthcheck m√† monitor 1 enpoint\nhealthcheck m√† monitor nh·ªØng healthcheck kh√°c\nhealthcheck m√† monitor Cloudwatch alarm\n Q. B·∫°n c·∫ßn investigate h·ªá th·ªëng n√™n c·∫ßn suspend ASG scaling process t√™n l√† Terminate, khi ƒë√≥ th√¨ n√≥ s·∫Ω ·∫£nh h∆∞·ªüng nh∆∞ n√†o ƒë·∫øn AZ rebalance process c·ªßa ASG?\n ASG s·∫Ω c√≥ th·ªÉ tƒÉng l√™n max l√† th√™m 10% so v·ªõi maximum size\nC√≤n n·∫øu suspend AZRebalance process, khi ƒë√≥ n·∫øu scale-out th√¨ n√≥ v·∫´n c·ªë g·∫Øng rebalance AZ v·ªõi b·∫±ng c√°ch launch th√™m 1 s·ªë √≠t nh·∫•t EC2\nC√ín n·∫øu suspend Launch process, AZRebalance s·∫Ω ko x√≥a hay t·∫°o th√™m EC2\n Q. B·∫°n c√≥ 1 bucket S3 c·∫ßn delete. Bucket ƒë√≥ ƒë√£ dc enable versioning v√† MFA delete. B·∫°n c·∫ßn l√†m g√¨ ƒë·ªÉ x√≥a?\n C√°ch 1_Remove c√°i bucket policy m√† y√™u c·∫ßu MFA delete, R·ªìi d√πng SDK ƒë·ªÉ x√≥a bucket\u0026rsquo;s markers v√† version, R·ªìi x√≥a l·∫°i b·∫±ng CLI command\nC√°ch 2_D√πng root account ƒë·ªÉ suspend MFA v√† versioning feature ƒëi. R·ªìi Config Lifecycle ƒë·ªÉ remove version, x√≥a object v√† markers, r·ªìi delete bucket again\n\u0026mdash;P2-udemy\u0026mdash;  Q. ƒê·ªÉ 1 EC2 c√≥ th·ªÉ communicate ra Internet qua IPv6 th√¨ VPC c·∫ßn config nh∆∞ n√†o?\n _associate /56 CIDR block cho VPC\n_associate /64 CIDR block cho subnet\n_t·∫°o custom route table r·ªìi associate cho subnet\nVi·ªác setup egress-only gateway ch·ªâ ƒë·ªÉ t√°c d·ª•ng l√† VPC ipv6 ch·∫∑n connect t·ª´ internet v√†o trong\n Q. B·∫°n c√≥ 1 S3 bucket m√† d·∫°o n√†y user complain l√† to√†n l·ªói 503. V√¨ sao?\n C√≥ object n√†o ƒë√≥ c√≥ h√†ng tri·ªáu version, khi ƒë√≥ s3 s·∫Ω throttle c√°c request ƒë·∫øn bucket\nGi·∫£i ph√°p l√† d√πng S3 inventory tool ƒë·ªÉ x√°c ƒë·ªãnh object n√†o\n Q. B·∫°n c√≥ 3 VPC v√† mu·ªën ensure l√† c√°c ec2 trong ƒë√≥ c√≥ th·ªÉ communicate v·ªõi nhau th√¨ l√†m n√†o?\n Setup VPC Peering full mesh config cho c·∫£ 3 VPC (full mesh nghƒ©a l√† 3 c√°i VPC s·∫Ω peering t·ª´ng c·∫∑p v·ªõi nhau)\nEnsure l√† all route table c·ªßa m·ªói VPC dc setup\nENsure l√† VPC ko b·ªã overlapping IPv4 CIDR block\n Q. B·∫°n d√πng PostgreSQL run tr√™n EC2, n√≥ s·∫Ω dc d√πng b·ªüi nhi·ªÅu internal app trong VPC c·ªßa b·∫°n. B·∫°n mu·ªën ƒë∆°n gi·∫£n vi·ªác naming convention b·∫±ng c√°ch ƒë·∫∑t 1 custom domain name cho c√°i DB server, l√†m n√†o?\n setup private hosted zone trong Route53.\nT·∫°o A ho·∫∑c AAAA record,\nspecify IP c·ªßa DB server v√†o record ƒë√≥\n Q. B·∫°n c·∫ßn 1 tool ƒë·ªÉ c√≥ th·ªÉ coordinate nhi·ªÅu AWS resources v·ªõi nhau, v√†o 1 serverless workflow, d√πng c√°i g√¨? SWF hay Step Function?\n D√πng Step Functions\nCh·ª© SWF l√† service ƒë·ªÉ track state v√† task, n√≥ c≈©ng ko ph·∫£i l√† serverless orchestration cho nhi·ªÅu resource\n Q. B·∫°n mu·ªën enable Cloudwatch Detailed monitoring cho Auto Scaling Group th√¨ c√≥ ƒë∆∞·ª£c ko?\n c√≥, nh∆∞ng s·∫Ω b·ªã t√≠nh ph√≠\nN·∫øu Basic monitoring cho ASG th√¨ ko t√≠nh th√™m ph√≠\nCloudwatch monitoring support cho h·∫ßu h·∫øt c√°c AWS Services\n Q. Cty b·∫°n c√≥ nhi·ªÅu thi·∫øt b·ªã ƒëo nhi·ªát ƒë·ªô mu·ªën g·ª≠i data custom metric l√™n AWS Cloudwatch ƒë·ªÉ xem tr√™n AWS 1 c√°ch graph visually, th√¨ c√≥ dc ko?\n D√πng CLI or API ƒë·ªÉ upload data metric l√™n Cloudwatch\n Q. Cty b·∫°n d√πng heavily used RDS, gi·ªù mu·ªën ch·∫Øc ch·∫Øn r·∫±ng ko c√≥ outage khi th·ª±c hi·ªán snapshot cho DB th√¨ l√†m n√†o? Multi-AZ hay Read replica?\n Design DB ki·ªÉu Multi-AZ v√† Snapshot s·∫Ω th·ª±c hi·ªán tr√™n con Standby\nc√≤n n·∫øu Read Replica th√¨ m·∫∑c d√π c≈©ng ko c√≥ outage nh∆∞ng m√† do replica lag n√™n c√≥ th·ªÉ c√≥ nhi·ªÅu transaction ch∆∞a k·ªãp ƒë·ªìng b·ªô t·ª´ con master sang con replica t·∫°i th·ªùi ƒëi·ªÉm ƒë√≥ (ƒë√¢y l√† heavily used RDS v√† ko quan t√¢m cost)\n Q. B·∫°n c√≥ 2 subnet trong VPC, EC2 trong 1 subnet ko ping dc EC2 c√≤n l·∫°i v√¨ sao?\n C·∫ßn check l·∫°i config c·ªßa SG\nv√† check Network ACL c√≥ cho ph√©p ICMP traffic ko\n Q. B·∫°n th·∫•y website c·ªßa b·∫°n c√≥ CPU usage tr√™n EC2 instance 90% v√† CPU usage DB ch·ªâ 20% n√™n l√†m g√¨ ƒë·ªÉ c·∫£i thi·ªán respone time?\n ƒê·ªïi lo·∫°i EC2 instance type kh√°c,\nConfig cho ASG c·ªßa EC2 s·∫Ω base tr√™n CPU load ƒë·ªÉ scale out\n Q. S3 bucket Access control list (ACL) c√≥ nh·ªØng lo·∫°i quy·ªÅn g√¨?\n READ\nWRITE\nREAD_ACP\nWRITE_ACP\nFULL_CONTROL\n Q. Khi b·∫°n c·∫ßn apply c√°i s3 server side encryption cho bucket c·ªßa b·∫°n, v√† b·∫°n s·∫Ω s·ª≠ d·ª•ng c√°i key c·ªßa b·∫°n th√¨ trong header c·ªßa reuqest c·∫ßn c√≥ g√¨?\n 1_ x-amz-server-side‚Äã-encryption‚Äã-customer-algorithm (n·∫øu d√πng c√°i n√†y th√¨ value ph·∫£i l√† AES256)\n2_ x-amz-server-side‚Äã-encryption‚Äã-customer-key\n3_ x-amz-server-side‚Äã-encryption‚Äã-customer-key-MD5\n Q. network packet sniffing thu·ªôc tr√°ch nhi·ªám c·ªßa Customer hay AWS?\n AWS ch·ªãu tr√°ch nhi·ªám protect ng∆∞·ªùi d√πng kh·ªèi vi·ªác ƒë√°nh c·∫Øp c√°c g√≥i tin network\n Q. Cty b·∫°n c√≥ 1 app host tr√™n EC2 v√† ALB, C√≥ 1 member thay ƒë·ªïi ALB d·∫´n ƒë·∫øn app b·ªã down, c·∫ßn monitoring changes dc t·∫°o ra tr√™n ALB, th√¨ c√°i ELB healthchecks c√≥ ph·∫£i l√† service gi√∫p monitor info c·ªßa ELB ko?\n ko, ELB healthchecks ch·ªâ check info c·ªßa EC2 c√≥ health hay ko th√¥i\n Q. Cloudfront c√≥ offer t√≠nh nƒÉng monitoring ko?\n C√≥, monitor, report, access log ƒë·ªÉ tracking activity\n Q. B·∫°n mu·ªën S3 t·ª± ph√¢n t√≠ch storage v√† khi m√† l∆∞·ª£ng access v√†o content √≠t th√¨ s·∫Ω transit ch√∫ng v√†o S3-IA, sau ƒë√≥ t·∫°o lifecycle policy ƒë·ªÉ transit ti·∫øp v√†o Glacier, d√πng g√¨?\n S3 analytics\n Q. B·∫°n c√≥ nh·ªØng RDS db instances, trong 1 VPC. C·∫ßn l√†m n√†o ƒë·ªÉ make maximum security cho db v√† db connection? C√≥ n√™n d√πng Db security group ko?\n t·∫°o snapshot c·ªßa db instance r·ªìi start 1 db m·ªõi dc encrypted t·ª´ b·∫£n snapshot ƒë√≥\nd√πng SSL cho RDS ƒë·ªÉ encrypt traffic\nt·∫°o ri√™ng VPC SG cho EC2 v√† VPC SG cho DB\n*ko n√™n d√πng DB SG v√¨ ƒë√≥ l√† EC2-clasic platform, ko ph·∫£i VPC platform\n Q. Auto Scaling Group ASG gi√∫p b·∫°n t·ª± ƒë·ªông scale c√°c lo·∫°i resource n√†o? c√≥ th·ªÉ scaling ra 1 region kh√°c ko?\n EC2,\nEC2 spot fleet,\nECS,\nDynamoDB,\nv√† Aurora\n*ko th·ªÉ l√†m v·ªõi resource ·ªü region kh√°c\n Q. S·ª± kh√°c bi·ªát gi·ªØa HTTP, TCP?\n ‚òÖ Tr∆∞·ªõc khi b·∫°n d√πng ELB, b·∫°n c·∫ßn config 1 or nhi·ªÅu listener cho CLB c·ªßa b·∫°n, listener c√≥ nhi·ªám v·ª• check c√°c request connection. Listener dc config v·ªõi 1 protocol v√† port cho LB c·ªßa front-end, 1 protocol v√† port cho LB c·ªßa back-end\n‚òÖ ng ta d√πng SSL protocol ƒë·ªÉ secure cho HTTP layer (khi ƒë√≥ g·ªçi l√† HTTPS protocol)\n‚òÖ c≈©ng c√≥ th·ªÉ d√πng SSL protocol d√πng ƒë·ªÉ secure cho TCP layer\n‚òÖ N·∫øu frontend d√πng TCP ho·∫∑c SSL th√¨ backend c≈©ng c√≥ th·ªÉ d√πng TCP ho·∫∑c SSL\n‚òÖ N·∫øu frontend d√πng HTTP ho·∫∑c HTTPS th√¨ backend c≈©ng c√≥ th·ªÉ d√πng HTTP ho·∫∑c HTTPS\n‚òÖ Khi d√πng TCP (layer 4) cho c·∫£ frontend v√† backend th√¨: LB s·∫Ω forward request ƒë·∫øn backend m√† kh√¥ng modify headers\n‚òÖ Khi d√πng HTTP (layer 7) cho c·∫£ frontend v√† backend th√¨: LB s·∫Ω modify (ho·∫∑c g·ªçi l√† parse) headers c·ªßa request, r·ªìi terminate connection, r·ªìi m·ªõi forward request ƒë·∫øn backend\n Q. Kh√°ch h√†ng ƒëang s·ª≠ d·ª•ng ELB, ƒë·ªÉ enhance security n√™n b·∫°n mu·ªën config ELB v·ªõi SSL d√πng 1 policy ƒë∆∞·ª£c define tr∆∞·ªõc qu√° tr√¨nh negotiate SSL connection gi·ªØa ELB v√† client. D√πng c√°i g√¨ ƒë·ªÉ ensure l√† ELB s·∫Ω quy·∫øt ƒë·ªãnh cipher n√†o s·∫Ω ƒë∆∞·ª£c d√πng cho SSL connection, (ch·ª© ko ph·∫£i tu√¢n theo th·ª© t·ª± cipher t·ª´ ph√≠a client) ?\n Server Order Preference\nDefault th√¨ c√°i cipher ƒë·∫ßu ti√™n trong client\u0026rsquo;s list m√† match v·ªõi 1 cipher n√†o ƒë√≥ c·ªßa ELB\u0026rsquo;s list s·∫Ω ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ d√πng cho SSL connection.\nNh∆∞ng n·∫øu ELB ƒë∆∞·ª£c config v·ªõi Server Order Preference th√¨ c√°i cipher ƒë·∫ßu ti√™n c·ªßa ELB\u0026rsquo;s list m√† match v·ªõi 1 cipher n√†o ƒë√≥ trong client\u0026rsquo;s list s·∫Ω ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ d√πng cho SSL connection. Config n√†y ƒë·∫£m b·∫£o ELB s·∫Ω ƒë∆∞·ª£c quy·∫øt ƒë·ªãnh cipher n√†o d√πng cho SSL\n Q. B·∫°n c√≥ th·ªÉ t·∫°o 1 ELB v·ªõi nh·ªØng feature n√†o v·ªÅ security? SSL Server Certificates, SSL Negotiation, Back-End Server Authentication, Front-End Server Authentication\n ko th·ªÉ c√≥ Front-End Server Authentication, ch·ªâ c√≤n 3 c√°i kia\n Q. B·∫°n mu·ªën cho ph√©p 1 admin v√†o setup EC2, EC2 ƒë√≥ c·∫ßn quy·ªÅn access v√†o DynamoDB. Nh·ªØng policy permission g√¨ c·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o security? C·∫ßn trust policy t√°c d·ª•ng g√¨?\nIAM permission policy t√°c d·ª•ng g√¨?\n Trust policy cho ph√©p EC2 Instance assume role\nIAM permission policy cho ph√©p User Pass role to EC2\nN√≥i chung ch·ªâ c·∫ßn nh·ªõ User Pass Role l√† ƒë∆∞·ª£c\n Q. C√≥ th·ªÉ l·∫•y ·ªü ƒë√¢u c√°i Credential Report c·ªßa List c√°c AWS User g·ªìm current status, access key age, MFA or not?\n AWS IAM Console -\u0026gt; download Credential Report\n Q. B·∫°n c√≥ 1 VPC with a CIDR block of 10.0.0.0/16, trong ƒë√≥ c√≥ 1 subnet c√≥ CIDR gi·ªëng v·ªõi VPC. B·∫°n ƒë·ªãnh t·∫°o 1 subnet th·ª© 2 (CIDR 10.0.1.0/24) nh∆∞ng b·ªã l·ªói, l√†m th·∫ø n√†o? C√≥ n√™n s·ª≠a c√°i CIDR c·ªßa subnet th·ª© 1 ko?\n B·∫°n ph·∫£i associate 1 secondary IPv4 CIDR block v√†o VPC ƒë·ªÉ tƒÉng size cho VPC ƒë√≥. Th√¨ sau ƒë√≥ m·ªõi add th√™m c√°i subnet th·ª© 2 dc\n Q. B·∫°n c√≥ 1 VPC v·ªõi primary IPv4 CIDR block of 10.0.0.0/24, v√† 4 secondary IPv4 CIDR block n·ªØa. G·∫ßn h·∫øt c√°c IP ƒë√£ ƒëc d√πng v√† b·∫°n ph·∫£i tƒÉng size ƒë·ªÉ c√≥ th·ªÉ add th√™m EC2. L√†m n√†o? l·∫°i add th√™m secondary IPv4 CIDR block √†?\n Ch·ªâ c√≥ th·ªÉ t·∫°o t·ªëi ƒëa 4 secondary IPv4 CIDR block th√¥i.\nTh·∫ø n√™n ph·∫£i x√≥a c√°i subnet n√†o ko s·ª≠ d·ª•ng ƒëi, ƒë·ªÉ c√≥ th·ªÉ t·∫°o subnet m·ªõi\n Q. B·∫°n d√πng API Gateway v√† Lambda, b·∫°n mu·ªën trace v√† analyze c√°c user request g·ª≠i ƒë·∫øn API Gateway th√¨ d√πng service n√†o c·ªßa AWS? VPC Flow Log hay Cloudtrail hay X-Ray?\n D√πng AWS X-Ray, n√≥ cho b·∫°n kh·∫£ nƒÉng debug v√† analyze vi·ªác tracing user request ƒë·ªÉ tim root cause v·ªÅ performances\nCloudtrail ch·ªâ d√πng cho API logging (ch·ª© ko ph·∫£i tracing),\nVPC Flow Log th√¨ ko t·ªët b·∫±ng X-Ray\n Q. B·∫°n ƒëang serve content t·ª´ S3 v√† d√πng CloudFront ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô deliver content ƒë·∫øn user to√†n c·∫ßu. B·∫°n mu·ªën improve service c·ªßa b·∫°n n√™n c·∫ßn l·∫•y report v·ªÅ h·ªá th·ªëng hi·ªán t·∫°i t·ª´ CloudFront th√¨ Cloudfront cung c·∫•p nh·ªØng lo·∫°i report nh∆∞ n√†o?\nCache Statistics Reports, Top Referrers Reports, Usage Reports, Popular Objects Report, Viewers Reports?\n Top Referrers Reports: List ra 25 website domain m√† t·∫°o ra HTTP/HTTPS request ƒë·∫øn object c·ªßa b·∫°n\nUsage Reports: S·ªë l∆∞·ª£ng v·ªÅ c√°c request HTTP/HTTPS m√† Cloudfront ph·∫£n h·ªìi t·ª´ edge location\nPopular Objects Report: Nh·ªØng object n√†o ƒë∆∞·ª£c access th∆∞·ªùng xuy√™n nh·∫•t\nViewers Reports: Xem location c·ªßa nh·ªØng user access v√†o xem content nhi·ªÅu nh·∫•t, v√† h·ªç ƒëang d√πng lo·∫°i browser n√†o\nCache Statistics Reports: Get d·ªØ li·ªáu c·ªßa c√°c request ƒë∆∞·ª£c nh√≥m l·∫°i b·ªüi HTTP status code\n Q. B·∫°n ƒëang d√πng RDS v√† mu·ªën monitoring v·ªÅ CPU v√† Memory c·ªßa c√°c process ch·∫°y trong RDS Instance. N√™n l√†m g√¨?\n Enable Enhanced Monitoring trong RDS\nch·ª© ko d√πng Cloudwatch, v√¨ Cloudwatch ch·ªâ l·∫•y metric v·ªÅ CPU utilization c·ªßa DB Instance th√¥i, c√≤n RDS Enhanced Monitoring th√¨ l·∫•y metric t·ª´ c√°i agent trong DB ƒë√≥\n\u0026mdash;P3-udemy\u0026mdash;  Q. Cty b·∫°n c√≥ 1 website v√† c√≥ 2 m√¥i tr∆∞·ªùng UAT v√† dev. B·∫°n deploy app l√™n 1 EC2 m1.small. Khi test nh·∫≠n th·∫•y l√† performance gi·∫£m xu·ªëng khi tƒÉng workload ·ªü m√¥i tr∆∞·ªùng UAT. L√†m n√†o? C√≥ n√™n enable Enhanced Networking ko?\n D√πng EC2 size to h∆°n m1.small, ch·ª© Enhanced Networking ko support EC2 m1.small (ch·ªâ support m4,m5,c5..etc th√¥i)\n Q. 1 cty ƒëang d√πng MongoDB v√† NGINX server. H·ªç mu·ªën 1 service c√≥ th·ªÉ cung c·∫•p 200,000 IOPS trong 4000 random I/O Read cho Database. Th√¨ n√™n d√πng lo·∫°i Volume n√†o? (io1? st1? sc1\u0026hellip;)\n Storage Optimized Instances\nStorage Optimized Instances ƒë∆∞·ª£c design cho workload l·ªõn, ƒë·ªçc v√† ghi (Read write) li√™n t·ª•c v√†o Dataset c·ª±c l·ªõn. V√≠ d·ª• i3.4xlarge c√≥ th·ªÉ cung c·∫•p 825,000 Random Read IOPS v√† 360,000 Write IOPS.\nch·ª© c√°c instance nh∆∞ Provisioned IOPS SSD (io1) EBS Volumes max c√≥ 64k IOPS th√¥i\n Q. B·∫°n ƒëang c√≥ 1 portal 20 EC2, chia 4 regions. 1 ALB dc setup tr√™n m·ªói region ƒë·ªÉ distribute traffic. L√†m n√†o ƒë·ªÉ duy tr√¨ availability n·∫øu 1 trong 4 regions b·ªã disconnect?\n Set Evaluate Target Health check = true\nv√† setup Latency Based Routing cho Route53, t·∫°o record set m√† s·∫Ω resolve ƒë·∫øn ALB tr√™n m·ªói regions\nV√≠ d·ª• nh∆∞ n√†y: B·∫°n c√≥ ELB ·ªü Oregon v√† Singapore, b·∫°n t·∫°o 1 latency record cho m·ªói ELB ƒë√≥. Khi 1 user ·ªü London truy c·∫≠p v√†o domain c·ªßa b·∫°n th√¨:\n1-DNS route c√°i query ƒë·∫øn R53\n2-R53 refer v·ªÅ ƒë·ªô tr·ªÖ gi·ªØa London ‚áî Singapore v√† London ‚áî Oregon\n3-N·∫øu ƒë·ªô tr·ªÖ gi·ªØa London ‚áî Oregon l√† th·∫•p h∆°n, th√¨ R53 s·∫Ω reponse c√°i IP c·ªßa ELB ·ªü Oregon\n Q. B·∫°n c√≥ v√†i S3 bucket gi·ªØ c√°c th√¥ng tin quan tr·ªçng. B·∫°n c√≥ 1 app (·ªü private subnet) c·∫ßn edit file trong S3 r·ªìi t·∫°o report g·ª≠i ƒë·∫øn c√¥ng ty partner qua internet public. B·∫°n ƒë√£ t·∫°o S3 VPC Enpoint r·ªìi c·∫ßn l√†m g√¨ n·ªØa?\n Update Route table c·ªßa Private subnet ƒë·ªÉ connect tr·ª±c ti·∫øp v·ªõi S3 VPC enpoint c√≤n send outbound traffic th√¨ qua NAT Gateway\n Q. Nh·ªØng service n√†o sau ƒë√¢y cung c·∫•p kh·∫£ nƒÉng backup automatic v√† rotation backup m√† user c√≥ th·ªÉ config? S3, EC2, RDS, Redshift, EFS?\n RDS v√† Redshift (Redshift default ƒë√£ b·∫≠t t√≠nh nƒÉng automated backup r·ªìi, v·ªõi 1 retention day. RDS th√¨ v·ªën ƒë√£ c√≥)\nS3 ko c√≥ backup policy, v·ªën n√≥ ƒë√£ r·∫•t durable v√† ko c·∫ßn option backup\nEC2 th√¨ mu·ªën backup th√¨ c·∫ßn l√†m tay (ho·∫∑c gi·ªù ƒë√£ c√≥ service AWS Backup g√¨ ƒë√≥, nh∆∞ng ƒëang gi·∫£ s·ª≠ ch∆∞a c√≥)\nEFS th√¨ v·ªën ƒë√£ HA v√† durable r·ªìi, ko c√≥ option backup\n Q. V·ªÅ Route53 health check. N·∫øu \u0026gt;18% Health checker report l√† healthy th√¨ R53 s·∫Ω consider n√≥ l√† healthy. V·∫≠y nh·ªØng HTTP/HTTPS status codes n√†o tr·∫£ v·ªÅ th√¨ R53 s·∫Ω nh·∫≠n l√† healthy?\n 2XX, 3XX\n Q. 1 Cty v·ªÅ data analytics s·∫Øp m·ªü 1 s·ªë service. B·∫°n c·∫ßn t·∫°o 1 Cloudformation template s·∫Ω init 1 s·ªë service quan tr·ªçng tr∆∞·ªõc, trong CF template ƒë√≥ c·∫ßn t·∫°o nh·ªØng g√¨?\n EC2, Redshift, EMR configuration\n(ch√∫ √Ω l√† c√¥ng ty v·ªÅ data analytics n√™n c·∫ßn Redshift, EMR\u0026hellip;)\n Q. B·∫°n l√† Database engineer, ƒëang c·∫ßn setup storage cho EC2 Server. M·ªói server c·∫ßn handle l∆∞·ª£ng l·ªõn DB workload. H·ªç c≈©ng c·∫ßn ph·∫£i secure v√† seprate ƒë·ªÉ keep c√°c file backup v√† logs. N√™n d√πng g√¨ l√†m root volume, d√πng g√¨ l√†m file system ƒë·ªÉ cost-effective?\n D√πng EBS-Provisioned IOPS SSD volume ƒë·ªÉ l√†m root volume (v√¨ n√≥ ƒë∆∞·ª£c optimized ƒë·ªÉ high disk read/write performance)\nD√πng Cold HDD volume ƒë·ªÉ l√†m file system (v√¨ n√≥ r·∫•t r·∫ª v√† great cho data access ko th∆∞·ªùng xuy√™n infrequently)\nKo d√πng EFS v√¨ ƒë·∫Øt, ko d√πng Throughput Optimized HDD v√¨ n√≥ t·∫≠p trung v√†o throughput h∆°n l√† IOPS n√™n c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn database performance\n Q. B·∫°n c√≥ subnet A c√≥ 2 EC2, gi·ªù t·∫°o th√™m subnet B. N·∫øu x√≥a subnet A th√¨ sao?\n S·∫Ω ko x√≥a dc, v√¨ ph·∫£i x√≥a h·∫øt EC2 trong ·∫•y ƒëi ƒë√£. C√≤n vi·ªác x√≥a route c·ªßa n√≥ th√¨ s·∫Ω t·ª± ƒë·ªông b·ªã x√≥a khi b·∫°n x√≥a th√†nh c√¥ng subnet\n Q. 1 Cty ƒëang c√≥ 1 platform host tr√™n EC2 sau ASG, v√† 1 CLB v·ªõi HTTPS listener. H·ªç y√™u c·∫ßu thay th·∫ø c√°i certificate hi·ªán t·∫°i trong load balancer b·∫±ng c√°i cert m·ªõi m√† ƒë√£ upload l√™n IAM. V√† c·∫ßn l√†m b·∫±ng CLI, l√†m n√†o?\n 1-aws iam get-server-certificate ƒë·ªÉ get ARN of the certificate.\n2-aws elb set-load-balancer-listener-ssl-certificate ƒë·ªÉ add cert m·ªõi v√†o LB\n Q. B·∫°n c√≥ 1 VPC v·ªõi CIDR block 10.0.0.0/16. B·∫°n c·∫ßn t·∫°o connect t·ª´ VPC t·ªõi data center. ƒê√£ c√≥ 1 public subnet CIDR (10.0.0.0/24) v√† 1 VPN-only subnet (10.0.1.0/24) c√πng v·ªõi VPN Gateway (vgw-51898). ƒê·ªÉ cho ph√©p traffic ra ngo√†i internet, b·∫°n ƒë√£ setup NAT instance (i-918273). Data center c√≥ CIDR 172.16.0.0/12. V·∫≠y config c·ªßa Main Route table v√† Custom Route table n√™n nh∆∞ n√†o?\n Custom Route table associate v·ªõi Public Subnet:    Destination Target     10.0.0.0/16 local   0.0.0.0/0 igw-id    Main Route table associate v·ªõi VPN-only Subnet:    Destination Target     10.0.0.0/16 local   0.0.0.0/0 vgw-id    Ch√∫ √Ω Destination l√† CIDR c·ªßa VPC ch·ª© ko ph·∫£i c·ªßa Subnet\n Q. V·ªÅ Cloudformation th√¨ c√≥ nh·ªØng c√°i anatomy n√†o l√† b·∫Øt bu·ªôc, c√°i n√†o l√† optional?\n Ch·ªâ c√≥ Resource l√† b·∫Øt bu·ªôc ph·∫£i c√≥. Ngay c·∫£ Format Version nh∆∞ AWSTemplateFormatVersion c≈©ng ch·ªâ l√† optional\n Q. Ph√≠a AWS ch·ªãu tr√°ch nhi·ªám g√¨ trong h·ªá th·ªëng infra c·ªßa b·∫°n?\n Ph√≠a AWS ch·ªãu tr√°ch nhi·ªám:\nQu·∫£n l√Ω Global infra c√°i run all your service\nPatch v√† update OS\nB·∫£o v·ªá Network traffic t·∫ßng abstract service nh∆∞ S3, Dynamo\nC√≤n ph√≠a b·∫°n c·∫ßn c√≥ tr√°ch nhi·ªám:\nClient side data encryption v√† data integrity authentication.\nServer side encryption (file system and data)\nB·∫£o v·ªá Network traffic c·ªßa b·∫°n\n Q. 1 Cty c√≥ nhi·ªÅu VPC tr√™n nhi·ªÅu regions. ƒê·ªÉ monitor h·ªá th·ªëng b·∫°n c·∫ßn l·∫•y aggregate CPU Utilization c·ªßa c√°c EC2 running tr√™n all VPC. L√†m n√†o?\n Setup Cloudwatch Dashboard, add 1 widget m·ªói region, l·∫•y aggregate CPU Utilization all EC2 trong m·ªói regions. (Ch·ª© ko th·ªÉ setup all regions dc)\nB·∫≠t Detailed Monitoring cho c√°c EC2 (B·∫Øt bu·ªôc ph·∫£i b·∫≠t c√°i n√†y v√¨ Cloudwatch ch·ªâ l·∫•y aggregate dc c·ªßa c√°c EC2 ƒë√£ b·∫≠t c√°i n√†y th√¥i)\n Q. 1 App mu·ªën user c·ªßa h·ªç c√≥ th·ªÉ send pic, videos l√™n. Login b·∫±ng Social media account v√† app s·∫Ω l∆∞u detail v√†o DynamoDB. C·∫ßn d√πng nh·ªØng service g√¨?\n D√πng AWS Cognito v√† AWS IAM Roles\nVi·ªác d√πng Federated Access l√† sai\n Q. B·∫°n c√≥ 1 h·ªá th·ªëng ASG c√°c EC2 ch·∫°y tr√™n c√°c AZ, v√† 1 CLB. B·∫°n nh·∫≠n th·∫•y c√≥ nhi·ªÅu l·ªói 4xx. Gi·ªù n√™n enable VPC Flow Logs hay Load Balancer access log ƒë·ªÉ l·∫•y Client IP v√† request path?\n N√™n d√πng enable access log c·ªßa Load Balancer, ch·ª© ko c·∫ßn l·∫•y IP address c·ªßa to√†n b·ªô traffic ra v√†o VPC\n Q. N·∫øu Instance b·ªã l·ªói Insufficient Instance Capacity th√¨ c√≥ th·ªÉ root cause l√† g√¨?\n Do hi·ªán t·∫°i AWS Ko c√≥ ƒë·ªß available capacity v·ªÅ s·ªë l∆∞·ª£ng instance On-demands\n Q. B·∫°n c√≥ 1 S3 bucket l∆∞u c√°c th√¥ng tin c·ªßa user. Bucket c·∫ßn dc monitor ƒë·ªÉ secure. Nhi·ªám v·ª• c·ªßa b·∫°n l√† automate the assessment of your resource configurations and resource changes, th√¨ c·∫ßn l√†m g√¨? Config bucket policy d√πng IAM role? Hay d√πng policy deny unauthored User? hay AWS Config?\n D√πng AWS Config, v√¨ y√™u c·∫ßu l√† automate vi·ªác ƒë√°nh gi√°/theo d√µi resource thay ƒë·ªïi\n Q. Gi·ªØa AWS Shield Advanced v√† AWS WAF kh√°c nhau nh∆∞ n√†o?\n AWS Shield Advanced l√† c√°i ƒë·∫Øt ti·ªÅn t√°c d·ª•ng ch·ªëng DDoS attacks\nch·ª© WAF ch·ªâ c√≥ th·ªÉ ch·∫∑n c√°c common attack nh∆∞ SQL injection, cross-site scripting th√¥i, ko ch·ªëng dc DDoS attack\n Q. B·∫°n ƒë∆∞·ª£c y√™u c·∫ßu assess (theo d√µi) behaivor c·ªßa c√°c resource nh∆∞ EC2, ƒë·ªÉ x√°c ƒë·ªãnh nh∆∞ng nguy hi·ªÉm v·ªÅ security. B·∫°n c·∫ßn b·∫£n report v·ªÅ c√°c activity di·ªÖn ra trong EC2 v√≠ d·ª• nh∆∞ detail v·ªÅ s·ª± giao ti·∫øp v·ªõi c√°c service kh√°c. Service n√†o s·∫Ω gi√∫p b·∫°n l√†m ƒëi·ªÅu n√†y? Trusted Advisor hay Inspector?\n AWS Inspector\nN√≥i chung c·ª© Security Assessment, Network Assessment th√¨ ph·∫£i l√† Inspector\n Q. 1 website cung c·∫•p livescore, tin t·ª©c, h√¨nh ·∫£nh th·ªÉ thao. ƒêang ƒëc d·ª±ng tr√™n h·ªá th·ªëng ASG multi-AZ, d√πng ALB (Application Load Balancer). G·∫ßn ƒë√¢y v√¨ Worldcup n√™n traffic tƒÉng l√™n, ng∆∞·ªùi d√πng b√°o c√°o l√† load ch·∫≠m. Gi·∫£i ph√°p n√†o ƒë·ªÉ scale v√† c·∫£i thi·ªán load time c·ªßa website? Cloudfront? hay NLB? hay b·∫≠t Enhaced Networking?\n D√πng Cloudfront v√¨ n√≥ cung c·∫•p kh·∫£ nƒÉng deliver content ƒë·∫øn user 1 c√°ch low latency, t·ªëc ƒë·ªô transfer cao.\nKo ph·∫£i NLB v√¨ n√≥ ch·ªß y·∫øu d√πng ƒë·ªÉ load balancing cho TCP traffic, handle h√†ng tri·ªáu request m·ªói gi√¢y v·ªõi ƒë·ªô tr·ªÖ c·ª±c th·∫•p, tuy c√≥ th·ªÉ tƒÉng performance nh∆∞ng n√≥ ko ph·∫£i gi·∫£i ph√°p ƒë·ªÉ tƒÉng th·ªùi gian load time c·ªßa website tr√™n to√†n th·∫ø gi·ªõi (respone time around globe)\nKo ph·∫£i Enhanced Networking v√¨ n√≥ ch·ªß y·∫øu ƒë·ªÉ cho ph√©p high-performance networking ƒë·ªëi v·ªõi 1 s·ªë lo·∫°i EC2 c·ª• th·ªÉ m·ªõi support\n Q. So s√°nh 4 c√°i sau: Cost Optimization Monitor? TCO calculator? Simple Monthly Calculator? AWS Pricing page?\n 1-Cost Optimization Monitor: report cho v·ªÅ chi ph√≠ operation chi ti·∫øt. Generate report v√† cung c·∫•p insight v·ªÅ service usage v√† cost\n2-TCO calculator: d√πng ƒë·ªÉ compare cost c·ªßa on-premise ho·∫∑c m√¥i tr∆∞·ªùng hosting truy·ªÅn th·ªëng so v·ªõi AWS\n3-Simple Monthly Calculator: d√πng ƒë·ªÉ estimate billing h√†ng th√°ng. Nh∆∞ng ko generate report 1 c√°ch chi ti·∫øt nh∆∞ c√°i Cost Optimization Monitor. V√† n√≥ c≈©ng ko monitor cost hi·ªán t·∫°i m√¨nh ƒëang ch·ªãu\n4-AWS Pricing page: ch·ªâ show ra th√¥i, ch·ª© ko gen ra report cho b·∫°n.\n Q. Nh·ªØng task m√† b·∫°n c√≥ th·ªÉ perform b·∫±ng s·ª± t·ª± ƒë·ªông c·ªßa System Manager cho EC2 c·ªßa b·∫°n?\n Build Automation workflow ƒë·ªÉ config v√† qu·∫£n l√Ω EC2\nT·∫°o custom workflow ho·∫∑c s·ª≠ d·ª•ng c√°i workflow m√† AWS ƒë√£ defined s·∫µn\nNh·∫≠n noti v·ªÅ Automation Workflow/Task b·∫±ng Cloudwatch Event\nMonitor qu√° tr√¨nh Automation v√† chi ti·∫øt v·ªÅ qu√° tr√¨nh execute\nN√≥i chung c√≥ ch·ªØ Automation ho·∫∑c workflow th√¨ l√† task c·ªßa SSM\n Q. B·∫°n mu·ªën monitor system infra. B·∫°n ƒë√£ c√≥ 1 custom shell script ƒë·ªÉ check memory ultilization c·ªßa EC2 r·ªìi. Gi·ªù b·∫°n mu·ªën dc notified m·ªói khi memory breach threshold th√¨ l√†m n√†o?\n C√†i Cloudwatch agent tr√™n EC2 n√≥ s·∫Ω ƒë·ªçc c√°i custom metric script m√† b·∫°n ƒë√£ chu·∫©n b·ªã\nSetup 1 c√°i CW alarm base tr√™n c√°i custom metric\nset SNS topic ƒë·ªÉ nh·∫≠n noti\n Q. CloudWatch alarms c√≥ th·ªÉ c√≥ nh·ªØng tr·∫°ng th√°i n√†o?\n OK\nALARM (metric outside kh·ªèi threshold ƒë√£ defined)\nINSUFFICIENT_DATA (alarm v·ª´a start, ho·∫∑c metric not available, ho·∫∑c ko ƒë·ªß data cho metric) ko c√≥ tr·∫°ng th√°i Pending ƒë√¢u nh√©\n Q. V·ªÅ AWS Risk and Compliance Whitepaper?\n Kh√°ch h√†ng dc t√πy ch·ªçn data c·ªßa h·ªç ·ªü trong region v·∫≠t l√Ω n√†o, AWS s·∫Ω ko di chuy·ªÉn m√† ko th√¥ng b√°o v·ªõi user, tr·ª´ khi c√≥ y√™u c·∫ßu c·ªßa ch√≠nh quy·ªÅn\nAWS ngƒÉn ch·∫∑n vi·ªác x√¢m nh·∫≠p v√†o host v·∫≠t l√Ω ho·∫∑c ·∫£o h√≥a m√† user ko c√≥ quy·ªÅn s·ªü h·ªØu, ko dc assign\nKh√°ch h√†ng retain quy·ªÅn v√† s·ª± s·ªü h·ªØu v·ªÅ data c·ªßa h·ªç (AWS ko can thi·ªáp)\nCh√∫ √Ω l√† AWS ko cho b√™n third-party n√†o deliver service c·ªßa AWS ƒë·∫øn KH\nAWS ko cho ph√©p c√°c auditor l√†m 1 tour v√†o data center ƒë·ªÉ validate h·ªá th·ªëng ƒë√¢u nh√©\nAWS ko notify khi h·ªá th·ªëng c·ªßa h·ªç c·∫ßn bring offline, b·ªüi v√¨ vi·ªác maintain v√† patch h·ªá th·ªëng s·∫Ω ko ·∫£nh h∆∞·ªüng ƒë·∫øn KH\n Q. 1 cty ƒëang host web app e-commerce v√† blog fetch data t·ª´ database. 1 s·ªë b√†i vi·∫øt (static web page) c√≥ nhi·ªÅu truy c·∫≠p l√†m app ho·∫°t ƒë·ªông ch·∫≠m. Ph·∫£i l√†m sao ƒë·ªÉ gi·∫£m load time nh∆∞ v·∫≠y? ElastiCache? hay S3? hay Read replica cho DB?\n D√πng S3 ƒë·ªÉ host c√°c web pages\nVi·ªác d√πng ElastiCache ch·ªâ ƒë·ªÉ caching content dc fetch th∆∞·ªùng xuy√™n t·ª´ DB\nRead replica ch·ªâ ƒë·ªÉ scaling nh·ªØng web m√† n·∫∑ng v·ªÅ ƒë·ªçc\n Q. B·∫°n qu·∫£n l√Ω 1 h·ªá th·ªëng batch ch·∫°y tr√™n EC2 v√† CLB. EC2 deploy tr√™n 3 AZ us-west-2a, 2b, 2c. B·∫°n nh·∫≠n th·∫•y EC2 trong 2b th·ª±c hi·ªán c√°c request r·∫•t ch·∫≠m. C·∫ßn monitor metric n√†o ƒë·ªÉ bi·∫øt dc t·ªïng s·ªë request ho·∫∑c connection ƒëang pending tr∆∞·ªõc khi dc route t·ªõi 1 EC2 kh·ªèe m·∫°nh healthy? SurgeQueueLength hay RequestCount hay BackendConnectionErrors hay SpilloverCount ?\n d√πng SurgeQueueLength: t·ªïng s·ªë request (HTTP listener) ho·∫∑c connection (TCP listener) ƒëang pending. Max c·ªßa queue l√† 1024. N·∫øu queue b·ªã full th√¨ request s·∫Ω b·ªã reject. S·ªë request b·ªã reject ƒë√≥ s·∫Ω hi·ªÉn th·ªã ·ªü metric SpilloverCount\nRequestCount: l√† s·ªë request completed trong 1 kho·∫£ng time nh·∫•t ƒë·ªãnh (1 or 5 ph√∫t)\nBackendConnectionErrors: s·ªë l∆∞·ª£ng connection fail khi connect gi·ªØa LB v√† Instances\n Q. B·∫°n ping 1 EC2 t·ª´ m√°y c·ªßa b·∫°n, nh∆∞ng ko nh·∫≠n dc respone, b·∫°n check VPC flow log th√¨ nh∆∞ sau:\n2 123456789010 eni-1235b8ca 110.237.99.166 172.31.17.140 0 0 1 4 336 1432917027 1432917142 ACCEPT OK\n2 123456789010 eni-1235b8ca 172.31.17.140 110.237.99.166 0 0 1 4 336 1432917094 1432917142 REJECT OK V√¨ sao?\n V√¨ Network ACL ƒëang ko cho ph√©p outbound ICMP traffic (v√¨ NACL l√† stateless)\nch·ª© c√≤n n·∫øu l√† SG th√¨ b·ªüi SG l√† stateful n√™n n·∫øu inbound traffic dc cho ph√©p th√¨ outbound c≈©ng s·∫Ω ƒë∆∞·ª£c cho ph√©p (ngay c·∫£ khi rule trong SG c·ªßa b·∫°n ch·∫∑n outbound traffic) üòÇ\n\u0026mdash;P4-udemy\u0026mdash;  Q. App c·ªßa b·∫°n l√† app share ·∫£nh, deploy tr√™n 1 group c√°c EC2. Team dev mu·ªën nhanh ch√≥ng push 1 b·∫£n update cho app, v√† b·∫£n update ƒë√≥ s·∫Ω ƒë∆∞·ª£c download b·ªüi user tr√™n global t·ª´ b√™n trong mobile app. L√†m n√†o ƒë·ªÉ ng d√πng c√≥ tr·∫£i nghi·ªám t·ªët.\n L∆∞u file tr√™n S3, Config sao cho Cloudfront s·ª≠ d·ª•ng S3 bucket nh∆∞ l√† 1 Origin.\nkh√¥ng th·ªÉ upload th·∫≥ng file update ƒë√≥ l√™n Cloudfront\n Q. App c·ªßa b·∫°n c√≥ nhi·ªÅu user, User tƒÉng l√™n n√™n c·∫ßn tƒÉng Storage l√™n. B·∫£n Backup c·∫ßn ƒë·ªÉ ·ªü ch·ªó n√†o secure v√† durable. B·∫°n mu·ªën d√πng AWS v√¨ n√≥ unlimited. Tuy nhi√™n b·∫°n v·∫´n mu·ªën gi·ªØ dc s·ª± access th∆∞·ªùng xuy√™n v√†o data. L√†m n√†o?\n D√πng Storage Gateway v√¨ n√≥ cho b·∫°n tr·∫£i nghi·ªám li·ªÅn m·∫°ch (seamlessly) trong vi·ªác integrate app v√† AWS block object storage\n Q. L√†m sao ƒë·ªÉ d√πng 1 Cloudformation Template ƒë·ªÉ t·∫°o stack tr√™n nhi·ªÅu AWS account?\n D√πng Cloudformation StackSet\n Q. Khi b·∫°n enable MFA Delete cho 1 S3 bucket, th√¨ s·∫Ω nh∆∞ n√†o?\n MFA Delete c·∫ßn enable c√πng v·ªõi Bucket Versioning\nPh·∫£i enable b·∫±ng CLI, b·∫±ng root account\nFeature n√†y ch·ªâ d√†nh cho root account. Ch·ªâ root account m·ªõi c√≥ th·ªÉ x√≥a Object trong S3 bucket ƒë√≥\n1 IAM User n·∫øu c√≥ cung c·∫•p MFA th√¨ c≈©ng ko th·ªÉ x√≥a dc object trong bucket ƒë√≥.\nhttps://www.cloudmantra.net/blog/how-to-enable-mfa-delete-for-s3-bucket/\n Q. Command n√†o ƒë·ªÉ put custom metric l√™n Cloudwatch?\n put-metric-data command CLI\n Q. C√°i helper script n√†o c·ªßa Cloudformation gi√∫p b·∫°n retrieve v√† interpret metadata, install package, t·∫°o file, start service?\n cfn-init\nko ph·∫£i cfn-signal v√¨ n√≥ ch·ªâ gi√∫p b√°o WaitCondition ch·ªù sync v·ªõi c√°c resource kh√°c c·∫£u stack khi app ready\nko ph·∫£i cfn-get-metadata v√¨ n√≥ ch·ªâ d√πng ƒë·ªÉ get metadata th√¥i\n Q. B·∫°n t·∫°o 1 Chef Server trong AWS Opswork, v·ªõi 3 node running. B·∫°n c·∫ßn ph·∫£i add nhi·ªÅu h∆°n, nh∆∞ng th·∫•y l√† m·ªói l·∫ßn add m·∫•t th·ªùi gian qu√°. C√≥ c√°ch n√†o t·ª± ƒë·ªông t·∫°o EC2 Node v√†o Chef Server ko?\n C√≥, d√πng Chef Client Cookbook\n Q. B·∫°n mu·ªën s·ª≠ d·ª•ng Pupet Enterpise, AWS c√≥ service n√†o l√†m c√°c nhi·ªám v·ª• nh∆∞ operation, backup, restore, upgrade software cho Pupet Enterprise ko?\n C√≥, AWS Opswork\n Q. B·∫°n mu·ªën secure cho data c·ªßa m√¨nh, 1 service c·ªßa AWS m√† d√πng FIPS 140-2 compliant, v√† n·∫±m trong VPC c·ªßa b·∫°n th√¨ d√πng c√°i g√¨? CloudHSM hay KMS ?\n CloudHSM v√¨ n√≥ n·∫±m trong VPC c·ªßa b·∫°n, do b·∫°n qu·∫£n l√Ω, b·∫°n t·ª± generate ra key v√† d√πng key ƒë√≥ trong AWS\nch·ª© KMS l√† Service n·∫±m ngo√†i VPC c·ªßa b·∫°n\n Q. V·ªÅ RDS th√¨ Cloudwatch c√≥ nh·ªØng metric nh∆∞ sau, ch√∫ng n√≥ √Ω nghƒ©a g√¨? FreeStorageSpace, BinLogDiskUsage, FreeableMemory, DiskQueueDepth?\n FreeStorageSpace: monitor s·ªë space c√≤n th·ª´a trong RDS instance,\nBinLogDiskUsage: monitor dung l∆∞·ª£ng ƒë√£ d√πng (ch·ªâ apply cho MySQL read replicas),\nFreeableMemory: s·ªë RAM c√≤n tr·ªëng,\nDiskQueueDepth: s·ªë I/O (read/write) request ƒëang ch·ªù access v√†o disk\n Q. B·∫°n c·∫ßn quick update RDS, launch th√™m v√†i EC2, new ALB v√†o c√°i Cloudformation Stack hi·ªán t·∫°i. B·∫°n mu·ªën c√≥ th·ªÉ preview nh·ªØng c√°i thay ƒë·ªïi trong stack m·ªõi, ch·∫Øc ch·∫Øn l√† new infra s·∫Ω dc provision ƒë√∫ng th√¨ l√†m n√†o?\n Cloudformation Change Sets (n√≥ s·∫Ω list ra nh·ªØng thay ƒë·ªïi cho b·∫°n review tr∆∞·ªõc khi th·ª±c s·ª± execute)\n Q. B·∫°n mu·ªën t·ª± ƒë·ªông x√≥a nh·ªØng EBS snapshot khi ƒë·ªß maximum snapshot th√¨ l√†m n√†o?\n D√πng Amazon Data Lifecycle Manager\n Q. Cross-zone Load balancing l√† nh∆∞ n√†o?\n xem h√¨nh sau:\nN·∫øu disable Cross-zone Load balancing th√¨ traffic t·ª´ client v√†o s·∫Ω chia ƒë·ªÅu 2 LB, m·ªói LB 50% (chia theo Route53 define), d·∫´n ƒë·∫øn m·ªói EC2 trong Zone A s·∫Ω l√† 25%, c√≤n m·ªói EC2 trong Zone B l√† 50/8 (%) -\u0026gt; ko ƒë·ªÅu.\nN·∫øu enable Cross-zone Load balancing th√¨ traffic s·∫Ω chia ƒë·ªÅu cho c√°c EC2, m·ªói c√°i 10%\n Q. B·∫°n mu·ªën m·ªói khi snapshot ƒëc t·∫°o ra t·ª´ EC2, ch√∫ng s·∫Ω dc copy qua region kh√°c ƒë·ªÉ ƒë·∫£m b·∫£o durability, l√†m n√†o?\n Cloudwatch Event cho EBS, r·ªìi trigger Lambda ƒë·ªÉ copy qua region kh√°c\n Q. RDS Multi-AZ ho·∫°t ƒë·ªông nh∆∞ n√†o?\n Con Primary Instance c·ªßa b·∫°n s·∫Ω dc synchonize replicate data sang con DB Instance standby ·ªü Zone kh√°c. 2 con n√†y ch·ªâ sync v·ªÅ Data, c√≤n infra c·ªßa ch√∫ng ƒë·ªôc l·∫≠p v·ªõi nhau.\n Q. V·ªÅ Cloudfront, h·ªá th·ªëng c·ªßa b·∫°n cung c·∫•p ·∫£nh, tin t·ª©c. C√≥ writer post 1 b·ª©c ·∫£nh fake v√† ƒë√£ b·ªã cache l·∫°i tr√™n Cloudfront, v√† c·∫ßn ph·∫£i ƒëc x√≥a ngay l·∫≠p t·ª©c (ko th·ªÉ ƒë·ª£i cache expire, m√† ph·∫£i lu√¥n v√† ngay), l√†m n√†o?\n ‚ñ† C√°ch 1: Invalidate file ƒë√≥ ·ªü edge cache b·∫±ng CLI command aws cloudfront create-invalidation, th√¨ t·ª´ l·∫ßn sau viewer v√†o request, Cloudfront s·∫Ω g·ª≠i ƒë·∫øn origin ƒë·ªÉ fetch file m·ªõi nh·∫•t\n‚ñ† C√°ch 2: D√πng file versioning ƒë·ªÉ serve 1 version kh√°c c·ªßa file ƒë√≥\nCh√∫ √Ω c√°ch invalidate file th√¨ ko th·ªÉ invalidate file media ƒë·ªãnh d·∫°ng Microsoft Smooth Streaming, ko th·ªÉ invalidate 1 file ƒë√£ dc serve b·ªüi RTMP (Real-Time Messaging Protocol)\n Q. B·∫°n c√≥ S3 bucket v√† mu·ªën secure data. Ch·ªâ nh·ªØng user c√≥ MFA th√¨ m·ªõi ƒëc v√†o access content bucket ƒë√≥. L√†m n√†o?\n COnfig bucket policy ch·ªâ cho ph√©p access n·∫øu User ƒë√£ authen b·∫±ng MFA\nEnsure MFA dc b·∫≠t cho c√°c User\n Q. 1 Cty d√πng DynamoDB, b·∫°n c·∫ßn ensure l√† backup c·∫ßn c√≥ s·∫µn available, l√†m n√†o?\n B·∫≠t ch·ª©c nƒÉng On-demands backup for DynamoDB tables (N√≥ cho ph√©p b·∫°n backup for long-term, b·∫°n c√≥ th·ªÉ backup v√† restore b·∫•t c·ª© l√∫c n√†o ch·ªâ v·ªõi v√†i click chu·ªôt, vi·ªác backup v√† restore s·∫Ω ko ·∫£nh h∆∞·ªüng ƒë·∫øn perfomance hay t√≠nh availability c·ªßa table)\n Q. 1 Cty mu·ªën b·∫°n manage h·ªá th·ªëng infa c·ªßa h·ªç tr√™n c·∫£ cloud v√† on-premise. H·ªç mu·ªën h·ªá th·ªëng backup v√† archive ·ªïn ƒë·ªãnh tr√™n c·∫£ on-premise v√† cloud cho Document c·ªßa h·ªç. Nh·ªØng document ph·∫£i lu√¥n ƒëc accessible ngay l·∫≠p t·ª©c trong 6 th√°ng, v√† sau 6 th√°ng th√¨ s·∫Ω dc archive cho 10 nƒÉm. D√πng g√¨?\n Storage Gateway v·ªõi File gateway connect ƒë·∫øn onpremise data center. D√πng lifecycle ƒë·ªÉ move sang Glacier\nKo d√πng Tape gateway v√¨ n√≥ l∆∞u data tr√™n Glacier n√™n ko th·ªÉ retrive data ngay l·∫≠p t·ª©c ƒë∆∞·ª£c nh∆∞ File gateway\nAWS Storage Gateway cung c·∫•p, File-based, Volume-based, Tape-based, th√¨:\nFile Gateway ch√≠nh l√† File-based,\nTape Gateway ch√≠nh l√† Tape-based.\n\u0026mdash;P5-udemy\u0026mdash;  Q. 1 Cty c√≥ nhi·ªÅu AWS account trong 1 Org. B·∫°n c·∫ßn ensure l√† c√°c tag c·∫ßn dc apply nh·∫•t qu√°n khi t·∫°o resource trong c√°c account. L√†m n√†o?\n D√πng Cloudformation Resource Tags\nD√πng AWS Service Catalogs\n Q. Cty b·∫°n qu·∫£n l√Ω nhi·ªÅu AWS resource, gi·ªù mu·ªën optimize cost c·ªßa c√°c resource th√¨ c·∫ßn d√πng c√°i g√¨? AWS Inspector? hay AWS Cost \u0026amp; Usage report?\n D√πng AWS Cost \u0026amp; Usage report\nV√¨ Inspector ch·ªâ ƒë·ªÉ check xem server c√≥ b·ªã vulnerabilities hay ko th√¥i\n Q. Nh·ªØng limit c·ªßa RDS encrpyted Database Instance l√† g√¨?\n Ch·ªâ c√≥ th·ªÉ enable encryption ngay khi t·∫°o DB, ch·ª© ko th·ªÉ sau khi DB ƒë√£ ƒë∆∞·ª£c t·∫°o\nDB ƒë√£ b·∫≠t encryption th√¨ ko th·ªÉ disable ƒëi ƒë∆∞·ª£c\nB·∫°n ko th·ªÉ c√≥ 1 b·∫£n Read Replica unencrypted c·ªßa 1 DB encrypted\nB·∫°n ko th·ªÉ c√≥ 1 b·∫£n Read Replica encrypted c·ªßa 1 DB unencrypted\nRead Replica v·ªõi DB g·ªëc c·∫ßn dc encrypt b·∫±ng c√πng 1 key\nB·∫°n ko th·ªÉ restore 1 b·∫£n backup/snapshot unencrypted v√†o 1 DB Instance ƒë√£ ƒë∆∞·ª£c encrypted\nKhi copy encrpyted snapshot t·ª´ 1 region sang regions kh√°c, b·∫°n c·∫ßn ch·ªâ ƒë·ªãnh key KMS v√¨ KMS ra specific cho m·ªói region b·∫°n t·∫°o DB\n Q. 1 Cty c√≥ 1 app host tr√™n EC2. Gi·ªù c·∫ßn 1 high throughput data storage cho qu·∫£n l√Ω content v√† c√≥ th·ªÉ cung c·∫•p parallel share access v√†o server c·ªßa h·ªç, th√¨ s·∫Ω d√πng g√¨? EBS Volume Throughput Optimize hay EFS hay S3?\n D√πng EFS\nV√¨ EBS ch·ªß y·∫øu d√πng cho local block level storage, m·∫∑c d√π c√≥ th·ªÉ cung c·∫•p high throughput nh∆∞ng ko th·ªÉ cung c·∫•p kh·∫£ nƒÉng parallel access v√†o multiple instances ƒë∆∞·ª£c\n Q. Cty b·∫°n c·∫ßn migrate app l√™n AWS. C·∫ßn infa c√≥ th·ªÉ HA v√† ch·ªëng dc cross-site scrpting, SQL injection, HTTP flood attacks. Th√¨ d√πng WAF hay Shield Advanced? d√πng Network Load Balancer hay Application Load Balancer?\n WAF v√† ALB (v√¨ WAF l√† ƒë·ªß ƒë·ªÉ ch·ªëng nh·ªØng c√°i tr√™n r·ªìi, bao gi·ªù c·∫ßn ch·ªëng DDos th√¨ m·ªõi c·∫ßn Shield. WAF th√¨ n√≥ ko th·ªÉ integrate v·ªõi NLB ƒë∆∞·ª£c, n√™n m·ªõi ch·ªçn ALB)\n Q. B·∫°n c√≥ 1 s·ªë EC2 trong ASG v√† 1 v√†i h√†m Lambda. M·ªói khi b·∫°n release 1 version code m·ªõi, x·∫£y ra s·ª± ko ·ªïn ƒë·ªãnh v√¨ b·∫°n ko th·ªÉ manually update all resource li√™n quan ƒë√∫ng c√°ch. V√¨ s·ªë l∆∞·ª£ng l·ªõn resource, b·∫°n c·∫ßn 1 c√°ch ƒë·ªÉ nh√≥m ch√∫ng l·∫°i v√† deploy new version c·ªßa code 1 c√°ch ·ªïn ƒë·ªãnh cho c√°i nh√≥m ƒë√≥ v·ªõi downtime b√© th√¥i. L√†m n√†o?\n D√πng deployment group trong AWS Code Deploy ƒë·ªÉ automate vi·ªác deploy code\n Q. Cloudwatch agent c√≥ th·ªÉ cung c·∫•p th√¥ng tin v·ªÅ vulnerabilities v√† errors trong system c·ªßa b·∫°n ko?\n KO, mu·ªën c√°i ƒë√≥ th√¨ d√πng Inspector\n Q. Data Lifecycle Manager cung c·∫•p 1 c√°ch ƒë∆°n gi·∫£n, t·ª± ƒë·ªông ƒë·ªÉ backup data tr√™n EBS volume c·ªßa b·∫°n. V·∫≠y n√≥ c√≥ kh·∫£ nƒÉng t·∫°o snapshot r·ªìi t·ª± ƒë·ªông copy sang region kh√°c hay ko?\n Kh√¥ng nh√©. B·∫°n ph·∫£i t·ª± l√†m vi·ªác move sang region kh√°c ƒë√≥\n Q. App c·ªßa b·∫°n d√πng ELB Clasic Load Balancer. B·∫°n mu·ªën enable ticky session ƒë·ªÉ CLB s·∫Ω bind c√°i user session v·ªõi 1 instance c·ª• th·ªÉ. Khi ƒë√≥ t√™n c·ªßa cookie m√† CLB s·∫Ω t·∫°o ƒë·ªÉ map v·ªõi user session l√† g√¨?\n AWSELB\nN·∫øu App c·ªßa b·∫°n c√≥ session cookie s·∫µn r·ªìi, b·∫°n c√≥ th·ªÉ config ELB ƒë·ªÉ c√°i session cookie c·ªßa ELB follow c√°i duration c·ªßa App\u0026rsquo;s session cookie.\nN·∫øu App c·ªßa b·∫°n ko c√≥ session cookie, b·∫°n c√≥ th·ªÉ config ELB ƒë·ªÉ n√≥ t·∫°o ra session cookie v√† ch·ªâ ƒë·ªãnh duration b·∫°n mu·ªën. Khi ELB t·∫°o cookie s·∫Ω c√≥ t√™n l√† AWSELB d√πng ƒë·ªÉ map c√°i session v√†o Instance\n Q. Khi b·∫°n delete CMK (Customer Master Key) c·ªßa KMS, th√¨ c·∫ßn ch√∫ √Ω g√¨? C√≥ th·ªÉ x√≥a ngay l·∫≠p t·ª©c ko? th·ªùi gian default?\n B·∫°n kh√¥ng th·ªÉ x√≥a CMK ngay l·∫≠p t·ª©c, b·∫°n ph·∫£i l√™n schedule cho n√≥\nTh·ªùi gian schedule ƒë·ªÉ x√≥a CMK default l√† 30 ng√†y (30 days)\nKMS s·∫Ω ko th·ª±c hi·ªán rotate v·ªõi nh∆∞ng CMK ƒëang tr·ªçng tr·∫°ng th√°i pending deletion\n1 CMK pending deletion ko th·ªÉ s·ª≠ d·ª•ng tron b·∫•t c·ª© ho·∫°t ƒë·ªông cryptographic n√†o\n Q. App c·ªßa b·∫°n d·ª±ng tr√™n EC2-Clastic Instance v√† trong 1 public subnet. Gi·ªù mu·ªën list c√°c IP truy c·∫≠p v√†o EC2 n√™n l√†m n√†o?\n D√πng VPC Flow log, nh∆∞ng tr∆∞·ªõc h·∫øt ph·∫£i migrate App sang EC2-VPC, ch·ª© lo·∫°i EC2-Clasic ko d√πng VPC Flow log ƒë∆∞·ª£c\n Q. Cloudformation c√≥ c√°i DeletionPolicy, c√°i n√†y c√≥ nh·ªØng option g√¨?\n Retain: gi·ªØ l·∫°i resource khi Stack b·ªã x√≥a\nDelete: delete all (default)\nSnapshot: t·∫°o snapshot tr∆∞·ªõc khi x√≥a\n Q. Default khi t·∫°o VPC th√¨ qua CLI/Console/VPC Wizard, nh·ªØng c√°i sau dc set true hay false: enableDnsHostnames v√† enableDnsSupport?\n Khi t·∫°o b·∫±ng VPC Wizard: c·∫£ 2 set true\nKhi t·∫°o b·∫±ng Console, CLI: ch·ªâ c√≥ enableDnsSupport=true\n Q. B·∫°n c·∫ßn setup 1 infra ƒë·ªÉ c√≥ th·ªÉ t·ª± ƒë·ªông provision ra h√†ng ng√†n EC2 at any scale, d√πng service g√¨?\n EMR (Elastic MapReduce), n√≥ gi√∫p b·∫°n d·ªÖ d√†ng tƒÉng gi·∫£m s·ªë l∆∞·ª£ng ec2 c√≥ th·ªÉ access v√† qu·∫£n l√Ω.\n Q. B·∫°n c√≥ 1 bucket S3, mu·ªën setting ƒë·ªÉ ch·ªâ c√≥ th·ªÉ ƒë·ªçc v√† vi·∫øt ch·ª© ko th·ªÉ x√≥a object th√¨ l√†m n√†o? MFA Delete hay attach S3 bucket policy?\n Attach 1 S3 bucket policy\nko ch·ªçn MFA delete v√¨ n√≥ d√πng ƒë·ªÉ ch·∫∑n tr∆∞·ªùng h·ª£p x√≥a 1 c√°ch v√¥ √Ω th√¥i, n·∫øu c√≥ quy·ªÅn v√† MFA code v·∫´n c√≥ th·ªÉ x√≥a dc\n Q. 1 Cty ƒëang consider d√πng RDS, khi d√πng RDS th√¨ nh·ªØng activities g√¨ s·∫Ω do AWS perform, ng d√πng ko c·∫ßn quan t√¢m?\n ‚ñ† create v√† maintain backup v·ªõi point in time recovery 5 minutes\n‚ñ† install c√°c b·∫£n v√° b·∫£o m·∫≠t (security patch)\nCh·ª© c√≤n nh·ªØng th∆∞ nh∆∞ config Multi-AZ t·ª± ƒë·ªông, t·∫°o Read Replica t·ª± ƒë·ªông khi utilization cao th√¨ AWS ko perform, m√† ph√≠a ng∆∞·ªùi d√πng ph·∫£i config/enable\n Q. 1 Game online ƒëang host tr√™n 1 EC2. H·ªç chu·∫©n b·ªã release 1 b·∫£n game m·ªõi, c·∫ßn ch·∫Øc ch·∫Øn l√† website c·ªßa h·ªç ƒë·ªß kh·∫£ nƒÉng support cho new players. C·∫ßn l√†m g√¨?\n T·∫°o 1 ASG t·ª´ c√°i EC2 ƒëang ch·∫°y\nC√≥ 2 ki·ªÉu t·∫°o ASG Launch Configuration t·ª´ scratch, v√† t·∫°o ASG Launch configuration t·ª´ exist EC2\n Q. B·∫°n c·∫ßn report v·ªÅ t√¨nh tr·∫°ng OS patching process c·ªßa RDS, xem c√≥ c√°i g√¨ vulnerability ko, th√¨ c√≥ c·∫ßn d√πng AWS Inspector ko?\n Kh√¥ng. Ch·ªâ c·∫ßn d√πng RDS Console l√† ƒë·ªß r·ªìi\n Q. B·∫°n c·∫ßn ensure l√† all object S3 bucket ƒë∆∞·ª£c encrypt at rest. C·∫ßn config S3 bucket ƒë·ªÉ n√≥ deny nh·ªØng request m√† ko c√≥ header l√† g√¨?\n x-amz-server-side-encryption\n Q. B·∫°n c√≥ 1 RDS database, gi·ªù mu·ªën chuy·ªÉn sang DynamoDB, n√™n b·∫°n c·∫ßn disable backup c·ªßa RDS v√¨ ko d√πng n·ªØa. B·∫°n c√≥ th·ªÉ set retention period xu·ªëng 1, nh∆∞ng khi set xu·ªëng 0 b·ªã l·ªói. V√¨ sao?\n V√¨ RDS c·ªßa b·∫°n ƒëang setting c√≥ Read Replica\n Q. B·∫°n migrate 1 online accounting app d√πng TCP protocol l√™n AWS. C·∫ßn ƒë·∫£m b·∫£o t√≠nh scalability v√† availability, ƒë·ªìng th·ªùi ph·∫£i record dc IP c·ªßa client ƒë·ªÉ tracking n·ªØa. D√πng c√°i g√¨ 2 c√°ch sau?\n1: ELB + 1 TCP Listener + enable Proxy Protocol\n2: ELB + 1 TCP Listener + enable Cross-Zone Load Balancing\n D√πng c√°ch 1, v√¨ enable Cross-Zone Load Balancing s·∫Ω ko th·ªÉ record dc IP c·ªßa User d√πng app\n Q. 1 Cty d√πng Kinesis ƒë·ªÉ li√™n t·ª•c collect data c·ªßa user. V√¨ data nh·∫°y c·∫£m n√™n c·∫ßn secure at rest. N√™n config Kinesis s·ª≠ d·ª•ng server-side encryption hay client-side encryption?\n D√πng Kinesis server-side encryption (ko n√™n d√πng client-side encrpytion v√¨ kh√≥ setup h∆°n nhi·ªÅu)\nC√¢u h·ªèi l√† secure at rest, n√™n d√πng server-side n√≥ s·∫Ω encrpyt khi n√≥ ch∆∞a ghi v√†o DB, v√† decrypt sau khi l·∫•y ra t·ª´ DB\nN·∫øu c·∫ßn secure in transit, th√¨ n√™n d√πng client-side, n√≥ s·∫Ω encrpyt khi n√≥ ƒëi ra kh·ªèi client, v√† ƒë·∫øn l√∫c v√†o DB th√¨ n√≥ ƒë√£ dc encrypt r·ªìi (qu√£ng ƒë∆∞·ªùng t·ª´ client ƒë·∫øn server th√¨ data ƒë√£ dc encrpyt n√™n ch√≠nh l√† in transit)\nC√°c c√¢u h·ªèi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh thi  Q. EFS encrpytion th√¨ c·∫ßn d√πng c√°i g√¨? CloudHSM hay Server-side Encryption c√≥ s·∫µn c·ªßa EFS\n  Q. Khi t·∫°o Billing alert th√¨ d√πng service n√†o? AWS budget? hay Trusted Advisor? hay Cost Explorer? hay Cost and Usage?\n  Q. Account A t·∫°o AWS Catalog portfolio r·ªìi share cho Account B, v·∫≠y th√¨ B c√≥ th·ªÉ l√†m g√¨ v·ªõi portfolio ƒë√≥? Create/Update/Delete ƒë∆∞·ª£c ko? hay ch·ªâ t·∫°o ƒë∆∞·ª£c ·ªü local?\n  Q. NAT Gateway n·∫±m trong public subnet hay private subnet?\n  Q. Cloudformation, sau khi ch·∫°y stack l·ªói, b·∫°n s·ª≠a l·∫°i template xong th√¨ x√≥a t·∫°o l·∫°i stack hay update stack ƒë√≥? hay t·∫°o ChangeSet?\n  Q. B·∫°n ƒë√£ issued 1 certificate, gi·ªù mu·ªën t·∫°o key trong 1 secure environment, c√≥ th·ªÉ l√†m c√°c crpytographic operation th√¨ d√πng service n√†o? CloudHSM hay KMS\n  Q. Cty b·∫°n ƒë√£ centralized t·∫•t c·∫£ log v√†o 1 log group. C·∫ßn ph·∫£i xem c√°i log ƒë·ªÉ notify cho c√°c team li√™n quan. Th√¨ c√≥ c√°ch n√†o most effective? C√≥ n√™n chia ra nhi·ªÅu log group r·ªìi subcribe m·ªói team 1 log group?\n  Q. Khi d√πng AWS Aurora th√¨ resposibility c·ªßa b·∫°n l√† g√¨, provisioning storage DB? hay Schedule patch, maintain, update OS? hay Excute patch, maintain, update OS?\n  Q. Lambda ·ªü account A push object l√™n s3 bucket ·ªü account B, nh∆∞ng B ko x√≥a dc object ƒë√≥ th√¨ n√™n l√†m n√†o? Lambda c·∫ßn put c·∫£ objectACl n·ªØa, define owner c√≥ quy·ªÅn full access\n  Q. Cloudtrail c√≥ kh√°i ni·ªám data event log v√† management event log ko? hay c√≥ c√°i n√†o trong 2 c√°i ƒë√≥?\n  Q. AWS Orgnization c·ªßa b·∫°n mu·ªën xem ai make request create new account th√¨ xem ·ªü ƒë√¢u? Cloudtrail log hay Organization access log? C√≥ c√°i g·ªçi l√† Orgnization access log ko?\n  Q. Khi t·∫°o S3 bucket policy, c√≥ c√°i resource arn ki·ªÉu ...bucket/* ko?\n  Q. 1 VPC c√≥ 2 public subnet, 2 private subnet, 1 igw, 1 natgw, c√≥ potential nguy hi·ªÉm ti·ªÅm t√†ng g√¨? Single natgw ko redundant?, Single igw ko redundant?, VPC default route table ch·ªâ associate dc v·ªõi 1 AZ?\n  Q. V·ªÅ cloudtrail intergrity, ƒë·ªÉ prevent modify/delete c√°i trail log, n√™n b·∫≠t mfa delele cho bucket ƒë√≥? Hay edit policy ƒë·ªÉ DENY ALL\n  Q. C√≥ 2 VPC, VPC 1 l√† ch√≠nh ·ªü 1 region, VPC 2 l√† c√°i d·ª± ph√≤ng ƒëang ƒë·ªÉ ·ªü region kh√°c, gi·ªù mu·ªën sync data t·ª´ VPC 1 sang 2, m√† traffic ko b·ªã expose qua internet th√¨ n√™n l√†m n√†o? t·∫°o 1 inter-vpc peering √†? hay t·∫°o 1 EC2 d√πng l√†m VPN?\n Done!\nCh√∫c c√°c b·∫°n th√†nh c√¥ng, m√¨nh passed r·ªìi üéâ üéâ üéâ\n","href":"/posts/encrypt-aws-certified-sysops-administrator-associate-note-soa/","title":"Aws Certified Sysops Administrator Associate Note (SOA)"},{"content":"B√†i n√†y h∆∞·ªõng d·∫´n t·∫°o 1 function lambda start EC2 v√† g·ª≠i message v·ªÅ Discord/Slack.\nNgo√†i ra, d·ª±a v√†o ƒë√≥, c√°c b·∫°n c√≥ th·ªÉ setting Cloudwatch rule ƒë·ªÉ trigger Lambda function theo 1 Schedule mong mu·ªën.\nI. Discord 1. Y√™u c·∫ßu ƒêang s·ª≠ d·ª•ng h·ªá ƒëi·ªÅu h√†nh Window (MacOS c≈©ng ƒë∆∞·ª£c, nh∆∞ng c√°c b·∫°n s·∫Ω c·∫ßn hi·ªÉu dc ch·ªó n√†o n√™n l√†m theo/ko l√†m theo)\n2. C√°ch l√†m 2.1. T·∫°o bot tr√™n discord V√†o discord t·∫°o server \u0026ldquo;Test\u0026rdquo;\nTrong server ƒë√≥, t·∫°o channel \u0026ldquo;test-channel\u0026rdquo;\nV√†o User setting b·∫≠t Developer Mode\nQuay l·∫°i channel \u0026ldquo;test-channel\u0026rdquo; chu·ªôt ph·∫£i ƒë·ªÉ l·∫•y channel ID\nL∆∞u l·∫°i Channel Id: 61623232544545162 ƒë·ªÉ l√°t n·ªØa d√πng.\nV√†o link sau: https://discordapp.com/developers/applications/\nCh·ªçn \u0026ldquo;New Application\u0026rdquo;\nƒë·∫∑t t√™n cho app: \u0026ldquo;test-app\u0026rdquo;\nV√†o tab BOT, ch·ªçn \u0026ldquo;Add Bot\u0026rdquo; ƒë·∫∑t t√™n cho bot: \u0026ldquo;test-bot\u0026rdquo;\nl∆∞u l·∫°i TOKEN c·ªßa BOT: NjEASSSSMzg2QRUFDJSHFJD.XAGGGGQ.BHASSSSFFFFlas4FucsS8M ƒë·ªÉ l√°t n·ªØa d√πng.\nV√†o tab \u0026ldquo;OAuth2\u0026rdquo;, ch·ªçn Scope l√† \u0026ldquo;bot\u0026rdquo;, ch·ªçn Permission b·∫±ng c√°ch select all option c·ªôt \u0026ldquo;Text permission\u0026rdquo;\ncopy URL c·ªßa Bot ra: https://discordapp.com/api/oauth2/authorize?client_id=616163386459160596\u0026amp;permissions=522304\u0026amp;scope=bot\nD√πng URL ƒë√≥ paste v√†o tr√¨nh duy·ªát Chrome ch·∫≥ng h·∫°n\nCh·ªçn server Test m√¨nh ƒë√£ t·∫°o, ·∫•n Authorize\nD·∫•u t√≠ch xanh hi·ªán ra l√† ok\n2.2. T·∫°o AWS Lambda layer \u0026ldquo;discord-layer\u0026rdquo; Launch 1 EC2 Amazon linux, SSH v√†o, run c√°c command sau:\nsudo yum install python36 python36-pip -y mkdir ~/schedule-start-ec2 cd ~/schedule-start-ec2 sudo python3 -m pip install --target ./package/python discord.py==0.16.12 cd package/ zip -r9 ${OLDPWD}/layer.zip . D√πng winSCP l·∫•y file layer.zip ra\nV√†o AWS Lambda, t·∫°o 1 layer ƒë·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán discord\nt√™n layer: \u0026ldquo;discord-layer\u0026rdquo;, upload file \u0026ldquo;layer.zip\u0026rdquo; v·ª´a t·∫°o ·ªü tr√™n\n2.3. T·∫°o AWS Lambda Function V√†o AWS Lambda, t·∫°o function schedule-start-ec2\nCh√∫ √Ω ch·ªçn python 3.6 nh√© (v√¨ b√†i n√†y m√¨nh s·ª≠ d·ª•ng 3.6 v√† ch∆∞a test c√°c version kh√°c) Add layer \u0026ldquo;discord-layer\u0026rdquo; m√† m√¨nh v·ª´a t·∫°o v√†o\nLayer c·ªßa b·∫°n c√≥ d·∫°ng arn:aws:lambda:us-east-1:973219233233:layer:discord-layer:1\nTh·ªùi gian Timeout c·ªßa Lambda n√™n set l√† 10s\nNh·ªõ ch·ªçn role ƒë·ªÉ Lambda c√≥ th·ªÉ start EC2\nN·ªôi dung function nh∆∞ sau: (Ch√∫ √Ω b·∫°n c·∫ßn s·ª≠a ph·∫ßn # Your personal info)\nimport boto3, logging, os from botocore.exceptions import ClientError import discord \u0026#34;\u0026#34;\u0026#34; Start a specific instance \u0026#34;\u0026#34;\u0026#34; # Logger settings - CloudWatch # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) # Your personal info region = \u0026#34;us-east-1\u0026#34; ec2_id = \u0026#34;i-00019a293857hfdk8a\u0026#34; TOKEN = \u0026#34;NjEASSSSMzg2QRUFDJSHFJD.XAGGGGQ.BHASSSSFFFFlas4FucsS8M\u0026#34; channel_id_number = \u0026#34;61623232544545162\u0026#34; # Define Discord bot client client = discord.Client() def start_ec2_command(region, ec2_id): \u0026#34;\u0026#34;\u0026#34; Tries to Start EC2. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;, region) # start ec2 ec2.start_instances(InstanceIds=[ec2_id]) return True except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info(\u0026#34;Start Ec2 command throttled, automatically retrying...\u0026#34;) start_ec2_command(region, ec2_id) else: logger.error(\u0026#34;Start Ec2 command Failed!\\n%s\u0026#34;, str(err)) return False except: raise @client.event async def on_ready(): print(\u0026#39;Logged in as %s\u0026#39; % client.user.name) print(\u0026#39;------------------\u0026#39;) ec2_start = start_ec2_command(region, ec2_id) if ec2_start == True: msg = \u0026#34;EC2 \\\u0026#34;%s\\\u0026#34; has been start as scheduled\u0026#34; % ec2_id else: msg = \u0026#34;I try to start EC2 \\\u0026#34;%s\\\u0026#34; but something go wrong, sorry :( \u0026#34; % ec2_id # send discord message channel_id = client.get_channel(str(channel_id_number)) await client.send_message(channel_id, msg, tts = True) await client.close() def lambda_handler(event, context): # Run Discord client  client.run(TOKEN) return True 2.4. Test Save l·∫°i v√† Test function lambda tr√™n\nFunction s·∫Ω ch·∫°y trong 10s r·ªìi return true.\nV√†o Discord ƒë·ªÉ check tin nh·∫Øn nh∆∞ h√¨nh l√† ok:\nDone!\nƒê·∫øn ƒë√¢y c√°c b·∫°n c√≥ th·ªÉ k·∫øt h·ª£p Cloudwatch Rule ƒë·ªÉ set trigger function lambda n√†y b·∫•t c·ª© khi n√†o b·∫°n mu·ªën, k·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ discord.\nS·∫Ω c√≥ r·∫•t nhi·ªÅu √Ω t∆∞·ªüng d·ª±a v√†o nh·ªØng s·ª± k·∫øt h·ª£p kh√°c nhau, t√πy kh·∫£ nƒÉng s√°ng t·∫°o m·ªói ng∆∞·ªùi üòÇ\nII. Slack ƒê·ªÉ g·ª≠i message l√™n Slack th√¨ ƒë∆°n gi·∫£n h∆°n Discord nhi·ªÅu, c√°c b·∫°n ch·ªâ c·∫ßn add Incoming Webhook v√†o, g·ª≠i message ƒë·∫øn Webhook ƒë√≥ l√† ƒë∆∞·ª£c\n1. Install Incoming WebHooks plugin into your slack. Goto App Directory \u0026gt; Search Incoming WebHooks.\nClick on Incoming WebHooks.\nClick on Add to Slack.\nConfigure Incoming WebHooks nh∆∞ h√¨nh sau.\n2. T·∫°o AWS Lambda Function V√†o AWS Lambda, t·∫°o function test-send-msg-to-slack\nCh√∫ √Ω ch·ªçn python \u0026gt;= 3.6 nh√©\nN·ªôi dung function nh∆∞ sau: (Ch√∫ √Ω b·∫°n c·∫ßn s·ª≠a url)\nimport boto3 import time import urllib3, json from botocore.vendored import requests http = urllib3.PoolManager() url = \u0026#34;https://hooks.slack.com/services/T0????D1B/B0?????CS/mHtL3PAM????????LTi\u0026#34; payload={\u0026#34;text\u0026#34;: \u0026#34;This is a line of text in a channel.\\nAnd this is another line of text.\u0026#34;} def lambda_handler(event, context): response = http.request(\u0026#39;POST\u0026#39;, url, body = json.dumps(payload), headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}, retries = False) return \u0026#34;Sent message to Slack!\u0026#34; 3. Test B√¢y gi·ªù c√°c b·∫°n c√≥ th·ªÉ run th·ª≠ function Lambda v√† check xem channel ƒë√£ c√≥ tin nh·∫Øn hay ch∆∞a nh√©.\n","href":"/bk/send-message-to-discord-slack-using-aws-lambda/","title":"Send Message to Discord/Slack Using Aws Lambda"},{"content":"B√†i n√†y h∆∞·ªõng d·∫´n t·∫°o 1 function lambda start EC2 v√† g·ª≠i message v·ªÅ Discord/Slack.\nNgo√†i ra, d·ª±a v√†o ƒë√≥, c√°c b·∫°n c√≥ th·ªÉ setting Cloudwatch rule ƒë·ªÉ trigger Lambda function theo 1 Schedule mong mu·ªën.\nI. Discord 1. Y√™u c·∫ßu ƒêang s·ª≠ d·ª•ng h·ªá ƒëi·ªÅu h√†nh Window (MacOS c≈©ng ƒë∆∞·ª£c, nh∆∞ng c√°c b·∫°n s·∫Ω c·∫ßn hi·ªÉu dc ch·ªó n√†o n√™n l√†m theo/ko l√†m theo)\n2. C√°ch l√†m 2.1. T·∫°o bot tr√™n discord V√†o discord t·∫°o server \u0026ldquo;Test\u0026rdquo;\nTrong server ƒë√≥, t·∫°o channel \u0026ldquo;test-channel\u0026rdquo;\nV√†o User setting b·∫≠t Developer Mode\nQuay l·∫°i channel \u0026ldquo;test-channel\u0026rdquo; chu·ªôt ph·∫£i ƒë·ªÉ l·∫•y channel ID\nL∆∞u l·∫°i Channel Id: 61623232544545162 ƒë·ªÉ l√°t n·ªØa d√πng.\nV√†o link sau: https://discordapp.com/developers/applications/\nCh·ªçn \u0026ldquo;New Application\u0026rdquo;\nƒë·∫∑t t√™n cho app: \u0026ldquo;test-app\u0026rdquo;\nV√†o tab BOT, ch·ªçn \u0026ldquo;Add Bot\u0026rdquo; ƒë·∫∑t t√™n cho bot: \u0026ldquo;test-bot\u0026rdquo;\nl∆∞u l·∫°i TOKEN c·ªßa BOT: NjEASSSSMzg2QRUFDJSHFJD.XAGGGGQ.BHASSSSFFFFlas4FucsS8M ƒë·ªÉ l√°t n·ªØa d√πng.\nV√†o tab \u0026ldquo;OAuth2\u0026rdquo;, ch·ªçn Scope l√† \u0026ldquo;bot\u0026rdquo;, ch·ªçn Permission b·∫±ng c√°ch select all option c·ªôt \u0026ldquo;Text permission\u0026rdquo;\ncopy URL c·ªßa Bot ra: https://discordapp.com/api/oauth2/authorize?client_id=616163386459160596\u0026amp;permissions=522304\u0026amp;scope=bot\nD√πng URL ƒë√≥ paste v√†o tr√¨nh duy·ªát Chrome ch·∫≥ng h·∫°n\nCh·ªçn server Test m√¨nh ƒë√£ t·∫°o, ·∫•n Authorize\nD·∫•u t√≠ch xanh hi·ªán ra l√† ok\n2.2. T·∫°o AWS Lambda layer \u0026ldquo;discord-layer\u0026rdquo; Launch 1 EC2 Amazon linux, SSH v√†o, run c√°c command sau:\nsudo yum install python36 python36-pip -y mkdir ~/schedule-start-ec2 cd ~/schedule-start-ec2 sudo python3 -m pip install --target ./package/python discord.py==0.16.12 cd package/ zip -r9 ${OLDPWD}/layer.zip . D√πng winSCP l·∫•y file layer.zip ra\nV√†o AWS Lambda, t·∫°o 1 layer ƒë·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán discord\nt√™n layer: \u0026ldquo;discord-layer\u0026rdquo;, upload file \u0026ldquo;layer.zip\u0026rdquo; v·ª´a t·∫°o ·ªü tr√™n\n2.3. T·∫°o AWS Lambda Function V√†o AWS Lambda, t·∫°o function schedule-start-ec2\nCh√∫ √Ω ch·ªçn python 3.6 nh√© (v√¨ b√†i n√†y m√¨nh s·ª≠ d·ª•ng 3.6 v√† ch∆∞a test c√°c version kh√°c) Add layer \u0026ldquo;discord-layer\u0026rdquo; m√† m√¨nh v·ª´a t·∫°o v√†o\nLayer c·ªßa b·∫°n c√≥ d·∫°ng arn:aws:lambda:us-east-1:973219233233:layer:discord-layer:1\nTh·ªùi gian Timeout c·ªßa Lambda n√™n set l√† 10s\nNh·ªõ ch·ªçn role ƒë·ªÉ Lambda c√≥ th·ªÉ start EC2\nN·ªôi dung function nh∆∞ sau: (Ch√∫ √Ω b·∫°n c·∫ßn s·ª≠a ph·∫ßn # Your personal info)\nimport boto3, logging, os from botocore.exceptions import ClientError import discord \u0026#34;\u0026#34;\u0026#34; Start a specific instance \u0026#34;\u0026#34;\u0026#34; # Logger settings - CloudWatch # Set level to DEBUG for debugging, INFO for general usage. logger = logging.getLogger() logger.setLevel(logging.INFO) # Your personal info region = \u0026#34;us-east-1\u0026#34; ec2_id = \u0026#34;i-00019a293857hfdk8a\u0026#34; TOKEN = \u0026#34;NjEASSSSMzg2QRUFDJSHFJD.XAGGGGQ.BHASSSSFFFFlas4FucsS8M\u0026#34; channel_id_number = \u0026#34;61623232544545162\u0026#34; # Define Discord bot client client = discord.Client() def start_ec2_command(region, ec2_id): \u0026#34;\u0026#34;\u0026#34; Tries to Start EC2. If a ThrottlingException is encountered recursively calls itself until success. \u0026#34;\u0026#34;\u0026#34; try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;, region) # start ec2 ec2.start_instances(InstanceIds=[ec2_id]) return True except ClientError as err: if \u0026#39;ThrottlingException\u0026#39; in str(err): logger.info(\u0026#34;Start Ec2 command throttled, automatically retrying...\u0026#34;) start_ec2_command(region, ec2_id) else: logger.error(\u0026#34;Start Ec2 command Failed!\\n%s\u0026#34;, str(err)) return False except: raise @client.event async def on_ready(): print(\u0026#39;Logged in as %s\u0026#39; % client.user.name) print(\u0026#39;------------------\u0026#39;) ec2_start = start_ec2_command(region, ec2_id) if ec2_start == True: msg = \u0026#34;EC2 \\\u0026#34;%s\\\u0026#34; has been start as scheduled\u0026#34; % ec2_id else: msg = \u0026#34;I try to start EC2 \\\u0026#34;%s\\\u0026#34; but something go wrong, sorry :( \u0026#34; % ec2_id # send discord message channel_id = client.get_channel(str(channel_id_number)) await client.send_message(channel_id, msg, tts = True) await client.close() def lambda_handler(event, context): # Run Discord client  client.run(TOKEN) return True 2.4. Test Save l·∫°i v√† Test function lambda tr√™n\nFunction s·∫Ω ch·∫°y trong 10s r·ªìi return true.\nV√†o Discord ƒë·ªÉ check tin nh·∫Øn nh∆∞ h√¨nh l√† ok:\nDone!\nƒê·∫øn ƒë√¢y c√°c b·∫°n c√≥ th·ªÉ k·∫øt h·ª£p Cloudwatch Rule ƒë·ªÉ set trigger function lambda n√†y b·∫•t c·ª© khi n√†o b·∫°n mu·ªën, k·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ discord.\nS·∫Ω c√≥ r·∫•t nhi·ªÅu √Ω t∆∞·ªüng d·ª±a v√†o nh·ªØng s·ª± k·∫øt h·ª£p kh√°c nhau, t√πy kh·∫£ nƒÉng s√°ng t·∫°o m·ªói ng∆∞·ªùi üòÇ\nII. Slack ƒê·ªÉ g·ª≠i message l√™n Slack th√¨ ƒë∆°n gi·∫£n h∆°n Discord nhi·ªÅu, c√°c b·∫°n ch·ªâ c·∫ßn add Incoming Webhook v√†o, g·ª≠i message ƒë·∫øn Webhook ƒë√≥ l√† ƒë∆∞·ª£c\n1. Install Incoming WebHooks plugin into your slack. Goto App Directory \u0026gt; Search Incoming WebHooks.\nClick on Incoming WebHooks.\nClick on Add to Slack.\nConfigure Incoming WebHooks nh∆∞ h√¨nh sau.\n2. T·∫°o AWS Lambda Function V√†o AWS Lambda, t·∫°o function test-send-msg-to-slack\nCh√∫ √Ω ch·ªçn python \u0026gt;= 3.6 nh√©\nN·ªôi dung function nh∆∞ sau: (Ch√∫ √Ω b·∫°n c·∫ßn s·ª≠a url)\nimport boto3 import time import urllib3, json from botocore.vendored import requests http = urllib3.PoolManager() url = \u0026#34;https://hooks.slack.com/services/T0????D1B/B0?????CS/mHtL3PAM????????LTi\u0026#34; payload={\u0026#34;text\u0026#34;: \u0026#34;This is a line of text in a channel.\\nAnd this is another line of text.\u0026#34;} def lambda_handler(event, context): response = http.request(\u0026#39;POST\u0026#39;, url, body = json.dumps(payload), headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}, retries = False) return \u0026#34;Sent message to Slack!\u0026#34; 3. Test B√¢y gi·ªù c√°c b·∫°n c√≥ th·ªÉ run th·ª≠ function Lambda v√† check xem channel ƒë√£ c√≥ tin nh·∫Øn hay ch∆∞a nh√©.\n","href":"/posts/send-message-to-discord-slack-using-aws-lambda/","title":"Send Message to Discord/Slack Using Aws Lambda"},{"content":"1. M√¥ t·∫£ Chia s·∫ª v·ªÅ 1 l·ªói khi s·ª≠ d·ª•ng AWS Lambda:\nM√¨nh d√πng Lambda ƒë·ªÉ get current timestamp, lambda n√†y ƒë∆∞·ª£c trigger b·ªüi Cloudwatch c·ª© 5 gi√¢y ch·∫°y 1 l·∫ßn.\nM√¨nh ph√°t hi·ªán ra l√† l·∫ßn ƒë·∫ßu th√¨ Lambda return k·∫øt qu·∫£ ƒë√∫ng 2019-08-27 08:46:54.867900.\nNh∆∞ng sau ƒë√≥ 5 gi√¢y v√†o check log trong Cloudwatch th√¨ th·∫•y v·∫´n tr·∫£ v·ªÅ timestamp c≈© 2019-08-27 08:46:54.867900.\nC·ª© th·∫ø, d√π 5 10 15 ph√∫t sau m√¨nh c·ª© ch·∫°y l·∫°i l√† log tr·∫£ v·ªÅ timestamp c≈©.\n2. T√°i hi·ªán l·ªói T·∫°o 1 funtion lambda get-current-time ƒë∆°n gi·∫£n nh∆∞ n√†y:\nimport datetime, boto3, time, logging from datetime import datetime logger = logging.getLogger() logger.setLevel(logging.INFO) #Get current datetime currentDT = datetime.now() def lambda_handler(event, context): logger.info(\u0026#34;Current datetime: %s\u0026#34; % currentDT) Test function Lambda tr√™n\nL·∫ßn ƒë·∫ßu tr·∫£ v·ªÅ ƒë√∫ng timestamp:\nCurrent datetime: 2019-08-27 08:46:54.867900 Ch·∫°y l·∫°i kho·∫£ng v√†i gi√¢y sau, k·∫øt qu·∫£ kh√¥ng ƒë·ªïi v·∫´n l√†:\nCurrent datetime: 2019-08-27 08:46:54.867900 3. Nguy√™n nh√¢n Nguy√™n nh√¢n l√† do Lambda ƒë√£ cache l·∫°i gi√° tr·ªã tr·∫£ v·ªÅ l·∫ßn ƒë·∫ßu\nv√† b·ªüi h√†m Lambda c·ªßa m√¨nh kh√¥ng thay ƒë·ªïi g√¨,\nc·∫£ event trigger c≈©ng ko thay ƒë·ªïi, th·∫ø n√™n Lambda c·ª© l·∫•y gi√° tr·ªã ƒë√£ cache ƒë∆∞·ª£c ·ªü l·∫ßn tr∆∞·ªõc th√¥i.\n4. Gi·∫£i ph√°p M√¨nh c·∫ßn t·∫°o 1 update ƒë·ªëi v·ªõi ch√≠nh h√†m get-current-time n√†y,\nƒë·ªÉ m·ªói l·∫ßn ch·∫°y, Lambda s·∫Ω nh·∫≠n ra l√† function c√≥ s·ª± thay ƒë·ªïi. Khi ƒë√≥ n√≥ s·∫Ω ko l·∫•y gi√° tr·ªã c≈© tr·∫£ v·ªÅ cho m√¨nh n·ªØa.\n·ªû ƒë√¢y m√¨nh s·∫Ω update l·∫°i ph·∫ßn Description c·ªßa function get-current-time.\nS·ª≠a function lambda l·∫°i nh∆∞ sau:\nimport datetime, boto3, time, logging from datetime import datetime logger = logging.getLogger() logger.setLevel(logging.INFO) #Get current datetime currentDT = datetime.now() def lambda_handler(event, context): logger.info(\u0026#34;Current datetime: %s\u0026#34; % currentDT) client = boto3.client(\u0026#39;lambda\u0026#39;) client.update_function_configuration( FunctionName=\u0026#34;get-current-time\u0026#34;, Description=\u0026#34;lambda run at [%s]\u0026#34; % currentDT ) Nh·ªõ Role c·ªßa Lambda function c·∫ßn c√≥ quy·ªÅn sau: lambda:UpdateFunctionConfiguration\nTest l·∫°i function tr√™n:\nCurrent datetime: 2019-08-27 13:58:52.281869 Test l·∫°i v√†i l·∫ßn:\nCurrent datetime: 2019-08-27 13:59:21.350455 Done!\n","href":"/bk/aws-lambda-get-current-date-time-wrong/","title":"Aws Lambda get Current Date Time Wrong"},{"content":"1. M√¥ t·∫£ Chia s·∫ª v·ªÅ 1 l·ªói khi s·ª≠ d·ª•ng AWS Lambda:\nM√¨nh d√πng Lambda ƒë·ªÉ get current timestamp, lambda n√†y ƒë∆∞·ª£c trigger b·ªüi Cloudwatch c·ª© 5 gi√¢y ch·∫°y 1 l·∫ßn.\nM√¨nh ph√°t hi·ªán ra l√† l·∫ßn ƒë·∫ßu th√¨ Lambda return k·∫øt qu·∫£ ƒë√∫ng 2019-08-27 08:46:54.867900.\nNh∆∞ng sau ƒë√≥ 5 gi√¢y v√†o check log trong Cloudwatch th√¨ th·∫•y v·∫´n tr·∫£ v·ªÅ timestamp c≈© 2019-08-27 08:46:54.867900.\nC·ª© th·∫ø, d√π 5 10 15 ph√∫t sau m√¨nh c·ª© ch·∫°y l·∫°i l√† log tr·∫£ v·ªÅ timestamp c≈©.\n2. T√°i hi·ªán l·ªói T·∫°o 1 funtion lambda get-current-time ƒë∆°n gi·∫£n nh∆∞ n√†y:\nimport datetime, boto3, time, logging from datetime import datetime logger = logging.getLogger() logger.setLevel(logging.INFO) #Get current datetime currentDT = datetime.now() def lambda_handler(event, context): logger.info(\u0026#34;Current datetime: %s\u0026#34; % currentDT) Test function Lambda tr√™n\nL·∫ßn ƒë·∫ßu tr·∫£ v·ªÅ ƒë√∫ng timestamp:\nCurrent datetime: 2019-08-27 08:46:54.867900 Ch·∫°y l·∫°i kho·∫£ng v√†i gi√¢y sau, k·∫øt qu·∫£ kh√¥ng ƒë·ªïi v·∫´n l√†:\nCurrent datetime: 2019-08-27 08:46:54.867900 3. Nguy√™n nh√¢n Nguy√™n nh√¢n l√† do Lambda ƒë√£ cache l·∫°i gi√° tr·ªã tr·∫£ v·ªÅ l·∫ßn ƒë·∫ßu\nv√† b·ªüi h√†m Lambda c·ªßa m√¨nh kh√¥ng thay ƒë·ªïi g√¨,\nc·∫£ event trigger c≈©ng ko thay ƒë·ªïi, th·∫ø n√™n Lambda c·ª© l·∫•y gi√° tr·ªã ƒë√£ cache ƒë∆∞·ª£c ·ªü l·∫ßn tr∆∞·ªõc th√¥i.\n4. Gi·∫£i ph√°p M√¨nh c·∫ßn t·∫°o 1 update ƒë·ªëi v·ªõi ch√≠nh h√†m get-current-time n√†y,\nƒë·ªÉ m·ªói l·∫ßn ch·∫°y, Lambda s·∫Ω nh·∫≠n ra l√† function c√≥ s·ª± thay ƒë·ªïi. Khi ƒë√≥ n√≥ s·∫Ω ko l·∫•y gi√° tr·ªã c≈© tr·∫£ v·ªÅ cho m√¨nh n·ªØa.\n·ªû ƒë√¢y m√¨nh s·∫Ω update l·∫°i ph·∫ßn Description c·ªßa function get-current-time.\nS·ª≠a function lambda l·∫°i nh∆∞ sau:\nimport datetime, boto3, time, logging from datetime import datetime logger = logging.getLogger() logger.setLevel(logging.INFO) #Get current datetime currentDT = datetime.now() def lambda_handler(event, context): logger.info(\u0026#34;Current datetime: %s\u0026#34; % currentDT) client = boto3.client(\u0026#39;lambda\u0026#39;) client.update_function_configuration( FunctionName=\u0026#34;get-current-time\u0026#34;, Description=\u0026#34;lambda run at [%s]\u0026#34; % currentDT ) Nh·ªõ Role c·ªßa Lambda function c·∫ßn c√≥ quy·ªÅn sau: lambda:UpdateFunctionConfiguration\nTest l·∫°i function tr√™n:\nCurrent datetime: 2019-08-27 13:58:52.281869 Test l·∫°i v√†i l·∫ßn:\nCurrent datetime: 2019-08-27 13:59:21.350455 Done!\n","href":"/posts/aws-lambda-get-current-date-time-wrong/","title":"Aws Lambda get Current Date Time Wrong"},{"content":"2019-August-15 Sau khi t·ª´ Nh·∫≠t v·ªÅ l·∫°i chi nh√°nh ·ªü VN th√¨ m√¨nh r·ª•c r·ªãch ƒëi t√¨m 1 c√¥ng ty m·ªõi.\nH√†nh tr√¨nh t√¨m ki·∫øm 1 c√¥ng vi·ªác m·ªõi ƒë√£ ƒë∆∞a m√¨nh ƒë·∫øn nhi·ªÅu v√≤ng ph·ªèng v·∫•n kh√°c nhau (kho·∫£ng 13 ch·ªó g·ª≠i CV th√¨ kho·∫£ng 6 ch·ªó g·ªçi ƒëi ph·ªèng v·∫•n tr·ª±c ti·∫øp).\nVi·∫øt b√†i n√†y ƒë·ªÉ note l·∫°i 1 s·ªë ƒëi·ªÉm l∆∞u √Ω v·ªÅ c√°c c√¥ng ty c≈©ng nh∆∞ t·ª± r√∫t kinh nghi·ªám cho nh·ªØng l·∫ßn xin vi·ªác sau.\nC√°c v·ªã tr√≠ m√¨nh ·ª©ng tuy·ªÉn ƒë·ªÅu l√† v·ªã tr√≠ DevOps.\n1. OSP Group Aipi C√¥ng ty th·ª© 1: OSP Group Aipi, T·∫ßng 7, To√† nh√† ƒê·∫°i Ph√°t, s·ªë 2 ng√µ 82 Duy T√¢n, C·∫ßu Gi·∫•y, H√† N·ªôi.\n(https://www.topcv.vn/viec-lam/nhan-vien-devops-engineer/238864.html) m√¥i tr∆∞·ªùng c√≥ v·∫ª OK, c√¥ng ty b√© ch·∫Øc 80 ng∆∞·ªùi th√¥i.\nƒê∆°n v·ªã c√≥ c√°i app ƒë·ªçc b√°o v√† mu·ªën tri·ªÉn khai Devops, nh∆∞ng ch∆∞a c√≥ nhi·ªÅu kinh nghi·ªám.\nKo c√≥ ai chuy√™n v·ªÅ DevOps ·ªü ƒë√≥ c·∫£. ƒê·ªãnh h∆∞·ªõng l√† d·ª± √°n l√†m v·ªÅ product duy tr√¨ trong kho·∫£ng 2 nƒÉm xem sao.\nM√¨nh n√≥i chuy·ªán vs HR v√† gi√°m ƒë·ªëc c√πng 2 b·∫°n backend (t·ªïng c·ªông 4) kh√° tho·∫£i m√°i, t·ª± tin. C√°c b·∫°n c≈©ng c√≥ kn l√†m docker k8s nh∆∞ng c≈©ng ch∆∞a s√¢u, n√≥i chuy·ªán vs m√¨nh c√≥ v·∫ª c√≤n run, anh gi√°m ƒë·ªëc bi·∫øt nhi·ªÅu, ch·∫Øc c·ª©ng nh·∫•t v·ªÅ tech, h·ªèi th√¨ c≈©ng c√≥ 1 v√†i c√°i m√¨nh ko bi·∫øt. M√¨nh deal 20 Net, ch·∫Øc ko tr·∫£ dc. N√™n d√π h·∫πn 5 ng√†y l√†m vi·ªác nh∆∞ng ƒë·∫øn 8 ng√†y l√†m vi·ªác v·∫´n ko tr·∫£ l·ªùi -\u0026gt; t·ª± bi·∫øt t·∫°ch\n2. IFI Solution NTT Data Corp C√¥ng ty th·ª© 2: IFI Solution NTT Data Corp, 12th Floor, Thang Long Tower, 98A Nguy Nhu Kon Tum, Hanoi, Vietnam.\n(https://itviec.com/it-jobs/devops-engineer-ifi-solution-an-ntt-data-company-5545)\nHR Dung n√≥i chuy·ªán ƒëi·ªáu, c√≥ v·∫ª b·ªÅ tr√™n.\nm·ªõi v√†o th√¨ ƒë∆∞a m√¨nh 1 ƒëo·∫°n vƒÉn ti·∫øng Vi·ªát ƒë·ªÉ d·ªãch sang ti·∫øng Anh, trong 15p.\na PM pv m√¨nh kh√° c·ª©ng v·ªÅ chuy√™n m√¥n. Ban ƒë·∫ßu v√†o b·∫Øt n√≥i chuy·ªán b·∫±ng ti·∫øng Anh, b·∫£o phong tr√†o ƒë√° b√≥ng ·ªü ƒë√¢y s√¥i n·ªïi, n·∫øu c√≥ ƒë√° b√≥ng th√¨ t·ªët. H·ªèi m√¨nh nhi·ªÅu c√°i m√¨nh ko bi·∫øt nh·∫•t (firewall, proxy, nginx\u0026hellip;).\nanh b·∫£o \u0026ldquo;kn m√¨nh c√≥ v·∫ª √≠t d√π ghi 4 nƒÉm, h·ªèi c√°i g√¨ c≈©ng ch·ªâ t√≠nh = th√°ng\u0026rdquo;. v·ªÅ sau b·∫£o m√¨nh c√≥ 2 nƒÉm l√†m trong 1 d·ª± √°n c√¥ng ngh√™ c≈© (colbol) n√™n ko lien quan nhi·ªÅu th√¨ m·ªõi \u0026ldquo;√† h√≥a ra th·∫ø\u0026rdquo;.\nAnh b·∫£o mu·ªën x√¢y dung team Devops kho·∫£ng 5 ng∆∞·ªùi, hi·ªán c√≥ 3. vi·ªác th√¨ v·∫´n l√† c√°c vi·ªác c≈©, nh∆∞ng team th√¨ c√≥ t√™n g·ªçi m·ªõi l√† team Devops. Qu·∫£n l√Ω v·∫≠n h√†nh c√≥ v·∫ª d√πng nhi·ªÅu ansible, docker, terraform, GitLab h·ªèi m√¨nh dung executor n√†o m√¨nh ko bi·∫øt.\nB·∫£o l√† ƒë√¢y ch·ªâ l√† pv v√≤ng 1. C√≤n 1 v√≤ng pv v·ªõi KH n·ªØa, pass th√¨ m·ªõi ok cho th·ª≠ vi√™c 2 th√°ng. ƒê·ªçc review tr√™n m·∫°ng th√¨ deal l∆∞∆°ng cao s·∫Ω c√≥ kh·∫£ nƒÉng t·∫°ch th·ª≠ vi·ªác n√™n m√¨nh c≈©ng h∆°i r√©n ch√∫t.\nH·∫πn 5.7 b√°o l·∫°i bao h h·∫πn pv v·ªõi KH\nCu·ªëi c√πng sang h·∫≥n 10.7 m·ªõi b√°o l·∫°i l√† 14.7 ƒëi pv l·∫ßn 2 (ch∆∞a r√µ k·∫øt qu·∫£)\nph·ªèng v·∫•n l·∫ßn 2 ƒë·∫øn c√≥ 1 b·∫°n n·ªØa l√† T√°m. Cty h·ªç th√¨ c√≥ th√™m 1 anh fullstack v√†o ki·ªÉu ch·ªâ ƒëi·ªÉm cho m√¨nh ( V√¨ l·∫ßn n√†y pv v·ªõi kh√°ch h√†ng). Ng∆∞·ªùi pv l√† 1 √¥ng ·∫§n ƒë·ªô ƒëang l√†m b√™n Ph√°p. √îng ·∫•y n√≥i r·∫•t nhi·ªÅu v√† lan man, m√¥ t·∫£ v·ªÅ h·ªá th·ªëng c·ªßa h·ªç. R·ªìi h·ªèi m√¨nh nh·ªØng c√¢u kh√° basic, c√≥ v·∫ª ch·ªß y·∫øu h·ªèi ƒë·ªÉ bi·∫øt chƒÉng (Ki·ªÉu network private c√≥ CIDR bock v√≠ d·ª• nh∆∞ n√†o, b√™n ngo√†i c√≥ anh Fullstack ch·ªâ ƒëi·ªÉm cho m√¨nh ki·ªÉu 10.0.0.1/8, 10.1.0.0/16), r·ªìi h·ªèi private cloud l√†m nh∆∞ n√†o ƒë·ªÉ secure resource (ƒë·ªÉ trong private subnet), th·ª±c ra nh·ªØng ki·∫øn th·ª©c n√†y r·∫•t c∆° b·∫£n. Nh∆∞ng v·∫•n ƒë·ªÅ ·ªü ch·ªó √¥ng ·∫§n ƒë·ªô ƒë√≥ (t√™n Louis) n√≥i r·∫•t kh√≥ nghe, accent kh√≥ nghe h∆°n c·∫£ tr√™n youtube, may m√† n√≥i to n√™n m√¨nh nghe keyword ƒëo√°n ch·ªØ dc, v√† nghe l√¢u th√¨ quen.\nSau ƒë√≥ th√¨ ko c√≥ k·∫øt qu·∫£ g√¨, m√£i ƒë·∫øn 22.7 m·ªõi g·ªçi l·∫°i cho m√¨nh th√¨ l√∫c ·∫•y m√¨nh ƒë√£ accept offer b√™n SmartOSC r·ªìi.\nH·ªç h·ªèi offer bao nhi√™u, m√¨nh b·∫£o 20net, th·∫ø l√† IFI g·ª≠i l·∫°i offer 32 gross lu√¥n. L√†m m√¨nh c√≥ 1 ch√∫t ƒë·∫Øn ƒëo khi ko bi·∫øt n√™n ch·ªçn b√™n n√†o.\n3. Sota Tek C√¥ng ty th·ª© 3: Sota Tek, 7th Floor, CIC Tower, No.2, Lane 219 Trung Kinh, Ha Noi\n(https://www.topcv.vn/viec-lam/devops-engineer/220257.html)\nC√¥ng ty c√≥ v·∫ª ƒë·∫πp. To ch·∫Øc ph·∫£i 150 ng. ph·ªèng v·∫•n v·ªõi 1 anh tech c≈©ng c·ª©ng v·ªÅ k·ªπ thu·∫≠t. c≈©ng h·ªèi nh·ªØng c√°i m√¨nh ko bi·∫øt. nh∆∞ng c√≥ v·∫ª m√¨nh n√≥i dc nhi·ªÅu h∆°n v√† t·ª± tin h∆°n, d√π 1 s·ªë ch·ªó solution m√¨nh c≈©ng ko t·ªët l·∫Øm. n√≥i xong 1 t√Ω anh ·∫•y ƒëi ra lu√¥n, ko cho m√¨nh h·ªèi v·ªÅ v·ªã tr√≠ c√¥ng vi·ªác nh∆∞ n√†o. 1 anh n·ªØa ph·ªèng v·∫•n ti·∫øng Nh·∫≠t, ƒë∆∞a 1 ƒëo·∫°n b√†i b√°o c√¥ng ty cho m√¨nh d·ªãch. ch·∫Øc v√¨ m√¨nh c√≥ N2 n√™n ch·∫Øc anh ·∫•y v√†o pv. Nh∆∞ng m√¨nh l√¢u ko ƒë·ª•ng t·ª´ v·ª±ng n√™n d·ªãch ko t·ªët. nhi·ªÅu ch·ªó ph·∫£i h·ªèi n√™n ch∆∞a d·ªãch xong anh ·∫•y ƒë√£ b·∫£o th√¥i r·ªìi. R·ªìi b·∫£o th√¥i ti·∫øng nh·∫≠t k√©m c≈©ng ko sao, v·ªã tr√≠ n√†y ko y√™u c·∫ßu l·∫Øm ti·∫øng nh·∫≠t l·∫Øm.\nCty c√≥ v√†i s√†n giao d·ªãch ti·ªÅn ·∫£o mu·ªën maintain. nh∆∞ng c·ª• th·ªÉ h∆°n l√†m v·ªÅ c√¥ng ngh·ªá g√¨ th√¨ anh ·∫•y l√† l√£nh ƒë·∫°o n√™n ch·∫Øc c≈©ng ch·∫£ bi√™t tr·∫£ l·ªùi nh∆∞ n√†o.\nV·ªÅ v·ªã tr√≠ Devops th·∫•y b·∫£o c√≥ 2 ng∆∞·ªùi expert r·ªìi. Nhu c·∫ßu tuy·ªÉn them 2 thi ph·∫£i.\nM√¨nh ƒë√≤i 20 net. ko th·∫•y h·ªèi bao h ƒëi l√†m dc. N√™n ch·∫Øc l√† t·∫°ch. V·ªÅ nh√† 1 2 ng√†y th√¨ th·∫•y mail t·∫°ch th·∫≠t.\n4. Hyperlogy C√¥ng ty th·ª© 4: Hyperlogy, VƒÉn ph√≤ng C√¥ng ty ‚Äì T·∫ßng 4, t√≤a nh√† 29T1, Ho√†ng ƒê·∫°o Th√∫y, C·∫ßu Gi·∫•y, H√† N·ªôi.\nC√¥ng ty c√≥ 2 m·∫£ng, 1 m·∫£ng v·ªÅ t∆∞ v·∫•n g√¨ ƒë√≥. 1 m·∫£ng IT. L√†m vi·ªác 1 tu·∫ßn 2 th·ª© 7 full time. C√≥ 1 ƒë·ªôi onsite c√¥ng ty kh√°c, t7 v·ªÅ l·∫°i c√¥ng ty ch√≠nh. M·∫£ng devops th√¨ ch·ªâ c√≥ 1 anh l√†m. L√†m ch·ªß y·∫øu tr√™n VM m√°y ·∫£o. ch·ª© ko ph·∫£i tr√™n Cloud. m·∫∑c d√π t∆∞∆°ng lai th√¨ s·∫Ω c√≥ l√™n Cloud. ph·ªèng v·∫•n c√≥ 1 ch·ªã HR, 1 anh tech lead, 1 anh Devops. C·∫£ 3 ƒë·ªÅu c√≥ v·∫ª th√¢n thi·ªán. L√†m vi·ªác c√≥ v·∫ª thi√™n nhi·ªÅu v·ªÅ l·∫Øp ƒë·∫∑t c√†i c·∫Øm server ph·∫ßn c·ª©ng h∆°n l√† ph·∫ßn m·ªÅm. Source code l∆∞u tr√™n SVN ch·ª© ko dung Git. GitLab c√≥ nhung b·ªè l√¢u ko dung. Jenkin c√≥, c≈©ng c√≥ vi·∫øt c√°c script pipeline. K8s c≈©ng ƒëang x√¢y dung, nh∆∞ng l√† l√†m tr√™n nh∆∞ng m√°y hardware ·ªü t·∫°i c√¥ng ty ho·∫∑c c·ªßa KH, ch·ª© ko ph·∫£i tr√™n Cloud. h·ªèi l∆∞∆°ng bao nhi√™u m√¨nh c√≥ n√≥i 18 net.\nV·ªÅ nh√† 1,2 h√¥m th√¨ c≈©ng b·∫Øn mail c·∫£m ∆°n (t·∫°ch)\n5. Smart OSC C√¥ng ty th·ª© 5: Smart OSC, 18th Floor, Handico Tower, Pham Hung, Me Tri, Nam Tu Liem, Ha Noi.\nC√¥ng ty to r·ªông, kho·∫£ng 400 ng∆∞·ªùi. chi·∫øm nguy√™n 1 tang 18 lu√¥n. v√† 1/3 t·∫ßng 19. HR anh Ki√™n kh√° th√¢n thi·ªán\nph·ªèng v·∫•n c√≥ 2 anh a To√†n v√† a H√†o. a To√†n tr∆∞·ªõc c≈©ng FU k5 ch∆∞a l·∫•y bang. c√≥ v·∫ª r·∫•t gi·ªèi, ti·∫øng Anh t·ªët.\n·ªü ƒë·∫•y ƒëang c√≥ 3 d·ª± √°n, d·ª± √°n th·ª© nh·∫•t t√™n l√† Central l√†m v·ªÅ e-commerce l√†m tr·ª±c ti·∫øp v·ªõi KH Thai Lan. Hi·ªán ·ªü cty c√≥ 20 Dev cho d·ª± √°n n√†y.\ntuy·ªÉn 7 Devops, m√† m·ªõi tuy·ªÉn dc 3 senior. c√≤n them 4 ng n·ªØa. N·∫øu pass v√≤ng n√†y th√¨ c√≤n 1 v√≤ng pv v·ªõi Kh n·ªØa bang ti·∫øng Anh. L√†m v·ªÅ magento php\nch·∫°y tr√™n AWS, k8s, bitbucket, Jenkins, goCD, spinnaker. D·ª± √°n si√™u to. (Gi·ªù ƒë·∫øn cu·ªëi nƒÉm c·∫ßn 30-100 Dev). l·ªõn v√† ri√™ng branch ƒë√£ c√≥ kho·∫£ng 10 m·∫•y branch.\nsite nh∆∞ tiki. v√†o th√¨ v·ª´a maintain v·ª´a dung them. assign v√†o c√°c d·ª± √°n nh·ªè c·ªßa h·ªç.\nd·ª± √°n th·ª© 2 thu·∫ßn Devops nhi·ªÅu h∆°n, manage service 247. ƒêang build, manage h·ªá th·ªëng cho kh√°ch. 1 th√°ng KH t·ªën 20-25k USD Azure (ch·ªâ cho CMS, show dashboard).\nd·ª± √°n th·ª© 3, team Solution, t·ª± l√†m product ƒëang c√≥ kho·∫£ng 15 ng∆∞·ªùi. t·ª´ sale ƒë·∫øn markeing, dev. Devops 1 ng th√¨ sang central r·ªìi. backend l√† Golang, frontend Vue JS, AWS, k8s, bitbucket. Jenkins simple (ch·ªâ down helm chart latest v·ªÅ execute t√™n k8s). app vi·∫øt bang Helm3. vi·∫øt helm package v√† install bang helm.\nMySQL 8. t·∫•t c·∫£ latest. Chu·∫©n b·ªã ƒë∆∞a Cassandra ƒë·ªÉ l√†m catalog product. c√≥ c√°i g·ªçi l√† CDI (Custom Data infra). g·ªìm Apache Druind NoSQL, ch·ª©a data metric. S·ª≠ dung Apache superset ƒë·ªÉ visualize, CDI s·∫Ω ƒëi collect source t·ª´ web, app, backend. route traffic ƒë·∫øn Kafka/druind r·ªìi v·ªÅ.\nN·∫øu pass v√≤ng n√†y t·∫°ch Central c√≤n 247 nh∆∞ng 247 hi·ªán t·∫°i ch∆∞a c√≥ d·ª± √°n.\nko d√πng Google Cloud, v·ªÅ sau th·∫•y c√πi qu√°, thi·∫øu ·ªïn ƒë·ªãnh, ch·∫≠m update, chuy·ªÉn sang dung EKS. Hay b·ªã s·∫≠p (S·∫≠p CloudSQL) D√πng Vault 3 node. data t·ª± replicate sang c√°c AZ kh√°c.\nCh∆∞a g·∫∑p issue m√† scale pvc sang AZ kh√°c c·ªßa EKS, do PVC s·∫Ω replicate ki·ªÉu kh√°c. N·∫øu Vault c·∫ßn scale them 2 th√¨ n√≥ s·∫Ω l·∫•y them 2 PVC tr·∫Øng tinh ƒë·ªÉ replicate data sang.\nH·ªèi c√¢u v·ªÅ dung VPC tr√™n AWS( ki·ªÉu architecture cho VPC c·ªßa 1 app tr√™n AWS)\nV·ªÅ sau anh Ki√™n HR c≈©ng nh·∫Øn tin skype b·∫£o l√† ƒë√°nh gi√° m√¨nh h∆°i ƒëu·ªëi ƒë·ªÉ c√≥ th·ªÉ v√†o l√†m ngay ƒë∆∞·ª£c, c·∫ßn ph·∫£i training on job, nh∆∞ng ch∆∞a r√µ t√¨nh h√¨nh c√≥ dc nh·∫≠n ƒë·ªÉ m√† v√†o training hay ko v√¨ c√°c s·∫øp v·∫´n ƒëang discuss. Hi·ªán v·∫´n ch·ªù. M√¨nh ƒë√°nh gi√° HR anh Ki√™n r·∫•t nice, c√≥ th√¥ng tin c√°i l√† b√°o cho m√¨nh ngay. Kh√¥ng lom dom im l√¨m ko n√≥i g√¨ nh∆∞ m·∫•y HR c√¥ng ty tr∆∞·ªõc.\nƒê·∫øn 15/7 th√¨ a Ki√™n g·ª≠i l·∫°i offer, ok v·ªõi m·ª©c 20 net c·ªßa m√¨nh. M√¨nh c≈©ng ok v√† ƒë·ªìng √Ω ƒëi l√†m v√†o 17/8. V·ªÅ Fsoft xin ngh·ªâ lu√¥n h√¥m sau.\n6. MOBILE HEALTH C√¥ng ty th·ª© 6: MOBILE HEALTH, Floor 22, ICON4 Building, no. 243A, De La Thanh street, Dong Da district, Hanoi (right next to University of Transport and Communications - Dai hoc Giao thong Van tai)\nc√≥ c√°i ph·∫ßn m·ªÅm ManaDR tr√™n ƒëi·ªán tho·∫°i.\nh·ªç g·ª≠i b√†i test v√† y√™u c·∫ßu m√¨nh l√†m solution CICD cho mobile app c·ªßa h·ªç.\nM√¨nh c≈©ng l√†m v√† g·ª≠i l·∫°i. M·∫∑c d√π m√¨nh l√†m kh√° chung chung, v√¨ m√¨nh ch∆∞a c√≥ kinh nghi·ªám l√†m Devops cho Mobile App bao gi·ªù, n√™n ch·ªâ d√°m t√¨m hi·ªÉu tr√™n m·∫°ng r·ªìi l√†m th√†nh b√†i c·ªßa m√¨nh th√¥i.\nB·∫µng ƒëi 1 tu·∫ßn m·ªõi g·ªçi m√¨nh di pv, H·∫πn t2 nh∆∞ng t·ªëi CN m·ªõi b·∫£o c√≥ h·ªçp ko pv dc, h·∫πn l·∫°i t3. nh∆∞ng m√¨nh Xin ngh·ªâ nhi·ªÅu qu√° ƒë·ªÉ ƒëi pv r·ªìi ko d√°m Xin ti·∫øp n√™n t·ª´ ch·ªói t3. b·∫£o t6 ho·∫∑c t2 tu·∫ßn sau. HR c√≥ v·∫ª that v·ªçng nh∆∞ng minh ko ti·∫øc l·∫Øm, v√¨ l√†m v·ªõi mobile app devops th√¨ m√¨nh c≈©ng ch∆∞a c√≥ nhi·ªÅu kinh nghi·ªám\nko pv th√¨ th√¥i c≈©ng ko sao.\nƒê·∫øn t6 th√¨ g·ªçi ƒëi·ªán ƒëi ph·ªèng v·∫•n, cty n√†y tr√™n review c√≥ r·∫•t √≠t th√¥ng tin.\nC√≥ v·∫ª l√† c√¥ng ty nh·ªè. ch·ªâ kho·∫£ng 50-70m2, kho·∫£ng 30-40 ng∆∞·ªùi th√¥i\npv c√≥ 3 ng∆∞·ªùi ch·∫Øc l√† gi√°m ƒë·ªëc v√† 2 anh chuy√™n v·ªÅ backend\nc√¥ng ty ƒëang c√≥ kho·∫£ng 20 microservice\nch·∫°y tr√™n AWS (production), GCP (Develop), v√† local cho dev\nƒêang d√πng Jenkins, GitHub, k8s\nch∆∞a c√≥ h·ªá th·ªëng qu·∫£n l√Ω API (API Gateway, Kong)\nƒêang c·∫ßn t√¨m senior Devops, 1 ng∆∞·ªùi v√†o l√†m t·∫•t, nh·ªØng ng∆∞·ªùi Backend hi·ªán t·∫°i th√¨ bi·∫øt c·∫£ Devops n√™n h·ªç ch·ªâ c·∫ßn 1 ng th√¥i.\nCty v·ªÅ product. ko ph·∫£i outsource.\nƒê√£ c√≥ CI, ch∆∞a c√≥ CD\nh·ªèi v·ªÅ c√¢u c√πng 1 Docker file c√≥ th·ªÉ chia ra dung cho 2 m√¥i tr∆∞·ªùng kh√°c nhau ko, m√¨nh ko tr·∫£ l·ªùi dc, (D√πng stage c·ªßa Docker) h·ªèi v·ªÅ b√†i to√°n d·ª± √°n solution m√† em th·∫•y hay\nb·∫£o l√† s·∫Ω c√≥ k·∫øt qu·∫£ v√†o kho·∫£ng cu·ªëi tu·∫ßn sau, nh∆∞ng ch·ªù ƒë·∫øn cu·ªëi tu·∫ßn ko th·∫•y th√¥ng b√°o g√¨. M√¨nh ƒë√°nh gi√° l√† l√†m vi·ªác lom dom. Ko c√≥ KQ c≈©ng ch·∫£ th√®m n√≥i g√¨ lu√¥n, l√†m ·ª©ng vi√™n kh√≥ ch·ªãu.\nM√£i v·ªÅ sau l√† 21/7 m·ªõi g·ª≠i mail c·∫£m ∆°n (t·∫°ch). L√†m vi·ªác lom dom h·∫øt s·ª©c.\n7. Teko m√¨nh t·∫°ch t·ª´ v√≤ng g·ª≠i CV, h·ªç g·ª≠i mail c·∫£m ∆°n lu√¥n\n8. Citigo kiotviet m√¨nh t·∫°ch t·ª´ v√≤ng g·ª≠i CV, h·ªç g·ª≠i mail c·∫£m ∆°n lu√¥n\n9. GotIt apply nh∆∞ng ko ph·∫£n h·ªìi g√¨, ho·∫∑c si√™u ch·∫≠m\n10. Esoft apply nh∆∞ng ko ph·∫£n h·ªìi g√¨, ho·∫∑c si√™u ch·∫≠m\n11. Studynow apply nh∆∞ng ko ph·∫£n h·ªìi g√¨, ho·∫∑c si√™u ch·∫≠m\n12. FinOS apply nh∆∞ng ko ph·∫£n h·ªìi g√¨, ho·∫∑c si√™u ch·∫≠m\n13. C√¥ng ty c·ªï ph·∫ßn an ninh m·∫°ng apply nh∆∞ng ko ph·∫£n h·ªìi g√¨, ho·∫∑c si√™u ch·∫≠m\nK·∫øt qu·∫£ K·∫øt qu·∫£ l√† sau kho·∫£ng 7 l·∫ßn ƒëi ph·ªèng v·∫•n th√¨ m√¨nh ƒë√£ ch·ªët ƒë∆∞·ª£c l√†m ·ªü IFI. L√∫c ph·∫£i xin l·ªói SmartOSC c≈©ng h∆°i ng·∫°i v√† suy nghƒ© kh√° nhi·ªÅu, anh HR Ki√™n c≈©ng r·∫•t nice khi n√≥i th√¥i ko sao, ok v√† ch√∫c m√¨nh may m·∫Øn. Th·∫≠m ch√≠ m√¨nh c√≤n ph·∫£i h·ªèi anh ch·ªã ƒë·ªÉ nh·ªù t∆∞ v·∫•n xem n√™n ch·ªçn c√¥ng ty n√†o. M·ªçi ng∆∞·ªùi c≈©ng khuy√™n thuy·ªÅn to s√≥ng c·∫£, n√™n ch·∫•p nh·∫≠n offer cao h∆°n ƒë·ªÉ th·ª≠ s·ª©c b·∫£n th√¢n. Sau khi ch·ªët xong th√¨ hi·ªán t·∫°i khi vi·∫øt nh·ªØng d√≤ng n√†y, m√¨nh ƒë√£ ƒëi l√†m ƒë∆∞·ª£c 1 tu·∫ßn r∆∞·ª°i, th·∫•y c≈©ng ch∆∞a c√≥ g√¨ qu√° kh√≥ khƒÉn. C√≥ l·∫Ω t∆∞∆°ng lai s·∫Ω th·ª≠ th√°ch nhi·ªÅu h∆°n.\nN√≥i chung, v·∫´n ch∆∞a ƒë·ªß ƒë·ªÉ ƒë√°nh gi√° ƒë√¢y ƒë√£ ph·∫£i l√† quy·∫øt ƒë·ªãnh ƒë√∫ng ƒë·∫Øn hay ch∆∞a\u0026hellip;\n2022-January-4 Th·ª≠ v·∫≠n may v·ªõi vi·ªác apply 1 v·ªã tr√≠ DevOps b√™n H√† Lan, hy v·ªçng h·ªç sponsor visa :))\n1. Rabobank apply v√† b·ªã reject ngay\n2. Inergy apply v√† b·ªã reject ngay\n3. a.s.r. apply v√† b·ªã reject ngay\n4. Valtech apply v√† b·ªã reject ngay\n5. Sogeti, Part of the Capgemini Group https://www.linkedin.com/jobs/view/2765512958/?refId=9536b709-3005-4a91-9868-fda511caa5b9\nH·ªç nh·∫≠n ƒë∆∞·ª£c CV c·ªßa m√¨nh v√† setup 1 cu·ªôc trao ƒë·ªïi Introduction qua Teams, c√≥ l·∫Ω l√† ƒë·ªÉ hi·ªÉu th√™m v·ªÅ ti·∫øng Anh c·ªßa m√¨nh.\nH∆°i ti·∫øc v√¨ ti·∫øng Anh c·ªßa m√¨nh v·∫´n h∆°i t·ªá.\nB·∫Øt ƒë·∫ßu v√†o th√¨ b·∫°n Marie-Louise gi·ªõi thi·ªáu v·ªÅ b·∫£n th√¢n, h∆°n 30, th√≠ch ch∆°i boardgame, th·ªùi gian r·∫£nh th√¨ ch∆°i v·ªõi b·∫°n. Ch∆∞a l·∫≠p gia ƒë√¨nh.\nPoint c·ªßa h·ªç l√† ph·∫ßn gi·ªõi thi·ªáu mu·ªën t√¨m hi·ªÉu b·∫°n l√† ai, ng∆∞·ªùi nh∆∞ th·∫ø n√†o l√† ch√≠nh. Ko n√™n b·ª•p lu√¥n v√†o technical, h√£y n√≥i v·ªÅ s·ªü th√≠ch, gia ƒë√¨nh, th·ªùi gian r·∫£nh, b·∫°n b√®. Sau ƒë√≥ khi n√†o ng∆∞·ªùi ph·ªèng v·∫•n h·ªèi v·ªÅ technical h√£y chuy·ªÉn sang technical.\nGi·ªõi thi·ªáu qua v·ªÅ t·∫≠p ƒëo√†n Capgemini, c√¥ng ty Sogeti l√† 1 ph·∫ßn c·ªßa h·ªç\nH·ªèi v·ªÅ l√Ω do apply c√¥ng vi·ªác n√†y?\nB·∫°n ƒë√£ t·ªët nghi·ªáp ƒë·∫°i h·ªçc ƒë√∫ng ko, b·∫°n c√≥ diploma/ch·ª©ng nh·∫≠n ch·ª©?\nT∆∞∆°ng lai xa h∆°n b·∫°n mu·ªën tr·ªü th√†nh g√¨?\nB·∫°n prefer AWS stack or Azure stack?\nL√†m vi·ªác 4 days per week, h·ª£p ƒë·ªìng 1 year contract, sau ƒë·∫•y s·∫Ω extension.\nC√≥ provide visa cho v·ª£, nh∆∞ng ko provide cho anh/ch·ªã/em\nD√π h·ªç c√≥ h·ªó tr·ª£ visa/bank account, financial housing nh∆∞ng resposibility l√† c·ªßa m√¨nh l√† ch√≠nh.\nH·ªç c√≥ standard package cho ng∆∞·ªùi ƒë·∫øn t·ª´ b√™n ngo√†i H√† Lan, 4050 Euro, 1 nƒÉm kho·∫£ng 50k gross H·ªç th·∫•y t∆∞∆°ng lai b·∫°n mu·ªën tr·ªü th√†nh Solution Architect, n√≥ c√≥ h∆°i kh√°c v·ªõi AWS DevOps engineer, li·ªáu c√≥ v·∫•n ƒë·ªÅ g√¨ ko?\nH·ªç l√†m ch·ªß y·∫øu AWS, c√≥ Azure v√† GCP nh∆∞ng √≠t h∆°n\n3 lo·∫°i d·ª± √°n: client, fix-price, smart digital\u0026hellip;\nN·∫øu h·ªç n√©m b·∫°n v√†o l√†m vi·ªác v·ªõi 1 client m√† all documents b·∫±ng ti·∫øng Anh h·∫øt, h·ªçp meeting ƒë·ªÅu ti·∫øng Anh th√¨ c√≥ v·∫•n ƒë·ªÅ g√¨ ko?\n(C√≥ l·∫Ω h·ªç th·∫•y m√¨nh h∆°i nervous, ti·∫øng Anh ko t·ª± tin)\nH·ªèi v·ªÅ c√¥ng ty m√¨nh c·∫ßn notice tr∆∞·ªõc bao l√¢u ƒë·ªÉ ngh·ªâ (tr·∫£ l·ªùi 45 ng√†y)\nD·∫°o n√†y c√≥ delay kh√¢u visa ·ªü H√† Lan n√™n s·∫Ω ph·∫£i k√©o d√†i 6-8 tu·∫ßn (th√¥ng th∆∞·ªùng h·ªìi x∆∞a l√† 2-4 tu·∫ßn)\nH·ªèi v·ªÅ situation c·ªßa v·ª£ m√¨nh, ƒë√£ apply PhD ch∆∞a?\nSau n√†y s·∫Ω c√≥ interview v·ªõi Hiring Manager, Senior\nN·∫øu ok s·∫Ω c√≥ interview v·ªÅ Technical n·ªØa\nSau ƒë·∫•y m·ªõi c√≥ offer\nN√≥i chung l√† c√≤n 2 v√≤ng ph·ªèng v·∫•n n·ªØa\nCh√†o t·∫°m bi·ªát\nSau v√†i ng·∫ßy th√¨ h·ªç g·ª≠i mail t·ª´ ch·ªëi. Ch·ªß y·∫øu v√¨ ti·∫øng Anh c·ªßa m√¨nh h∆°i k√©m, h·ªç s·ª£ s·∫Ω ko l√†m vi·ªác ƒëc.\n2022-March-12 1. FPT Germany G·∫ßn 2 nƒÉm l√†m vi·ªác ·ªü IFI, m√¨nh h·ªçc ƒë∆∞·ª£c nhi·ªÅu th·ª©, ch·ªß y·∫øu l√† t·ª± h·ªçc :)))\nB·∫Øt ƒë·∫ßu nghƒ© ƒë·∫øn chuy·ªán r·ªùi ƒëi.\nV√¨ Linh mu·ªën sang H√† Lan n√™n m√¨nh c·ªë g·∫Øng t√¨m ki·∫øm 1 c√¥ng vi·ªác ·ªü H√† Lan.\nPh·ªèng v·∫•n 1 l·∫ßn ·ªü c√¥ng ty b√™n ƒë√≥ (Sogeti - apply qua Linkedin), v√† b·ªã h·ªç ƒë√°nh fail ngay t·ª´ l·∫ßn ƒë·∫ßu ti√™n v√¨ ti·∫øng Anh k√©m.\n1 Th·ªùi gian sau qua li√™n h·ªá v·ªõi anh D≈©ng (JP) th√¨ m√¨nh bi·∫øt F s·∫Øp c√≥ c∆° s·ªü b√™n H√† Lan,\nH·ªç ƒëang t√¨m ng∆∞·ªùi v√† m√¨nh c≈©ng apply cho vui xem k·∫øt qu·∫£ th·∫ø n√†o,\nƒêi·ªÉm ƒë·∫∑c bi·ªát c·ªßa d·ª± √°n l√† c√¥ng ty m·∫π ·ªü Nh·∫≠t, c√¥ng ty con ·ªü H√† Lan n√™n vi·ªác bi·∫øt c·∫£ ti·∫øng Nh·∫≠t v√† ti·∫øng Anh nh∆∞ m√¨nh c√≥ th·ªÉ l√† l·ª£i th·∫ø. C√¥ng vi·ªác ch·ªß y·∫øu th·ªùi gian ƒë·∫ßu l√† review code c·ªßa 1 b√™n partner kh√°c, r·ªìi t√¨m hi·ªÉu h·ªá th·ªëng ƒë·ªÉ sau n√†y chuy·ªÉn d·ª± √°n v·ªÅ cho offshore, v·ªÅ sau th√¨ m√¨nh s·∫Ω th√†nh ng∆∞·ªùi x·ª≠ l√Ω s·ª± c·ªë/l·ªói h·ªá th·ªëng b√™n HL, v√† g·∫ßn nh∆∞ l√†m c√¥ng vi·ªác 1 BrSE cho ƒë·ªôi d·ª± √°n ·ªü VN. Stack k·ªπ thu·∫≠t ch·ªß y·∫øu tr√™n AWS (Aurora MySQL, S3, SES, Firebase Push, Cognito, Typescript, AWS Lambda, python, API Batch)\nPh·ªèng v·∫•n m·ªçi th·ª© kh√° OK, th·∫ø nh∆∞ng ƒë·∫øn ph·∫ßn deal l∆∞∆°ng v·ªõi F b√™n ƒê·ª©c th√¨ kh√¥ng th·ªÉ th√†nh c√¥ng. M√¨nh expect 80k EURO gross nh∆∞ng h·ªç ch·ªâ tr·∫£ 68k gross m√† th√¥i. Th·ªùi ƒëi·ªÉm n√†y IFI c≈©ng ƒë√†m ph√°n ƒë·ªÉ tƒÉng l∆∞∆°ng cho m√¨nh (c√≥ l·∫Ω v√¨ h·ªç bi·∫øt sau T·∫øt l√† th·ªùi ƒëi·ªÉm nhi·ªÅu ng∆∞·ªùi mu·ªën r·ªùi ƒëi). M√¨nh expect 45M gross v√† IFI ƒë·ªìng √Ω ngay.\nCu·ªëi c√πng m√¨nh ƒë√£ t·ª´ ch·ªëi offer v√† th·∫ø l√† m·ªçi th·ª© ƒë·∫£o chi·ªÅu. T·ª´ ch·ªó r·ª•c r·ªãch ƒëi HL th√¨ m√¨nh ti·∫øp t·ª•c ·ªü l·∫°i IFI. Hy v·ªçng 1 ng√†y ko xa m√¨nh c√≥ th·ªÉ ti·∫øp t·ª•c gi·∫•c m∆° l√†m vi·ªác b√™n H√† Lan\u0026hellip;\n2022-November-28 Ban ƒë·∫ßu t√≠nh t√¨m c√¥ng ty c√≥ th·ªÉ l√†m remote b√™n H√† Lan\nNh∆∞ng v·ªÅ sau apply qua Linkedin th·∫•y ko hy v·ªçng l·∫Øm, n√™n m√¨nh apply c·∫£ nh·ªØng c√¥ng ty n∆∞·ªõc ngo√†i cho l√†m remote ·ªü VN n·ªØa\n1. DevHunt DevOps/ Golang engineer:\nhttps://www.linkedin.com/jobs/view/3361068660\n2. Annapurna Recruitment European Union Remote Junior DevOps Engineer: https://www.linkedin.com/jobs/view/3358803027\n3. Otrium Netherlands Remote Senior DevOps Engineer:\nhttps://www.linkedin.com/jobs/view/3324063530\n-\u0026gt; ƒë√£ b·ªã reject b·∫±ng mail\n4. MentorMate\u0026rsquo;s Partner Network Cloud \u0026amp; DevOps Engineer - Contractor:\nhttps://www.linkedin.com/jobs/view/3362963704\n5. Numbrs Personal Finance AG Site Reliability Engineer - Remote:\nhttps://numbrs.applytojob.com/apply/wPuuphiAVz/Site-Reliability-Engineer-Remote\n-\u0026gt; ƒë√£ b·ªã reject b·∫±ng mail\n6. Numbrs Self Custody European Economic Area Remote DevOps Engineer:\nhttps://www.linkedin.com/jobs/view/3359547583\n7. Scalac Netherlands Remote DevOps Engineer: https://www.linkedin.com/jobs/view/3355606879\n-\u0026gt; ƒë√£ b·ªã reject b·∫±ng mail\n8. Toptal European Union Remote Freelance DevOps Engineer Remote:\nhttps://www.linkedin.com/jobs/view/3300238612\n9. Opus Recruitment Solutions Netherlands Remote DevOps Engineer: https://www.linkedin.com/jobs/view/3365879887\n10. HTH Communications Devops Engineer: c√¥ng ty c·ªßa ƒê·∫°t refer,\nn·ªôp CV xong ko th·∫•y process ti·∫øp\nhttps://docs.google.com/document/d/1vVEmfmfkrx47KgvnRXe7TttlRZmPimI1w0-s4dQMo0E/edit?usp=gmail\nL√∫c h·ªç h·ªèi th√°ng 12, th√¨ m√¨nh l·∫°i ƒëang pv 2 ch·ªó kia n√™n h·∫πn sau\n11. Peer39 DEVOPS ENGINEER:\nCompany: Peer39\nWorking Location: Remote Fulltime\nSalary: $3.000 - $3.500 (3-5 years) / $3.500 - $5.000 (6 - 8 years))\nJD file trong folder CV-answer-question: PEER39 - DEVOPS ENGINEER.pdf\n12. NSCSOFTWARE.COM (REMOTE Freelancer SENIOR AZURE ENGINEER) @nscsoftware.com Freelancer | Contractor | C·ªông t√°c vi√™n\n‚úÖT·ªïng Quan\n‚óè T·ªëi thi·ªÉu 4 nƒÉm kinh nghi·ªám v·ªõi Microsoft Azure.\n‚óè L∆∞∆°ng NET: 12 - 16 USD/gi·ªù. Tr·∫£ l∆∞∆°ng h√†ng th√°ng v√†o ng√†y 10.\n‚óè Linh ƒë·ªông th·ªùi gian l√†m vi·ªác.\n‚óè L√†m vi·ªác trong m√¥i tr∆∞·ªùng qu·ªëc t·∫ø, c√°c th√†nh vi√™n trong team ·ªü kh·∫Øp c√°c ch√¢u l·ª•c.\n‚óè C∆° h·ªôi ƒë·ªÉ trau d·ªìi kh·∫£ nƒÉng Ti·∫øng Anh v·ªõi ng∆∞·ªùi b·∫£n ƒë·ªãa.\n‚óè KH √öc - h·ªá th·ªëng qu·∫£n l√Ω an to√†n cho lƒ©nh v·ª±c x√¢y d·ª±ng.\n‚óè C√≥ th·ªÉ l√†m 2-3 job c√πng l√∫c JD file: Senior-Azure-Engineer-Remote-FreelanceTV.pdf\n-\u0026gt; ph·ªèng v·∫•n v·ªõi 1 anh t√™n ƒê·ª©c sn1985, HR Th√†nh add Zalo. Job freelancer y√™u c·∫ßu senior c·∫ßn fluent English. Ph·ªèng v·∫•n h·ªèi ti·∫øng Anh gi·∫£i th√≠ch m·∫•y kh√°i ni·ªám ƒë∆°n gi·∫£n nh∆∞ Azure l√† g√¨, VNET, SUBNET l√† g√¨? Azure Fabric l√† g√¨, m√¨nh ko bi·∫øt, H·ªèi n·∫øu 1 KH c√≥ nhu c·∫ßu l√†m 1 trang web v·ªõi data t·ª´ server onpremise th√¨ c·∫ßn d√πng service g√¨, m√¨nh ko bi·∫øt n√™n ko tr·∫£ l·ªùi dc, n√≥i chung th·∫•y English m√¨nh ko t·ªët l·∫Øm -\u0026gt; t·∫°ch\n13. Bull IT Services DevOps:\nhttps://www.linkedin.com/jobs/view/3378120406\n-\u0026gt; reject lu√¥n t·ª´ v√≤ng scan cv\n14. The Digital Group Inc Hanoi Capital Region DevOps On-site:\nhttps://www.linkedin.com/jobs/view/3387409983 1 √¥ng ng∆∞·ªùi nc ngo√†i (PARESHWAR BHATT) h·ªèi qua Linkedin, ch·∫Øc ·∫§n ƒë·ªô, m√¨nh mu·ªën th·ª≠ pv xem sao.\nƒë√¢y l√† full time job ko th·ªÉ l√†m remote ƒëc, nh·∫≠n l∆∞∆°ng VNƒê. Ng∆∞·ªùi ·∫§n ƒë·ªô g·ªçi ƒëi·ªán tho·∫°i m√¨nh ch·∫£ nghe r√µ g√¨ c·∫£.\nNh∆∞ng m√† l√†m ·ªü vƒÉn ph√≤ng VN v·ªõi ng ·∫§n th√¨ m√¨nh th·∫•y k√®o ko th∆°m l·∫Øm. ƒê√£ t·ª´ ch·ªëi sau khi h·ªç li√™n h·ªá h·ªèi current CTC (cost-to-cost), m·ª©c l∆∞∆°ng cty hi·ªán t·∫°i tr·∫£ theo nƒÉm l√† bn?\n15. Hexience company Hexience Job Application DevOps:\nHexience is a technology services company based in Hong Kong. Our services cover web, ecommerce, mobile app, ERP and embedded system design \u0026amp; development. We are expanding our software development team to cope with the ever changing market needs. We eagerly invite brilliant talents to join us. https://docs.google.com/forms/d/e/1FAIpQLSe-ifwQ1TR-m4Xo7utcZ0BLyhUCjt37zVNCDFluMqPOopbfjg/viewform\n-\u0026gt; ph·ªèng v·∫•n l·∫ßn 1 v·ªõi 1 anh t√™n Charles ng∆∞·ªùi Vi·ªát, m√¨nh n√≥i c√≥ v·∫ª OK, h·ªèi l√∫c apply m√¨nh expect 1500$ sao l√∫c pv l·∫°i n√≥i 3000$? m√¨nh ch·ªãu. H·ªèi k·ªπ thu·∫≠t v·ªÅ c∆° b·∫£n c≈©ng d·ªÖ, m√¨nh th·∫•y ok, ch·ªó n√†o l√†m vi·ªác t·ª´ 9-18h, gi·ªù h√†nh ch√≠nh. HR Hao Hao k√©m tu·ªïi m√¨nh. Th·∫•y b·∫£o gi·ªù gi·∫•c c≈©ng d·ªÖ l√†m remote. Cty ko d√≥ng BHXH, ko ƒë√≥ng thu·∫ø. Nh·∫≠n ti·ªÅn xong th√¨ t·ª± l√†m m·∫•y c√°i ƒë√≥. M√¨nh th√≠ch nh∆∞ v·∫≠y h∆°n, nh∆∞ng ch∆∞a bi·∫øt n·∫øu k√Ω hƒë th√¨ c·∫ßn th√¥ng tin g√¨.\n-\u0026gt; ph·ªèng v·∫•n l·∫ßn 2 (8h30) th√¨ ko h·∫≥n l√† ph·ªèng v·∫•n, ki·ªÉu nh∆∞ b√™n Hexience mu·ªën b√°n m√¨nh cho Deloit HongKong ƒë·ªÉ l√†m d·ª± v·ªõi team Deloit, n√™n h·ªç train m√¨nh c√°c ki·∫øn th·ª©c c√≥ th·ªÉ b·ªã h·ªèi v√†o cu·ªôc pv l·∫ßn 3 sau ƒë√≥. 1 b·∫°n Doris (n·ªØ) n√≥i chung mu·ªën m√¨nh b·∫£o l√† H√£y n√≥i \u0026ldquo;Im joining Deloit, I have can-do attitude, s·∫µn s√†ng h·ªçc h·ªèi l√†m b·∫•t c·ª© c√°i g√¨.\u0026rdquo;\nT·∫≠p introduction.\nDoris b·∫£o l√†m ch·ªß y·∫øu v·ªõi Java n√™n s·∫Ω h·ªèi Java khuy√™n n√™n google some basic knowledge,\nB·∫£o m√¨nh H√£y n√≥i I have minimum knowledge in Java but mostly working with Python. (ƒë·ª´ng n√≥i t√¥i ko bi·∫øt g√¨).\nN·∫øu h·ªç h·ªèi c√≥ ki·∫øn th·ª©c v·ªÅ Banking ko th√¨ h√£y b·∫£o I have similar knowledge in financing suchas I know banking have Bank Retail and Bank Commercial, trong ƒë√≥ Bank retail (B2C, bussiness to Customer, people ƒë·∫øn bank g·ª≠i ti·ªÅn), c√≤n bank commercial (B2B, business to business, MacDonald v√† HSBC l√†m vi·ªác v·ªõi nhau).\nL√†m ·ª©ng d·ª•ng mobile c·ªßa HSBC n√™n c≈©ng khuy√™n m√¨nh google some basic about mobile app. V√≠ d·ª• mobile c√≥ Payment gateway and frontend\u0026hellip;\nH√£y b·∫£o T√¥i adapt quickly\n-\u0026gt; ph·ªèng v·∫•n l·∫ßn 3 (10h15): c√≥ kho·∫£ng 2 ng∆∞·ªùi h·ªèi tr·ª±c ti·∫øp. 1 ng m√¨nh nghƒ© l√† Solution Architect.\nH·ªèi ch·∫£ c√≥ g√¨ li√™n quan ƒë·∫øn nh·ªØng th·ª© m√† Doris b·∫£o chu·∫©n b·ªã c·∫£ =))\nCh·ªß y·∫øu l√† scan CV c·ªßa m√¨nh r·ªìi h·ªèi v·ªÅ c√°c d·ª± √°n m√¨nh ƒë√£ l√†m, gi·∫£i th√≠ch c√°c tool.\nTerraform, qu·∫£n l√Ω credetial gi·ªØa c√°c m√¥i tr∆∞·ªùng ntn?\nD√πng gitlab v√† branch ƒë·ªÉ h·∫°n ch·∫ø quy·ªÅn c·ªßa Dev.\nAnsible th√¨ c≈©ng qu·∫£n l√Ω credetial gi·ªØa c√°c m√¥i tr∆∞·ªùng ntn?\nGrafana+Prometheus: n·∫øu noti c·ªßa Grafana m√† n√≥ alert m√¨nh nhi·ªÅu l·∫ßn c·ª© warning r·ªìi solved warning r·ªìi solved th√¨ l√†m th·∫ø n√†o ƒë·ªÉ h·∫°n ch·∫ø s·ª± th√¥ng b√°o phi·ªÅn ph·ª©c ƒë√≥ (m√¨nh ch·ªãu ko tr·∫£ l·ªùi dc, nh∆∞ng v·ªÅ sau nghƒ© l·∫°i th√¨ c√≥ th·ªÉ tƒÉng c√°i delay gi·ªØa m·ªói l·∫ßn scan)\nAzure Container Registry l∆∞u artifact.\nBlue/Green deployment.\nArgoCD l√† g√¨. B·∫°n recommend c√°c service n√™n s·ª≠ d·ª•ng trong CICD? (m√¨nh recommend gitlab cho CI, argoCd cho CD)\nB·∫°n c√≥ scan c√°c security vulnebility c·ªßa c√°c Docker Image tr√™n registry ko? (m√¨nh ko) B·∫°n aware v·ªÅ n√≥ l√† dc. Bi·∫øt DevSecOps l√† g√¨ ko?\nJenkins file c√≥ bi·∫øt ko, ch·ªçn Pipeline type job ch·ª© ko ch·ªçn c√°i lo·∫°i c√≤n l·∫°i c√≥ bi·∫øt ko?\nH·ªèi c·∫£ v·ªÅ EFK stack.\n-\u0026gt; 1 th√°ng tr√¥i qua ko th·∫•y ph·∫£n h·ªìi -\u0026gt; t·ª± bi·∫øt t·∫°ch\n16. Avaiga apply qua facebook:\nhttps://www.facebook.com/albert.antoine.14\nFile DevOps-Azure-facebookAlbert Antoine.pdf\nDevOps-Azure:\nAvaiga is an open-source software company founded in July 2021. It is natively international. Today, Avaiga has about 20 employees all over the world (Brazil, US, Vietnam, France, Singapour, Luxembourg). The main objective of Avaiga is to make the data scientist more autonomous in a complete data science project. In particular, Avaiga focuses on helping data scientists put algorithms and data visualization into their end user‚Äôs hands.\nC√¥ng ty n√†y c√≥ 1 c√°i python package: Taipy\n-\u0026gt; ph·ªèng v·∫•n l·∫ßn 1, M√¨nh ph·ªèng v·∫•n v·ªõi 1 √¥ng ng∆∞·ªùi Ph√°p t√™n Florian. H·ªèi v·ªÅ Azure kh√° d·ªÖ, H·∫πn T6 ph·ªèng v·∫•n ti·∫øp v2. Cho 2 b√†i test v·ªÅ nh√†.\nM√¨nh nghƒ© c√≥ th·ªÉ l√†m ƒë∆∞·ª£c. Ch∆∞a h·ªèi c·ª• th·ªÉ v·ªÅ l∆∞·ª£ng l·∫≠u, c√¥ng vi·ªác, n√™n ƒë·ªÉ sau.\n2 b√†i test nh∆∞ n√†y:\nBtw, if you have time, can you perform both exercises please? First: - Create a GitHub action that will build and publish a \u0026quot;hello world\u0026quot; python application on Docker Hub. I will test your application by doing: - In a terminal, I will run: \u0026quot;docker pull \u0026lt;yourimage\u0026gt;\u0026quot; - In a terminal, I will run: \u0026quot;docker run \u0026lt;yourimage\u0026gt;\u0026quot; I want to show a \u0026quot;hello world\u0026quot; in the terminal. - I will ask you to change \u0026quot;hello world\u0026quot; to \u0026quot;Hi Taipy\u0026quot; on GitHub and then run your GitHub action. - In a terminal, I will run: \u0026quot;docker pull \u0026lt;yourimage\u0026gt;\u0026quot; to get the new image. - In a terminal, I will run: \u0026quot;docker run \u0026lt;yourimage\u0026gt;\u0026quot; I want to show a \u0026quot;Hi Taipy\u0026quot; in the terminal. Second: - Create a YAML script to expose an FTP server (vsftpd - Secure, fast FTP server for UNIX-like systems (security.appspot.com) via Kubernetes. Working with Minikube is enough for me. - The FTP server should be accessible with FileZilla. - Volume should be persistent. b√†i 1 th√¨ m√¨nh l√†m ok ko v·∫•n ƒë·ªÅ g√¨, b√†i 2 th√¨ m√¨nh ph·∫£i c√†i minikube tr√™n 1 Azure VM, nh∆∞ng m√† n√≥i chung ko th·ªÉ l√†m cho FileZilla access dc server tr√™n minikube. M√¨nh nghƒ© n·∫øu c√†i minikube trong m·∫°ng n·ªôi b·ªô th√¨ c√≥ th·ªÉ d√πng FileZilla tr√™n Laptop ƒë·ªÉ access. nh∆∞ng m√¨nh c√†i minikube tr√™n Azure VM th√¨ ko th·ªÉ (ƒë√£ th·ª≠ Wireguard vpn, tinyproxy)\nb√†i 1 v·ªÅ Github Action workflow m√¨nh ƒë·ªÉ ·ªü ƒë√¢y: https://github.com/terabithians/gh-action-python-app\nb√†i 2 v·ªÅ minikube m√¨nh ƒë√£ vi·∫øt l·∫°i ·ªü Install Minikube on Linux Ubuntu arm64 (not amd64)\n-\u0026gt; ph·ªèng v·∫•n l·∫ßn 2, l·∫ßn n√†y c√≥ kho·∫£ng 3 ng∆∞·ªùi (ch·∫Øc to√†n ng∆∞·ªùi Ph√°p, da tr·∫Øng).\nL√¥i 2 b√†i tr∆∞·ªõc ra gi·∫£i th√≠ch, share screen.\ngi·∫£i th√≠ch t·ª´ng d√≤ng trong file Github action workflow.\ngi·∫£i th√≠ch Dockerfile v√¨ sao ch·ªçn ADD m√† ko ch·ªçn COPY.\ntrong Dockerfile c√≥ 2 d√≤ng c√≥ v·∫Ω gi·ªëng nhau, WORKDIR src v·ªõi RUN cd /src. b√†i 2 th√¨ l√¥i file yaml k8s ra ƒë·ªÉ gi·∫£i th√≠ch,\nkh√°c nhau gi·ªØa accessmode: ReadwriteOnce, ReadWriteMany (m√¨nh ko ch·∫Øc l·∫Øm n√™n ko d√°m tr·∫£ l·ªùi).\nV√¨ sao c·∫ßn c√≥ PVC v√† PV.\nH·ªèi v·ªÅ Python, bi·∫øt lambda trong python l√† g√¨ ko? -\u0026gt; m√¨nh ko bi·∫øt :(\nBi·∫øt ki·∫øn tr√∫c micro services l√† g√¨ ko?\nb√†i 3 l√† h·ªç s·∫Ω g·ª≠i 1 b√†i tr·ª±c ti·∫øp qua mail v√† code LIVE python ƒë·ªÉ l√†m cho h·ªç lu√¥n:\nWe built three serverless endpoints that do complex processing on numbers. We want to expose these three endpoints as one and create a function that takes a list of integers then returns a Map/Dict with the initial value and the result of the endpoint ‚Äúhitchhiker‚Äù. Example: hitchhiker([43, 1971, 1992, 2022]) == {43: 42, 1971: 1970, 1992: 1991, 2022: 2021} Here the list of available endpoints: Endpoint Hitchhiker. Requires the output of the endpoint Bill and the output of the endpoint Linus then returns the wanted result Endpoint Bill. Requires an integer and returns the result of the algorithm. Endpoint Linus. Requires the initial integer + the result of the endpoint Bill and calculates the output. Interface note: Hitchhiker: https://taipy-algorithms.azurewebsites.net/api/HttpTrigger3 Query arguments: ‚Äúcode‚Äù, ‚Äúlinus‚Äù, ‚Äúbill‚Äù Code: sVzxyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxzFu_Hc5Nw== Bill: https://taipy-algorithms.azurewebsites.net/api/HttpTrigger1 Query arguments: ‚Äúcode‚Äù, ‚Äúnumber‚Äù Code: jsm60xxxxxxxxxxxxxxxxxxxxxxxxgikA== Linus: https://taipy-algorithms.azurewebsites.net/api/HttpTrigger2 Query arguments: ‚Äúcode‚Äù, ‚Äúnumber‚Äù, ‚Äúbill‚Äù Code: wbe-sW9xxxxxxxxxxxxxxxxxxxxxxxxxxxxFu-1JF2Q== th√¨ n√≥i chung l√† may m√† chia m√†n h√¨nh ra ƒë·ªÉ l√†m, c√≤n nh√¨n v√†o code c≈© ƒë·ªÉ bi·∫øt ƒë∆∞·ªùng.\nimport requests def call_url(bill): data_bill = { \u0026#34;code\u0026#34;: \u0026#34;jsm60xxxxxxxxxxxxxxxxxxxxxxxxxgikA==\u0026#34;, \u0026#34;number\u0026#34;: bill } bill_res = requests.get(\u0026#34;https://taipy-algorithms.azurewebsites.net/api/HttpTrigger1\u0026#34;, data_bill) # print(bill_res.json()) res_elment=bill_res.json()[\u0026#39;res\u0026#39;] data_linus = { \u0026#34;code\u0026#34;: \u0026#34;wbe-sWxxxxxxxxxxxxxxxxxxxxxxxxxxFu-1JF2Q==\u0026#34;, \u0026#34;number\u0026#34;: bill, \u0026#34;bill\u0026#34;: res_elment, } linus_res = requests.get(\u0026#34;https://taipy-algorithms.azurewebsites.net/api/HttpTrigger2\u0026#34;,data_linus) # print(linus_res.json()) linus_element=linus_res.json()[\u0026#39;res\u0026#39;] data_hit = { \u0026#34;code\u0026#34;: \u0026#34;sVzxxxxxxxxxxxxxxxxxxxxxxxxxc5Nw==\u0026#34;, \u0026#34;linus\u0026#34;: linus_element, \u0026#34;bill\u0026#34;: res_elment, } hit_res = requests.get(\u0026#34;https://taipy-algorithms.azurewebsites.net/api/HttpTrigger3\u0026#34;,data_hit) # print(hit_res.json()) return hit_res.json() bill = 43 print(call_url(bill)) N√≥i chung t·ª´ng d√≤ng 1 ph·∫£i code tr∆∞·ªõc m·∫∑t h·ªç ƒë·ªÉ h·ªç hi·ªÉu m√¨nh hi·ªÉu. code th√¨ tr∆∞·ªõc gi·ªù to√†n copy paste n√™n h∆°i ng·ª£p.\nM√£i m·ªõi xong.\nC√≥ v·∫ª kh√° nice, h∆∞·ªõng d·∫´n m√¨nh m√£i c√°i ch·ªó parse ra json, ƒë·ªÉ l·∫•y value c·ªßa field.\nH·∫πn tu·∫ßn sau n·ªØa m·ªõi tr·∫£ l·ªùi dc, v√¨ c√≤n pv nh·ªØng ng kh√°c. t·ª´ 26/12\u0026hellip;\n-\u0026gt; 1 th√°ng tr√¥i qua ko th·∫•y ph·∫£n h·ªìi -\u0026gt; t·ª± bi·∫øt t·∫°ch\n17. Scopic software Remote JS/TS Developer with Devops Experience,\nhttps://www.linkedin.com/jobs/view/3390151049,\nhttps://scopicsoftware.hire.trakstar.com/jobs/fk0sois?source=Linkedin,\nC√¥ng ty scopicsoftware,\nƒë·ªãnh apply nh∆∞ng ƒëang suy nghƒ©, ch·ªù 2 job ph√≠a tr√™n xem th·∫ø n√†o ƒë√£.\n18. IBM CIC https://www.linkedin.com/jobs/view/3403273823\napply qua Link Linh g·ª≠i: Krb-sjobs.brasshing.com\n19. Devoteam Belgium Azure Devops Engineer:\nhttps://www.glassdoor.com/job-listing/azure-devops-engineer-devoteam-belgium-JV_IC2345196_KO0,21_KE22,38.htm?jl=1008290546001\u0026amp;pos=128\u0026amp;ao=1136043\u0026amp;s=58\u0026amp;guid=0000018597960a6e96cf405f8a0a2549\u0026amp;src=GD_JOB_AD\u0026amp;t=SR\u0026amp;vt=w\u0026amp;ea=1\u0026amp;cs=1_693d1fbf\u0026amp;cb=1673325692963\u0026amp;jobListingId=1008290546001\u0026amp;jrtk=3-0-1gmbpc2l12hs8001-1gmbpc2lujihk800-72c55dae154ca079-\u0026amp;cancel=true\u0026amp;smart-apply-action=POST_APPLY\u0026amp;ctt=1673326013098\n-\u0026gt; t·∫°ch\n20. Explore Group Europe Azure DevOps Engineer ‚Äì Freelance ‚Äì Remote:\nhttps://www.linkedin.com/jobs/view/3430368727\n21. Thailand Remote 1 job Th√°i Lan app qua FB t√™n H·∫±ng -\u0026gt; reject t·ª´ v√≤n CV v√¨ c·∫ßn NodeJS\n22. Regal Cloud - Senior Devops Engineer https://www.linkedin.com/jobs/view/3407129072\n23. Leaseweb Amsterdam (Infrastructure Engineer) https://www.leaseweb.com/career/infrastructure-engineer\n2023.02.01 ph·ªèng v·∫•n v·ªõi Merve (ƒë·ªçc l√† Mai-v√™)\nc√¥ng ty to, ƒë·∫øn c√¥ng ty 2 l·∫ßn/week\nsalary range: 60-65k euro per year (gross)\nc√≥ sponsor visa\nN·∫øu join th√¨ l√†m v·ªõi 2 team 1 l√† team website, 2 l√† Customer\nC√≤n ph·ªèng v·∫•n v·ªõi Hiring Manager v√† Technical team\nH·ªèi m√¨nh:\nGi·ªõi thi·ªáu b·∫£n th√¢n?\nMotivation ƒë·ªÉ chuy·ªÉn sang H√† lan l√† g√¨?\nT·∫°i sao l·∫°i l√† HL m√† ko ph·∫£i ƒê·ª©c?\nH·ªèi v√¨ sao ngh·ªâ 2 c√¥ng ty c≈©?\n-\u0026gt; reject\n24. Ataccama (Job ·ªü Vietnam) Cloud Engineer https://atmosjobs.com/job/0a8174cd0ade8763f1dd0cb48750d08b67d9c445db\n2023.02.01 ph·ªèng v·∫•n v·ªõi Ana (b·∫°n n√†y contact m√¨nh qua linkedin)\nc√¥ng ty c√≥ 1 team ·ªü HN ƒëang ·ªü Toong coworking space, Ho√†ng ƒë·∫°o Th√∫y, Nguy·ªÖn th·ªã th·∫≠p (kh√° g·∫ßn ch·ªó c≈©)\njob full time, c√≥ th·ªÉ l√™n office ko th∆∞·ªùng xuy√™n, c√≥ k√Ω h·ª£p ƒë·ªìng\n·ªü VN, team c√≥ kho·∫£ng 9 ng∆∞·ªùi, 7 ng∆∞·ªùi l√† Java, c√≤n l·∫°i Cloud Enginneer c√≥ 2 ng∆∞·ªùi.\nc√≥ c∆° h·ªôi relocation nh∆∞ng ph·∫£i ch·ª©ng minh m√¨nh l√†m t·ªët\nch∆∞a c√≥ ƒë·ª£t laidoff n√†o, ·ªïn ƒë·ªãnh\nH·ªèi m√¨nh v√¨ sao l·∫°i mu·ªën chuy·ªÉn vi·ªác?\n-\u0026gt; reject sau 1 tu·∫ßn\n25. Swisscom/DevOps Engineer https://www.linkedin.com/jobs/view/3439445121\n-\u0026gt; reject sau 1 tu·∫ßn\n26. AZURE CLOUD ENGINEER - RKITEK https://www.linkedin.com/jobs/view/3418385120\n27. EyeTech-Solutions DevOps Engineer https://www.linkedin.com/jobs/view/3452358823\n-\u0026gt; reject sau 1 tu·∫ßn\n28. CLOUD ENGINEER - Ordina https://www.ordina.be/vacature/cloud-engineer/\n29. delaware Belux - Cloud Infrastructure Engineer https://www.linkedin.com/jobs/view/3442065192\nph·ªèng v·∫•n h·ªèi v·ªÅ motivation, tr·∫£ l·ªùi v√¨ v·ª£ sang h·ªçc PhD, -\u0026gt; v·ª£ b·∫°n h·ªçc tr∆∞·ªùng n√†o? m√¨nh ko tr·∫£ l·ªùi ƒë∆∞·ª£c B·∫°n bi·∫øt l√°i xe ko? - ko bi·∫øt B·∫°n ƒë√£ t·ª´ng sang Euro bao h ch∆∞a? - ch∆∞a\nH√¥m 17/2 h·∫πn ph·ªèng v·∫•n v√≤ng 2 v·ªõi 2 Manager, c√≥ l·∫Ω nh·ªù b·∫°n c·ªßa Linh (Bob Nguy·ªÖn Nghƒ©a) refer n√™n m·ªõi qua ƒë∆∞·ª£c v√≤ng ƒë·∫ßu\nGi·ªõi thi·ªáu c√¥ng ty, big service, consultant, 80 countries, 20 offices. ch√¢u √Å c√≥ Malay, philipins, China\nMicrosoft SAP, d√πng Azure ho√†n to√†n, ko c√≥ Data center\nƒêang c√≥ 8-9 person\nL√†m vi·ªác theo ticket, tr√™n VDI, monitoring tr√™n Zabix\nR·∫•t nhanh fast-foward, cut-in-edge technology\nKo ch·∫Øc 6 th√°ng n·ªØa s·∫Ω c√≥ g√¨ thay ƒë·ªïi, nhi·ªÅu project\nH·ªèi m√¨nh t·∫°i sao l·∫°i ch·ªçn Delaware,\nT·∫°i sao c√≥ 1 nƒÉm b√™n Japan xong l·∫°i v·ªÅ\nT·∫°i sao ch·ªçn relocation, ph·∫£i left behind nhi·ªÅu th·ª© ƒë·∫•y, ch∆∞a k·ªÉ paper work?\n1 ng√†y l√†m vi·ªác b√¨nh th∆∞·ªùng th√¨ c√≥ th·ªÉ ph·∫£i join nhi·ªÅu project kh√°c nhau,\nC√≥ v·∫•n ƒë·ªÅ g√¨ n·∫øu ki·ªÉu trong cu·ªôc h·ªçp n√†y nh∆∞ng l·∫°i ph·∫£i ƒëi x·ª≠ l√Ω v·∫•n ƒë·ªÅ ·ªü project kh√°c ko?\nƒêang d√πng bicep c·ªßa Azure\nH·ªèi m√¨nh c√≥ kinh nghi·ªám Azure AD ko, r·ªìi kinh nghi·ªám ElasticSearch\ntraffic t·ª´ region n√†y qua region kh√°c\n-\u0026gt; ko kh·∫£ quan l·∫Øm, kh·∫£ nƒÉng t·∫°ch cao\n30. epilot (Germany) - Cloud Infrastructure Engineer https://epilot.recruitee.com/l/en/o/cloud-engineer-mfd-koln/?source=Germantechjobs\n-\u0026gt; reject\n31. Senior Kubernetes Engineer at Accure Senior Kubernetes Engineer (d/f/m) at ACCURE Battery Intelligence GmbH\nhttps://germantechjobs.de/jobs/ACCURE-Battery-Intelligence-GmbH-Senior-Kubernetes-Engineer-dfm\n32. IBM Cloud Engineer - Ref:612331BR\nt·∫°ch v√† mail reject sau v√†i ng√†y\n33. Cloud/Workplace Engineer at iStorm https://www.linkedin.com/jobs/view/3417501143\n-\u0026gt; reject\n34. DevOps Engineer at Delta10 Netherlands Remote https://www.linkedin.com/jobs/view/3452387542\n-\u0026gt; reject\n35. Azure Devops engineer at Atos Brussels Metropolitan Area Hybrid https://www.linkedin.com/jobs/view/3479254936\n-\u0026gt; t·∫°ch\n36. DevOps at Jefferson Frank AWS DevOps Engineer - FinTech SaaS - Leuven:\nhttps://www.jeffersonfrank.com/job/CS192_1672732145/aws-devops-engineer-fintech-saas-leuven\nJunior Site Reliability Engineer - AWS SaaS Product - Hybrid:\nhttps://www.jeffersonfrank.com/job/JunDevSRE281122_1669653736/junior-site-reliability-engineer-aws-saas-product-hybrid\n","href":"/secrets/encrypt-my-journey-of-finding-new-job/","title":"My journey of finding new job"},{"content":"","href":"/tags/packer/","title":"Packer"},{"content":"Y√™u c·∫ßu Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒë√£ c√≥ 1 t√†i kho·∫£n AWS r·ªìi, c√≥ th·ªÉ t·∫°o ƒë∆∞·ª£c EC2\nC√≥ base ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ AWS, Linux bi·∫øt c√°ch SSH v√†o EC2\nC√°ch l√†m 1. T·∫°o b·ªô AWS key cho Packer user V√†o AWS IAM t·∫°o user \u0026ldquo;packer\u0026rdquo; (ho·∫∑c b·∫•t c·ª© t√™n g√¨, ·ªü ƒë√¢y m√¨nh l·∫•y ·∫£nh c≈© ch·ªçn user \u0026ldquo;ansible\u0026rdquo;) c·∫•p policy ph√π h·ª£p (v√≠ d·ª• ch·ªçn AdministratorAccess).\nV√†o tab Security cedentials t·∫°o Key cho User ƒë√≥.\nCopy b·ªô access key id v√† sceret access key ra ƒë·ªÉ b∆∞·ªõc sau d√πng. Launch 1 Amazon Linux EC2, SSH v√†o v√† l√†m c√°c b∆∞·ªõc ti·∫øp theo:\n2. Install Packer export PACKER_RELEASE=\u0026#34;1.4.3\u0026#34; cd /tmp/ wget --no-check-certificate https://releases.hashicorp.com/packer/${PACKER_RELEASE}/packer_${PACKER_RELEASE}_linux_amd64.zip unzip packer_${PACKER_RELEASE}_linux_amd64.zip sudo mv packer /usr/local/bin export PATH=$PATH:/usr/local/bin/packer source ~/.bashrc packer version N·∫øu install th√†nh c√¥ng s·∫Ω check ƒë∆∞·ª£c version c·ªßa packer:\n[ec2-user@ip-172-31-16-113 ~]$ packer version Packer v1.4.3 3. Export AWS KEY c·ªßa Packer user S·ª≠ d·ª•ng 2 c√°i key ƒë√£ chu·∫©n b·ªã ·ªü step 1:\nexport AWS_ACCESS_KEY_ID=\u0026#39;AKIARRRRRRRRRRRRRRZ6\u0026#39; export AWS_SECRET_ACCESS_KEY=\u0026#39;ICUZE333sjfio899EEEEEEEEEEUr7\u0026#39; ƒê·∫øn ƒë√¢y m√¨nh s·∫Ω ƒë∆∞a ra 2 demo:\nDemo 1 l√† D√πng Packer ƒë·ªÉ build Amazon image (AMI)\nDemo 2 l√† D√πng Packer + Ansible ƒë·ªÉ build Docker image\nT·∫•t nhi√™n c√≥ th·ªÉ c√≥ nhi·ªÅu c√°ch k·∫øt h·ª£p, nh∆∞ng m√¨nh ch·ªâ ƒë∆∞a 2 c√°i v√≠ d·ª• c∆° b·∫£n\n4. D√πng Packer ƒë·ªÉ build Amazon image 4.1. T·∫°o file Packer template file m√¨nh ƒë·∫∑t l√† basic.json Ch√∫ √Ω ƒëang l√†m tr√™n \u0026ldquo;region\u0026rdquo;: \u0026ldquo;us-east-1\u0026rdquo;, n√™n n·∫øu b·∫°n l√†m kh√°c region th√¨ ph·∫£i ch·ªçn ami ph√π h·ª£p\n{ \u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;{{env `AWS_ACCESS_KEY_ID`}}\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;{{env `AWS_SECRET_ACCESS_KEY`}}\u0026#34; }, \u0026#34;builders\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;source_ami_filter\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;virtualization-type\u0026#34;: \u0026#34;hvm\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ubuntu/images/*/ubuntu-xenial-16.04-amd64-server-*\u0026#34;, \u0026#34;root-device-type\u0026#34;: \u0026#34;ebs\u0026#34; }, \u0026#34;owners\u0026#34;: [\u0026#34;099720109477\u0026#34;], \u0026#34;most_recent\u0026#34;: true }, \u0026#34;instance_type\u0026#34;: \u0026#34;t2.micro\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;packer-example {{timestamp}}\u0026#34; }], \u0026#34;provisioners\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;inline\u0026#34;: [ \u0026#34;sleep 30\u0026#34;, \u0026#34;sudo apt-get update\u0026#34;, \u0026#34;sudo apt-get install -y redis-server\u0026#34; ] }] } 4.2. Validate Packer template packer validate basic.json N·∫øu template c·ªßa b·∫°n ko c√≥ l·ªói syntax g√¨ th√¨ hi·ªán nh∆∞ sau:\n[ec2-user@ip-172-31-16-113 ~]$ packer validate basic.json Template validated successfully. 4.3. Build Packer template packer build basic.json N√≥ s·∫Ω t·∫°o ra AMI tr√™n AWS console cho m√¨nh, trong AMI ƒë·∫•y ƒë√£ install redis nh∆∞ ƒëc define trong basic.json v√†o Console check n·∫øu t·∫°o ra AMI l√† ok\n5. D√πng Packer + Ansible ƒë·ªÉ build Docker image 5.1. Install Docker sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user exit exit r·ªìi SSH l·∫°i v√†o EC2, v√† c√°c l√†m b∆∞·ªõc sau\n5.2. T·∫°o file Packer template M√¨nh ƒë·∫∑t t√™n file l√† container.json\n{ \u0026#34;builders\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;centos:7\u0026#34;, \u0026#34;export_path\u0026#34;: \u0026#34;image.tar\u0026#34; }], \u0026#34;provisioners\u0026#34;:[{ \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;inline\u0026#34;: [ \u0026#34;yum -y update\u0026#34;, \u0026#34;yum -y install epel-release\u0026#34;, \u0026#34;yum -y install python-pip\u0026#34;, \u0026#34;pip install --upgrade pip\u0026#34;, \u0026#34;pip install ansible==2.5.0\u0026#34; ]}, { \u0026#34;type\u0026#34;: \u0026#34;ansible-local\u0026#34;, \u0026#34;playbook_file\u0026#34;: \u0026#34;container_base.yml\u0026#34; }], \u0026#34;post-processors\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;docker-import\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;ansible-dockerimage\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;0.1.0\u0026#34; }] } 5.3. T·∫°o file Ansible playbook V√¨ trong file container.json s·∫Ω g·ªçi ƒë·∫øn Ansible playbook container_base.yml\nN√™n m√¨nh ƒë·∫∑t t√™n file l√† container_base.yml\n- hosts: all become: True tasks: - name: install package yum: name={{ item }} state=present with_items: - git - htop 5.4. Build Packer template packer build container.json ch·∫°y c√≥ th·ªÉ b·ªã l·ªói TLS handshake timeout th√¨ ch·∫°y l·∫°i l√† ƒë∆∞·ª£c (·∫£nh) ch·∫°y OK s·∫Ω th·∫•y to√†n m√†u xanh v√† khi g√µ \u0026ldquo;docker images\u0026rdquo; s·∫Ω th·∫•y images dc t·∫°o ra (·∫£nh) Done!\n","href":"/bk/using-packer-ansible-to-create-image/","title":"Using Packer + Ansible to Create Image"},{"content":"Y√™u c·∫ßu Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒë√£ c√≥ 1 t√†i kho·∫£n AWS r·ªìi, c√≥ th·ªÉ t·∫°o ƒë∆∞·ª£c EC2\nC√≥ base ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ AWS, Linux bi·∫øt c√°ch SSH v√†o EC2\nC√°ch l√†m 1. T·∫°o b·ªô AWS key cho Packer user V√†o AWS IAM t·∫°o user \u0026ldquo;packer\u0026rdquo; (ho·∫∑c b·∫•t c·ª© t√™n g√¨, ·ªü ƒë√¢y m√¨nh l·∫•y ·∫£nh c≈© ch·ªçn user \u0026ldquo;ansible\u0026rdquo;) c·∫•p policy ph√π h·ª£p (v√≠ d·ª• ch·ªçn AdministratorAccess).\nV√†o tab Security cedentials t·∫°o Key cho User ƒë√≥.\nCopy b·ªô access key id v√† sceret access key ra ƒë·ªÉ b∆∞·ªõc sau d√πng. Launch 1 Amazon Linux EC2, SSH v√†o v√† l√†m c√°c b∆∞·ªõc ti·∫øp theo:\n2. Install Packer export PACKER_RELEASE=\u0026#34;1.4.3\u0026#34; cd /tmp/ wget --no-check-certificate https://releases.hashicorp.com/packer/${PACKER_RELEASE}/packer_${PACKER_RELEASE}_linux_amd64.zip unzip packer_${PACKER_RELEASE}_linux_amd64.zip sudo mv packer /usr/local/bin export PATH=$PATH:/usr/local/bin/packer source ~/.bashrc packer version N·∫øu install th√†nh c√¥ng s·∫Ω check ƒë∆∞·ª£c version c·ªßa packer:\n[ec2-user@ip-172-31-16-113 ~]$ packer version Packer v1.4.3 3. Export AWS KEY c·ªßa Packer user S·ª≠ d·ª•ng 2 c√°i key ƒë√£ chu·∫©n b·ªã ·ªü step 1:\nexport AWS_ACCESS_KEY_ID=\u0026#39;AKIARRRRRRRRRRRRRRZ6\u0026#39; export AWS_SECRET_ACCESS_KEY=\u0026#39;ICUZE333sjfio899EEEEEEEEEEUr7\u0026#39; ƒê·∫øn ƒë√¢y m√¨nh s·∫Ω ƒë∆∞a ra 2 demo:\nDemo 1 l√† D√πng Packer ƒë·ªÉ build Amazon image (AMI)\nDemo 2 l√† D√πng Packer + Ansible ƒë·ªÉ build Docker image\nT·∫•t nhi√™n c√≥ th·ªÉ c√≥ nhi·ªÅu c√°ch k·∫øt h·ª£p, nh∆∞ng m√¨nh ch·ªâ ƒë∆∞a 2 c√°i v√≠ d·ª• c∆° b·∫£n\n4. D√πng Packer ƒë·ªÉ build Amazon image 4.1. T·∫°o file Packer template file m√¨nh ƒë·∫∑t l√† basic.json Ch√∫ √Ω ƒëang l√†m tr√™n \u0026ldquo;region\u0026rdquo;: \u0026ldquo;us-east-1\u0026rdquo;, n√™n n·∫øu b·∫°n l√†m kh√°c region th√¨ ph·∫£i ch·ªçn ami ph√π h·ª£p\n{ \u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;{{env `AWS_ACCESS_KEY_ID`}}\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;{{env `AWS_SECRET_ACCESS_KEY`}}\u0026#34; }, \u0026#34;builders\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;source_ami_filter\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;virtualization-type\u0026#34;: \u0026#34;hvm\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ubuntu/images/*/ubuntu-xenial-16.04-amd64-server-*\u0026#34;, \u0026#34;root-device-type\u0026#34;: \u0026#34;ebs\u0026#34; }, \u0026#34;owners\u0026#34;: [\u0026#34;099720109477\u0026#34;], \u0026#34;most_recent\u0026#34;: true }, \u0026#34;instance_type\u0026#34;: \u0026#34;t2.micro\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;packer-example {{timestamp}}\u0026#34; }], \u0026#34;provisioners\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;inline\u0026#34;: [ \u0026#34;sleep 30\u0026#34;, \u0026#34;sudo apt-get update\u0026#34;, \u0026#34;sudo apt-get install -y redis-server\u0026#34; ] }] } 4.2. Validate Packer template packer validate basic.json N·∫øu template c·ªßa b·∫°n ko c√≥ l·ªói syntax g√¨ th√¨ hi·ªán nh∆∞ sau:\n[ec2-user@ip-172-31-16-113 ~]$ packer validate basic.json Template validated successfully. 4.3. Build Packer template packer build basic.json N√≥ s·∫Ω t·∫°o ra AMI tr√™n AWS console cho m√¨nh, trong AMI ƒë·∫•y ƒë√£ install redis nh∆∞ ƒëc define trong basic.json v√†o Console check n·∫øu t·∫°o ra AMI l√† ok\n5. D√πng Packer + Ansible ƒë·ªÉ build Docker image 5.1. Install Docker sudo yum install -y docker sudo service docker start sudo usermod -a -G docker ec2-user exit exit r·ªìi SSH l·∫°i v√†o EC2, v√† c√°c l√†m b∆∞·ªõc sau\n5.2. T·∫°o file Packer template M√¨nh ƒë·∫∑t t√™n file l√† container.json\n{ \u0026#34;builders\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;centos:7\u0026#34;, \u0026#34;export_path\u0026#34;: \u0026#34;image.tar\u0026#34; }], \u0026#34;provisioners\u0026#34;:[{ \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;inline\u0026#34;: [ \u0026#34;yum -y update\u0026#34;, \u0026#34;yum -y install epel-release\u0026#34;, \u0026#34;yum -y install python-pip\u0026#34;, \u0026#34;pip install --upgrade pip\u0026#34;, \u0026#34;pip install ansible==2.5.0\u0026#34; ]}, { \u0026#34;type\u0026#34;: \u0026#34;ansible-local\u0026#34;, \u0026#34;playbook_file\u0026#34;: \u0026#34;container_base.yml\u0026#34; }], \u0026#34;post-processors\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;docker-import\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;ansible-dockerimage\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;0.1.0\u0026#34; }] } 5.3. T·∫°o file Ansible playbook V√¨ trong file container.json s·∫Ω g·ªçi ƒë·∫øn Ansible playbook container_base.yml\nN√™n m√¨nh ƒë·∫∑t t√™n file l√† container_base.yml\n- hosts: all become: True tasks: - name: install package yum: name={{ item }} state=present with_items: - git - htop 5.4. Build Packer template packer build container.json ch·∫°y c√≥ th·ªÉ b·ªã l·ªói TLS handshake timeout th√¨ ch·∫°y l·∫°i l√† ƒë∆∞·ª£c (·∫£nh) ch·∫°y OK s·∫Ω th·∫•y to√†n m√†u xanh v√† khi g√µ \u0026ldquo;docker images\u0026rdquo; s·∫Ω th·∫•y images dc t·∫°o ra (·∫£nh) Done!\n","href":"/posts/using-packer-ansible-to-create-image/","title":"Using Packer + Ansible to Create Image"},{"content":"C√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh h·ªçc \u0026mdash;P1\u0026mdash;\n Q. 1 cty c·∫ßn migrate 10TB data l√™n aws. Y√™u c·∫ßu replica lag \u0026lt; 100 miliseconds, data c√≥ th·ªÉ scale g·∫•p 4? d√πng c√°i RDS n√†o c·ªßa amazon?\n aurora, c√≥ th·ªÉ scale 64TB v√† keep replica lag d∆∞·ªõi 100 milisec\n Q. \u0026ldquo;IOPS‚Äù stands for input/output operations per second=s·ªë l·∫ßn ƒë·ªçc/ghi max trong 1 gi√¢y, \u0026ldquo;throughput\u0026rdquo; l√† dung l∆∞·ª£ng traffic 1 gi√¢y\n C√°c lo·∫°i EBS:\nGeneral Purpose SSD (gp2): b√¨nh th∆∞·ªùng, d√πng cho m√¥i tr∆∞·ªùng Dev/test, volume size (1gb-16TB), max IOPS=16k, max throughput/volume=250M/s\nProvisioned IOPS SSD (io1): performance kinh nh·∫•t, d√πng cho data quan h·ªá RDS, volume size (4gb-64TB), max IOPS=64k, max throughput/volume=1000M/s\nThroughput Optimized HDD (st1): gi√° r·∫ª, d√πng cho Bigdata, streaming, access th∆∞·ªùng xuy√™n, volume size (500gb-16TB), max IOPS=500, max throughput/volume=500M/s\nCold HDD (sc1): gi√° r·∫ª nh·∫•t, d√πng cho Bigdata, access ko th∆∞·ªùng xuy√™n, volume size (500gb-16TB), max IOPS=250, max throughput/volume=250M/s\n Q. 1 App run 150GB relational database on EC2 instance. App ch·∫°y ko th∆∞·ªùng xuy√™n v√† ƒë√¥i khi c√≥ peak time. ƒë·ª° t·ªën ti·ªÅn n√™n ch·ªçn lo·∫°i EBS n√†o?\n s·∫Ω d√πng SSD (gp2),\n1- v√¨ c√°i Hdd (st1, sc1) s·∫Ω c√≥ minimum 500GB m√† m√¨nh ch·ªâ c·∫ßn 150GB th√¥i\n2- v√¨ l√† RDS n√™n ∆∞u ti√™n IOPS -\u0026gt; ∆∞u ti√™n SSD\nch·ª© n·∫øu Big data, stream data th√¨ ∆∞u ti√™n Throughtput MB/s -\u0026gt; ∆∞u ti√™n Hdd\n Q. Though the images will be retrieved infrequently, they must be available for retrieval immediately. Data dc truy xu·∫•t ko li√™n t·ª•c, nh∆∞ng ph·∫£i available ƒë·ªÉ l·∫•y ngay l·∫≠p t·ª©c\n S3 IA\n Q. V·ªÅ s3 glacier, kh√°c bi·ªát gi·ªØa c√°c option Retrieval sau: expedited, standard, bulk?\n Expedited data s·∫Ω c√≥ th·ªÉ access trong v√≤ng 1-5 min,\nStandard data s·∫Ω c√≥ th·ªÉ access trong v√≤ng 3-5 hour,\nBulk r·∫ª nh·∫•t, data s·∫Ω c√≥ th·ªÉ access trong v√≤ng 5-12 hour\n Q. 1 app tr√™n aws c·∫ßn pull data t·ª´ internet, SA c·∫ßn thi·∫øt k·∫ø HA solution ƒë·ªÉ access v√†o data m√† ko c·∫ßn ƒë·∫∑t bandwidth constraints v√†o c√°i app c·ªßa m√¨nh? th√¨ s·∫Ω l√†m g√¨?\n g·∫Øn IGW v√† route 0.0.0.0/0,\nNATGW c≈©ng HA nh∆∞ng ch·ªâ d√πng cho private subnet, v√† bandwidth ch·ªâ cho scale up 45Gbps,\nNAT instance th√¨ bandwidth ph·ª• thu·ªôc v√†o lo·∫°i instance,\nVPC endpoint th√¨ d√πng ƒë·ªÉ connect t·ªõi c√°c service c·ªßa aws th√¥i, ko d√πng cho internet\n Q. asG, b·∫°n nh·∫≠n ra app c·ªßa b·∫°n ƒëang scale asG up and down nhi·ªÅu l·∫ßn, c·∫ßn l√†m g√¨?\n S·ª≠a asg cool down timer, v√† check c√°i cloudwatch alarm ƒëang trigger vi·ªác scale down asg.\n Q. asg cool down timer l√† g√¨?\n l√† th·ªùi gian sau khi scale activity c·ªßa asg th·ª±c hi·ªán, n√≥ s·∫Ω t·∫°m suspend 1 tgian. (default l√† 300s)\n Q. recommend apply traffic cho ·ª©ng d·ª•ng web app, n√™n apply cho 2 tier web v√† db nh∆∞ n√†o?\n web sg: allow port 443 https (SSL) ho·∫∑c 80 http from source: anywhere\ndb sg: allow port 3306 (mysql) from source: \u0026ldquo;web sg\u0026rdquo;\nSG th√¨ m√¨nh ch·ªâ c√≥ th·ªÉ set rule ƒë·ªÉ allow ip, c√≤n ACL th√¨ c√≥ th·ªÉ set rule ƒë·ªÉ deny/allow ip n√†o ƒë√≥.\nSet rule inbound cho SG th√¨ outbound c≈©ng apply (stateful), nh∆∞ng ACL th√¨ ph·∫£i set c·∫£ inbound v√† outbound (stateless)\n Q. C√°c rule trong ACL ƒëc apply ntn? N·∫øu s·ªë 100 allow ip 1, s·ªë 101 deny ip 1 th√¨?\n t·ª´ s·ªë b√© tr∆∞·ªõc r·ªìi ƒë·∫øn s·ªë l·ªõn sau.\nN·∫øu s·ªë 100 allow ip 1, s·ªë 101 deny ip 1 th√¨ k·∫øt qu·∫£ l√† ip 1 v·∫´n dc allow v√¨ n√≥ dc apply tr∆∞·ªõc\n Q. 1 website ch·∫°y tr√™n ec2 sau 1 ALB, h·ªç mu·ªën tr√°nh vi·ªác serving file c·ªßa ec2 m·ªói l·∫ßn ng d√πng request assest th√¨ l√†m g√¨ ƒë·ªÉ c·∫£i thi·ªán ux?\n cache static content tr√™n Cloudfront\n Q. 1 app c·∫ßn encrypt data trc khi ghi v√†o disk, n√™n d√πng service n√†o?\n KMS, ko d√πng Certificate manager v√¨ awS cert manager d√πng ƒë·ªÉ gen ra certificate ƒë·ªÉ encrypt traffic in transit\n Q. 1 cty export daily data v√†o S3. team data warehouse mu·ªën l·∫•y data t·ª´ s3 ƒë√≥ import v√†o Redshift, y√™u c·∫ßu l√† data ch·ªâ dc transport trong 1 vpc. l√†m g√¨ cho secure?\n b·∫≠t Redshift Enhanced vpc Routing v√† create s3 vpc endpoint\n Q. 1 website cung c·∫•p nhi·ªÅu language, define theo htttp request:\nhttp://d11111f8.cloudfront.net/main.html?language=de;\nhttp://d11111f8.cloudfront.net/main.html?language=en\nCloudfront c·∫ßn configure nh∆∞ n√†o?\n Base on query string parameters\n Q. 3 c√°ch cache content tr√™n cloudfront l√†?\n base on query string parameter, base on cookie, base on request headers\n Q. ecs, c√°c container ko dc access vao c√°c container kh√°c c·ªßa customer kh√°c. v·∫≠y ph·∫£i d√πng n√†o?\n d√πng IAM role for task (trong CDA th√¨ c√≥ 1 c√¢u nh∆∞ n√†y nh∆∞ng b·∫£o config security group ho·∫∑c IAM role for task -\u0026gt; c√≥ th·ªÉ d√πng 1 trong 2 c√°ch)\n Q. 1 c√¥ng ty ƒëang generate 1 c·ª•c dataset h√†ng tri·ªáu row th√†nh d·∫°ng c·ªôt, c√°c tool kh√°c s·∫Ω build report daily t·ª´ c√°c dataset ƒë√≥. N√™n d√πng service g√¨?\n Redshift, d√πng v·ªõi datawarehouse, ƒë·ªÉ t·∫°o insights cho c√¥ng ty b·∫°n\n Q. 1 cty c·∫ßn 1 db tr√™n aws ph√π h·ª£p y√™u c·∫ßu:\nkh·ªüi t·∫°o 8TB, tƒÉng tr∆∞·ªùng 8GB 1 ng√†y, c√≥ 4 read replica\n N√™n D√πng awS Aurora (limit 64TB v√† 15 read replica),\nkh√¥ng d√πng dynamoDB v√¨ ph·∫£i vi·∫øt r√µ l√† DAX (DynamoDB Accelerator) m·ªõi c√≥ 9 Read Replicas, DynamoDB ko limit size c·ªßa table\naurora l√† Relational db, dynamoDB l√† noSQL\n Q. 1 app c·∫ßn 1 file system. c√°i n√†o ƒë·ªÉ c√≥ th·ªÉ access b·ªüi ec2?\n EFS, ko d√πng S3 v√¨ l√† object base storage th√¥i. (ch√∫ √Ω efs ko th·ªÉ c√†i tr√™n window ec2)\n Q. H√†ng tri·ªáu ·∫£nh c·∫ßn up l√™n s3? l√†m g√¨ ƒë·ªÉ tƒÉng performance?\n d√πng hexadecimal hash for prefix\n Q. C·∫ßn get IP address c·ªßa c√°c resource accessed in private subnet. D√πng c√°i n√†o?\n VPC flow log\n Q. 1 app 2 tier c·∫ßn db. DB s·∫Ω y√™u c·∫ßu multiple schema changes. durable, ACID. D√πng db n√†o? ACID: Atomicity (n·∫øu transaction li√™n quan \u0026gt;2 x·ª≠ l√Ω, th√¨ ho·∫∑c all x·ª≠ l√Ω dc th·ª±c hi·ªán, ho·∫∑c l√† ko c√°i n√†o dc th·ª±c hi·ªán), Consistency (ko c√≥ transaction d·ªü dang,nghƒ©a l√† ho·∫∑c m·ªõi ho·∫∑c rollback), isolation (c√°c transaction lu√¥n t√°ch r·ªùi nhau, ko tr·ªôn l·∫´n), Durability (data never lost n·∫øu ·ª©ng d·ª•ng b·ªã crash)\n D√πng Aurora.\nV√¨ DynamoDB ko support schema change, ko support ACID (ch·ªâ C v√† D).\nRedshift c≈©ng ko ACID (ch·ªâ AID)\n Q. Redshift cluster m√† mu·ªën recovery khi disaster ƒë·∫øn 1 ch·ªó c√°ch xa 600km th√¨ c·∫ßn l√†m g√¨?\n B·∫≠t cross-region snapshot . V√¨ m·ªói AZ c√°ch nhau ch·ªâ kho·∫£ng 100km n√™n ko th·ªÉ b·∫≠t cross-az snapshot ƒëc (M√† n√≥ c≈©ng ko c√≥ kh√°i ni·ªám l√† \u0026ldquo;cross AZ\u0026rdquo; ch·ªâ c√≥ kh√°i ni·ªám \u0026ldquo;multi AZ/multi region\u0026rdquo; nh∆∞ng l·∫°i ko d√πng cho snapshot m√† d√πng ch·ªâ cluster)\n Q. 1 cty ƒëang c√≥ APi ri√™ng v·ªõi 1000 request per second ? ƒë·ªÉ cost effective d√πng g√¨?\n API gateway + lambda\n Q. 1 app host tr√™n ec2 ƒëang c√≥ 1 campaign start trong 2 tu·∫ßn. Mu·ªën ƒë·∫£m b·∫£o performance trong th·ªùi gian ƒë√≥, c·∫ßn config asG nh∆∞ n√†o?\n Dynamic scaling + target tracking scaling policy\n Q. 1 app host tr√™n aws sit sau 1 ELB. Notfy to admin n·∫øu read request \u0026gt; 1000 req/min. Notify n·∫øu latency \u0026gt; 10s. B·∫•t ky API activity m√† call ƒë·∫øn sensitive data s·∫Ω dc monitor\n Cloudtrail v√† cloudwatch metric v·ªõi setup alarm\n Q. 1 company mu·ªën monitor all API activity v√† c·∫ßn apply cho future regions. C·∫ßn l√†m g√¨?\n Ensure 1 trail trong cloudtrail dc enable all region\n Q. C√≥ y√™u c·∫ßu cho thi·∫øt b·ªã ISCSI device, v√† 1 app legacy c·∫ßn local storage. C·∫ßn d√πng service n√†o? iscsi device l√† c√°c thi·∫øt b·ªã li√™n quan ƒë·∫øn ip TCP/IP, truy·ªÅn d·ªØ li·ªáu qua LAN, WAN\n D√πng Aws Storage gateway stored volume\nPh√¢n bi·ªát aws gateway cached volume v√† aws gateway stored volume:\nAws gateway cached volume: l∆∞u data tr√™n s3, 1 c·ª•c c·∫ßn access th∆∞·ªùng xuy√™n s·∫Ω l∆∞u locally. d·ªÖ scale, ƒë·ªô tr·ªÖ th·∫•p\nAws gateway stored volume: data s·∫Ω l∆∞u ho√†n to√†n ·ªü locally, r·ªìi snapshot s·∫Ω dc async l√™n s3\n(gi·∫£i ph√°p cho vi·ªác backup recover data cho local data center c·ªßa m√¨nh ho·∫∑c tr√™n aws)\nGateway-virtual tape library (VTL): data l∆∞u ·ªü Glacier\n Q. Y√™u c·∫ßu l√† EC2 trong private subnet c·∫ßn access v√†o S3 bucket. Traffic ko dc qua internet. D√πng c√°i g√¨?\n VPC Enpoint\n Q. C√≥ 1 app ch·∫°y tr√™n c√°c EC2 v√† sit sau clasic ELB, 1 EC2 proxy dc d√πng cho qu·∫£n l√Ω content v√† nhi·ªÅu Ec2 d√πng cho Backend, V·∫•n ƒë·ªÅ l√† app c·ªßa h·ªç scale ko ƒë√∫ng. N√™n l√†m n√†o?\n D√πng asg cho c·∫£ proxy server v√† backend instance\n Q. 1 web host tr√™n aws c√≥ dc r·∫•t nhi·ªÅu traffic. L√†m sao ƒë·ªÉ gi·∫£m thi·ªÉu r·ªßi ro disruption c·ªßa web n√†y trong tr∆∞·ªùng h·ª£p disaster?\n D√πng Route53 ƒë·ªÉ Route to static website.\nKo th·ªÉ d√πng ELB ƒë·ªÉ chuy·ªÉn traffic ƒë·∫øn region kh√°c v√¨ ELB ch·ªâ d√πng ƒë·ªÉ balance trong 1 region ch·ª© ko multi region dc, k·ªÉ c·∫£ trong 1 region th√¨ vi·ªác backup cross AZ ko d√πng ƒë·ªÉ recovery\nH·∫ßu h·∫øt cty c·ªë g·∫Øng implement HA thay v√¨ DR, trong case HA th√¨ app s·∫Ω host kh√°c AZ ch·ª© ko kh√°c region, ko ƒë·∫£m b·∫£o trong TH disaster to√†n region. DR th√¨ s·∫Ω recovery t·ª´ 1 region kh√°c (\u0026gt;250miles). DR implement m√¥ h√¨nh Active/Passive, nghƒ©a l√† lu√¥n lu√¥n c√≥ 1 static site OR service nh·ªè run ·ªü 1 region kh√°c.\n Q. B·∫°n ƒë∆∞·ª£c y√™u c·∫ßu host static web cho domain mycompany.com tr√™n aws, y√™u c·∫ßu traffic dc scale ƒë√∫ng c√°ch, d√πng services n√†o?\n Route53 v√† static website tr√™n s3,\nnh·∫≠p NS record c·ªßa R53 v√†o domain registrar ƒë·ªÉ registrar qu·∫£n l√Ω c√°i domain ƒë√≥, ko cho ng kh√°c ƒëƒÉng k√Ω l·∫°i domain c·ªßa m√¨nh\n Q. B·∫°n ƒë∆∞·ª£c y√™u c·∫ßu analyze behavior qua nh·ªØng c√°i g√¨ customer click v√†o, c√°i n√†o ƒë·ªÉ capture data v√† analyze?\n Push nh·ªØng c√°i click c·ªßa customer l√™n kinesis v√† analyze b·∫±ng kinesis worker\n Q. 1 c√¥ng ty c√≥ 1 infra g·ªìm c√°c machine send log every 5 minutes, s·ªë l∆∞·ª£ng machine c√≥ th·ªÉ h√†ng ng√†n, data s·∫Ω analyze ·ªü stage kh√°c, th√¨ c·∫ßn d√πng g√¨?\n D√πng kinesis firehose v·ªõi s3 ƒë·ªÉ take log, store n√≥ tr√™n s3 cho c√°c process sau n√†y\nKinesis Firehose capture, transform, load stream data v√†o s3, redshift, elasticsearch\n Q. 1 app host tr√™n aws cho ph√©p user upload video to s3. 1 user ph·∫£i c√≥ quy·ªÅn ƒë·ªÉ upload video l√™n s3 bucket trong 1 tu·∫ßn base on profile, L√†m th·∫ø n√†o?\n T·∫°o pre-signed urL h·∫øt h·∫°n trong 1 tu·∫ßn cho m·ªói user profile.\n Q. 1 cty host 5 web server tr√™n aws. H·ªç mu·ªën Route53 route user traffic to random web server. Policy n√†o c·ªßa R53 th·ªèa m√£n?\n Multivalue answer routing policy: route traffic 1 c√°ch ng·∫´u nhi√™n ƒë·∫øn max l√† 8 record healthy.\nNgo√†i ra:\nSimple routing policy: D√πng 1 resource cho 1 domain.\nLatency routing policy: N·∫øu c√≥ nhi·ªÅu resource th√¨ s·∫Ω route traffic ƒë·∫øn location c√≥ best latency nh·∫•t.\nWeighted routing policy: route traffic ƒë·∫øn nhi·ªÅu resoure theo t·ªâ l·ªá b·∫°n defined.\nGeoproximity routing policy: route traffic d·ª±a tr√™n location c·ªßa resource c·ªßa b·∫°n.\nGeolocation routing policy: route traffic d·ª±a tr√™n location c·ªßa user.\nFailover routing policy: d√πng ƒë·ªÉ configure Active/passive failover.\n Q. C√¥ng ty c·∫ßn 1 db c√≥ th·ªÉ perform c√¢u query underlying (truy v·∫•n c∆° b·∫£n) nh∆∞ JOIN th√¨ d√πng db n√†o? aurora? dynamodb? redshift? s3?\n aurora (ch·ª© dynamo ko d√πng join dc )\n\u0026mdash;P2\u0026mdash;\n Q. 1 customer mu·ªën t·∫°o EBS volume in AWS, data on volume c·∫ßn encrypt at rest. C·∫ßn l√†m g√¨?\n v√¨ at rest n√™n d√πng KMS ƒë·ªÉ gen ra key r·ªìi s·ª≠ dung key ƒë√≥ encrypt data. Kh√¥ng d√πng SSL v√¨ SSL ƒë·ªÉ encrypt data in transit\n Q. Cty b·∫°n c√≥ 1 s·ªë EC2, c·∫ßn monitor state c·ªßa EC2 v√† record l·∫°i, l√†m n√†o?\n Cloudwatch log ƒë·ªÉ store state changes c·ªßa EC2, D√πng cloudwatch Event ƒë·ªÉ monitor changes of event ƒë·ªëi v·ªõi EC2\n Q. B·∫°n t·∫°o 1 VPC v√† 3 public subnet, ·ªü m·ªói AZ 1 public subnet, m·ªói subnet 1 ec2. Gi·ªù mu·ªën t·∫°o ELB c·ªßa 2 Az. bao nhi√™u Private IP dc d√πng khi kh·ªüi t·∫°o ELB?\n 2 AZ -\u0026gt; 2 subnet -\u0026gt; 2 ec2 -\u0026gt; 2 private IP\n Q. Amazon RDS backup process l√† g√¨? default l√† g√¨?\n default RDS s·∫Ω th·ª±c hi·ªán backup c√°i storage volume c·ªßa DB Instance, nghƒ©a l√† to√†n b·ªô DB instance ch·ª© ko ri√™ng r·∫Ω DB.\nƒêi·ªÅu n√†y l√†m m·ªói ng√†y 1 l·∫ßn. C√πng v·ªõi capture transaction log every 5min v√† l∆∞u s3\nC√≤n Database Snapshot th√¨ RDS ko t·ª± l√†m, l√† do User l√†m, c√≥ th·ªÉ l√†m b·∫•t k√Ω l√∫c n√†o.\n Q. 1 app l∆∞u th√¥ng tin quan tr·ªçng tr√™n s3. th√¥ng tin c√≥ th·ªÉ access qua internet, c√¥ng ty lo r·∫±ng connect ƒë·∫øn s3 c√≥ th·ªÉ l√† risk v·ªÅ security, l√†m n√†o?\n access data th√¥ng qua VPC enpoint cho S3.\n Q. B·∫°n ƒëang c√≥ 1 NAT Gateway cho c√°c ec2 private. B·∫°n mu·ªën NAT Gateway c≈©ng ph·∫£i HA, l√†m n√†o?\n T·∫°o th√™m NAT Gateway ·ªü 1 Az kh√°c, V·ªÅ document c·ªßa AWS c√≥ n√≥i, mu·ªën t·∫°o 1 h·ªá th·ªëng ko phu thu·ªôc Az th√¨ m·ªói Az 1 NAT Gateway v√† ensure r·∫±ng c√°c resource s·ª≠ dung NAT ƒë√≥ th√¨ ·ªü c√πng 1 Az.\nCh·ª© ko th·ªÉ dung ASG cho NAT Gateway\n Q. 1 cty mu·ªën c√≥ 1 fully managed data store tr√™n Aws, compatible v·ªõi MySQL, database engine n√†o c√≥ th·ªÉ s·ª≠ dung? AWS Aurora hay AWS RDS\n Ch·ªçn Aurora v√¨ RDS ko ph·∫£i l√† database engine, RDS l√† 1 service support 6 database engines.\nAurora l√† 1 DB engines nh∆∞ng n√≥ ch·ªâ support 2 DB engines kh√°c l√† MySQL v√† PostgresSQL\n Q. 1 cty mu·ªën ch·ªâ nh·ªØng kh√°ch hang x√°c ƒë·ªãnh c√≥ th·ªÉ upload ·∫£nh to s3 trong 1 giai ƒëo·∫°n nh·∫•t ƒë·ªãnh. N√™n l√†m n√†o?\n t·∫°o Pre-signed URL ƒë·ªÉ upload images, th√™m ƒëi·ªÅu ki·ªán expired time.\n Q. Ph√¢n bi·ªát c√°c lo·∫°i ELB: CLB, NLB, ALB (c√°i n√†o c≈©ng c√≥ t√≠nh scalability):\n Classic Load Balancer: cung c·∫•p (HTTP, HTTPS or TCP listeners) cho only 1 backend port. Ch·∫°y b√™n ngo√†i VPC. ( N·∫øu app c·ªßa b·∫°n ch·∫°y ngo√†i VPC). Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\nNetwork Load Balancer: Layer 4, cung c·∫•p only TCP. D√πng EIP/static IP. Nhi·ªÅu ports. D√πng v·ªõi nh·ªØng app m√† ch·ªâ c·∫ßn s·ª≠ d·ª•ng TCP. V√† mu·ªën qu·∫£n l√Ω b·∫±ng whilelist IP. C√≥ th·ªÉ scale ƒë·ªÉ handle workload l√™n ƒë·∫øn h√†ng tri·ªáu request 1 gi√¢y (millions of requests per second)\nApplication Load Balancer: Layer7, cung c·∫•p c√¢n b·∫±ng t·∫£i cho only HTTP/HTTPS. Nhi·ªÅu ports. flexible h∆°n CLB. Nhi·ªÅu ng d√πng. Cung c·∫•p t√≠nh nƒÉng n√¢ng cao v·ªÅ routing ƒë·∫øn target, d√πng cho c√°c app hi·ªán ƒë·∫°i, micro-services, containers. Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\n Q. Third-party sign-in (Federation) co dc d√πng ƒë·ªÉ authorize service ko?\n ko, Federation is used to authenticate users, not to authorize services\n Q. B·∫°n c√≥ 1 v√†i ec2 trong asG, nh∆∞ng c√≥ v·∫ª ec2 ko dc scale out ƒë√∫ng l√∫c. C·∫ßn check l·∫°i g√¨?\n check l·∫°i xem c√≥ ƒë√∫ng metric dc d√πng ƒë·ªÉ trigger vi·ªác scale out ko?\n Q. Cty b·∫°n thi·∫øt k·∫ø 1 app t∆∞∆°ng t√°c v·ªõi dynamodb, v√† c√°c user c·ªßa app th√¨ sign-in s·ª≠ d·ª•ng GG, FB. C·∫ßn c√°i g√¨ ƒë·ªÉ app c√≥ th·ªÉ get temporary access to dynamodb?\n 1 c√°i role ƒë·ªÉ app c√≥ th·ªÉ access v√†o dynamodb, user s·∫Ω ph·∫£i assume c√°i role ƒë√≥\n Q. Mu·ªën all traffic ra/v√†o Redshift ko ƒëi ra internet, l√†m n√†o?\n Enable Redshift Enhanced VPC routing\n Q. 1 cty ƒëang c√≥ 1 set HyperV machines and VMware virtual machine. Mu·ªën migrate to cloud. C·∫ßn d√πng c√°i g√¨?\n aWS server Migration service\n Q. Trust advisor ƒë·ªÉ l√†m g√¨?\n l√† tool ƒë·ªÉ cung c·∫•p real time guidance to provison resource theo best practice\n Q. 1 cty c√≥ nhi·ªÅu data tr√™n on premise. H·∫øt storage, cty mu·ªën 1 gi·∫£i ph√°p aws ƒë·ªÉ d·ªÖ d√†ng extension data to aws. N√≥i chung gi·∫£i ph√°p ƒë·ªÉ migrate ho·∫∑c transfer data on-premise to s3.\n Ch·ªçn aws gateway cached volumes\nPh√¢n bi·ªát aws gateway cached volume v√† aws gateway stored volume:\nAws gateway cached volume: l∆∞u data tr√™n s3, 1 c·ª•c c·∫ßn access th∆∞·ªùng xuy√™n s·∫Ω l∆∞u locally. d·ªÖ scale, ƒë·ªô tr·ªÖ th·∫•p\nAws gateway stored volume: data s·∫Ω l∆∞u ho√†n to√†n ·ªü locally, r·ªìi snapshot s·∫Ω dc async l√™n s3\n(gi·∫£i ph√°p cho vi·ªác backup recover data cho local data center c·ªßa m√¨nh ho·∫∑c tr√™n aws)\nGateway-virtual tape library (VTL): data l∆∞u ·ªü Glacier\n Q. 1 Cty mu·ªën l∆∞u document v√† ƒë·ªÅ ph√≤ng tr∆∞·ªùng h·ª£p b·ªã x√≥a. D√πng g√¨?\n S3, enable versioning\n Q. 1 cty c√≥ 1 app deliver object t·ª´ s3 ƒë·∫øn user. user ph·∫£n h·ªìi l√† respone time ch·∫≠m. Ph·∫£i l√†m sao ƒë·ªÉ gi·∫£m respone time v√† ti·∫øt ki·ªám chi ph√≠?\n ƒë·ªÉ s3 bucket sau CloudFront distribution. N·∫øu workload ch·ªß y·∫øu l√† GET request, th√¨ s·ª≠ d·ª•ng Cloudfront s·∫Ω cache request l·∫°i, gi·∫£m request t·ªõi S3, reduce cost.\nKh√¥ng n√™n d√πng S3 replication cross regions v√¨ t·ªën ti·ªÅn h∆°n.\nS3 Transfer Acceleration c≈©ng t·ªën ti·ªÅn h∆°n.\nKh√¥ng d√πng ELB v·ªõi s3 dc v√¨ ch·ªâ ƒë·ªÉ balance traffic cho ec2.\n Q. ƒê·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± message v√† dublicated message ko dc g·ª≠i th√¨ n√™n dung?\n SQS FIFO, ko n√™n d√πng SNS v√¨ SNS c√≥ th·ªÉ g·ª≠i message duplicated\n Q. 1 cty c√≥ 1 v√†i ec2 ƒë·ªÉ qu·∫£n l√Ω 1 web app. C·∫ßn gi·∫£i ph√°p ƒë·ªÉ fault tolerant?\n Ensure c√°c Ec2 ·ªü c√°c Az ri√™ng r·∫Ω. D√πng ELB ƒë·ªÉ distribute traffic, attach ELB v√†o ASG\n Q. 1 cty store log tr√™n s3, c√≥ y√™u c·∫ßu l√† c·∫ßn search ·ªü trong ƒë·ªëng data trong s3 ƒë·∫•y. L√†m n√†o?\n AWS Athena ƒë·ªÉ query tr√™n s3 bucket, Load data v√†o Amazon Elasticsearch ƒë·ªÉ search\n Q. Cty b·∫°n dung KMS qu·∫£n l√Ω key ƒë·ªÉ n√≥ encrypt decrypt data, gi·ªù b·∫°n mu·ªën secure h∆°n n√™n b·∫°n enable ch·ª©c nƒÉng rotate key. th√¨ sao?\n KMS s·∫Ω rotate key h√†ng nƒÉm v√† s·ª≠ dung 1 key ph√π h·ª£p ƒë·ªÉ encrypt/decrypt data (KMS keep c√°c key c≈©)\n Q. RDS MySQL size v√† Aurora Size?\n RDS MySQL up to 16TB\nAurora 10GB to 64TB\n Q. Cognito c√≥ th·ªÉ t·∫°o th√™m identity provider dc ko?\n c√≥\n Q. ƒê·ªÉ allow SSH cho 1 ec2, c·∫ßn config SG v√† NACL nh∆∞ n√†o?\n SG: Inbound (allow), Outbound (deny)\nNACL: Inbound (allow), Outbound (allow)\nv√¨ SG l√† stateful, nh·ªØng request incoming n·∫øu dc granted th√¨ outgoing request c≈©ng dc granted\nc√≤n NACL stateless, n·∫øu config cho grant nh·ªØng incoming request th√¨ c≈©ng ph·∫£i config grant cho nh·ªØng outgoing request n·ªØa\n Q. 1 cty ƒëang c√≥ 1 web distribution v√† dung Cloudfront. IT confirm l√† c√°i web n√†y ƒëang thu·ªôc scope PCI compliance. C·∫ßn l√†m g√¨?\n Enable Cloudfront access log, Capture nh·ªØng requests m√† dc g·ª≠i t·ªõi CloudFront API\nV√¨ ƒëang run PCI or HIPAA-compliant workloads based on the AWS Shared Responsibility Model, AWS recommend b·∫°n log l·∫°i CloudFront usage data c·ªßa 365 ng√†y g·∫ßn nh·∫•t ƒë·ªÉ audit trong t∆∞∆°ng lai, ƒë·ªÉ log usage data th√¨ c·∫ßn:\nEnable Cloudfront access log, Capture nh·ªØng requests m√† dc g·ª≠i t·ªõi CloudFront API\n Q. VPC Flow log l√†m g√¨?\n capture th√¥ng tin IP traffic ƒëi ra ƒëi v√†o Network interface trong VPC th√¥i, ko c√≥ Cloudfront\n\u0026mdash;P3\u0026mdash;\n Q. 1 cty d√πng RDS, mu·ªën encrypt data at rest, l√†m n√†o?\n Enable t√≠nh nƒÉng encryption ngay t·ª´ l√∫c t·∫°o DB ( ch·ª© t·∫°o r·ªìi ko enable dc n·ªØa).\nCh·ªçn ƒë√∫ng lo·∫°i instance class support vi·ªác encrypt v√¨ ko ph·∫£i lo·∫°i n√†o c≈©ng support (v√≠ d·ª• General purpose db.m1.small/medium/large/xlarge ko th·ªÉ encrypt).\nc√≥ 1 trick ƒë·ªÉ encrypt db c√≥ s·∫µn l√†: Ban ƒë·∫ßu b·∫°n c√≥ 1 db ch∆∞a encrypt, t·∫°o b·∫£n snapshot c·ªßa n√≥, encrypt b·∫£n snapshot ƒë√≥, restore l·∫°i t·ª´ snapshot ra DB instance -\u0026gt; v·∫≠y l√† ƒë√£ c√≥ DB instance dc encrypt.\nDB ƒë√£ b·∫≠t encryption th√¨ ko th·ªÉ disable encryption dc.\nMu·ªën encrypt read replicas th√¨ ph·∫£i d√πng c√πng 1 key v·ªõi DB instance.\nRDS encryption c≈©ng not available cho nh·ªØng DB instance ch·∫°y SQL Server express.\n Q. DynamoDB c√≥ HA ko?\n c√≥, t·ª± ƒë·ªông replicated in multi AZ. N·∫øu mu·ªën ·ªü level region th√¨ b·∫≠t t√≠nh nƒÉng global table.\n Q. 1 DB host tr√™n aws ƒëang b·ªã v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng write v√† ko th·ªÉ handle load. Sao ƒë·ªÉ ch·∫Øc ch·∫Øn s·ªë l∆∞·ª£ng write ko b·ªã m·∫•t trong b·∫•t c·ª© tr∆∞·ªùng h·ª£p n√†o?\n D√πng SQS FIFO ƒë·ªÉ query db writes,\nHo·∫∑c add th√™m IOPS v√†o existing volume d√πng cho DB (Tuy nhi√™n IOPS c√≥ gi·ªõi h·∫°n 40k th√¥i, trong khi SQS c√≥ th·ªÉ handle 120k message in flight)\n Q. B·∫°n ƒëang host data tr√™n Aurora. Mu·ªën nh·ªØng data n√†y available in another region in case disaster th√¨ l√†m n√†o?\n Create read replica of Aurora in cross regions.\nC·∫£ RDS v√† Aurora ƒë·ªÅu c√≥ th·ªÉ t·∫°o read replica cross regions\n Q. B·∫°n qu·∫£n l√Ω nh·ªØng user c·ªßa t·ªï ch·ª©c l·ªõn, h·ªç ƒëang move nhi·ªÅu service to AWS, b·∫°n mu·ªën c√°c user c√≥ th·ªÉ nhanh ch√≥ng login v√† s·ª≠ d·ª•ng cloud service, B·∫°n ƒë·ªãnh d√πng AWS Managed Microsoft AD, c·∫ßn l√†m g√¨?\n B·∫°n c·∫ßn setup VPN ho·∫∑c Direct Connect ƒë·ªÉ sau ƒë√≥ AWS MS AD c√≥ th·ªÉ s·ª≠ d·ª•ng dc cho c·∫£ Cloud v√† Onpremise\n Q. Cty b·∫°n d√πng Route53 as DNS Provider, C·∫ßn tr·ªè DNS c·ªßa cty t·ªõi CloudFront th√¨ l√†m n√†o?\n T·∫°o Alias record point t·ªõi CloudFront. Ch·ªçn Alias ch·ª© ko ch·ªçn CNAME record v√¨ Alias th√¨ c√≥ th·ªÉ t·∫°o cho c·∫£ root domain v√† sub domain, trong khi CNAME th√¨ ch·ªâ c√≥ th·ªÉ t·∫°o cho sub domain.\n Q. 1 cty ƒëang c√≥ c√°c app d√πng Docker containers. C·∫ßn move l√™n aws, setup nh·ªØng Docker containers ƒë√≥ tr√™n c√°c m√¥i tr∆∞·ªùng ri√™ng bi·ªát, l√†m n√†o?\n D√πng Elastic Beanstalk, ho·∫∑c D√πng ECS (Nh∆∞ng Beanstalk c√≥ ∆∞u ƒëi·ªÉm n·∫øu 1 Docker container ƒëang running b·ªã crash or killed, Beanstalk s·∫Ω t·ª± ƒë·ªông restart)\n Q. 1 cty c√≥ 1 workflow l√† send video t·ª´ on-premise sang AWS ƒë·ªÉ transcoding. H·ªç d√πng ec2 pull job from SQS. V√¨ sao d√πng SQS?\n ƒê·ªÉ gi√∫p vi·ªác scaling theo chi·ªÅu ngang c·ªßa c√°c task. ƒê√≥ l√† m·ª•c ƒë√≠ch ch√≠nh, ngo√†i ra th√¨ n√≥ SQS c≈©ng ƒë·∫£m b·∫£o th·ª© t·ª± message (SQS FIFO)\n Q. 1 cty mu·ªën extend onpremise to cloud. H·ªç mu·ªën communication gi·ªØa 2 m√¥i tr∆∞·ªùng possible over internet. L√†m n√†o?\n T·∫°o VPN connection gi·ªØa 2 onpremise v√† AWS\nKo th·ªÉ d√πng AWS Direct connect v√¨ Direct connect ko d√πng internet\n Q. 1 Cloudfront d√πng ƒë·ªÉ distribute content from S3. Y√™u c·∫ßu l√† ch·ªâ cho 1 s·ªë user tr·∫£ ph√≠ v√†o xem n·ªôi dung content th√¥i, l√†m n√†o?\n T·∫°o Cloudfront signed URLs v√† distribute n√≥ cho User\n Q. B·∫°n c√≥ 1 cty nh·ªè, ch·ªâ c·∫©n chuy·ªÉn resource to cloud nh∆∞ AWS workspace, AWS Workmail. C·∫ßn solution ƒë·ªÉ user management v√† set policy. D√πng AWS Directory Service n√†o?\n Simple AD l√† ƒë·ªß (support c·∫£ WIndow workload v√† Linux workload)\nKO c·∫ßn d√πng AD Connector v√¨ b·∫°n ko c√≥ on premise app.\nKo c·∫ßn Cognito, AWS Managed Microsoft AD v√¨ 2 c√°i ·∫•y nhi·ªÅu h∆°n nh·ªØng g√¨ b·∫°n c·∫ßn.\n Q. C√°c data trong S3 mu·ªën avalablity in another regions l√†m n√†o?\n Enable cross region cho S3 bucket\n Q. 1 Cty ƒëang c√≥ y√™u c·∫ßu 1 model Stack-based cho resource tr√™n AWS. C·∫ßn c√≥ nh·ªØng stack kh√°c nhau cho Dev v√† Prod. D√πng g√¨?\n AWS Opswork. N√≥ s·∫Ω gi√∫p b·∫°n manage app tr√™n AWS v√† Onpremise. T·∫°o c√°c stack bao g·ªìm c√°c Layer (layer LB, DB, App).\n1 Stack basic cho web server th√¨ g·ªìm: 1 set instance cho application server (qu·∫£n l√Ω incoming traffic), 1 LB instance (distribute traffic to apllication server), 1 DB instance (backend)\n1 Common practice l√† nhi·ªÅu stack cho c√°c m√¥i tr∆∞·ªùng: Dev stack ƒë·ªÉ develop, add feature, fix bug\u0026hellip;Staging stack ƒë·ªÉ verify update, hotfix. Production stack ƒë·ªÉ publish v√† handle request of customer.\n Q. B·∫°n c√≥ 1 app ƒëang deploy tr√™n 2 Az, d√πng 1 LB v√† ASG. Mu·ªën l√∫c n√†o c≈©ng available 100% n·∫øu 1 Az godown th√¨ n√™n l√†m n√†o?\n Deploy tr√™n 3 Az v·ªõi ASG minimum set 50% load per zone. -\u0026gt; N·∫øu set l√† 33% m·ªói zone th√¨ khi 1 Az go down th√¨ max ch·ªâ c√≥ 66% load dc handle, c√≤n 34% ko handle dc (ko th·ªÉ ƒë·ªß 100%)\n Q. 1 app ƒëang host tr√™n ec2 v√† ƒëang ch·ªçn EBS type, data tr√™n volume n√†y dc access kho·∫£ng 1 tu·∫ßn r·ªìi sau ƒë√≥ dc move to infrequent access storage. D√πng lo·∫°i EBS n√†o ƒë·ªÉ cost efficiency?\n Ch·ªâ access kho·∫£ng 1 tu·∫ßn -\u0026gt; ko access th∆∞·ªùng xuy√™n -\u0026gt; EBS Cold HDD\n Q. 1 KH mu·ªën import virtual machine c√≥ s·∫µn l√™n cloud. D√πng c√°i g√¨?\n VM import/export\nKh√¥ng d√πng AWS Import/Export v√¨ ƒë·∫•y l√† service ƒë·ªÉ transport data to Cloud.\nKh√¥ng d√πng AWS Storage Gateway v√¨ ƒë·∫•y l√† service connect an on-premise to cloud-based, cung c·∫•p access v√†o s3 object nh∆∞ l√† file.\nKh√¥ng d√πng DB Migration Service v√¨ ƒë·∫•y l√† service ƒë·ªÉ migrate data t·ª´ ho·∫∑c ƒë·∫øn c√°c c·ªü s·ªü DB th∆∞∆°ng m·∫°i ho·∫∑c open.\n Q. B·∫°n c√≥ 1 app m√† application user s·∫Ω initiate TCP request to server, server respone c√°c request ƒë√≥. H·ªá th·ªëng g·ªìm c√°c VPC ·ªü c√°c Az kh√°c nhau trong 1 region. Gi·ªù c·∫ßn thi·∫øt l·∫≠p connection gi·ªØa c√°c VPC ƒë·ªÉ c√≥ 1 solution high scalable, and secure. D√πng c√°i g√¨?\n AWS PrivateLink v√† Network LB.\nKo d√πng IGW v√¨ ko secure.\nKo d√πng VPC peering v√¨ n√≥ s·∫Ω cho all resource c·ªßa VPC n√†y access v√†o resource VPC kh√°c, trong khi app m√¨nh ch·ªâ c·∫ßn user initiate request t·ªõi server th√¥i.\nKo d√πng VPN v√¨ ko th·ªÉ scalable.\n Q. B·∫°n c·∫ßn host static web tr√™n aws. N√™n d√πng c√°i g√¨? C√≥ n√™n d√πng RDS v√† EC2 ƒë·ªÉ store/host data ko?\n S3 v√† DynamoDB\n Q. B·∫°n ƒëang l∆∞u data tr√™n s3, gi·ªù c·∫ßn all data ph·∫£i encrypt at rest. L√†m sao?\n Enable serverside encryption on s3.\n Q. 1 KH c√≥ 3TB data on-premise, tƒÉng 500GB 1 nƒÉm. Kh√°ch ƒëang ng√†y c√†ng b·ªã h·∫°n ch·∫ø b·ªüi dung l∆∞·ª£ng data local tƒÉng l√™n. Mu·ªën backup 1 ph·∫ßn data nh∆∞ng v·∫´n duy tr√¨ ƒë·ªô tr·ªÖ th·∫•p v·ªõi nh·ªØng data access th∆∞·ªùng xuy√™n. N√™n d√πng AwS storage Gateway n√†o?\n Gateway-cached volume + schedule snapshot to s3 (l∆∞u data tr√™n s3, 1 c·ª•c access th∆∞·ªùng xuy√™n th√¨ ƒë·ªÉ local)\nko n√™n d√πng Gateway storage volume v√¨ lo·∫°i n√†y l∆∞u to√†n b·ªô data locally (trong khi KH ng√†y c√†ng b·ªã h·∫°n ch·∫ø b·ªüi dung l∆∞·ª£ng data local)\n Q. C·∫ßn deploy app to aws. C·∫ßn monitor web app log ƒë·ªÉ ph√°t hi·ªán nguy hi·ªÉm. D√πng g√¨?\n Cloudwatch log v√† Cloudtrail\nCloudtrail m·ª•c ƒë√≠ch ch√≠nh l√† : who did what on AWS? (trace log c·ªßa aws account activity tr√™n console, cli, sdk)\nCloudwatch m·ª•c ƒë√≠ch ch√≠nh l√†: what happening on AWS? by logging all event c·ªßa service or app\n Q. Y√™u c·∫ßu nh·ªØng ng ch·ªãu tr√°ch nhi·ªám cho Development instances th√¨ ko dc quy·ªÅn access v√†o Production instance. L√†m n√†o?\n Define tag cho 2 Prod v√† Dev, add condition v√†o IAM Policy ch·ªâ cho ph√©p nh·ªØng tag c·ª• th·ªÉ\n\u0026mdash;P4\u0026mdash;\n Q. 1 cty host DB tr√™n RDS, c√≥ Read replica, c√°c b·∫£n report c·ªßa app dc gen tr√™n Read replica, Nh∆∞ng ƒë√¥i l√∫c report show data c≈©. V√¨ sao?\n Do replication lag, ƒë·ªô tr·ªÖ khi async gi·ªØa origin v·ªõi replica\n Q. B·∫°n setup Cloudtrail cho cty, y√™u c·∫ßu l√† log n√†y c·∫ßn dc encrypt, l√†m sao?\n Ch·∫≥ng c·∫ßn l√†m g√¨, v√¨ log c·ªßa Cloudtrail default dc encypted r·ªìi (n√≥ d√πng Server side encrypt SSE)\nC√≥ th·ªÉ d√πng KMS, ho·∫∑c store tr√™n S3\n Q. 1 cty d√πng S3 l√† t·∫ßng Data layer, c√≥ c√°c request ƒë·∫øn s3 ƒë·ªÉ read/write/update object, User ph·∫£n √°nh l√† sometime c√°c update ko dc ph·∫£n √°nh. V√¨ sao?\n V√¨ Update object s3 l√† Eventual consistency model (y·∫øu), c√≥ 1 kho·∫£ng time delay, c·ª© F5 l√† cu·ªëi c√πng s·∫Ω th·∫•y update th√¥i\n Q. B·∫°n b·∫≠t CORS cho S3 bucket r·ªìi, b·∫°n ƒë√£ nh·∫≠p 3 domains l√† origin ƒë·ªÉ allow r·ªìi. Nh∆∞ng khi test th·∫•y 2 method OPTION v√† CONNECT ko work, trong khi c√°c method kh√°c ƒë·ªÅu work, v√¨ sao?\n B·ªüi v√¨ CORS S3 bucket ch·ªâ support 5 method l√† GET POST PUT DELETE HEAD th√¥i\n Q. 1 cty c√≥ 1 contest cho user up ·∫£nh thi. Cu·ªôc thi d√†i 2 tu·∫ßn, ·∫£nh s·∫Ω d√πng ƒë·ªÉ ph√¢n t√≠ch trong 3 th√°ng. C√°c file s·∫Ω dc access 1 l·∫ßn r·ªìi x√≥a. D√πng C√°i g√¨ cho kinh t·∫ø, c√≥ th·ªÉ scale?\n S3 IA\n Q. 1 cty host nhi·ªÅu data tr√™n on premise, v√† gi·ªù mu·ªën backup this data l√™n AWS, l√†m n√†o?\n D√πng Storage Gateway Stored volumes (data v·∫´n ·ªü local v√† dc async to s3)\n Q. 1 cty mu·ªën move Postgres SQL to AWS, h·ªç mu·ªën c√≥ c√°c replicas v√† auto backup, D√πng c√°i n√†o? Aurora PostgresSQL hay RDS PostgresSQL?\n Aurora (c≈©ng ƒë√°p ·ª©ng dc nhu c·∫ßu nh∆∞ RDS nh∆∞ng Aurora c√≥ performance g·∫•p 3 l·∫ßn RDS)\n Q. User trong cty c·∫ßn n∆°i l∆∞u documents, m·ªói user c·∫ßn c√≥ 1 location ri√™ng, v√† c√°c user kh√°c ko dc view documents c·ªßa nhau. D·ªÖ dang retrive docs c·ªßa m√¨nh, l√†m n√†o?\n S3 (c√≥ th·ªÉ restrict user ƒë·∫øn t·ª´ng folder file tr√™n s3)\n Q. 1 cty d√πng Glacier ƒë·ªÉ archive data. Mu·ªën time ƒë·ªÉ retrieved data trong kho·∫£ng 3-5h th√¨ d√πng c√°i feature n√†o c·ªßa Glacier?\n Standard retrieval (3-5h)\nBulk retrieval (5-12h)\nExpedited retrieval (1-5min)\n Q. Mu·ªën repicate current resource AWS to another region l√†m n√†o?\n Cloudformation\n Q. B·∫°n c√≥ c√°c IIS Server ch·∫°y tr√™n EC2. Gi·ªù mu·ªën collect v√† process logs file dc gen t·ª´ EC2 ra, l√†m n√†o?\n Store logs tr√™n S3 v√† D√πng AWS EMR ƒë·ªÉ process log file\nko n√™n d√πng DynamoDB ƒë·ªÉ l∆∞u log files\n Q. B·∫°n mu·ªën d√πng AWS host website, www.example.com. C·∫ßn deploy quickly v√† ko c·∫ßn server-side script. C√°i g√¨ n√†o?\n ƒêƒÉng k√Ω 1 domain b·∫±ng Route53 v√† verify tr∆∞·ªõc r·∫±ng S3 bucketname match v·ªõi domain s·∫Ω ƒëƒÉng k√Ω tr√™n R53\n Q. B·∫°n l√† SA, qu·∫£n l√Ω 1 web server, d√πng Cloudtrail v√† store all cloud trail event log files v√†o S3 bucket. Auditor c·ªßa AWS n√≥i r·∫±ng ko ƒë·∫£m b·∫£o t√≠nh x√°c th·ª±c c·ªßa nh·ªØng file log l∆∞u tr√™n s3 bucket ƒë√≥, ko c√≥ g√¨ ƒë·∫£m b·∫£o nh·ªØng file Cloudtrail log ƒë·∫•y ko b·ªã ch·ªânh s·ª≠a. L√†m n√†o?\n Enable t√≠nh nƒÉng Cloudtrail log file integrity validation.\nKhi b·∫≠t l√™n th√¨ n√≥ s·∫Ω t·∫°o 1 hash file (digest file) cho m·ªói Cloudtrail log file dc sinh ra. N√≥ dc save ·ªü 1 folder kh√°c c√πng bucket. M·ªói file c√≥ 1 public key v√† private key. N√≥ ƒë·∫£m b·∫£o r·∫±ng t·∫•t c·∫£ c√°c nh·ªØng ch·ªânh s·ª≠a ƒë·ªëi v·ªõi Cloudtrail log file ƒë·ªÅu dc record l·∫°i.\nD√πng SSE (serverside-encryption) l√† sai v√¨ n√≥ l√† m·∫∑c ƒë·ªãnh c·ªßa cloudtrail log r·ªìi, nh∆∞ng ko ƒë·∫£m b·∫£o dc t√≠nh authenticity c·ªßa log files.\nKMS c≈©ng v·∫≠y, ko ƒë·∫£m b·∫£o dc t√≠nh authenticity (x√°c th·ª±c).\n Q. B·∫°n c·∫ßn ensure l√† data tr√™n s3 dc encryted nh∆∞ng ko mu·ªën qu·∫£n l√Ω encryption key. D√πng c√°i g√¨?\n SSE-S3, v√¨ S3 s·∫Ω qu·∫£n l√Ω c·∫£ data key v√† master key.\nko d√πng SSE-KMS v√¨ KMS th√¨ n√≥ qu·∫£n l√Ω data key b·∫°n ph·∫£i qu·∫£n l√Ω master key.\n Q. 1 Cty ƒëang c√≥ Redshift cluster tr√™n aws. C·∫ßn monitor performance v√† ensure ƒë·∫•y l√† performance hi·ªáu qu·∫£ nh·∫•t c√≥ th·ªÉ th√¨ d√πng g√¨?\n Cloudwatch v√† AWS trusted advisor\n Q. Web c·ªßa b·∫°n d√πng s3 v√† cloudfront ƒë·ªÉ cache. B·∫°n t·∫°o 1 cloudtrail ·ªü m·ªói region v√† c√°c event log file dc l∆∞u v√†o s3 bucket ·ªü us-west-1. C√≥ 1 thay ƒë·ªïi c·ªßa cloudfront l√†m all traffic route to origin, tƒÉng ƒë·ªô tr·ªÖ cho user. Ph√°t hi·ªán ra l√† v√¨ c√≥ nh·ªØng event log b·ªã doublicate ·ªü Cloudfront. L√†m sao?\n D√πng AWS CLI ƒë·ªÉ ƒë·ªÉ disable global service event ƒëang deliver log file to all region ngo·∫°i tr·ª´ us-west-1\nC√°i n√†y ko th·ªÉ d√πng AWS Console dc.\n Q. Cloudfront Query string forwarding c·∫ßn ch√∫ √Ω g√¨?\n Ch·ªâ support web distribution (ko support RMTP distribution - Real Time Message Protocol).\nPh√¢n c√°ch gi·ªØa c√°c query string parameter ph·∫£i l√† k√Ω t·ª± \u0026amp;.\nParam name v√† Param value l√† case-sensitive (ph√¢n bi·ªát hoa th∆∞·ªùng).\n Q. 1 IOT sensor ƒë·ªÉ ƒë·∫øm s·ªë bag ·ªü s√¢n bay. Data g·ª≠i t·ªõi kinesis v·ªõi default setting. M·ªói alternate-day (48h) data dc send to S3 ƒë·ªÉ processs. Nh·∫≠n ra l√† S3 ko nh·∫≠n all data t·ª´ kinesis stream, v√¨ sao?\n V√¨ kinesis default ch·ªâ l∆∞u data 24h (c√≥ th·ªÉ setting 168h)\n Q. C∆° ch·∫ø l√†m vi·ªác c·ªßa AWS CloudHSM l√† g√¨?\n Ephemeral backup key (EBK) d√πng ƒë·ªÉ encrpyt data, Persistent backup key (PBK) ƒë·ªÉ encrpyt EBK sau ƒë√≥ save to S3 in same region v·ªõi CloudHSM.\nD·ªãch v·ª• CloudHSM n√†y g·∫ßn gi·ªëng v·ªõi KMS nh∆∞ng d√πng kh√°i ni·ªám key kh√°c nhau th√¥i.\n Q. B·∫°n qu·∫£n l√Ω app tr√™n AWS. G·ªìm ec2, route53, CLB, ASG. C·∫ßn d√πng blue-green deployment th√¨ c·∫ßn d√πng policy n√†o c·ªßa R53?\n Weighted Policy\n Q. (from Airbus documents) AWS Direct Connect c√≥ encrypt data ko?\n ko. V√¨ n√≥ s·ª≠ d·ª•ng m·∫°ng ri√™ng v√† private ho√†n to√†n. Mu·ªën encrypt data in transit th√¨ d√πng th√™m VPN\n Q. B·∫°n dev 1 app d√πng AWS Polly (text to speech), b·∫°n mu·ªën test nh∆∞ sau,ch·ªØ S3 ƒë·∫ßu ti√™n th√¨ ƒë·ªçc l√† \u0026ldquo;Simple Storage Service\u0026rdquo;, c√≤n nh·ªØng ch·ªØ S3 sau ƒë√≥ th√¨ ƒë·ªçc l√† \u0026ldquo;S3\u0026rdquo; th√¥i. C√°i case n√†y c·∫ßn test ·ªü 2 region us-east v√† us-west. L√†m n√†o?\n D√πng nhi·ªÅu Lexicons, t·∫°o c√°c alias kh√°c nhau cho t·ª´ \u0026ldquo;S3\u0026rdquo;, v√† apply theo th·ª© t·ª± kh√°c nhau. R·ªìi upload Lexicons ƒë√≥ l√™n c·∫£ 2 regions us-east v√† us-west\n Q. AWS polly m√† b·∫°n d√πng cho web app c·ªßa b·∫°n b·ªã user ph·∫£n h·ªìi l√† ƒë·ªçc qu√° nhanh v√† li√™n t·ª•c. L√†m sao?\n Convert d·∫•u ph·∫©y trong content th√†nh period.\nAdd a pause b·∫±ng c√°ch s·ª≠ d·ª•ng \u0026lt;break\u0026gt; tag gi·ªØa c√°c ƒëo·∫°n ph√π h·ª£p.\nAdd \u0026lt;emphasis\u0026gt; tag v·ªõi value = Strong v·ªõi t·ª´ ph√π h·ª£p ƒë·ªÉ nh·∫•n m·∫°nh.\n Q. B·∫°n c√≥ 1 web server cung c·∫•p docs l∆∞u s3, c√≥ d√πng Cloudfront, nh∆∞ng ko mu·ªën user c√≥ th·ªÉ access tr·ª±c ti·∫øp v√†o s3. L√†m n√†o?\n T·∫°o OAI (origin access identity) cho Cloudfront v√† grant access v√†o obbject trong s3 cho CFront.\nL√†m ƒëi·ªÅu n√†y th√¨ User ch·ªâ c√≥ th·ªÉ access v√†o s3 object b·∫±ng c√°ch ƒëi t·ª´ Cloudfront, ch·ª© ko th·ªÉ ƒëi tr·ª±c ti·∫øp ƒë·∫øn s3.\nB·∫°n t·∫°o Cfront signned URL/cookie ƒë·ªÉ access v√†o S3 bucket, sau ƒë√≥ t·∫°o 1 Cloudfront User ƒë·∫∑c bi·ªát g·ªçi l√† OAI. R·ªìi config ƒë·ªÉ Cfront ƒë·ªÉ n√≥ d√πng OAI access v√† serve file tr√™n S3 (tr√™n S3 bucket c·∫ßn config permission ƒë·ªÉ ch·ªâ OAI c√≥ quy·ªÅn access v√†o c√°c object). Nh·ªØng user th√¥ng th∆∞·ªùng s·∫Ω ko th·ªÉ s·ª≠ d·ª•ng S3 URL ƒë·ªÉ access S3 n·ªØa.\n Q. App c·ªßa b·∫°n ƒëang l√†m Load test. DB b·∫°n d√πng RDS MySQL. App ko ph·∫£n h·ªìi khi CPU 100%. App th√¨ l√† lo·∫°i read-heavy. L√†m n√†o?\n Add Read replicas.\nAdd Elasticache to RDS.\nShard your data set among nhi·ªÅu RDS DB instances (Sharding l√† 1 concept split data th√†nh nhi·ªÅu table trong 1 database).\nKo th·ªÉ d√πng ASG v·ªõi RDS dc.\n\u0026mdash;P5\u0026mdash;\n Q. Mu·ªën m·ªói file up l√™n s3 ph·∫£i dc l∆∞u file name v√†o DynamodB l√†m n√†o?\n S3 trigger tr·ª±c ti·∫øp event cho Lambda. (ko c·∫ßn qua Cloudwatch)\n Q. 1 app c·∫ßn DB tr√™n aws. Read/write on DB ch·ªâ c·∫ßn s·ªë nh·ªè th√¥i. Th√¨ n√™n ch·ªçn c√°i n√†o? EBS General Purpose SSD hay EBS Throughput Optimized HDD?\n d√πng EBS General Purpose SSD.\nEBS Throughput Optimized HDD (max throughput/volume=500M/s)\nEBS General Purpose SSD (max throughput/volume=250M/s)\n Q. B·∫°n c√≥ 1 web d√πng Network LB save log v√†o S3. User ph√†n n√†n v·ªÅ delay? L√†m n√†o ƒë·ªÉ t√¨m nh·ªØng Client IP c√≥ th·ªùi gian delay khi access (TLS handshake time)?\n AWS Athena.\nko c·∫ßn d√πng Quicksight v√¨ ƒë·∫•y ch·ªâ c·∫ßn trong TH mu·ªën visualization/t·∫°o report th√¥i.\n Q. 1 cty upload 1 l∆∞·ª£ng l·ªõn data to S3 v√† li√™n t·ª•c t·ª´ nhi·ªÅu n∆°i tr√™n TG. D√πng Athena ƒë·ªÉ query data ƒë√≥ v√† Quicksight ƒë·ªÉ t·∫°o report daily. Nh∆∞ng thi tho·∫£ng b·ªã l·ªói exception t·ª´ s3. Cost c≈©ng m·∫•t nhi·ªÅu ti·ªÅn, L√†m sao ƒë√¢y?\n Partition data based on date v√† location.\nT·∫°o 1 workgroup ri√™ng base on user group.\nV√¨ gi√° ti·ªÅn Athena base tr√™n s·ªë query v√† l∆∞·ª£ng data scan dc m·ªói query. N√™n c·∫ßn chia data ra m√† query.\n Q. 1 Cty c·∫ßn store 10TB file ƒë√£ scan. Y√™u c·∫ßu 1 search application ƒë·ªÉ search c√°c file ƒë√≥, l√†m n√†o?\n Store ·ªü S3 standard redundancy, v√† d√πng CloudSearch ƒë·ªÉ query, Beanstalk ƒë·ªÉ host webssite multiAZ.\nAmazon CloudSearch: B·∫°n t·∫°o 1 domain v√† upload data l√™n ƒë√≥ ƒë·ªÉ search.\n Q. 1 Cty d√πng AWS Glue v√† Athena ƒë·ªÉ analyze data, c√≥ upload data 1 l∆∞·ª£ng l·ªõn l√™n S3 daily v√† li√™n t·ª•c. ƒê·ªÉ gi·∫£m th·ªùi gian scan data th√¨ c·∫ßn ensure l√† ch·ªâ scan c√°c changes in dataset th√¥i. L√†m sao?\n Enable Job Bookmark in AWS Glue.\nN·∫øu Reset Job Bookmark s·∫Ω reprocess all data.\nN·∫øu Disable Job Bookmark s·∫Ω process all data each time.\nN·∫øu Pause Job Bookmark th√¨ m·ªói l·∫ßn scan n√≥ s·∫Ω scan t·ª´ l·∫ßn cu·ªëi Bookmark dc enable.\n Q. 1 Cty s·ª£ dev c√≥ th·ªÉ x√≥a Ec2 resource trong m√¥i tr∆∞·ªùng Production, l√†m sao?\n Tag production resource v√† deny quy·ªÅn x√≥a resource c√≥ tag Prod.\nT·∫°o ri√™ng 1 AWS Account v√† move developers v√†o account ƒë√≥.\n Q. B·∫°n ƒëang D√πng AWS X-ray ƒë·ªÉ tracing, setting g√¨ ƒë·ªÉ cost in limit?\n Sampling at low rate\n Q. 1 cty lo ng·∫°i v·ªÅ t√≠nh redundancy c·ªßa EBS Volume m√† h·ªç d√πng, l√†m n√†o? redundancy: l√† ph∆∞∆°ng ph√°p l∆∞u 1 th√¥ng tin ·ªü nhi·ªÅu ·ªï, khi c√≥ s·ª± c·ªë th√¨ c√≥ th·ªÉ nhanh ch√≥ng l·∫•y l·∫°i\n Default l√† EBS ƒë√£ dc replicated trong Az r·ªìi ƒë·ªÉ c√≥ HA v√† Durability (n√™n ko c·∫ßn lo ng·∫°i v·ªÅ t√≠nh redundancy)\n Q. Mu·ªën x√¢y d·ª±ng h·ªá th·ªëng decoubled n√™n d√πng service n√†o?\n SQS v√† SNS.\nko ph·∫£i ELB hay ASG.\n Q. 1 Cty d√πng AWS WorkDocs ƒë·ªÉ share document vs third party. B·ªã leak th√¥ng tin nh·∫°y c·∫£m n√™n gi·ªù team security c·∫ßn ph√¢n quy·ªÅn l·∫°i nh∆∞ n√†o?\n ch·ªâ cho quy·ªÅn cho Power users c√≥ kh·∫£ nƒÉng invite ng m·ªõi.\nCh·ªâ cho Power user c√≥ kh·∫£ nƒÉng share publicly.\n Q. B·∫°n ƒëang d√πng SQS, ƒë√¥i khi b·ªã l·ªói client get price tr∆∞·ªõc khi login v√†o site, n√™n b·∫°n chuy·ªÉn sang d√πng SQS FIFO, c·∫ßn l√†m tr∆∞·ªõc g√¨ ƒë·ªÉ vi·ªác migrate sang SQS FIFO d·ªÖ d√†ng?\n M·ªói FIFO Queue c·∫ßn c√≥ Message group ID b·∫•t k·ªÉ khi n√†o.\nV·ªõi nh·ªØng message bodies m√† \u0026ldquo;gi·ªëng h·ªát nhau\u0026rdquo; (identical): h√£y d√πng unique deduplication ID\nV·ªõi nh·ªØng message bodies m√† \u0026ldquo;unique\u0026rdquo;: h√£y d√πng content-based deduplication ID. (khi ƒë√≥ SQS d√πng SHA-256 generate ra deduplication ID)\n Q. 1 cty mu·ªën d√πng 1 ph∆∞∆°ng th·ª©c deployment t·ª± ƒë·ªông nh∆∞ t·∫°o LAMP stack, download latest PHP t·ª´ s3, setup ELB. D√πng c√°i g√¨? Beanstalk hay Cloudformation?\n Beanstalk. V√¨ Beanstalk ƒë·ªÉ deploy app c√≤n CF ƒë·ªÉ t·∫°o resource\n Q. B·∫°n D√πng EMR ƒë·ªÉ run big data framwork, mu·ªën gi·∫£m cost th√¨ l√†m g√¨?\n D√πng Spot instance for underlying node\n Q. 1 application cho traders in market, c√≥ th·ªÉ c√≥ multiple trading c·ªßa client. B·∫°n c√≥ nhi·ªÅu ec2 ƒë·ªÉ process c√°c trading n√†y song song. V√† m·ªói trade ph·∫£i stateful v√† x·ª≠ l√Ω ƒë·ªôc l·∫≠p. D√πng SQS v·ªõi setting g√¨?\n SQS FIFO Queue s·ª≠ d·ª•ng Message Group ID\nMessage Group ID: ch·ªâ ƒë·ªãnh cho 1 group c√°c messages, nh·ªØng msg thu·ªôc c√πng group s·∫Ω lu√¥n process l·∫ßn l∆∞·ª£t. ƒê·ªÉ xen k·∫Ω c√°c message group trong 1 single FIFO queue (V√≠ d·ª• session data c·ªßa nhi·ªÅu user), trong scenario ƒë√≥ nhi·ªÅu user c√≥ th·ªÉ process queue nh∆∞ng session data c·ªßa m·ªói user v·∫´n dc process theo FIFO manner.\nReceive Request Attempt ID: d√πng ƒë·ªÉ retry same request nh·∫≠n ƒë∆∞·ª£c trong TH SDK th·ª±c hi·ªán b·ªã l·ªói\nDeduplication ID: N·∫øu 1 msg dc g·ª≠i th√†nh c√¥ng, c√°c msg sau v·ªõi c√πng Deduplication ID s·∫Ω ƒëc nh·∫≠n nh∆∞ng ko dc deliver trong v√≤ng 5 minutes.\n Q. Ph√≤ng IT y√™u c√¢u all IP traffic in out c·ªßa 1 EC2 c·∫ßn monitor, d√πng c√°i g√¨?\n VPC Flow log. S·∫Ω monitor all IP traffic ƒë·∫øn v√† ra kh·ªèi ENI c·ªßa VPC b·∫°n.\nko d√πng CloudTrail v√¨ n√≥ ƒë·ªÉ: ai made API call? khi n√†o, call c√°i g√¨?\n B·∫°n c√≥ RDS PostgreSQL. B·∫°n c·∫ßn backup DB v√† dc asynchronous copy. D√πng c√°i g√¨?  Enable Read Replicas for DB. Khi ƒë√≥ b·∫°n ph·∫£i ch·ªâ ƒë·ªãnh DB, r·ªìi AWS s·∫Ω t·∫°o snapshot c·ªßa DB instance, r·ªìi t·∫°o read-only instance t·ª´ snapshot. R·ªìi n√≥ d√πng Async ƒë·ªÉ update Replicas.\nKo d√πng Enable Multi-AZ v√¨ n√≥ s·∫Ω d√πng synchronous ch·ª© ko ph·∫£i asynchronous.\n Q. B·∫°n d√πng AWS Batch Job, data l∆∞u trong S3, trigger Lambda, submit AWS Batch Job in a queue. Queue s·ª≠ d·ª•ng EC2 v√† ECS ƒë·ªÉ compute. Job c·ªßa b·∫°n b·ªã stuck ·ªü tr·∫°ng th√°i RUNNABLE, V√¨ sao?\n C√°c kh·∫£ nƒÉng c√≥ th·ªÉ x·∫£y ra v·ªõi vi·ªác stuck ·ªü RUNNABLE:\nawslogs log driver ch∆∞a dc configure trong EC2/ECS.\ninsufficient resources: job c·ªßa b·∫°n define 4GB memory nh∆∞ng ec2 ko c√≥ ƒë·ªß.\nno internet access v·ªõi EC2/ECS.\ns·ªë l∆∞·ª£ng EC2 ƒë·∫øn limit.\nC√°c tr·∫°ng th√°i (state) c·ªßa Job:\nSUBMIITED: 1 job v·ª´a dc submit v√†o queue, ch∆∞a dc evaluated,\nPENDING: job c√°c ch·ªù c√°c dependencies v·ªõi job kh√°c ho·∫∑c resource kh√°c,\nRUNNABLE: job c√≥ th·ªÉ ch·∫°y nh∆∞ng maybe stuck v√¨ nh·ªØng l√Ω do tr√™n,\nSTARTING: container image dc pull v·ªÅ v√† running,\nRUNNING: job ƒëang ch·∫°y,\nSUCCEEDED: job exit with code 0,\nFAILED: job exit failed\n Q. C√≥ 1 l∆∞·ª£ng l·ªõn data onpremise c·∫ßn l√™n S3, D√ông g√¨ ƒë·ªÉ transfer?\n Direct Connect, Snowball\n Q. B·∫°n v·ª´a t·∫°o 1 Redshift cluster, ƒëang mu·ªën d√πng SQL client from EC2 connect ƒë·∫øn Redshift cluster nh∆∞ng ko dc. L√†m n√†o?\n S·ª≠a VPC SG (Security Group)\n Q. 1 cty c·∫ßn block level storage l∆∞u 800GB data. C·∫ßn encrypt data, n√™n d√πng c√°i g√¨? EBS? Glacier? EFS? S3?\n EBS\nGlacier v√† S3: object level storage\nEFS: file level storage\n Q. B·∫°n d√πng AWS Batch Job. 1 s·ªë job th√¨ c·∫ßn l√†m nhanh v√¨ quan tr·ªçng, 1 s·ªë job th√¨ ko quan tr·ªçng l·∫Øm. N√™n setting Job queue cho ƒë·ª° t·ªën ti·ªÅn nh∆∞ n√†o?\n Nhi·ªÅu jobs, 1 job th√¨ c√≥ EC2 ondemand instance, high priority, 1 job th√¨ dung Spot instance v·ªõi low priority\n Q. EC2 b·ªã restart li√™n t·ª•c, gi·ªù c·∫ßn check log v√† analyze system log. D√πng c√°i g√¨?\n AWS Cloudwatch Logs\nko d√πng Cloudtrail dc.\n Q. B·∫°n d√πng RDS MySQL l√†m DB layer, gi·ªù mu·ªën tƒÉng connection t·ªõi DB th√¨ l√†m n√†o?\n t·∫°o 1 DB parameter group, s·ª≠a c√°i max connection theo √Ω m√¨nh, r·ªìi attach c√°i DB parameter m·ªõi ƒë√≥ v√†o DB instance\nDB parameter group default th√¨ ko th·ªÉ s·ª≠a dc, ph·∫£i t·∫°o c√°i m·ªõi, r·ªìi s·ª≠a, r·ªìi attach v√†o DB\n Q. B·∫°n c√≥ 1 Redis cluster deploy in VPC ·ªü us-east-1. C·∫ßn secure c√°c access to Redis t·ª´ EC2 ·ªü 1 VPC kh√°c c√πng region th√¨ l√†m n√†o?\n Enable Redis AUTH with in-transit encryption cho Cluster\nVPC peering\nKh√¥ng n√™n d√πng VPC transit v√¨ ƒë√≥ l√† gi·∫£i ph√°p cho TH 2 VPC ·ªü kh√°c region.\nKh√¥ng d√πng VPN Connection v√¨ c√°i ƒë√≥ l√† gi·∫£i ph√°p cho on-premise Server.\n Q. B·∫°n d√πng Cloudfront cho 1 web distribute media content. Dc user truy c·∫≠p th∆∞·ªùng xuy√™n. Gi·ªù mu·ªën tƒÉng performance l√†m n√†o?\n TƒÉng th·ªùi gian cache expiration l√™n.\n Q. Disaster Recovery solution m√† cost ph·∫£i minimum th√¨ d√πng DR mechanism n√†o? Backup and Restore, Pilot Light, Warm standby, Multi-Site ?\n Backup \u0026amp; Restore\n Q. 1 cty c√≥ 1 s·ªë EC2 sau 1 ELB, gi·ªù mu·ªën dc notification m·ªói khi latency v∆∞·ª£t qu√° 10s. L√†m n√†o?\n Enable log on ELB v·ªõi Latency alarm s·∫Ω send mail v√† ph√¢n t√≠ch log b·∫•t c·ª© khi n√†o c√≥ issue\n Q. 1 cty mu·ªën integrate data on premise l√™n AWS storage. Y√™u c·∫ßu l√† all data v·∫´n ph·∫£i keep low latency. N√™n l√†m n√†o?\n Storage Gateway Stored Volume (all data v·∫´n ·ªü local, r·ªìi async backup snapshot l√™n s3)\nKo d√πng Storage Gateway Cached Volume v√¨ y√™u c·∫ßu all data v·∫´n ph·∫£i low latency\n Q. 1 cty c√≥ 1 web infrastructure tr√™n aws, (EC2,ELB,RDS) mu·ªën h·ªá th·ªëng dc self-healing th√¨ c·∫ßn g√¨?\n self-healing = HA\nB·∫≠t Multi-AZ feature c·ªßa RDS l√™n.\nD√πng Cloudwatch metric check utilization c·ªßa web layer, r·ªìi theo ƒë√≥ m√† ƒëi·ªÅu ch·ªânh ASG\n Q. Nh·ªØng c√°i AWS Config c√≥ th·ªÉ l√†m?\n -Evaluate aws resource configuration\n-L·∫•y snapshot c·ªßa config hi·ªán t·∫°i c·ªßa c√°c resource nh·∫•t ƒë·ªãnh\n-Retrieve configuration c·ªßa resource -Retrieve history config c·ªßa resource\n-Nh·∫≠n noti khi resource dc t·∫°o, s·ª≠a, x√≥a\n-Xem m·ªëi quan h·ªá gi·ªØa c√°c resource\n\u0026mdash;P6\u0026mdash;\n Q. B·∫°n mu·ªën t·∫°o 1 instance m·ªõi ·ªü 1 Az kh√°c, EBS volume c·ªßa instance m·ªõi c·∫ßn l·∫•y t·ª´ volume c·ªßa 1 instance c≈©, l√†m n√†o?\n t·∫°o snapshot c·ªßa EBS c≈© r·ªìi t·∫°o volume m·ªõi t·ª´ snapshot ƒë√≥\nƒê·ªÉ c√≥ th·ªÉ t·∫°o 1 volume available trong 1 AZ m·ªõi th√¨ c·∫ßn t·∫°o snapshot c·ªßa c√°i c≈© r·ªìi t·ª´ snapshot ƒë√≥ t·∫°o volume m·ªõi.\nKo th·ªÉ detach volume c≈© r·ªìi g·∫Øn v√†o c√°i instance m·ªõi dc v√¨ Kh√°c AZ\n Q. 1 Cty mu·ªën move container-based app l√™n AWS. C·∫ßn 1 service m√† h·ªç ko ph·∫£i qu·∫£n l√Ω infra cho orchestration service. D√πng g√¨? ECS fargate hay Beanstalk?\n ECS Fargate, v√¨ Beanstalk ko cung c·∫•p service qu·∫£n l√Ω orchestration\n Q. 1 cty v·ª´a develop 1 app m·ªõi, h·ªç d√πng Codepipeline gi·ªù source stage th√¨ ch·ªçn serice n√†o? v√† deploy stage n√™n ch·ªçn service n√†o?\n source stage: CodeCommit\ndeploy stage: Beanstalk\ndeploy stage ko d√πng CodeDeploy v√¨ ko c√≥ exist infrastructure, m√† c√°c resource c·∫ßn dc deploy m·ªõi, v√¨ v·∫≠y d√πng Beanstalk\nCodeDeploy ch·ªâ deploy app l√™n 1 EC2/onpremise server ƒë√£ c√≥ s·∫µn (n√≥ ko provision ra c√°c resource m·ªõi)\n Q. 1 cty d√πng CodePipeline, source code l∆∞u S3, thay ƒë·ªïi tr√™n s3 s·∫Ω trigger CodepiPeline d√πng Beanstalk ƒë·ªÉ provision resource, C·∫ßn config g√¨ ƒë·ªÉ trigger CodePipeline nhanh h∆°n?\n ƒê·ªÉ trigger Pipeline auto khi c√≥ thay ƒë·ªïi S3 th√¨ c·∫ßn Disable periodic checks c·ªßa Pipeline\nv√† t·∫°o Cloudwatch event rule, t·∫°o Cloudtrail trail ƒë·ªÉ detect thay ƒë·ªïi.\nC∆° ch·∫ø nh∆∞ sau:\nM·ªói khi S3 c√≥ thay ƒë·ªïi, event dc filter trong Cloudtrail r·ªìi Cloudwatch event s·∫Ω trigger start Pipeline. Nh·ªõ l√† CodePipeline ph·∫£i disable c√°i Periodic check ƒë·ªÉ n√≥ d√πng event-based trigger\nC√≥ nhi·ªÅu c√°ch ƒë·ªÉ start 1 Pipeline c·ªßa AWS Codepipeline (b√™n tr√™n l√† khi d√πng S3 l√†m source):\nN·∫øu d√πng CodeCommit l√†m source: C·∫ßn d√πng Cloudwatch Event rule detect thay ƒë·ªïi, disable periodic check\nN·∫øu d√πng Github l√†m source: C·∫ßn d√πng Webhook ƒë·ªÉ detect thay ƒë·ªïi, c≈©ng disable periodic check\nN·∫øu d√πng ECR l√†m source: C·∫ßn d√πng Cloudwatch Event Rule ƒë·ªÉ detect thay ƒë·ªïi, periodic m·∫∑c ƒë·ªãnh ko th·ªÉ enable ƒë∆∞·ª£c.\n Q. 1 cty c√≥ c√°c ph√≤ng ban, m·ªói ph√≤ng ban 1 AWS account, c·∫ßn c√°ch ƒë·ªÉ security policy dc ƒë·∫£m b·∫£o theo level account, d√πng c√°i g√¨?\n AWS Organizations v√† Service control policies\nAWS Organizations gi√∫p qu·∫£n l√Ω nhi·ªÅu account 1 c√°ch centrally. Service control policies s·∫Ω cho ph√©p b·∫°n define nh·ªØng AWS API n√†o c√≥ th·ªÉ/ko th·ªÉ call b·ªüi IAM thu·ªôc Acount trong Organization. SCPs dc t·∫°o b·ªüi master account, l√† account d√πng ƒë·ªÉ t·∫°o ra Organization\n Q. B·∫°n mu·ªën reboot instance n·∫øu status check FAILED , l√†m n√†o?\n D√πng Cloudwatch alarm action\n Q. C√¥ng ty b·∫°n l∆∞u data tr√™n EFS. C·∫ßn ensure l√† all client access v√†o EFS ph·∫£i dc encrypt b·∫±ng TLS 1.2. C√°ch n√†o ƒë·ªÉ seccure data in transit khi access v√†o EFS?\n D√πng EFS mount helper ƒë·ªÉ encrypt data in transit (c√°ch n√†y ti·ªán v√† d·ªÖ h∆°n c√°ch 2)\nD√πng EFS m√† mu·ªën encrypt data in transit b·∫±ng TLS th√¨ c√≥ 2 c√°ch:\nc√°ch 1 l√† Mount EFS v·ªõi mount helper\nsudo mount -t efs -o tls fs-12345678:/ /mnt/efs\nc√°ch 2 l√† ko d√πng mount helper, th√¨ s·∫Ω ph·∫£i download v√† install stunnel, run stunnel on port 2049, d√πng NFS mount localport v√†o port c·ªßa application.\n Q. Khi t·∫°o VPC Peering c·∫ßn ch√∫ √Ω g√¨?\n C√°c VPC ko b·ªã overlapping CIDR block\nKo c√≥ y√™u c·∫ßu g√¨ v·ªÅ onpremise communication\n Q. B·∫°n c·∫ßn t·∫°o connection site-to-site VPN t·ª´ on premise network l√™n 1 AWS VPC. C·∫ßn ch√∫ √Ω g√¨?\n Virtual private gateway attach v√†o VPC\nTrong Customer Gateway c√≥ public IP c·ªßa on-premise network\n Q. C√¥ng ty b·∫°n mu·ªën encrypt data at rest v√† mu·ªën to√†n quy·ªÅn qu·∫£n l√Ω key v√† lifecycle c·ªßa key, th√¨ d√πng service g√¨?\n CloudHSM, b·∫°n t·ª± gen ra encryption key v√† qu·∫£n l√Ω key ƒë√≥\nko d√πng SSE S3, KMS v√¨ nh·ªØng c√°i ƒë√≥ ko cho b·∫°n to√†n quy·ªÉn qu·∫£n l√Ω key (SSE S3 th√¨ n√≥ full managed key, KMS th√¨ n√≥ manage data key v√† lifecycle c·ªßa key c√≤n b·∫°n manage master key)\n Q. B·∫°n c√≥ c√°c b·∫£n snapshot c·ªßa EBS volume, n√™n gi·ªØ nh·ªØng b·∫£n n√†o? original snapshot v√† latest snapshot hay c·∫£ 2?\n Ch·ªâ c·∫ßn gi·ªØ 1 latest snapshot v√¨ n√≥ v·ª´a l√† incremental v√† complete snapshot r·ªìi\n Q. B·∫°n ƒëang c√≥ 1 EC2 m1.small 300Gb EBS General purpose volume host 1 relational DB. Mu·ªën tƒÉng throughput c·ªßa DB th√¨ l√†m n√†o?\n D√πng lo·∫°i volume Provisioned IOPS volumes ho·∫∑c D√πng 1 EC2 l·ªõn h∆°n m1.small\n Q. Cty b·∫°n d√πng RDS instances, b·∫°n c·∫ßn disable automate backup ƒë·ªÉ save cost. N·∫øu disable th√¨ c·∫ßn ch√∫ √Ω g√¨?\n B·∫°n ƒëang disable t√≠nh nƒÉng point-in-time recovery\nRDS default l√† t·ª± ƒë·ªông t·∫°o b·∫£n storage volume snapshot c·ªßa to√†n b·ªô DB instance 1 ng√†y 1 l·∫ßn. Retention c≈©ng l√† 1 ng√†y. (c√≥ th·ªÉ s·ª≠a th√†nh 0-35 days)\nN·∫øu disable t√≠nh nƒÉng t·ª± ƒë·ªông n√†y th√¨: Khi disable r·ªìi re-enable l·∫°i, b·∫°n ch·ªâ c√≥ th·ªÉ restore t·ª´ th·ªùi ƒëi·ªÉm re-enable t√≠nh nƒÉng th√¥i, ko th·ªÉ restore th·ªùi ƒëi·ªÉm tr∆∞·ªõc khi disable n·ªØa\n Q. 1 cty c√≥ 1 web app. C·∫ßn user across world access v√†o site v√†o xem c√°c pages c·ªßa site v·ªõi ƒë·ªô tr·ªÖ th·∫•p nh·∫•t, d√πng g√¨? Route53 v·ªõi latency routing hay Cloudfront?\n ƒë·∫∑t Cloudfront l√†m distribution trc web app\nko d√πng R53 v·ªõi latency-based routing v√¨ c√°i ƒë·∫•y d√πng cho multi site v√† latency based s·∫Ω routing gi·ªØa c√°c sites.\nko hi·ªÉu gi·∫£i th√≠ch c·ªßa c√¢u n√†y\n Q. 1 Cty c√≥ 1 web app, d√πng Route53 ƒë·ªÉ qu·∫£n l√Ω DNS. Configure R53 th·∫ø n√†o ƒë·ªÉ ensure l√† custom domain point to LB?\n Ensure l√† hosted zone dc t·∫°o\nT·∫°o 1 alias record for a CNAME record to Load balancer DNS Name\n Q. 1 Cty ƒëang c√≥ 1 workflow l√† g·ª≠i c√°c video t·ª´ on premise l√™n aws ƒë·ªÉ ec2 th·ª±c hi·ªán transcoding b·∫±ng c√°ch pull from SQS. SQS dc s·ª≠ d·ª•ng ƒë·ªÉ c√≥ vai tr√≤ g√¨ trong h·ªá th·ªëng ƒë√≥? ƒê·∫£m b·∫£o th·ª© t·ª± c·ªßa msg √†? hay ƒë·ªÉ scaling c√°c task encoding theo chi·ªÅu ngang?\n ƒë·ªÉ scaling c√°c task encoding theo chi·ªÅu ngang\nv√¨ vi·ªác ƒë·∫£m b·∫£o order ko ƒë·ªÅ c·∫≠p ƒë·∫øn trong c√¢u h·ªèi\n Q. 1 Cty c·∫ßn deploy 1 app l√™n 1 s·ªë ec2, y√™u c·∫ßu low latency gi·ªØa c√°c ec2 n√†y? l√†m n√†o?\n Cluster Placement Group\n Q. B·∫°n design 1 web app host tr√™n ec2 sau 1 ELB. C·∫ßn consider c√°i g√¨ ƒë·ªÉ x√¢y d·ª±ng web ƒë√≥?\n C·∫ßn x√°c ƒë·ªãnh I/O operations\nX√°c ƒë·ªãnh minimum memory c·∫ßn cho app\nCh·ª© c√≤n peak usage cho client th√¨ ko c·∫ßn v√¨ ELB s·∫Ω take care c√°i ƒë√≥\n Q. Network Load balancer c√≥ support custom security policies ko? default security policy c·ªßa n√≥ l√† g√¨?\n NLB ko support custom security policies\ndefault security policy c·ªßa NLB l√† k·∫øt h·ª£p Protocols \u0026amp; Ciphers\n Q. B·∫°n l√† AWS consultant cho shop online. 2 tier web app, web server tr√™n AWS, data th√¨ on premise data centre. D√ông NLB, all traffic gi·ªØa client v√† server dc encrypted. ƒê·ªÉ gi·∫£m load v√†o backend server, h·ªç t√¨m 1 solution ƒë·ªÉ terminate TLS connection c·ªßa NLB. H·ªç mu·ªën 1 solution ƒë·ªÉ qu·∫£n l√Ω certificate s·ª≠ d·ª•ng khi terminate TLS connection c·ªßa NLB.\n D√πng 1 single certificate cho m·ªói TLS listener (cung c·∫•p b·ªüi AWS Certificate Manager)\nDoc c·ªßa AWS vi·∫øt, B·∫°n c·∫ßn ch·ªâ ƒë·ªãnh ch√≠nh x√°c 1 server certificate cho m·ªói TLS Listener. LB s·∫Ω d√πng cert ƒë√≥ d·ªÉ terminate connection v√† decrypt request c·ªßa client tr∆∞·ªõc khi g·ª≠i request ƒë√≥ ƒëi.\nELB s·ª≠ d·ª•ng security policy l√† \u0026ldquo;TLS\u0026rdquo; ƒë·ªÉ negotiate connection gi·ªØa client v√† LB. 1 security policy n√†y bao g·ªìm Protocols v√† Ciphers.\nProtocol d√πng ƒë·ªÉ thi·∫øt l·∫≠p connection gi·ªØa client v√† server, ƒë·∫£m b·∫£o data gi·ªØa client v√† LB dc private.\n1 Cipher l√† 1 thu·∫≠t to√°n encrypt ƒë·ªÉ t·∫°o nh∆∞ng message dc m√£ h√≥a.\nProtocol s·ª≠ d·ª•ng cipher ƒë·ªÉ encrypt data.\nQu√° tr√¨nh negotiate connection, B√™n Client v√† b√™n LB s·∫Ω ƒë∆∞a ra c√°c list cipher v√† protocols m√† m·ªói b√™n support, c√°i cipher ƒë·∫ßu ti√™n c·ªßa b√™n Server match v·ªõi b√™n Client s·∫Ω dc ch·ªçn l√† k·∫øt n·ªëi an to√†n (secure connection)\nNLB ch·ªâ support only single certificate per TLS Listener th√¥i.\nNLB ko support nh·ªØng certificate l·ªõn h∆°n 2048 bits\nAWS Certificate Manager s·∫Ω qu·∫£n l√Ω certificate cho b·∫°n, t·ª± ƒë·ªông renew khi cert h·∫øt h·∫°n\n Q. AWS Managed Blockchain, gi·ªØa c√°c member th√¨ 1 member c√≥ format resource enpoint l√† g√¨?\n ResourceID.MemberID.NetworkID.managedblockchain.us-east-1.amazonaws.com:PortNumber\nC√°c kh√°i ni·ªám:\nm·ªói member trong network blockchain l∆∞u 1 b·∫£n copy c·ªßa s·ªï c√°i (ledger) ·ªü local\nvi·ªác add member v√†o blockchain network c·∫ßn th√¥ng qua proposal v√† qu√° tr√¨nh vote. 1 member trong network t·∫°o proposal ƒë·ªÉ invite 1 account v√†o m·∫°ng, nh·ªØng member kh√°c vote, n·∫øu proposal dc approve th√¨ s·∫Ω g·ª≠i invitation ƒëi t·ªõi account kia.\nvi·ªác remove member c≈©ng v·∫≠y. Tuy nhi√™n Principal (quy·ªÅn to) c·ªßa network c√≥ th·ªÉ remove member ko c·∫ßn t·∫°o proposal v√† voting\nm·ªói member ph·∫£i tr·∫£ ti·ªÅn membership khi v√†o network, v√† c·∫£ ti·ªÅn peer node h·ªç t·∫°o ra, storage v√† l∆∞·ª£ng data dc write v√†o network\nblockchain network ch·ªâ b·ªã delete khi m√† member cu·ªëi c√πng t·ª± x√≥a kh·ªèi m·∫°ng\nkhi 1 member join network, vi·ªác ƒë·∫ßu ti√™n ph·∫£i l√†m l√† t·∫°o peer node (m·ªói peer node l∆∞u 1 local copy c·ªßa s·ªï c√°i ledger)\n Q. Khi 1 ec2 t·ª´ Running -\u0026gt; hibernate (stop) -\u0026gt; Running th√¨ c√°c lo·∫°i IP thay ƒë·ªïi nh∆∞ n√†o? Private IPv4 \u0026amp; IPv6, Public IPv4?\n Private IPv4 \u0026amp; IPv6 s·∫Ω retain, c√≤n Public IPv4 th√¨ release r·ªìi renew\n Q. Khi 1 EC2 v√†o tr·∫°ng th√°i hibernate (stop theo ki·ªÉu hibernate ch·ªâ c√≥ tr√™n c√°c d√≤ng EC2 M3, M4, M5, C3, C4, C5, R3, R4, R5, v·ªõi OS Amazon Linux 1, v√† type On demand v√† Reserved) th√¨ s·∫Ω t√≠nh ti·ªÅn nh·ªØng c√°i g√¨?\n T√≠nh ti·ªÅn EIP (N·∫øu c√≥) v√† storage space of EBS volume\nko t√≠nh ti·ªÅn compute capacity (c√¥ng su·∫•t t√≠nh to√°n)\n\u0026mdash;P7\u0026mdash;\n Q. Mu·ªën EC2 khi m√¨nh terminate v·∫´n gi·ªØ l·∫°i EBS volume th√¨ l√†m n√†o?\n ch·ªçn c√°i DeleteOnTermination for EBS = False\n Q. B·∫°n d√πng RDS MySQL, C·∫ßn ch·∫Øc ch·∫Øn l√† ho·∫°t ƒë·ªông backup DB s·∫Ω ko g√¢y ƒë·ªô tr·ªÖ cho nh·ªØng I/O operation b√¨nh th∆∞·ªùng l√™n DB. N√™n l√†m sao?\n ensure ƒë√£ enable Multi-AZ cho RDS\nQu√° tr√¨nh automate backup c·ªßa DB s·∫Ω l√†m I/O activity treo 1 th·ªùi gian ng·∫Øn kho·∫£ng v√†i gi√¢y. V·ªõi MariaDB, MySQL, Oracle, and PostgreSQL th√¨ ko suspend primary db v√¨ n√≥ backup tr√™n DB standby, not primary.\nV·ªõi SQL server th√¨ s·∫Ω b·ªã suspended 1 time ng·∫Øn\nAutomate Backup ch·ªâ th·ª±c hi·ªán v·ªõi DB instance ·ªü tr·∫°ng th√°i ACTIVE\nKhi x√≥a DB instance n·∫øu b·∫°n ch·ªçn \u0026ldquo;Retain automated backups\u0026rdquo; th√¨ n√≥ s·∫Ω gi·ªØ l·∫°i backup, c√≤n n·∫øu ko ch·ªçn th√¨ n√≥ x√≥a h·∫øt v√† ko th·ªÉ recover\n Q. 1 app dc x√¢y d·ª±ng nh∆∞ sau: 1 nh√≥m ec2 accept video upload from users, 1 nh√≥m ec2 process videos ƒë√≥. L√†m g√¨ ƒë·ªÉ ki·∫øn tr√∫c ƒë√≥ tr·ªü th√†nh excellent?\n t·∫°o SQS queue ƒë·ªÉ l∆∞u th√¥ng tin video uploads. C√°c ec2 process video g·∫Øn v√†o ASG. Vi·ªác scale ASG d·ª±a tr√™n size c·ªßa queue.\nDoc c·ªßa AWS vi·∫øt:\n1 ASG qu·∫£n l√Ω c√°c EC2 process message from sqs\n1 custom metric g·ª≠i ƒë·∫øn Cloudwatch s·ªë l∆∞·ª£ng msg trong queue per ec2\n1 policy c·ªßa ASG, scale d·ª±a tr√™n custom metric, cloudwatch alarm invoke scaling policy ƒë√≥\nc√°i policy c·∫ßn c√≥ 1 value ƒë·ªÉ d·ª±a tr√™n value ƒë√≥ scale ASG, th√¨ c√°ch t√≠nh nh∆∞ sau:\nGi·∫£ s·ª≠ hi·ªán t·∫°i SQS c√≥ ApproximateNumberOfMessages = 1500 msg, capacity (s·ªë l∆∞·ª£ng ec2 process) = 10 instances.\nN·∫øu th·ªùi gian trung b√¨nh x·ª≠ l√Ω 1 msg = 0.1s, v√† ƒë·ªô tr·ªÖ l·ªõn nh·∫•t c√≥ th·ªÉ ch·∫•p nh·∫≠n (longest acceptable latency) = 10s, th√¨ acceptable backlog per instance = 10/0.1 = 100.\n100 s·∫Ω l√† target value cho c√°i policy c·ªßa b·∫°n.\nNh∆∞ng hi·ªán t·∫°i backlog per instance = 1500/10 = 150.\nSuy ra c·∫ßn gi·∫£m backlog per instance xu·ªëng 100, mu·ªën v·∫≠y th√¨ ph·∫£i scale out th√†nh 1500/100 = 15 instances. 15-10 = 5 instances s·∫Ω dc ASG t·∫°o th√™m.\n Q. 1 cty s·∫Øp host 1 app tr√™n aws. H·ªç mu·ªën load balance traffic d·ª±a tr√™n which route user choose. 2 route c·ªßa app l√† /customer v√† /orders. N√™n d√πng lo·∫°i ELB n√†o?\n D√πng lo·∫°i c√≥ path-based routing, ƒë√≥ l√† ALB\n Q. App c·ªßa b·∫°n t∆∞∆°ng t√°c v·ªõi DynamoDB, ƒëang c√≥ Read capacity l√† 10. G·∫ßn ƒë√¢y ph√°t hi·ªán hay b·ªã throttling ·ªü DynamoDB, l√†m sao?\n Enable Autoscaling cho DynamoDB\nƒêi·ªÅu n√†y s·∫Ω khi·∫øn khi traffic tƒÉng l√™n, n√≥ s·∫Ω provision read/write capacity ƒë·ªÉ handle traffic m√† ko b·ªã throttling.\nN·∫øu b·∫°n ch·ªâ ƒë∆°n gi·∫£n tƒÉng Read Capacity c·ªßa table l√™n 20 th√¨ ch·ªâ l√† gi·∫£i ph√°p t·∫°m th·ªùi.\n Q. B·∫°n l∆∞u data documents tr√™n S3, ko truy c·∫≠p th∆∞·ªùng xuy√™n, nh∆∞ng khi truy c·∫≠p th√¨ mu·ªën access ƒëc trong kho·∫£ng 20p, c·∫ßn d√πng lo·∫°i g√¨ cho ti·∫øt ki·ªám?\n S3 IA\nko d√πng Glacier Bulk retrieval (5-12h), Glacier Standard retrieval (3-5h)\n Q. B·∫°n c√≥ 1 app ch·∫°y tr√™n c√°c ec2 trong private subnet, gi·ªù mu·ªën access v√†o Kinessis stream m√† ko ƒëi qua internet, l√†m n√†o?\n T·∫°o VPC Interface Endpoint cho ph√©p Kinesis Streams\nko d√πng VPC Gateway Endpoint v√¨ c√°i ƒë√≥ d√πng cho S3 v√† DynamoDB\nVPC Interface Endpoint dc cung c·∫•p b·ªüi AWS PrivateLink, n√≥ ko y√™u c·∫ßu IGW, NAT device, VPN, Direct Connect, n√≥ cung c·∫•p kh·∫£ nƒÉng comunicate gi·ªØa c√°c AWS service\n Q. C·∫ßn monitor API activity cho m·ª•c ƒë√≠ch audit th√¨ c·∫ßn d√πng service n√†o? Cloudtrail? AWS Config? AWS Inspector?\n Cloudtrail th√¥i\n Q. 1 architecture nh∆∞ sau:\n1 set EC2 l√† web layer ƒë·ª©ng sau 1 CLB\n1 set EC2 l√† proxy server\n1 set EC2 l√† backend server\n c·∫ßn l√†m g√¨ ƒë·ªÉ c√≥ kh·∫£ nƒÉng scalability?\nASG cho proxy server\nASG cho backend server\nko c·∫ßn thay CLB b·∫±ng ALB (Application LB) v√¨ Clasic Load Balancer v·ªën ƒë√£ c√≥ kh·∫£ nƒÉng scalability r·ªìi\n Q. 1 cty c·∫ßn host 1 app tr√™n ec2. C√≥ y√™u c·∫ßu l√† b·∫°n c·∫ßn c√≥ quy·ªÅn control s·ªë l∆∞·ª£ng core d√†nh cho application. D√πng lo·∫°i EC2 n√†o?\n D√πng EC2 - Dedicated Host\nAWS cho b·∫°n to√†n quy·ªÅn tr√™n 1 con server v·∫≠t l√Ω lu√¥n, ko chia s·∫ª v·ªõi ai, cho b·∫°n to√†n quy·ªÅn control s·ªë l∆∞·ª£ng core, socket‚Ä¶, cho b·∫°n quy·ªÅn s·ª≠ d·ª•ng licence c√≥ s·∫µn, ko m·∫•t ti·ªÅn mua license c·ªßa AWS n·ªØa. B·∫°n c√≥ th·ªÉ run nhi·ªÅu instances tr√™n c√°i host ƒë√≥\nnh·ªõ ph√¢n bi·ªát v·ªõi Dedicated Instance:\nEC2 - Dedicated Instance:\nT·∫°o Ec2 tr√™n 1 con server v·∫≠t l√Ω, ƒë·∫£m b·∫£o duy nh·∫•t cho b·∫°n (tu√¢n th·ªß v·ªÅ m·∫∑t compliance)\n Q. Cty b·∫°n l∆∞u docs tr√™n s3, mu·ªën ensure l√† object s3 dc encrypt at rest, l√†m n√†o? c√≥ th·ªÉ l√†m trong ph·∫ßn bucket policy v√† bucket ACL ko?\n ko.\nC√≥ 3 c√°ch encrypt data s3 l√†:\nSSE with S3 managed key\nSSE with KMS managed key\nSSE with Customer-provided key\n Q. 1 cty host 1 active-active site. 1 site tr√™n AWS, 1 site ·ªü Onpremise. C·∫ßn c√°c traffic dc distributed ph√π h·ª£p tr√™n c·∫£ 2 site. D√πng c√°i g√¨ c·ªßa Route53? Simple / Failover / Latency / Weighted Routing ?\n ·ªü ƒë√¢y b·∫°n c√≥ 2 resource.\nch·ªçn Weighted Routing - cho ph√©p b·∫°n associate multiple resource v·ªõi 1 single domain name, ch·ªçn bao nhi√™u traffic s·∫Ω dc route ƒë·∫øn m·ªói resource\nko ch·ªçn m·∫•y c√°i c√≤n l·∫°i:\nV√¨ Simple ch·ªâ d√πng khi b·∫°n mu·ªën configure ƒë∆°n gi·∫£n, 1 resource cho 1 domain, standard DNS record\nFailover d√πng khi b·∫°n mu·ªën route traffic ƒë·∫øn 1 resource khi resource c√≤n l·∫°i unhealthy\nLatency d√πng khi b·∫°n mu·ªën improve performance c·ªßa user theo region ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ\n Q. Gi·ªØa c√°c VPC ·ªü c√°c regions kh√°c nhau, mu·ªën connect v√† ko pass ra internet? l√†m n√†o?\n VPC peering v√† AWS Direct Connect\n Q. App c·ªßa b·∫°n c√≥ web tier v√† db tier tr√™n ec2. ƒêang d√πng General purpose SSD cho volume type. ƒêang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ read write to DB. L√†m n√†o?\n v·∫•n ƒë·ªÅ v·ªÅ read write to DB -\u0026gt; li√™n quan ƒë·∫øn volume type, ko ph·∫£i instance type nh∆∞ t2, m5 etc\u0026hellip;\nc·∫ßn change to Provisioned IOPS SSD\n Q. App c·ªßa cty b·∫°n c√≥ ec2 v·ªõi ALB. Cty mu·ªën b·∫£o v·ªá app kh·ªèi application level attack. D√ông g√¨? Cloudfront/ WAF?\n WAF\nCh·ª© Cloudfront ch·ªâ l√† service ph·ª•c v·ª• CDN Content delivery\n Q. Cty ƒëang setup app nh∆∞ sau: 1 set EC2 host web app, ELB, User access v√†o app qua ELB, app connect to backend db, NAT gateway. N√™n setup c√°c server v√† ELB ·ªü c√°c subnet nh∆∞ n√†o ƒë·ªÉ HA v√† ƒë√∫ng architect?\n 2 public subnet cho ELB, 2 private subnet cho web server, 2 private subnet cho db\nch√∫ √Ω web server ko c·∫ßn ph·∫£i ·ªü ngo√†i public subnet, nh∆∞ v·∫≠y secure h∆°n\n Q. 1 app g·ªìm c√°c componnents, 2 primary component c·∫ßn ch·∫°y 3h m·ªói ng√†y. C√°c components kh√°c ch·∫°y 6-8h m·ªói ng√†y. N√™n d√πng c√°i type instance n√†o cho ti·∫øt ki·ªám?\n On-demand instance cho primary components, Reserved Instance cho c√≤n l·∫°i\nPrimary component ch·ªâ run 3h/day th√¨ ko c·∫ßn d√πng Reserved\nKo d√πng Spot instance v√¨ ko bi·∫øt lo·∫°i workload (data analysis, batch jobs, background processing, and optional tasks?) ƒë·ªÉ quy·∫øt ƒë·ªãnh s·ª≠ d·ª•ng spot\nReserved s·∫Ω dc discount v√† r·∫ª h∆°n On-demand\nC√°c c√¢u h·ªèi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh thi (ch∆∞a add c√¢u tr·∫£ l·ªùi)  Q. 1 cty mu·ªën encrypt data at rest, nh∆∞ng mu·ªën encryption key dc managed ·ªü on-premise. D√πng c√°i n√†o? S3 SSE-C (Server side encryption with customer provided key)? Hay CloudHSM?\n  Q. 1 app c√≥ private subnet v√† public subnet, gi·ªù private subnet c·∫ßn ra ngo√†i internet, th√¨ c·∫ßn l√†m g√¨?\nG·∫Øn IGW v√† update private subnet route table?\nHay g·∫Øn NatGW v√†o public subnet v√† update private route table?\n  Q. NAT Gatway s·∫Ω add v√†o public subnet hay private subnet?\n  Q. Gi·∫£i ph√°p cho web static content, ch·ªçn S3 static website hay cloudfront + s3 bucket as origin?\n  Q. 1 App ghi v√†o s3, user ph·∫£n √°nh hi·ªÉn th·ªã data c≈© v√¨ sao?\nV√¨ app ƒëang read parallel request?\nHay v√¨ app ƒëang read theo range of header?\nHay v√¨ app ƒëang write ki·ªÉu ghi ƒë√® l√™n object c√≥ s·∫µn?\n  Q. App tr√™n Ec2 memory intensive, c·∫ßn config policy base tr√™n cloudwatch metric nh∆∞ n√†o?\nT·∫°o custom metric from app?\nHay t·∫°o high resolution alarm?\n  Q. App d√πng Cloudfront, nh∆∞ng user ph·∫£n √°nh l√† h·ªç ƒëang th·∫•y c√°c data c≈©, c·∫ßn l√†m g√¨?\nPerform cloudfront refresh?\nHay change TTL c·ªßa cache tr√™n cloudfront?\n  Q. App ghi v√†o db b·ªã throttle th√¨ n√™n l√†m g√¨?\nD√πng elastic cache v√† lazy loading?\nHay elasticsearch v√† write-through db?\nHay g·∫Øn ASG cho db?\n  Q. App c√≥ 10 micro service m·ªói c√°i 1 Clasic Load Balancer, Mu·ªën gi·∫£m cost th√¨ n√™n l√†m j?\nThay h·∫øt CLBs b·∫±ng 1 single ALB?\nHay change ASG desired?\n  Q. ƒêang migrate on-premise proxy server l√™n aws, quy·∫øt ƒë·ªãnh d√πng ALB, c·∫ßn secure communication, th√¨ d√πng c√°i g√¨?\nD√πng Third party ƒë·ªÉ qu·∫£n l√Ω certificate manager?\nHay t·∫°o 1 SNI certificate v√† up l√™n ALB?\nHay t·∫°o 1 wildcard certificate v√† up l√™n ALB?\nHay t·∫°o 1 second proxy server ƒë·ªÉ terminate SSL traffic?\n  Q. SG c·∫ßn allow trafic t·ª´ ALB, th√¨ l√†m n√†o?\nG·∫Øn ALB ip v√†o sg?, Hay g·∫Øn subnet ip v√†o sg?, Hay g·∫Øn vpc cidr v√†o sg?\n  Q. 1 app d·ª± ƒëo√°n tu·∫ßn sau s·∫Ω ph·∫£i x2 c√¥ng su·∫•t th√¨ asg config nh∆∞ n√†o?\nD√πng Scheduled scaling policy?\nHay Dynamic Scaling policy?\n  Q. 1 app d√πng EC2 24/7 trong 1 nƒÉm, n√™n ch·ªçn type EC2 n√†o?\nConvertible Reserved EC2?\nStandard Reserved EC2?\nScheduled Reserved EC2?\n  Q. Kh√°i ni·ªám RTO RPO trong Disaster Recovery?\n ","href":"/bk/encrypt-aws-certified-solution-architect-associate-note-saa/","title":"Aws Certified Solution Architect Associate Note (SAA)"},{"content":"C√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh h·ªçc \u0026mdash;P1\u0026mdash;\n Q. 1 cty c·∫ßn migrate 10TB data l√™n aws. Y√™u c·∫ßu replica lag \u0026lt; 100 miliseconds, data c√≥ th·ªÉ scale g·∫•p 4? d√πng c√°i RDS n√†o c·ªßa amazon?\n aurora, c√≥ th·ªÉ scale 64TB v√† keep replica lag d∆∞·ªõi 100 milisec\n Q. \u0026ldquo;IOPS‚Äù stands for input/output operations per second=s·ªë l·∫ßn ƒë·ªçc/ghi max trong 1 gi√¢y, \u0026ldquo;throughput\u0026rdquo; l√† dung l∆∞·ª£ng traffic 1 gi√¢y\n C√°c lo·∫°i EBS:\nGeneral Purpose SSD (gp2): b√¨nh th∆∞·ªùng, d√πng cho m√¥i tr∆∞·ªùng Dev/test, volume size (1gb-16TB), max IOPS=16k, max throughput/volume=250M/s\nProvisioned IOPS SSD (io1): performance kinh nh·∫•t, d√πng cho data quan h·ªá RDS, volume size (4gb-64TB), max IOPS=64k, max throughput/volume=1000M/s\nThroughput Optimized HDD (st1): gi√° r·∫ª, d√πng cho Bigdata, streaming, access th∆∞·ªùng xuy√™n, volume size (500gb-16TB), max IOPS=500, max throughput/volume=500M/s\nCold HDD (sc1): gi√° r·∫ª nh·∫•t, d√πng cho Bigdata, access ko th∆∞·ªùng xuy√™n, volume size (500gb-16TB), max IOPS=250, max throughput/volume=250M/s\n Q. 1 App run 150GB relational database on EC2 instance. App ch·∫°y ko th∆∞·ªùng xuy√™n v√† ƒë√¥i khi c√≥ peak time. ƒë·ª° t·ªën ti·ªÅn n√™n ch·ªçn lo·∫°i EBS n√†o?\n s·∫Ω d√πng SSD (gp2),\n1- v√¨ c√°i Hdd (st1, sc1) s·∫Ω c√≥ minimum 500GB m√† m√¨nh ch·ªâ c·∫ßn 150GB th√¥i\n2- v√¨ l√† RDS n√™n ∆∞u ti√™n IOPS -\u0026gt; ∆∞u ti√™n SSD\nch·ª© n·∫øu Big data, stream data th√¨ ∆∞u ti√™n Throughtput MB/s -\u0026gt; ∆∞u ti√™n Hdd\n Q. Though the images will be retrieved infrequently, they must be available for retrieval immediately. Data dc truy xu·∫•t ko li√™n t·ª•c, nh∆∞ng ph·∫£i available ƒë·ªÉ l·∫•y ngay l·∫≠p t·ª©c\n S3 IA\n Q. V·ªÅ s3 glacier, kh√°c bi·ªát gi·ªØa c√°c option Retrieval sau: expedited, standard, bulk?\n Expedited data s·∫Ω c√≥ th·ªÉ access trong v√≤ng 1-5 min,\nStandard data s·∫Ω c√≥ th·ªÉ access trong v√≤ng 3-5 hour,\nBulk r·∫ª nh·∫•t, data s·∫Ω c√≥ th·ªÉ access trong v√≤ng 5-12 hour\n Q. 1 app tr√™n aws c·∫ßn pull data t·ª´ internet, SA c·∫ßn thi·∫øt k·∫ø HA solution ƒë·ªÉ access v√†o data m√† ko c·∫ßn ƒë·∫∑t bandwidth constraints v√†o c√°i app c·ªßa m√¨nh? th√¨ s·∫Ω l√†m g√¨?\n g·∫Øn IGW v√† route 0.0.0.0/0,\nNATGW c≈©ng HA nh∆∞ng ch·ªâ d√πng cho private subnet, v√† bandwidth ch·ªâ cho scale up 45Gbps,\nNAT instance th√¨ bandwidth ph·ª• thu·ªôc v√†o lo·∫°i instance,\nVPC endpoint th√¨ d√πng ƒë·ªÉ connect t·ªõi c√°c service c·ªßa aws th√¥i, ko d√πng cho internet\n Q. asG, b·∫°n nh·∫≠n ra app c·ªßa b·∫°n ƒëang scale asG up and down nhi·ªÅu l·∫ßn, c·∫ßn l√†m g√¨?\n S·ª≠a asg cool down timer, v√† check c√°i cloudwatch alarm ƒëang trigger vi·ªác scale down asg.\n Q. asg cool down timer l√† g√¨?\n l√† th·ªùi gian sau khi scale activity c·ªßa asg th·ª±c hi·ªán, n√≥ s·∫Ω t·∫°m suspend 1 tgian. (default l√† 300s)\n Q. recommend apply traffic cho ·ª©ng d·ª•ng web app, n√™n apply cho 2 tier web v√† db nh∆∞ n√†o?\n web sg: allow port 443 https (SSL) ho·∫∑c 80 http from source: anywhere\ndb sg: allow port 3306 (mysql) from source: \u0026ldquo;web sg\u0026rdquo;\nSG th√¨ m√¨nh ch·ªâ c√≥ th·ªÉ set rule ƒë·ªÉ allow ip, c√≤n ACL th√¨ c√≥ th·ªÉ set rule ƒë·ªÉ deny/allow ip n√†o ƒë√≥.\nSet rule inbound cho SG th√¨ outbound c≈©ng apply (stateful), nh∆∞ng ACL th√¨ ph·∫£i set c·∫£ inbound v√† outbound (stateless)\n Q. C√°c rule trong ACL ƒëc apply ntn? N·∫øu s·ªë 100 allow ip 1, s·ªë 101 deny ip 1 th√¨?\n t·ª´ s·ªë b√© tr∆∞·ªõc r·ªìi ƒë·∫øn s·ªë l·ªõn sau.\nN·∫øu s·ªë 100 allow ip 1, s·ªë 101 deny ip 1 th√¨ k·∫øt qu·∫£ l√† ip 1 v·∫´n dc allow v√¨ n√≥ dc apply tr∆∞·ªõc\n Q. 1 website ch·∫°y tr√™n ec2 sau 1 ALB, h·ªç mu·ªën tr√°nh vi·ªác serving file c·ªßa ec2 m·ªói l·∫ßn ng d√πng request assest th√¨ l√†m g√¨ ƒë·ªÉ c·∫£i thi·ªán ux?\n cache static content tr√™n Cloudfront\n Q. 1 app c·∫ßn encrypt data trc khi ghi v√†o disk, n√™n d√πng service n√†o?\n KMS, ko d√πng Certificate manager v√¨ awS cert manager d√πng ƒë·ªÉ gen ra certificate ƒë·ªÉ encrypt traffic in transit\n Q. 1 cty export daily data v√†o S3. team data warehouse mu·ªën l·∫•y data t·ª´ s3 ƒë√≥ import v√†o Redshift, y√™u c·∫ßu l√† data ch·ªâ dc transport trong 1 vpc. l√†m g√¨ cho secure?\n b·∫≠t Redshift Enhanced vpc Routing v√† create s3 vpc endpoint\n Q. 1 website cung c·∫•p nhi·ªÅu language, define theo htttp request:\nhttp://d11111f8.cloudfront.net/main.html?language=de;\nhttp://d11111f8.cloudfront.net/main.html?language=en\nCloudfront c·∫ßn configure nh∆∞ n√†o?\n Base on query string parameters\n Q. 3 c√°ch cache content tr√™n cloudfront l√†?\n base on query string parameter, base on cookie, base on request headers\n Q. ecs, c√°c container ko dc access vao c√°c container kh√°c c·ªßa customer kh√°c. v·∫≠y ph·∫£i d√πng n√†o?\n d√πng IAM role for task (trong CDA th√¨ c√≥ 1 c√¢u nh∆∞ n√†y nh∆∞ng b·∫£o config security group ho·∫∑c IAM role for task -\u0026gt; c√≥ th·ªÉ d√πng 1 trong 2 c√°ch)\n Q. 1 c√¥ng ty ƒëang generate 1 c·ª•c dataset h√†ng tri·ªáu row th√†nh d·∫°ng c·ªôt, c√°c tool kh√°c s·∫Ω build report daily t·ª´ c√°c dataset ƒë√≥. N√™n d√πng service g√¨?\n Redshift, d√πng v·ªõi datawarehouse, ƒë·ªÉ t·∫°o insights cho c√¥ng ty b·∫°n\n Q. 1 cty c·∫ßn 1 db tr√™n aws ph√π h·ª£p y√™u c·∫ßu:\nkh·ªüi t·∫°o 8TB, tƒÉng tr∆∞·ªùng 8GB 1 ng√†y, c√≥ 4 read replica\n N√™n D√πng awS Aurora (limit 64TB v√† 15 read replica),\nkh√¥ng d√πng dynamoDB v√¨ ph·∫£i vi·∫øt r√µ l√† DAX (DynamoDB Accelerator) m·ªõi c√≥ 9 Read Replicas, DynamoDB ko limit size c·ªßa table\naurora l√† Relational db, dynamoDB l√† noSQL\n Q. 1 app c·∫ßn 1 file system. c√°i n√†o ƒë·ªÉ c√≥ th·ªÉ access b·ªüi ec2?\n EFS, ko d√πng S3 v√¨ l√† object base storage th√¥i. (ch√∫ √Ω efs ko th·ªÉ c√†i tr√™n window ec2)\n Q. H√†ng tri·ªáu ·∫£nh c·∫ßn up l√™n s3? l√†m g√¨ ƒë·ªÉ tƒÉng performance?\n d√πng hexadecimal hash for prefix\n Q. C·∫ßn get IP address c·ªßa c√°c resource accessed in private subnet. D√πng c√°i n√†o?\n VPC flow log\n Q. 1 app 2 tier c·∫ßn db. DB s·∫Ω y√™u c·∫ßu multiple schema changes. durable, ACID. D√πng db n√†o? ACID: Atomicity (n·∫øu transaction li√™n quan \u0026gt;2 x·ª≠ l√Ω, th√¨ ho·∫∑c all x·ª≠ l√Ω dc th·ª±c hi·ªán, ho·∫∑c l√† ko c√°i n√†o dc th·ª±c hi·ªán), Consistency (ko c√≥ transaction d·ªü dang,nghƒ©a l√† ho·∫∑c m·ªõi ho·∫∑c rollback), isolation (c√°c transaction lu√¥n t√°ch r·ªùi nhau, ko tr·ªôn l·∫´n), Durability (data never lost n·∫øu ·ª©ng d·ª•ng b·ªã crash)\n D√πng Aurora.\nV√¨ DynamoDB ko support schema change, ko support ACID (ch·ªâ C v√† D).\nRedshift c≈©ng ko ACID (ch·ªâ AID)\n Q. Redshift cluster m√† mu·ªën recovery khi disaster ƒë·∫øn 1 ch·ªó c√°ch xa 600km th√¨ c·∫ßn l√†m g√¨?\n B·∫≠t cross-region snapshot . V√¨ m·ªói AZ c√°ch nhau ch·ªâ kho·∫£ng 100km n√™n ko th·ªÉ b·∫≠t cross-az snapshot ƒëc (M√† n√≥ c≈©ng ko c√≥ kh√°i ni·ªám l√† \u0026ldquo;cross AZ\u0026rdquo; ch·ªâ c√≥ kh√°i ni·ªám \u0026ldquo;multi AZ/multi region\u0026rdquo; nh∆∞ng l·∫°i ko d√πng cho snapshot m√† d√πng ch·ªâ cluster)\n Q. 1 cty ƒëang c√≥ APi ri√™ng v·ªõi 1000 request per second ? ƒë·ªÉ cost effective d√πng g√¨?\n API gateway + lambda\n Q. 1 app host tr√™n ec2 ƒëang c√≥ 1 campaign start trong 2 tu·∫ßn. Mu·ªën ƒë·∫£m b·∫£o performance trong th·ªùi gian ƒë√≥, c·∫ßn config asG nh∆∞ n√†o?\n Dynamic scaling + target tracking scaling policy\n Q. 1 app host tr√™n aws sit sau 1 ELB. Notfy to admin n·∫øu read request \u0026gt; 1000 req/min. Notify n·∫øu latency \u0026gt; 10s. B·∫•t ky API activity m√† call ƒë·∫øn sensitive data s·∫Ω dc monitor\n Cloudtrail v√† cloudwatch metric v·ªõi setup alarm\n Q. 1 company mu·ªën monitor all API activity v√† c·∫ßn apply cho future regions. C·∫ßn l√†m g√¨?\n Ensure 1 trail trong cloudtrail dc enable all region\n Q. C√≥ y√™u c·∫ßu cho thi·∫øt b·ªã ISCSI device, v√† 1 app legacy c·∫ßn local storage. C·∫ßn d√πng service n√†o? iscsi device l√† c√°c thi·∫øt b·ªã li√™n quan ƒë·∫øn ip TCP/IP, truy·ªÅn d·ªØ li·ªáu qua LAN, WAN\n D√πng Aws Storage gateway stored volume\nPh√¢n bi·ªát aws gateway cached volume v√† aws gateway stored volume:\nAws gateway cached volume: l∆∞u data tr√™n s3, 1 c·ª•c c·∫ßn access th∆∞·ªùng xuy√™n s·∫Ω l∆∞u locally. d·ªÖ scale, ƒë·ªô tr·ªÖ th·∫•p\nAws gateway stored volume: data s·∫Ω l∆∞u ho√†n to√†n ·ªü locally, r·ªìi snapshot s·∫Ω dc async l√™n s3\n(gi·∫£i ph√°p cho vi·ªác backup recover data cho local data center c·ªßa m√¨nh ho·∫∑c tr√™n aws)\nGateway-virtual tape library (VTL): data l∆∞u ·ªü Glacier\n Q. Y√™u c·∫ßu l√† EC2 trong private subnet c·∫ßn access v√†o S3 bucket. Traffic ko dc qua internet. D√πng c√°i g√¨?\n VPC Enpoint\n Q. C√≥ 1 app ch·∫°y tr√™n c√°c EC2 v√† sit sau clasic ELB, 1 EC2 proxy dc d√πng cho qu·∫£n l√Ω content v√† nhi·ªÅu Ec2 d√πng cho Backend, V·∫•n ƒë·ªÅ l√† app c·ªßa h·ªç scale ko ƒë√∫ng. N√™n l√†m n√†o?\n D√πng asg cho c·∫£ proxy server v√† backend instance\n Q. 1 web host tr√™n aws c√≥ dc r·∫•t nhi·ªÅu traffic. L√†m sao ƒë·ªÉ gi·∫£m thi·ªÉu r·ªßi ro disruption c·ªßa web n√†y trong tr∆∞·ªùng h·ª£p disaster?\n D√πng Route53 ƒë·ªÉ Route to static website.\nKo th·ªÉ d√πng ELB ƒë·ªÉ chuy·ªÉn traffic ƒë·∫øn region kh√°c v√¨ ELB ch·ªâ d√πng ƒë·ªÉ balance trong 1 region ch·ª© ko multi region dc, k·ªÉ c·∫£ trong 1 region th√¨ vi·ªác backup cross AZ ko d√πng ƒë·ªÉ recovery\nH·∫ßu h·∫øt cty c·ªë g·∫Øng implement HA thay v√¨ DR, trong case HA th√¨ app s·∫Ω host kh√°c AZ ch·ª© ko kh√°c region, ko ƒë·∫£m b·∫£o trong TH disaster to√†n region. DR th√¨ s·∫Ω recovery t·ª´ 1 region kh√°c (\u0026gt;250miles). DR implement m√¥ h√¨nh Active/Passive, nghƒ©a l√† lu√¥n lu√¥n c√≥ 1 static site OR service nh·ªè run ·ªü 1 region kh√°c.\n Q. B·∫°n ƒë∆∞·ª£c y√™u c·∫ßu host static web cho domain mycompany.com tr√™n aws, y√™u c·∫ßu traffic dc scale ƒë√∫ng c√°ch, d√πng services n√†o?\n Route53 v√† static website tr√™n s3,\nnh·∫≠p NS record c·ªßa R53 v√†o domain registrar ƒë·ªÉ registrar qu·∫£n l√Ω c√°i domain ƒë√≥, ko cho ng kh√°c ƒëƒÉng k√Ω l·∫°i domain c·ªßa m√¨nh\n Q. B·∫°n ƒë∆∞·ª£c y√™u c·∫ßu analyze behavior qua nh·ªØng c√°i g√¨ customer click v√†o, c√°i n√†o ƒë·ªÉ capture data v√† analyze?\n Push nh·ªØng c√°i click c·ªßa customer l√™n kinesis v√† analyze b·∫±ng kinesis worker\n Q. 1 c√¥ng ty c√≥ 1 infra g·ªìm c√°c machine send log every 5 minutes, s·ªë l∆∞·ª£ng machine c√≥ th·ªÉ h√†ng ng√†n, data s·∫Ω analyze ·ªü stage kh√°c, th√¨ c·∫ßn d√πng g√¨?\n D√πng kinesis firehose v·ªõi s3 ƒë·ªÉ take log, store n√≥ tr√™n s3 cho c√°c process sau n√†y\nKinesis Firehose capture, transform, load stream data v√†o s3, redshift, elasticsearch\n Q. 1 app host tr√™n aws cho ph√©p user upload video to s3. 1 user ph·∫£i c√≥ quy·ªÅn ƒë·ªÉ upload video l√™n s3 bucket trong 1 tu·∫ßn base on profile, L√†m th·∫ø n√†o?\n T·∫°o pre-signed urL h·∫øt h·∫°n trong 1 tu·∫ßn cho m·ªói user profile.\n Q. 1 cty host 5 web server tr√™n aws. H·ªç mu·ªën Route53 route user traffic to random web server. Policy n√†o c·ªßa R53 th·ªèa m√£n?\n Multivalue answer routing policy: route traffic 1 c√°ch ng·∫´u nhi√™n ƒë·∫øn max l√† 8 record healthy.\nNgo√†i ra:\nSimple routing policy: D√πng 1 resource cho 1 domain.\nLatency routing policy: N·∫øu c√≥ nhi·ªÅu resource th√¨ s·∫Ω route traffic ƒë·∫øn location c√≥ best latency nh·∫•t.\nWeighted routing policy: route traffic ƒë·∫øn nhi·ªÅu resoure theo t·ªâ l·ªá b·∫°n defined.\nGeoproximity routing policy: route traffic d·ª±a tr√™n location c·ªßa resource c·ªßa b·∫°n.\nGeolocation routing policy: route traffic d·ª±a tr√™n location c·ªßa user.\nFailover routing policy: d√πng ƒë·ªÉ configure Active/passive failover.\n Q. C√¥ng ty c·∫ßn 1 db c√≥ th·ªÉ perform c√¢u query underlying (truy v·∫•n c∆° b·∫£n) nh∆∞ JOIN th√¨ d√πng db n√†o? aurora? dynamodb? redshift? s3?\n aurora (ch·ª© dynamo ko d√πng join dc )\n\u0026mdash;P2\u0026mdash;\n Q. 1 customer mu·ªën t·∫°o EBS volume in AWS, data on volume c·∫ßn encrypt at rest. C·∫ßn l√†m g√¨?\n v√¨ at rest n√™n d√πng KMS ƒë·ªÉ gen ra key r·ªìi s·ª≠ dung key ƒë√≥ encrypt data. Kh√¥ng d√πng SSL v√¨ SSL ƒë·ªÉ encrypt data in transit\n Q. Cty b·∫°n c√≥ 1 s·ªë EC2, c·∫ßn monitor state c·ªßa EC2 v√† record l·∫°i, l√†m n√†o?\n Cloudwatch log ƒë·ªÉ store state changes c·ªßa EC2, D√πng cloudwatch Event ƒë·ªÉ monitor changes of event ƒë·ªëi v·ªõi EC2\n Q. B·∫°n t·∫°o 1 VPC v√† 3 public subnet, ·ªü m·ªói AZ 1 public subnet, m·ªói subnet 1 ec2. Gi·ªù mu·ªën t·∫°o ELB c·ªßa 2 Az. bao nhi√™u Private IP dc d√πng khi kh·ªüi t·∫°o ELB?\n 2 AZ -\u0026gt; 2 subnet -\u0026gt; 2 ec2 -\u0026gt; 2 private IP\n Q. Amazon RDS backup process l√† g√¨? default l√† g√¨?\n default RDS s·∫Ω th·ª±c hi·ªán backup c√°i storage volume c·ªßa DB Instance, nghƒ©a l√† to√†n b·ªô DB instance ch·ª© ko ri√™ng r·∫Ω DB.\nƒêi·ªÅu n√†y l√†m m·ªói ng√†y 1 l·∫ßn. C√πng v·ªõi capture transaction log every 5min v√† l∆∞u s3\nC√≤n Database Snapshot th√¨ RDS ko t·ª± l√†m, l√† do User l√†m, c√≥ th·ªÉ l√†m b·∫•t k√Ω l√∫c n√†o.\n Q. 1 app l∆∞u th√¥ng tin quan tr·ªçng tr√™n s3. th√¥ng tin c√≥ th·ªÉ access qua internet, c√¥ng ty lo r·∫±ng connect ƒë·∫øn s3 c√≥ th·ªÉ l√† risk v·ªÅ security, l√†m n√†o?\n access data th√¥ng qua VPC enpoint cho S3.\n Q. B·∫°n ƒëang c√≥ 1 NAT Gateway cho c√°c ec2 private. B·∫°n mu·ªën NAT Gateway c≈©ng ph·∫£i HA, l√†m n√†o?\n T·∫°o th√™m NAT Gateway ·ªü 1 Az kh√°c, V·ªÅ document c·ªßa AWS c√≥ n√≥i, mu·ªën t·∫°o 1 h·ªá th·ªëng ko phu thu·ªôc Az th√¨ m·ªói Az 1 NAT Gateway v√† ensure r·∫±ng c√°c resource s·ª≠ dung NAT ƒë√≥ th√¨ ·ªü c√πng 1 Az.\nCh·ª© ko th·ªÉ dung ASG cho NAT Gateway\n Q. 1 cty mu·ªën c√≥ 1 fully managed data store tr√™n Aws, compatible v·ªõi MySQL, database engine n√†o c√≥ th·ªÉ s·ª≠ dung? AWS Aurora hay AWS RDS\n Ch·ªçn Aurora v√¨ RDS ko ph·∫£i l√† database engine, RDS l√† 1 service support 6 database engines.\nAurora l√† 1 DB engines nh∆∞ng n√≥ ch·ªâ support 2 DB engines kh√°c l√† MySQL v√† PostgresSQL\n Q. 1 cty mu·ªën ch·ªâ nh·ªØng kh√°ch hang x√°c ƒë·ªãnh c√≥ th·ªÉ upload ·∫£nh to s3 trong 1 giai ƒëo·∫°n nh·∫•t ƒë·ªãnh. N√™n l√†m n√†o?\n t·∫°o Pre-signed URL ƒë·ªÉ upload images, th√™m ƒëi·ªÅu ki·ªán expired time.\n Q. Ph√¢n bi·ªát c√°c lo·∫°i ELB: CLB, NLB, ALB (c√°i n√†o c≈©ng c√≥ t√≠nh scalability):\n Classic Load Balancer: cung c·∫•p (HTTP, HTTPS or TCP listeners) cho only 1 backend port. Ch·∫°y b√™n ngo√†i VPC. ( N·∫øu app c·ªßa b·∫°n ch·∫°y ngo√†i VPC). Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\nNetwork Load Balancer: Layer 4, cung c·∫•p only TCP. D√πng EIP/static IP. Nhi·ªÅu ports. D√πng v·ªõi nh·ªØng app m√† ch·ªâ c·∫ßn s·ª≠ d·ª•ng TCP. V√† mu·ªën qu·∫£n l√Ω b·∫±ng whilelist IP. C√≥ th·ªÉ scale ƒë·ªÉ handle workload l√™n ƒë·∫øn h√†ng tri·ªáu request 1 gi√¢y (millions of requests per second)\nApplication Load Balancer: Layer7, cung c·∫•p c√¢n b·∫±ng t·∫£i cho only HTTP/HTTPS. Nhi·ªÅu ports. flexible h∆°n CLB. Nhi·ªÅu ng d√πng. Cung c·∫•p t√≠nh nƒÉng n√¢ng cao v·ªÅ routing ƒë·∫øn target, d√πng cho c√°c app hi·ªán ƒë·∫°i, micro-services, containers. Ko th·ªÉ handle ƒëc h√†ng tri·ªáu request 1 gi√¢y nh∆∞ NLB\n Q. Third-party sign-in (Federation) co dc d√πng ƒë·ªÉ authorize service ko?\n ko, Federation is used to authenticate users, not to authorize services\n Q. B·∫°n c√≥ 1 v√†i ec2 trong asG, nh∆∞ng c√≥ v·∫ª ec2 ko dc scale out ƒë√∫ng l√∫c. C·∫ßn check l·∫°i g√¨?\n check l·∫°i xem c√≥ ƒë√∫ng metric dc d√πng ƒë·ªÉ trigger vi·ªác scale out ko?\n Q. Cty b·∫°n thi·∫øt k·∫ø 1 app t∆∞∆°ng t√°c v·ªõi dynamodb, v√† c√°c user c·ªßa app th√¨ sign-in s·ª≠ d·ª•ng GG, FB. C·∫ßn c√°i g√¨ ƒë·ªÉ app c√≥ th·ªÉ get temporary access to dynamodb?\n 1 c√°i role ƒë·ªÉ app c√≥ th·ªÉ access v√†o dynamodb, user s·∫Ω ph·∫£i assume c√°i role ƒë√≥\n Q. Mu·ªën all traffic ra/v√†o Redshift ko ƒëi ra internet, l√†m n√†o?\n Enable Redshift Enhanced VPC routing\n Q. 1 cty ƒëang c√≥ 1 set HyperV machines and VMware virtual machine. Mu·ªën migrate to cloud. C·∫ßn d√πng c√°i g√¨?\n aWS server Migration service\n Q. Trust advisor ƒë·ªÉ l√†m g√¨?\n l√† tool ƒë·ªÉ cung c·∫•p real time guidance to provison resource theo best practice\n Q. 1 cty c√≥ nhi·ªÅu data tr√™n on premise. H·∫øt storage, cty mu·ªën 1 gi·∫£i ph√°p aws ƒë·ªÉ d·ªÖ d√†ng extension data to aws. N√≥i chung gi·∫£i ph√°p ƒë·ªÉ migrate ho·∫∑c transfer data on-premise to s3.\n Ch·ªçn aws gateway cached volumes\nPh√¢n bi·ªát aws gateway cached volume v√† aws gateway stored volume:\nAws gateway cached volume: l∆∞u data tr√™n s3, 1 c·ª•c c·∫ßn access th∆∞·ªùng xuy√™n s·∫Ω l∆∞u locally. d·ªÖ scale, ƒë·ªô tr·ªÖ th·∫•p\nAws gateway stored volume: data s·∫Ω l∆∞u ho√†n to√†n ·ªü locally, r·ªìi snapshot s·∫Ω dc async l√™n s3\n(gi·∫£i ph√°p cho vi·ªác backup recover data cho local data center c·ªßa m√¨nh ho·∫∑c tr√™n aws)\nGateway-virtual tape library (VTL): data l∆∞u ·ªü Glacier\n Q. 1 Cty mu·ªën l∆∞u document v√† ƒë·ªÅ ph√≤ng tr∆∞·ªùng h·ª£p b·ªã x√≥a. D√πng g√¨?\n S3, enable versioning\n Q. 1 cty c√≥ 1 app deliver object t·ª´ s3 ƒë·∫øn user. user ph·∫£n h·ªìi l√† respone time ch·∫≠m. Ph·∫£i l√†m sao ƒë·ªÉ gi·∫£m respone time v√† ti·∫øt ki·ªám chi ph√≠?\n ƒë·ªÉ s3 bucket sau CloudFront distribution. N·∫øu workload ch·ªß y·∫øu l√† GET request, th√¨ s·ª≠ d·ª•ng Cloudfront s·∫Ω cache request l·∫°i, gi·∫£m request t·ªõi S3, reduce cost.\nKh√¥ng n√™n d√πng S3 replication cross regions v√¨ t·ªën ti·ªÅn h∆°n.\nS3 Transfer Acceleration c≈©ng t·ªën ti·ªÅn h∆°n.\nKh√¥ng d√πng ELB v·ªõi s3 dc v√¨ ch·ªâ ƒë·ªÉ balance traffic cho ec2.\n Q. ƒê·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± message v√† dublicated message ko dc g·ª≠i th√¨ n√™n dung?\n SQS FIFO, ko n√™n d√πng SNS v√¨ SNS c√≥ th·ªÉ g·ª≠i message duplicated\n Q. 1 cty c√≥ 1 v√†i ec2 ƒë·ªÉ qu·∫£n l√Ω 1 web app. C·∫ßn gi·∫£i ph√°p ƒë·ªÉ fault tolerant?\n Ensure c√°c Ec2 ·ªü c√°c Az ri√™ng r·∫Ω. D√πng ELB ƒë·ªÉ distribute traffic, attach ELB v√†o ASG\n Q. 1 cty store log tr√™n s3, c√≥ y√™u c·∫ßu l√† c·∫ßn search ·ªü trong ƒë·ªëng data trong s3 ƒë·∫•y. L√†m n√†o?\n AWS Athena ƒë·ªÉ query tr√™n s3 bucket, Load data v√†o Amazon Elasticsearch ƒë·ªÉ search\n Q. Cty b·∫°n dung KMS qu·∫£n l√Ω key ƒë·ªÉ n√≥ encrypt decrypt data, gi·ªù b·∫°n mu·ªën secure h∆°n n√™n b·∫°n enable ch·ª©c nƒÉng rotate key. th√¨ sao?\n KMS s·∫Ω rotate key h√†ng nƒÉm v√† s·ª≠ dung 1 key ph√π h·ª£p ƒë·ªÉ encrypt/decrypt data (KMS keep c√°c key c≈©)\n Q. RDS MySQL size v√† Aurora Size?\n RDS MySQL up to 16TB\nAurora 10GB to 64TB\n Q. Cognito c√≥ th·ªÉ t·∫°o th√™m identity provider dc ko?\n c√≥\n Q. ƒê·ªÉ allow SSH cho 1 ec2, c·∫ßn config SG v√† NACL nh∆∞ n√†o?\n SG: Inbound (allow), Outbound (deny)\nNACL: Inbound (allow), Outbound (allow)\nv√¨ SG l√† stateful, nh·ªØng request incoming n·∫øu dc granted th√¨ outgoing request c≈©ng dc granted\nc√≤n NACL stateless, n·∫øu config cho grant nh·ªØng incoming request th√¨ c≈©ng ph·∫£i config grant cho nh·ªØng outgoing request n·ªØa\n Q. 1 cty ƒëang c√≥ 1 web distribution v√† dung Cloudfront. IT confirm l√† c√°i web n√†y ƒëang thu·ªôc scope PCI compliance. C·∫ßn l√†m g√¨?\n Enable Cloudfront access log, Capture nh·ªØng requests m√† dc g·ª≠i t·ªõi CloudFront API\nV√¨ ƒëang run PCI or HIPAA-compliant workloads based on the AWS Shared Responsibility Model, AWS recommend b·∫°n log l·∫°i CloudFront usage data c·ªßa 365 ng√†y g·∫ßn nh·∫•t ƒë·ªÉ audit trong t∆∞∆°ng lai, ƒë·ªÉ log usage data th√¨ c·∫ßn:\nEnable Cloudfront access log, Capture nh·ªØng requests m√† dc g·ª≠i t·ªõi CloudFront API\n Q. VPC Flow log l√†m g√¨?\n capture th√¥ng tin IP traffic ƒëi ra ƒëi v√†o Network interface trong VPC th√¥i, ko c√≥ Cloudfront\n\u0026mdash;P3\u0026mdash;\n Q. 1 cty d√πng RDS, mu·ªën encrypt data at rest, l√†m n√†o?\n Enable t√≠nh nƒÉng encryption ngay t·ª´ l√∫c t·∫°o DB ( ch·ª© t·∫°o r·ªìi ko enable dc n·ªØa).\nCh·ªçn ƒë√∫ng lo·∫°i instance class support vi·ªác encrypt v√¨ ko ph·∫£i lo·∫°i n√†o c≈©ng support (v√≠ d·ª• General purpose db.m1.small/medium/large/xlarge ko th·ªÉ encrypt).\nc√≥ 1 trick ƒë·ªÉ encrypt db c√≥ s·∫µn l√†: Ban ƒë·∫ßu b·∫°n c√≥ 1 db ch∆∞a encrypt, t·∫°o b·∫£n snapshot c·ªßa n√≥, encrypt b·∫£n snapshot ƒë√≥, restore l·∫°i t·ª´ snapshot ra DB instance -\u0026gt; v·∫≠y l√† ƒë√£ c√≥ DB instance dc encrypt.\nDB ƒë√£ b·∫≠t encryption th√¨ ko th·ªÉ disable encryption dc.\nMu·ªën encrypt read replicas th√¨ ph·∫£i d√πng c√πng 1 key v·ªõi DB instance.\nRDS encryption c≈©ng not available cho nh·ªØng DB instance ch·∫°y SQL Server express.\n Q. DynamoDB c√≥ HA ko?\n c√≥, t·ª± ƒë·ªông replicated in multi AZ. N·∫øu mu·ªën ·ªü level region th√¨ b·∫≠t t√≠nh nƒÉng global table.\n Q. 1 DB host tr√™n aws ƒëang b·ªã v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng write v√† ko th·ªÉ handle load. Sao ƒë·ªÉ ch·∫Øc ch·∫Øn s·ªë l∆∞·ª£ng write ko b·ªã m·∫•t trong b·∫•t c·ª© tr∆∞·ªùng h·ª£p n√†o?\n D√πng SQS FIFO ƒë·ªÉ query db writes,\nHo·∫∑c add th√™m IOPS v√†o existing volume d√πng cho DB (Tuy nhi√™n IOPS c√≥ gi·ªõi h·∫°n 40k th√¥i, trong khi SQS c√≥ th·ªÉ handle 120k message in flight)\n Q. B·∫°n ƒëang host data tr√™n Aurora. Mu·ªën nh·ªØng data n√†y available in another region in case disaster th√¨ l√†m n√†o?\n Create read replica of Aurora in cross regions.\nC·∫£ RDS v√† Aurora ƒë·ªÅu c√≥ th·ªÉ t·∫°o read replica cross regions\n Q. B·∫°n qu·∫£n l√Ω nh·ªØng user c·ªßa t·ªï ch·ª©c l·ªõn, h·ªç ƒëang move nhi·ªÅu service to AWS, b·∫°n mu·ªën c√°c user c√≥ th·ªÉ nhanh ch√≥ng login v√† s·ª≠ d·ª•ng cloud service, B·∫°n ƒë·ªãnh d√πng AWS Managed Microsoft AD, c·∫ßn l√†m g√¨?\n B·∫°n c·∫ßn setup VPN ho·∫∑c Direct Connect ƒë·ªÉ sau ƒë√≥ AWS MS AD c√≥ th·ªÉ s·ª≠ d·ª•ng dc cho c·∫£ Cloud v√† Onpremise\n Q. Cty b·∫°n d√πng Route53 as DNS Provider, C·∫ßn tr·ªè DNS c·ªßa cty t·ªõi CloudFront th√¨ l√†m n√†o?\n T·∫°o Alias record point t·ªõi CloudFront. Ch·ªçn Alias ch·ª© ko ch·ªçn CNAME record v√¨ Alias th√¨ c√≥ th·ªÉ t·∫°o cho c·∫£ root domain v√† sub domain, trong khi CNAME th√¨ ch·ªâ c√≥ th·ªÉ t·∫°o cho sub domain.\n Q. 1 cty ƒëang c√≥ c√°c app d√πng Docker containers. C·∫ßn move l√™n aws, setup nh·ªØng Docker containers ƒë√≥ tr√™n c√°c m√¥i tr∆∞·ªùng ri√™ng bi·ªát, l√†m n√†o?\n D√πng Elastic Beanstalk, ho·∫∑c D√πng ECS (Nh∆∞ng Beanstalk c√≥ ∆∞u ƒëi·ªÉm n·∫øu 1 Docker container ƒëang running b·ªã crash or killed, Beanstalk s·∫Ω t·ª± ƒë·ªông restart)\n Q. 1 cty c√≥ 1 workflow l√† send video t·ª´ on-premise sang AWS ƒë·ªÉ transcoding. H·ªç d√πng ec2 pull job from SQS. V√¨ sao d√πng SQS?\n ƒê·ªÉ gi√∫p vi·ªác scaling theo chi·ªÅu ngang c·ªßa c√°c task. ƒê√≥ l√† m·ª•c ƒë√≠ch ch√≠nh, ngo√†i ra th√¨ n√≥ SQS c≈©ng ƒë·∫£m b·∫£o th·ª© t·ª± message (SQS FIFO)\n Q. 1 cty mu·ªën extend onpremise to cloud. H·ªç mu·ªën communication gi·ªØa 2 m√¥i tr∆∞·ªùng possible over internet. L√†m n√†o?\n T·∫°o VPN connection gi·ªØa 2 onpremise v√† AWS\nKo th·ªÉ d√πng AWS Direct connect v√¨ Direct connect ko d√πng internet\n Q. 1 Cloudfront d√πng ƒë·ªÉ distribute content from S3. Y√™u c·∫ßu l√† ch·ªâ cho 1 s·ªë user tr·∫£ ph√≠ v√†o xem n·ªôi dung content th√¥i, l√†m n√†o?\n T·∫°o Cloudfront signed URLs v√† distribute n√≥ cho User\n Q. B·∫°n c√≥ 1 cty nh·ªè, ch·ªâ c·∫©n chuy·ªÉn resource to cloud nh∆∞ AWS workspace, AWS Workmail. C·∫ßn solution ƒë·ªÉ user management v√† set policy. D√πng AWS Directory Service n√†o?\n Simple AD l√† ƒë·ªß (support c·∫£ WIndow workload v√† Linux workload)\nKO c·∫ßn d√πng AD Connector v√¨ b·∫°n ko c√≥ on premise app.\nKo c·∫ßn Cognito, AWS Managed Microsoft AD v√¨ 2 c√°i ·∫•y nhi·ªÅu h∆°n nh·ªØng g√¨ b·∫°n c·∫ßn.\n Q. C√°c data trong S3 mu·ªën avalablity in another regions l√†m n√†o?\n Enable cross region cho S3 bucket\n Q. 1 Cty ƒëang c√≥ y√™u c·∫ßu 1 model Stack-based cho resource tr√™n AWS. C·∫ßn c√≥ nh·ªØng stack kh√°c nhau cho Dev v√† Prod. D√πng g√¨?\n AWS Opswork. N√≥ s·∫Ω gi√∫p b·∫°n manage app tr√™n AWS v√† Onpremise. T·∫°o c√°c stack bao g·ªìm c√°c Layer (layer LB, DB, App).\n1 Stack basic cho web server th√¨ g·ªìm: 1 set instance cho application server (qu·∫£n l√Ω incoming traffic), 1 LB instance (distribute traffic to apllication server), 1 DB instance (backend)\n1 Common practice l√† nhi·ªÅu stack cho c√°c m√¥i tr∆∞·ªùng: Dev stack ƒë·ªÉ develop, add feature, fix bug\u0026hellip;Staging stack ƒë·ªÉ verify update, hotfix. Production stack ƒë·ªÉ publish v√† handle request of customer.\n Q. B·∫°n c√≥ 1 app ƒëang deploy tr√™n 2 Az, d√πng 1 LB v√† ASG. Mu·ªën l√∫c n√†o c≈©ng available 100% n·∫øu 1 Az godown th√¨ n√™n l√†m n√†o?\n Deploy tr√™n 3 Az v·ªõi ASG minimum set 50% load per zone. -\u0026gt; N·∫øu set l√† 33% m·ªói zone th√¨ khi 1 Az go down th√¨ max ch·ªâ c√≥ 66% load dc handle, c√≤n 34% ko handle dc (ko th·ªÉ ƒë·ªß 100%)\n Q. 1 app ƒëang host tr√™n ec2 v√† ƒëang ch·ªçn EBS type, data tr√™n volume n√†y dc access kho·∫£ng 1 tu·∫ßn r·ªìi sau ƒë√≥ dc move to infrequent access storage. D√πng lo·∫°i EBS n√†o ƒë·ªÉ cost efficiency?\n Ch·ªâ access kho·∫£ng 1 tu·∫ßn -\u0026gt; ko access th∆∞·ªùng xuy√™n -\u0026gt; EBS Cold HDD\n Q. 1 KH mu·ªën import virtual machine c√≥ s·∫µn l√™n cloud. D√πng c√°i g√¨?\n VM import/export\nKh√¥ng d√πng AWS Import/Export v√¨ ƒë·∫•y l√† service ƒë·ªÉ transport data to Cloud.\nKh√¥ng d√πng AWS Storage Gateway v√¨ ƒë·∫•y l√† service connect an on-premise to cloud-based, cung c·∫•p access v√†o s3 object nh∆∞ l√† file.\nKh√¥ng d√πng DB Migration Service v√¨ ƒë·∫•y l√† service ƒë·ªÉ migrate data t·ª´ ho·∫∑c ƒë·∫øn c√°c c·ªü s·ªü DB th∆∞∆°ng m·∫°i ho·∫∑c open.\n Q. B·∫°n c√≥ 1 app m√† application user s·∫Ω initiate TCP request to server, server respone c√°c request ƒë√≥. H·ªá th·ªëng g·ªìm c√°c VPC ·ªü c√°c Az kh√°c nhau trong 1 region. Gi·ªù c·∫ßn thi·∫øt l·∫≠p connection gi·ªØa c√°c VPC ƒë·ªÉ c√≥ 1 solution high scalable, and secure. D√πng c√°i g√¨?\n AWS PrivateLink v√† Network LB.\nKo d√πng IGW v√¨ ko secure.\nKo d√πng VPC peering v√¨ n√≥ s·∫Ω cho all resource c·ªßa VPC n√†y access v√†o resource VPC kh√°c, trong khi app m√¨nh ch·ªâ c·∫ßn user initiate request t·ªõi server th√¥i.\nKo d√πng VPN v√¨ ko th·ªÉ scalable.\n Q. B·∫°n c·∫ßn host static web tr√™n aws. N√™n d√πng c√°i g√¨? C√≥ n√™n d√πng RDS v√† EC2 ƒë·ªÉ store/host data ko?\n S3 v√† DynamoDB\n Q. B·∫°n ƒëang l∆∞u data tr√™n s3, gi·ªù c·∫ßn all data ph·∫£i encrypt at rest. L√†m sao?\n Enable serverside encryption on s3.\n Q. 1 KH c√≥ 3TB data on-premise, tƒÉng 500GB 1 nƒÉm. Kh√°ch ƒëang ng√†y c√†ng b·ªã h·∫°n ch·∫ø b·ªüi dung l∆∞·ª£ng data local tƒÉng l√™n. Mu·ªën backup 1 ph·∫ßn data nh∆∞ng v·∫´n duy tr√¨ ƒë·ªô tr·ªÖ th·∫•p v·ªõi nh·ªØng data access th∆∞·ªùng xuy√™n. N√™n d√πng AwS storage Gateway n√†o?\n Gateway-cached volume + schedule snapshot to s3 (l∆∞u data tr√™n s3, 1 c·ª•c access th∆∞·ªùng xuy√™n th√¨ ƒë·ªÉ local)\nko n√™n d√πng Gateway storage volume v√¨ lo·∫°i n√†y l∆∞u to√†n b·ªô data locally (trong khi KH ng√†y c√†ng b·ªã h·∫°n ch·∫ø b·ªüi dung l∆∞·ª£ng data local)\n Q. C·∫ßn deploy app to aws. C·∫ßn monitor web app log ƒë·ªÉ ph√°t hi·ªán nguy hi·ªÉm. D√πng g√¨?\n Cloudwatch log v√† Cloudtrail\nCloudtrail m·ª•c ƒë√≠ch ch√≠nh l√† : who did what on AWS? (trace log c·ªßa aws account activity tr√™n console, cli, sdk)\nCloudwatch m·ª•c ƒë√≠ch ch√≠nh l√†: what happening on AWS? by logging all event c·ªßa service or app\n Q. Y√™u c·∫ßu nh·ªØng ng ch·ªãu tr√°ch nhi·ªám cho Development instances th√¨ ko dc quy·ªÅn access v√†o Production instance. L√†m n√†o?\n Define tag cho 2 Prod v√† Dev, add condition v√†o IAM Policy ch·ªâ cho ph√©p nh·ªØng tag c·ª• th·ªÉ\n\u0026mdash;P4\u0026mdash;\n Q. 1 cty host DB tr√™n RDS, c√≥ Read replica, c√°c b·∫£n report c·ªßa app dc gen tr√™n Read replica, Nh∆∞ng ƒë√¥i l√∫c report show data c≈©. V√¨ sao?\n Do replication lag, ƒë·ªô tr·ªÖ khi async gi·ªØa origin v·ªõi replica\n Q. B·∫°n setup Cloudtrail cho cty, y√™u c·∫ßu l√† log n√†y c·∫ßn dc encrypt, l√†m sao?\n Ch·∫≥ng c·∫ßn l√†m g√¨, v√¨ log c·ªßa Cloudtrail default dc encypted r·ªìi (n√≥ d√πng Server side encrypt SSE)\nC√≥ th·ªÉ d√πng KMS, ho·∫∑c store tr√™n S3\n Q. 1 cty d√πng S3 l√† t·∫ßng Data layer, c√≥ c√°c request ƒë·∫øn s3 ƒë·ªÉ read/write/update object, User ph·∫£n √°nh l√† sometime c√°c update ko dc ph·∫£n √°nh. V√¨ sao?\n V√¨ Update object s3 l√† Eventual consistency model (y·∫øu), c√≥ 1 kho·∫£ng time delay, c·ª© F5 l√† cu·ªëi c√πng s·∫Ω th·∫•y update th√¥i\n Q. B·∫°n b·∫≠t CORS cho S3 bucket r·ªìi, b·∫°n ƒë√£ nh·∫≠p 3 domains l√† origin ƒë·ªÉ allow r·ªìi. Nh∆∞ng khi test th·∫•y 2 method OPTION v√† CONNECT ko work, trong khi c√°c method kh√°c ƒë·ªÅu work, v√¨ sao?\n B·ªüi v√¨ CORS S3 bucket ch·ªâ support 5 method l√† GET POST PUT DELETE HEAD th√¥i\n Q. 1 cty c√≥ 1 contest cho user up ·∫£nh thi. Cu·ªôc thi d√†i 2 tu·∫ßn, ·∫£nh s·∫Ω d√πng ƒë·ªÉ ph√¢n t√≠ch trong 3 th√°ng. C√°c file s·∫Ω dc access 1 l·∫ßn r·ªìi x√≥a. D√πng C√°i g√¨ cho kinh t·∫ø, c√≥ th·ªÉ scale?\n S3 IA\n Q. 1 cty host nhi·ªÅu data tr√™n on premise, v√† gi·ªù mu·ªën backup this data l√™n AWS, l√†m n√†o?\n D√πng Storage Gateway Stored volumes (data v·∫´n ·ªü local v√† dc async to s3)\n Q. 1 cty mu·ªën move Postgres SQL to AWS, h·ªç mu·ªën c√≥ c√°c replicas v√† auto backup, D√πng c√°i n√†o? Aurora PostgresSQL hay RDS PostgresSQL?\n Aurora (c≈©ng ƒë√°p ·ª©ng dc nhu c·∫ßu nh∆∞ RDS nh∆∞ng Aurora c√≥ performance g·∫•p 3 l·∫ßn RDS)\n Q. User trong cty c·∫ßn n∆°i l∆∞u documents, m·ªói user c·∫ßn c√≥ 1 location ri√™ng, v√† c√°c user kh√°c ko dc view documents c·ªßa nhau. D·ªÖ dang retrive docs c·ªßa m√¨nh, l√†m n√†o?\n S3 (c√≥ th·ªÉ restrict user ƒë·∫øn t·ª´ng folder file tr√™n s3)\n Q. 1 cty d√πng Glacier ƒë·ªÉ archive data. Mu·ªën time ƒë·ªÉ retrieved data trong kho·∫£ng 3-5h th√¨ d√πng c√°i feature n√†o c·ªßa Glacier?\n Standard retrieval (3-5h)\nBulk retrieval (5-12h)\nExpedited retrieval (1-5min)\n Q. Mu·ªën repicate current resource AWS to another region l√†m n√†o?\n Cloudformation\n Q. B·∫°n c√≥ c√°c IIS Server ch·∫°y tr√™n EC2. Gi·ªù mu·ªën collect v√† process logs file dc gen t·ª´ EC2 ra, l√†m n√†o?\n Store logs tr√™n S3 v√† D√πng AWS EMR ƒë·ªÉ process log file\nko n√™n d√πng DynamoDB ƒë·ªÉ l∆∞u log files\n Q. B·∫°n mu·ªën d√πng AWS host website, www.example.com. C·∫ßn deploy quickly v√† ko c·∫ßn server-side script. C√°i g√¨ n√†o?\n ƒêƒÉng k√Ω 1 domain b·∫±ng Route53 v√† verify tr∆∞·ªõc r·∫±ng S3 bucketname match v·ªõi domain s·∫Ω ƒëƒÉng k√Ω tr√™n R53\n Q. B·∫°n l√† SA, qu·∫£n l√Ω 1 web server, d√πng Cloudtrail v√† store all cloud trail event log files v√†o S3 bucket. Auditor c·ªßa AWS n√≥i r·∫±ng ko ƒë·∫£m b·∫£o t√≠nh x√°c th·ª±c c·ªßa nh·ªØng file log l∆∞u tr√™n s3 bucket ƒë√≥, ko c√≥ g√¨ ƒë·∫£m b·∫£o nh·ªØng file Cloudtrail log ƒë·∫•y ko b·ªã ch·ªânh s·ª≠a. L√†m n√†o?\n Enable t√≠nh nƒÉng Cloudtrail log file integrity validation.\nKhi b·∫≠t l√™n th√¨ n√≥ s·∫Ω t·∫°o 1 hash file (digest file) cho m·ªói Cloudtrail log file dc sinh ra. N√≥ dc save ·ªü 1 folder kh√°c c√πng bucket. M·ªói file c√≥ 1 public key v√† private key. N√≥ ƒë·∫£m b·∫£o r·∫±ng t·∫•t c·∫£ c√°c nh·ªØng ch·ªânh s·ª≠a ƒë·ªëi v·ªõi Cloudtrail log file ƒë·ªÅu dc record l·∫°i.\nD√πng SSE (serverside-encryption) l√† sai v√¨ n√≥ l√† m·∫∑c ƒë·ªãnh c·ªßa cloudtrail log r·ªìi, nh∆∞ng ko ƒë·∫£m b·∫£o dc t√≠nh authenticity c·ªßa log files.\nKMS c≈©ng v·∫≠y, ko ƒë·∫£m b·∫£o dc t√≠nh authenticity (x√°c th·ª±c).\n Q. B·∫°n c·∫ßn ensure l√† data tr√™n s3 dc encryted nh∆∞ng ko mu·ªën qu·∫£n l√Ω encryption key. D√πng c√°i g√¨?\n SSE-S3, v√¨ S3 s·∫Ω qu·∫£n l√Ω c·∫£ data key v√† master key.\nko d√πng SSE-KMS v√¨ KMS th√¨ n√≥ qu·∫£n l√Ω data key b·∫°n ph·∫£i qu·∫£n l√Ω master key.\n Q. 1 Cty ƒëang c√≥ Redshift cluster tr√™n aws. C·∫ßn monitor performance v√† ensure ƒë·∫•y l√† performance hi·ªáu qu·∫£ nh·∫•t c√≥ th·ªÉ th√¨ d√πng g√¨?\n Cloudwatch v√† AWS trusted advisor\n Q. Web c·ªßa b·∫°n d√πng s3 v√† cloudfront ƒë·ªÉ cache. B·∫°n t·∫°o 1 cloudtrail ·ªü m·ªói region v√† c√°c event log file dc l∆∞u v√†o s3 bucket ·ªü us-west-1. C√≥ 1 thay ƒë·ªïi c·ªßa cloudfront l√†m all traffic route to origin, tƒÉng ƒë·ªô tr·ªÖ cho user. Ph√°t hi·ªán ra l√† v√¨ c√≥ nh·ªØng event log b·ªã doublicate ·ªü Cloudfront. L√†m sao?\n D√πng AWS CLI ƒë·ªÉ ƒë·ªÉ disable global service event ƒëang deliver log file to all region ngo·∫°i tr·ª´ us-west-1\nC√°i n√†y ko th·ªÉ d√πng AWS Console dc.\n Q. Cloudfront Query string forwarding c·∫ßn ch√∫ √Ω g√¨?\n Ch·ªâ support web distribution (ko support RMTP distribution - Real Time Message Protocol).\nPh√¢n c√°ch gi·ªØa c√°c query string parameter ph·∫£i l√† k√Ω t·ª± \u0026amp;.\nParam name v√† Param value l√† case-sensitive (ph√¢n bi·ªát hoa th∆∞·ªùng).\n Q. 1 IOT sensor ƒë·ªÉ ƒë·∫øm s·ªë bag ·ªü s√¢n bay. Data g·ª≠i t·ªõi kinesis v·ªõi default setting. M·ªói alternate-day (48h) data dc send to S3 ƒë·ªÉ processs. Nh·∫≠n ra l√† S3 ko nh·∫≠n all data t·ª´ kinesis stream, v√¨ sao?\n V√¨ kinesis default ch·ªâ l∆∞u data 24h (c√≥ th·ªÉ setting 168h)\n Q. C∆° ch·∫ø l√†m vi·ªác c·ªßa AWS CloudHSM l√† g√¨?\n Ephemeral backup key (EBK) d√πng ƒë·ªÉ encrpyt data, Persistent backup key (PBK) ƒë·ªÉ encrpyt EBK sau ƒë√≥ save to S3 in same region v·ªõi CloudHSM.\nD·ªãch v·ª• CloudHSM n√†y g·∫ßn gi·ªëng v·ªõi KMS nh∆∞ng d√πng kh√°i ni·ªám key kh√°c nhau th√¥i.\n Q. B·∫°n qu·∫£n l√Ω app tr√™n AWS. G·ªìm ec2, route53, CLB, ASG. C·∫ßn d√πng blue-green deployment th√¨ c·∫ßn d√πng policy n√†o c·ªßa R53?\n Weighted Policy\n Q. (from Airbus documents) AWS Direct Connect c√≥ encrypt data ko?\n ko. V√¨ n√≥ s·ª≠ d·ª•ng m·∫°ng ri√™ng v√† private ho√†n to√†n. Mu·ªën encrypt data in transit th√¨ d√πng th√™m VPN\n Q. B·∫°n dev 1 app d√πng AWS Polly (text to speech), b·∫°n mu·ªën test nh∆∞ sau,ch·ªØ S3 ƒë·∫ßu ti√™n th√¨ ƒë·ªçc l√† \u0026ldquo;Simple Storage Service\u0026rdquo;, c√≤n nh·ªØng ch·ªØ S3 sau ƒë√≥ th√¨ ƒë·ªçc l√† \u0026ldquo;S3\u0026rdquo; th√¥i. C√°i case n√†y c·∫ßn test ·ªü 2 region us-east v√† us-west. L√†m n√†o?\n D√πng nhi·ªÅu Lexicons, t·∫°o c√°c alias kh√°c nhau cho t·ª´ \u0026ldquo;S3\u0026rdquo;, v√† apply theo th·ª© t·ª± kh√°c nhau. R·ªìi upload Lexicons ƒë√≥ l√™n c·∫£ 2 regions us-east v√† us-west\n Q. AWS polly m√† b·∫°n d√πng cho web app c·ªßa b·∫°n b·ªã user ph·∫£n h·ªìi l√† ƒë·ªçc qu√° nhanh v√† li√™n t·ª•c. L√†m sao?\n Convert d·∫•u ph·∫©y trong content th√†nh period.\nAdd a pause b·∫±ng c√°ch s·ª≠ d·ª•ng \u0026lt;break\u0026gt; tag gi·ªØa c√°c ƒëo·∫°n ph√π h·ª£p.\nAdd \u0026lt;emphasis\u0026gt; tag v·ªõi value = Strong v·ªõi t·ª´ ph√π h·ª£p ƒë·ªÉ nh·∫•n m·∫°nh.\n Q. B·∫°n c√≥ 1 web server cung c·∫•p docs l∆∞u s3, c√≥ d√πng Cloudfront, nh∆∞ng ko mu·ªën user c√≥ th·ªÉ access tr·ª±c ti·∫øp v√†o s3. L√†m n√†o?\n T·∫°o OAI (origin access identity) cho Cloudfront v√† grant access v√†o obbject trong s3 cho CFront.\nL√†m ƒëi·ªÅu n√†y th√¨ User ch·ªâ c√≥ th·ªÉ access v√†o s3 object b·∫±ng c√°ch ƒëi t·ª´ Cloudfront, ch·ª© ko th·ªÉ ƒëi tr·ª±c ti·∫øp ƒë·∫øn s3.\nB·∫°n t·∫°o Cfront signned URL/cookie ƒë·ªÉ access v√†o S3 bucket, sau ƒë√≥ t·∫°o 1 Cloudfront User ƒë·∫∑c bi·ªát g·ªçi l√† OAI. R·ªìi config ƒë·ªÉ Cfront ƒë·ªÉ n√≥ d√πng OAI access v√† serve file tr√™n S3 (tr√™n S3 bucket c·∫ßn config permission ƒë·ªÉ ch·ªâ OAI c√≥ quy·ªÅn access v√†o c√°c object). Nh·ªØng user th√¥ng th∆∞·ªùng s·∫Ω ko th·ªÉ s·ª≠ d·ª•ng S3 URL ƒë·ªÉ access S3 n·ªØa.\n Q. App c·ªßa b·∫°n ƒëang l√†m Load test. DB b·∫°n d√πng RDS MySQL. App ko ph·∫£n h·ªìi khi CPU 100%. App th√¨ l√† lo·∫°i read-heavy. L√†m n√†o?\n Add Read replicas.\nAdd Elasticache to RDS.\nShard your data set among nhi·ªÅu RDS DB instances (Sharding l√† 1 concept split data th√†nh nhi·ªÅu table trong 1 database).\nKo th·ªÉ d√πng ASG v·ªõi RDS dc.\n\u0026mdash;P5\u0026mdash;\n Q. Mu·ªën m·ªói file up l√™n s3 ph·∫£i dc l∆∞u file name v√†o DynamodB l√†m n√†o?\n S3 trigger tr·ª±c ti·∫øp event cho Lambda. (ko c·∫ßn qua Cloudwatch)\n Q. 1 app c·∫ßn DB tr√™n aws. Read/write on DB ch·ªâ c·∫ßn s·ªë nh·ªè th√¥i. Th√¨ n√™n ch·ªçn c√°i n√†o? EBS General Purpose SSD hay EBS Throughput Optimized HDD?\n d√πng EBS General Purpose SSD.\nEBS Throughput Optimized HDD (max throughput/volume=500M/s)\nEBS General Purpose SSD (max throughput/volume=250M/s)\n Q. B·∫°n c√≥ 1 web d√πng Network LB save log v√†o S3. User ph√†n n√†n v·ªÅ delay? L√†m n√†o ƒë·ªÉ t√¨m nh·ªØng Client IP c√≥ th·ªùi gian delay khi access (TLS handshake time)?\n AWS Athena.\nko c·∫ßn d√πng Quicksight v√¨ ƒë·∫•y ch·ªâ c·∫ßn trong TH mu·ªën visualization/t·∫°o report th√¥i.\n Q. 1 cty upload 1 l∆∞·ª£ng l·ªõn data to S3 v√† li√™n t·ª•c t·ª´ nhi·ªÅu n∆°i tr√™n TG. D√πng Athena ƒë·ªÉ query data ƒë√≥ v√† Quicksight ƒë·ªÉ t·∫°o report daily. Nh∆∞ng thi tho·∫£ng b·ªã l·ªói exception t·ª´ s3. Cost c≈©ng m·∫•t nhi·ªÅu ti·ªÅn, L√†m sao ƒë√¢y?\n Partition data based on date v√† location.\nT·∫°o 1 workgroup ri√™ng base on user group.\nV√¨ gi√° ti·ªÅn Athena base tr√™n s·ªë query v√† l∆∞·ª£ng data scan dc m·ªói query. N√™n c·∫ßn chia data ra m√† query.\n Q. 1 Cty c·∫ßn store 10TB file ƒë√£ scan. Y√™u c·∫ßu 1 search application ƒë·ªÉ search c√°c file ƒë√≥, l√†m n√†o?\n Store ·ªü S3 standard redundancy, v√† d√πng CloudSearch ƒë·ªÉ query, Beanstalk ƒë·ªÉ host webssite multiAZ.\nAmazon CloudSearch: B·∫°n t·∫°o 1 domain v√† upload data l√™n ƒë√≥ ƒë·ªÉ search.\n Q. 1 Cty d√πng AWS Glue v√† Athena ƒë·ªÉ analyze data, c√≥ upload data 1 l∆∞·ª£ng l·ªõn l√™n S3 daily v√† li√™n t·ª•c. ƒê·ªÉ gi·∫£m th·ªùi gian scan data th√¨ c·∫ßn ensure l√† ch·ªâ scan c√°c changes in dataset th√¥i. L√†m sao?\n Enable Job Bookmark in AWS Glue.\nN·∫øu Reset Job Bookmark s·∫Ω reprocess all data.\nN·∫øu Disable Job Bookmark s·∫Ω process all data each time.\nN·∫øu Pause Job Bookmark th√¨ m·ªói l·∫ßn scan n√≥ s·∫Ω scan t·ª´ l·∫ßn cu·ªëi Bookmark dc enable.\n Q. 1 Cty s·ª£ dev c√≥ th·ªÉ x√≥a Ec2 resource trong m√¥i tr∆∞·ªùng Production, l√†m sao?\n Tag production resource v√† deny quy·ªÅn x√≥a resource c√≥ tag Prod.\nT·∫°o ri√™ng 1 AWS Account v√† move developers v√†o account ƒë√≥.\n Q. B·∫°n ƒëang D√πng AWS X-ray ƒë·ªÉ tracing, setting g√¨ ƒë·ªÉ cost in limit?\n Sampling at low rate\n Q. 1 cty lo ng·∫°i v·ªÅ t√≠nh redundancy c·ªßa EBS Volume m√† h·ªç d√πng, l√†m n√†o? redundancy: l√† ph∆∞∆°ng ph√°p l∆∞u 1 th√¥ng tin ·ªü nhi·ªÅu ·ªï, khi c√≥ s·ª± c·ªë th√¨ c√≥ th·ªÉ nhanh ch√≥ng l·∫•y l·∫°i\n Default l√† EBS ƒë√£ dc replicated trong Az r·ªìi ƒë·ªÉ c√≥ HA v√† Durability (n√™n ko c·∫ßn lo ng·∫°i v·ªÅ t√≠nh redundancy)\n Q. Mu·ªën x√¢y d·ª±ng h·ªá th·ªëng decoubled n√™n d√πng service n√†o?\n SQS v√† SNS.\nko ph·∫£i ELB hay ASG.\n Q. 1 Cty d√πng AWS WorkDocs ƒë·ªÉ share document vs third party. B·ªã leak th√¥ng tin nh·∫°y c·∫£m n√™n gi·ªù team security c·∫ßn ph√¢n quy·ªÅn l·∫°i nh∆∞ n√†o?\n ch·ªâ cho quy·ªÅn cho Power users c√≥ kh·∫£ nƒÉng invite ng m·ªõi.\nCh·ªâ cho Power user c√≥ kh·∫£ nƒÉng share publicly.\n Q. B·∫°n ƒëang d√πng SQS, ƒë√¥i khi b·ªã l·ªói client get price tr∆∞·ªõc khi login v√†o site, n√™n b·∫°n chuy·ªÉn sang d√πng SQS FIFO, c·∫ßn l√†m tr∆∞·ªõc g√¨ ƒë·ªÉ vi·ªác migrate sang SQS FIFO d·ªÖ d√†ng?\n M·ªói FIFO Queue c·∫ßn c√≥ Message group ID b·∫•t k·ªÉ khi n√†o.\nV·ªõi nh·ªØng message bodies m√† \u0026ldquo;gi·ªëng h·ªát nhau\u0026rdquo; (identical): h√£y d√πng unique deduplication ID\nV·ªõi nh·ªØng message bodies m√† \u0026ldquo;unique\u0026rdquo;: h√£y d√πng content-based deduplication ID. (khi ƒë√≥ SQS d√πng SHA-256 generate ra deduplication ID)\n Q. 1 cty mu·ªën d√πng 1 ph∆∞∆°ng th·ª©c deployment t·ª± ƒë·ªông nh∆∞ t·∫°o LAMP stack, download latest PHP t·ª´ s3, setup ELB. D√πng c√°i g√¨? Beanstalk hay Cloudformation?\n Beanstalk. V√¨ Beanstalk ƒë·ªÉ deploy app c√≤n CF ƒë·ªÉ t·∫°o resource\n Q. B·∫°n D√πng EMR ƒë·ªÉ run big data framwork, mu·ªën gi·∫£m cost th√¨ l√†m g√¨?\n D√πng Spot instance for underlying node\n Q. 1 application cho traders in market, c√≥ th·ªÉ c√≥ multiple trading c·ªßa client. B·∫°n c√≥ nhi·ªÅu ec2 ƒë·ªÉ process c√°c trading n√†y song song. V√† m·ªói trade ph·∫£i stateful v√† x·ª≠ l√Ω ƒë·ªôc l·∫≠p. D√πng SQS v·ªõi setting g√¨?\n SQS FIFO Queue s·ª≠ d·ª•ng Message Group ID\nMessage Group ID: ch·ªâ ƒë·ªãnh cho 1 group c√°c messages, nh·ªØng msg thu·ªôc c√πng group s·∫Ω lu√¥n process l·∫ßn l∆∞·ª£t. ƒê·ªÉ xen k·∫Ω c√°c message group trong 1 single FIFO queue (V√≠ d·ª• session data c·ªßa nhi·ªÅu user), trong scenario ƒë√≥ nhi·ªÅu user c√≥ th·ªÉ process queue nh∆∞ng session data c·ªßa m·ªói user v·∫´n dc process theo FIFO manner.\nReceive Request Attempt ID: d√πng ƒë·ªÉ retry same request nh·∫≠n ƒë∆∞·ª£c trong TH SDK th·ª±c hi·ªán b·ªã l·ªói\nDeduplication ID: N·∫øu 1 msg dc g·ª≠i th√†nh c√¥ng, c√°c msg sau v·ªõi c√πng Deduplication ID s·∫Ω ƒëc nh·∫≠n nh∆∞ng ko dc deliver trong v√≤ng 5 minutes.\n Q. Ph√≤ng IT y√™u c√¢u all IP traffic in out c·ªßa 1 EC2 c·∫ßn monitor, d√πng c√°i g√¨?\n VPC Flow log. S·∫Ω monitor all IP traffic ƒë·∫øn v√† ra kh·ªèi ENI c·ªßa VPC b·∫°n.\nko d√πng CloudTrail v√¨ n√≥ ƒë·ªÉ: ai made API call? khi n√†o, call c√°i g√¨?\n B·∫°n c√≥ RDS PostgreSQL. B·∫°n c·∫ßn backup DB v√† dc asynchronous copy. D√πng c√°i g√¨?  Enable Read Replicas for DB. Khi ƒë√≥ b·∫°n ph·∫£i ch·ªâ ƒë·ªãnh DB, r·ªìi AWS s·∫Ω t·∫°o snapshot c·ªßa DB instance, r·ªìi t·∫°o read-only instance t·ª´ snapshot. R·ªìi n√≥ d√πng Async ƒë·ªÉ update Replicas.\nKo d√πng Enable Multi-AZ v√¨ n√≥ s·∫Ω d√πng synchronous ch·ª© ko ph·∫£i asynchronous.\n Q. B·∫°n d√πng AWS Batch Job, data l∆∞u trong S3, trigger Lambda, submit AWS Batch Job in a queue. Queue s·ª≠ d·ª•ng EC2 v√† ECS ƒë·ªÉ compute. Job c·ªßa b·∫°n b·ªã stuck ·ªü tr·∫°ng th√°i RUNNABLE, V√¨ sao?\n C√°c kh·∫£ nƒÉng c√≥ th·ªÉ x·∫£y ra v·ªõi vi·ªác stuck ·ªü RUNNABLE:\nawslogs log driver ch∆∞a dc configure trong EC2/ECS.\ninsufficient resources: job c·ªßa b·∫°n define 4GB memory nh∆∞ng ec2 ko c√≥ ƒë·ªß.\nno internet access v·ªõi EC2/ECS.\ns·ªë l∆∞·ª£ng EC2 ƒë·∫øn limit.\nC√°c tr·∫°ng th√°i (state) c·ªßa Job:\nSUBMIITED: 1 job v·ª´a dc submit v√†o queue, ch∆∞a dc evaluated,\nPENDING: job c√°c ch·ªù c√°c dependencies v·ªõi job kh√°c ho·∫∑c resource kh√°c,\nRUNNABLE: job c√≥ th·ªÉ ch·∫°y nh∆∞ng maybe stuck v√¨ nh·ªØng l√Ω do tr√™n,\nSTARTING: container image dc pull v·ªÅ v√† running,\nRUNNING: job ƒëang ch·∫°y,\nSUCCEEDED: job exit with code 0,\nFAILED: job exit failed\n Q. C√≥ 1 l∆∞·ª£ng l·ªõn data onpremise c·∫ßn l√™n S3, D√ông g√¨ ƒë·ªÉ transfer?\n Direct Connect, Snowball\n Q. B·∫°n v·ª´a t·∫°o 1 Redshift cluster, ƒëang mu·ªën d√πng SQL client from EC2 connect ƒë·∫øn Redshift cluster nh∆∞ng ko dc. L√†m n√†o?\n S·ª≠a VPC SG (Security Group)\n Q. 1 cty c·∫ßn block level storage l∆∞u 800GB data. C·∫ßn encrypt data, n√™n d√πng c√°i g√¨? EBS? Glacier? EFS? S3?\n EBS\nGlacier v√† S3: object level storage\nEFS: file level storage\n Q. B·∫°n d√πng AWS Batch Job. 1 s·ªë job th√¨ c·∫ßn l√†m nhanh v√¨ quan tr·ªçng, 1 s·ªë job th√¨ ko quan tr·ªçng l·∫Øm. N√™n setting Job queue cho ƒë·ª° t·ªën ti·ªÅn nh∆∞ n√†o?\n Nhi·ªÅu jobs, 1 job th√¨ c√≥ EC2 ondemand instance, high priority, 1 job th√¨ dung Spot instance v·ªõi low priority\n Q. EC2 b·ªã restart li√™n t·ª•c, gi·ªù c·∫ßn check log v√† analyze system log. D√πng c√°i g√¨?\n AWS Cloudwatch Logs\nko d√πng Cloudtrail dc.\n Q. B·∫°n d√πng RDS MySQL l√†m DB layer, gi·ªù mu·ªën tƒÉng connection t·ªõi DB th√¨ l√†m n√†o?\n t·∫°o 1 DB parameter group, s·ª≠a c√°i max connection theo √Ω m√¨nh, r·ªìi attach c√°i DB parameter m·ªõi ƒë√≥ v√†o DB instance\nDB parameter group default th√¨ ko th·ªÉ s·ª≠a dc, ph·∫£i t·∫°o c√°i m·ªõi, r·ªìi s·ª≠a, r·ªìi attach v√†o DB\n Q. B·∫°n c√≥ 1 Redis cluster deploy in VPC ·ªü us-east-1. C·∫ßn secure c√°c access to Redis t·ª´ EC2 ·ªü 1 VPC kh√°c c√πng region th√¨ l√†m n√†o?\n Enable Redis AUTH with in-transit encryption cho Cluster\nVPC peering\nKh√¥ng n√™n d√πng VPC transit v√¨ ƒë√≥ l√† gi·∫£i ph√°p cho TH 2 VPC ·ªü kh√°c region.\nKh√¥ng d√πng VPN Connection v√¨ c√°i ƒë√≥ l√† gi·∫£i ph√°p cho on-premise Server.\n Q. B·∫°n d√πng Cloudfront cho 1 web distribute media content. Dc user truy c·∫≠p th∆∞·ªùng xuy√™n. Gi·ªù mu·ªën tƒÉng performance l√†m n√†o?\n TƒÉng th·ªùi gian cache expiration l√™n.\n Q. Disaster Recovery solution m√† cost ph·∫£i minimum th√¨ d√πng DR mechanism n√†o? Backup and Restore, Pilot Light, Warm standby, Multi-Site ?\n Backup \u0026amp; Restore\n Q. 1 cty c√≥ 1 s·ªë EC2 sau 1 ELB, gi·ªù mu·ªën dc notification m·ªói khi latency v∆∞·ª£t qu√° 10s. L√†m n√†o?\n Enable log on ELB v·ªõi Latency alarm s·∫Ω send mail v√† ph√¢n t√≠ch log b·∫•t c·ª© khi n√†o c√≥ issue\n Q. 1 cty mu·ªën integrate data on premise l√™n AWS storage. Y√™u c·∫ßu l√† all data v·∫´n ph·∫£i keep low latency. N√™n l√†m n√†o?\n Storage Gateway Stored Volume (all data v·∫´n ·ªü local, r·ªìi async backup snapshot l√™n s3)\nKo d√πng Storage Gateway Cached Volume v√¨ y√™u c·∫ßu all data v·∫´n ph·∫£i low latency\n Q. 1 cty c√≥ 1 web infrastructure tr√™n aws, (EC2,ELB,RDS) mu·ªën h·ªá th·ªëng dc self-healing th√¨ c·∫ßn g√¨?\n self-healing = HA\nB·∫≠t Multi-AZ feature c·ªßa RDS l√™n.\nD√πng Cloudwatch metric check utilization c·ªßa web layer, r·ªìi theo ƒë√≥ m√† ƒëi·ªÅu ch·ªânh ASG\n Q. Nh·ªØng c√°i AWS Config c√≥ th·ªÉ l√†m?\n -Evaluate aws resource configuration\n-L·∫•y snapshot c·ªßa config hi·ªán t·∫°i c·ªßa c√°c resource nh·∫•t ƒë·ªãnh\n-Retrieve configuration c·ªßa resource -Retrieve history config c·ªßa resource\n-Nh·∫≠n noti khi resource dc t·∫°o, s·ª≠a, x√≥a\n-Xem m·ªëi quan h·ªá gi·ªØa c√°c resource\n\u0026mdash;P6\u0026mdash;\n Q. B·∫°n mu·ªën t·∫°o 1 instance m·ªõi ·ªü 1 Az kh√°c, EBS volume c·ªßa instance m·ªõi c·∫ßn l·∫•y t·ª´ volume c·ªßa 1 instance c≈©, l√†m n√†o?\n t·∫°o snapshot c·ªßa EBS c≈© r·ªìi t·∫°o volume m·ªõi t·ª´ snapshot ƒë√≥\nƒê·ªÉ c√≥ th·ªÉ t·∫°o 1 volume available trong 1 AZ m·ªõi th√¨ c·∫ßn t·∫°o snapshot c·ªßa c√°i c≈© r·ªìi t·ª´ snapshot ƒë√≥ t·∫°o volume m·ªõi.\nKo th·ªÉ detach volume c≈© r·ªìi g·∫Øn v√†o c√°i instance m·ªõi dc v√¨ Kh√°c AZ\n Q. 1 Cty mu·ªën move container-based app l√™n AWS. C·∫ßn 1 service m√† h·ªç ko ph·∫£i qu·∫£n l√Ω infra cho orchestration service. D√πng g√¨? ECS fargate hay Beanstalk?\n ECS Fargate, v√¨ Beanstalk ko cung c·∫•p service qu·∫£n l√Ω orchestration\n Q. 1 cty v·ª´a develop 1 app m·ªõi, h·ªç d√πng Codepipeline gi·ªù source stage th√¨ ch·ªçn serice n√†o? v√† deploy stage n√™n ch·ªçn service n√†o?\n source stage: CodeCommit\ndeploy stage: Beanstalk\ndeploy stage ko d√πng CodeDeploy v√¨ ko c√≥ exist infrastructure, m√† c√°c resource c·∫ßn dc deploy m·ªõi, v√¨ v·∫≠y d√πng Beanstalk\nCodeDeploy ch·ªâ deploy app l√™n 1 EC2/onpremise server ƒë√£ c√≥ s·∫µn (n√≥ ko provision ra c√°c resource m·ªõi)\n Q. 1 cty d√πng CodePipeline, source code l∆∞u S3, thay ƒë·ªïi tr√™n s3 s·∫Ω trigger CodepiPeline d√πng Beanstalk ƒë·ªÉ provision resource, C·∫ßn config g√¨ ƒë·ªÉ trigger CodePipeline nhanh h∆°n?\n ƒê·ªÉ trigger Pipeline auto khi c√≥ thay ƒë·ªïi S3 th√¨ c·∫ßn Disable periodic checks c·ªßa Pipeline\nv√† t·∫°o Cloudwatch event rule, t·∫°o Cloudtrail trail ƒë·ªÉ detect thay ƒë·ªïi.\nC∆° ch·∫ø nh∆∞ sau:\nM·ªói khi S3 c√≥ thay ƒë·ªïi, event dc filter trong Cloudtrail r·ªìi Cloudwatch event s·∫Ω trigger start Pipeline. Nh·ªõ l√† CodePipeline ph·∫£i disable c√°i Periodic check ƒë·ªÉ n√≥ d√πng event-based trigger\nC√≥ nhi·ªÅu c√°ch ƒë·ªÉ start 1 Pipeline c·ªßa AWS Codepipeline (b√™n tr√™n l√† khi d√πng S3 l√†m source):\nN·∫øu d√πng CodeCommit l√†m source: C·∫ßn d√πng Cloudwatch Event rule detect thay ƒë·ªïi, disable periodic check\nN·∫øu d√πng Github l√†m source: C·∫ßn d√πng Webhook ƒë·ªÉ detect thay ƒë·ªïi, c≈©ng disable periodic check\nN·∫øu d√πng ECR l√†m source: C·∫ßn d√πng Cloudwatch Event Rule ƒë·ªÉ detect thay ƒë·ªïi, periodic m·∫∑c ƒë·ªãnh ko th·ªÉ enable ƒë∆∞·ª£c.\n Q. 1 cty c√≥ c√°c ph√≤ng ban, m·ªói ph√≤ng ban 1 AWS account, c·∫ßn c√°ch ƒë·ªÉ security policy dc ƒë·∫£m b·∫£o theo level account, d√πng c√°i g√¨?\n AWS Organizations v√† Service control policies\nAWS Organizations gi√∫p qu·∫£n l√Ω nhi·ªÅu account 1 c√°ch centrally. Service control policies s·∫Ω cho ph√©p b·∫°n define nh·ªØng AWS API n√†o c√≥ th·ªÉ/ko th·ªÉ call b·ªüi IAM thu·ªôc Acount trong Organization. SCPs dc t·∫°o b·ªüi master account, l√† account d√πng ƒë·ªÉ t·∫°o ra Organization\n Q. B·∫°n mu·ªën reboot instance n·∫øu status check FAILED , l√†m n√†o?\n D√πng Cloudwatch alarm action\n Q. C√¥ng ty b·∫°n l∆∞u data tr√™n EFS. C·∫ßn ensure l√† all client access v√†o EFS ph·∫£i dc encrypt b·∫±ng TLS 1.2. C√°ch n√†o ƒë·ªÉ seccure data in transit khi access v√†o EFS?\n D√πng EFS mount helper ƒë·ªÉ encrypt data in transit (c√°ch n√†y ti·ªán v√† d·ªÖ h∆°n c√°ch 2)\nD√πng EFS m√† mu·ªën encrypt data in transit b·∫±ng TLS th√¨ c√≥ 2 c√°ch:\nc√°ch 1 l√† Mount EFS v·ªõi mount helper\nsudo mount -t efs -o tls fs-12345678:/ /mnt/efs\nc√°ch 2 l√† ko d√πng mount helper, th√¨ s·∫Ω ph·∫£i download v√† install stunnel, run stunnel on port 2049, d√πng NFS mount localport v√†o port c·ªßa application.\n Q. Khi t·∫°o VPC Peering c·∫ßn ch√∫ √Ω g√¨?\n C√°c VPC ko b·ªã overlapping CIDR block\nKo c√≥ y√™u c·∫ßu g√¨ v·ªÅ onpremise communication\n Q. B·∫°n c·∫ßn t·∫°o connection site-to-site VPN t·ª´ on premise network l√™n 1 AWS VPC. C·∫ßn ch√∫ √Ω g√¨?\n Virtual private gateway attach v√†o VPC\nTrong Customer Gateway c√≥ public IP c·ªßa on-premise network\n Q. C√¥ng ty b·∫°n mu·ªën encrypt data at rest v√† mu·ªën to√†n quy·ªÅn qu·∫£n l√Ω key v√† lifecycle c·ªßa key, th√¨ d√πng service g√¨?\n CloudHSM, b·∫°n t·ª± gen ra encryption key v√† qu·∫£n l√Ω key ƒë√≥\nko d√πng SSE S3, KMS v√¨ nh·ªØng c√°i ƒë√≥ ko cho b·∫°n to√†n quy·ªÉn qu·∫£n l√Ω key (SSE S3 th√¨ n√≥ full managed key, KMS th√¨ n√≥ manage data key v√† lifecycle c·ªßa key c√≤n b·∫°n manage master key)\n Q. B·∫°n c√≥ c√°c b·∫£n snapshot c·ªßa EBS volume, n√™n gi·ªØ nh·ªØng b·∫£n n√†o? original snapshot v√† latest snapshot hay c·∫£ 2?\n Ch·ªâ c·∫ßn gi·ªØ 1 latest snapshot v√¨ n√≥ v·ª´a l√† incremental v√† complete snapshot r·ªìi\n Q. B·∫°n ƒëang c√≥ 1 EC2 m1.small 300Gb EBS General purpose volume host 1 relational DB. Mu·ªën tƒÉng throughput c·ªßa DB th√¨ l√†m n√†o?\n D√πng lo·∫°i volume Provisioned IOPS volumes ho·∫∑c D√πng 1 EC2 l·ªõn h∆°n m1.small\n Q. Cty b·∫°n d√πng RDS instances, b·∫°n c·∫ßn disable automate backup ƒë·ªÉ save cost. N·∫øu disable th√¨ c·∫ßn ch√∫ √Ω g√¨?\n B·∫°n ƒëang disable t√≠nh nƒÉng point-in-time recovery\nRDS default l√† t·ª± ƒë·ªông t·∫°o b·∫£n storage volume snapshot c·ªßa to√†n b·ªô DB instance 1 ng√†y 1 l·∫ßn. Retention c≈©ng l√† 1 ng√†y. (c√≥ th·ªÉ s·ª≠a th√†nh 0-35 days)\nN·∫øu disable t√≠nh nƒÉng t·ª± ƒë·ªông n√†y th√¨: Khi disable r·ªìi re-enable l·∫°i, b·∫°n ch·ªâ c√≥ th·ªÉ restore t·ª´ th·ªùi ƒëi·ªÉm re-enable t√≠nh nƒÉng th√¥i, ko th·ªÉ restore th·ªùi ƒëi·ªÉm tr∆∞·ªõc khi disable n·ªØa\n Q. 1 cty c√≥ 1 web app. C·∫ßn user across world access v√†o site v√†o xem c√°c pages c·ªßa site v·ªõi ƒë·ªô tr·ªÖ th·∫•p nh·∫•t, d√πng g√¨? Route53 v·ªõi latency routing hay Cloudfront?\n ƒë·∫∑t Cloudfront l√†m distribution trc web app\nko d√πng R53 v·ªõi latency-based routing v√¨ c√°i ƒë·∫•y d√πng cho multi site v√† latency based s·∫Ω routing gi·ªØa c√°c sites.\nko hi·ªÉu gi·∫£i th√≠ch c·ªßa c√¢u n√†y\n Q. 1 Cty c√≥ 1 web app, d√πng Route53 ƒë·ªÉ qu·∫£n l√Ω DNS. Configure R53 th·∫ø n√†o ƒë·ªÉ ensure l√† custom domain point to LB?\n Ensure l√† hosted zone dc t·∫°o\nT·∫°o 1 alias record for a CNAME record to Load balancer DNS Name\n Q. 1 Cty ƒëang c√≥ 1 workflow l√† g·ª≠i c√°c video t·ª´ on premise l√™n aws ƒë·ªÉ ec2 th·ª±c hi·ªán transcoding b·∫±ng c√°ch pull from SQS. SQS dc s·ª≠ d·ª•ng ƒë·ªÉ c√≥ vai tr√≤ g√¨ trong h·ªá th·ªëng ƒë√≥? ƒê·∫£m b·∫£o th·ª© t·ª± c·ªßa msg √†? hay ƒë·ªÉ scaling c√°c task encoding theo chi·ªÅu ngang?\n ƒë·ªÉ scaling c√°c task encoding theo chi·ªÅu ngang\nv√¨ vi·ªác ƒë·∫£m b·∫£o order ko ƒë·ªÅ c·∫≠p ƒë·∫øn trong c√¢u h·ªèi\n Q. 1 Cty c·∫ßn deploy 1 app l√™n 1 s·ªë ec2, y√™u c·∫ßu low latency gi·ªØa c√°c ec2 n√†y? l√†m n√†o?\n Cluster Placement Group\n Q. B·∫°n design 1 web app host tr√™n ec2 sau 1 ELB. C·∫ßn consider c√°i g√¨ ƒë·ªÉ x√¢y d·ª±ng web ƒë√≥?\n C·∫ßn x√°c ƒë·ªãnh I/O operations\nX√°c ƒë·ªãnh minimum memory c·∫ßn cho app\nCh·ª© c√≤n peak usage cho client th√¨ ko c·∫ßn v√¨ ELB s·∫Ω take care c√°i ƒë√≥\n Q. Network Load balancer c√≥ support custom security policies ko? default security policy c·ªßa n√≥ l√† g√¨?\n NLB ko support custom security policies\ndefault security policy c·ªßa NLB l√† k·∫øt h·ª£p Protocols \u0026amp; Ciphers\n Q. B·∫°n l√† AWS consultant cho shop online. 2 tier web app, web server tr√™n AWS, data th√¨ on premise data centre. D√ông NLB, all traffic gi·ªØa client v√† server dc encrypted. ƒê·ªÉ gi·∫£m load v√†o backend server, h·ªç t√¨m 1 solution ƒë·ªÉ terminate TLS connection c·ªßa NLB. H·ªç mu·ªën 1 solution ƒë·ªÉ qu·∫£n l√Ω certificate s·ª≠ d·ª•ng khi terminate TLS connection c·ªßa NLB.\n D√πng 1 single certificate cho m·ªói TLS listener (cung c·∫•p b·ªüi AWS Certificate Manager)\nDoc c·ªßa AWS vi·∫øt, B·∫°n c·∫ßn ch·ªâ ƒë·ªãnh ch√≠nh x√°c 1 server certificate cho m·ªói TLS Listener. LB s·∫Ω d√πng cert ƒë√≥ d·ªÉ terminate connection v√† decrypt request c·ªßa client tr∆∞·ªõc khi g·ª≠i request ƒë√≥ ƒëi.\nELB s·ª≠ d·ª•ng security policy l√† \u0026ldquo;TLS\u0026rdquo; ƒë·ªÉ negotiate connection gi·ªØa client v√† LB. 1 security policy n√†y bao g·ªìm Protocols v√† Ciphers.\nProtocol d√πng ƒë·ªÉ thi·∫øt l·∫≠p connection gi·ªØa client v√† server, ƒë·∫£m b·∫£o data gi·ªØa client v√† LB dc private.\n1 Cipher l√† 1 thu·∫≠t to√°n encrypt ƒë·ªÉ t·∫°o nh∆∞ng message dc m√£ h√≥a.\nProtocol s·ª≠ d·ª•ng cipher ƒë·ªÉ encrypt data.\nQu√° tr√¨nh negotiate connection, B√™n Client v√† b√™n LB s·∫Ω ƒë∆∞a ra c√°c list cipher v√† protocols m√† m·ªói b√™n support, c√°i cipher ƒë·∫ßu ti√™n c·ªßa b√™n Server match v·ªõi b√™n Client s·∫Ω dc ch·ªçn l√† k·∫øt n·ªëi an to√†n (secure connection)\nNLB ch·ªâ support only single certificate per TLS Listener th√¥i.\nNLB ko support nh·ªØng certificate l·ªõn h∆°n 2048 bits\nAWS Certificate Manager s·∫Ω qu·∫£n l√Ω certificate cho b·∫°n, t·ª± ƒë·ªông renew khi cert h·∫øt h·∫°n\n Q. AWS Managed Blockchain, gi·ªØa c√°c member th√¨ 1 member c√≥ format resource enpoint l√† g√¨?\n ResourceID.MemberID.NetworkID.managedblockchain.us-east-1.amazonaws.com:PortNumber\nC√°c kh√°i ni·ªám:\nm·ªói member trong network blockchain l∆∞u 1 b·∫£n copy c·ªßa s·ªï c√°i (ledger) ·ªü local\nvi·ªác add member v√†o blockchain network c·∫ßn th√¥ng qua proposal v√† qu√° tr√¨nh vote. 1 member trong network t·∫°o proposal ƒë·ªÉ invite 1 account v√†o m·∫°ng, nh·ªØng member kh√°c vote, n·∫øu proposal dc approve th√¨ s·∫Ω g·ª≠i invitation ƒëi t·ªõi account kia.\nvi·ªác remove member c≈©ng v·∫≠y. Tuy nhi√™n Principal (quy·ªÅn to) c·ªßa network c√≥ th·ªÉ remove member ko c·∫ßn t·∫°o proposal v√† voting\nm·ªói member ph·∫£i tr·∫£ ti·ªÅn membership khi v√†o network, v√† c·∫£ ti·ªÅn peer node h·ªç t·∫°o ra, storage v√† l∆∞·ª£ng data dc write v√†o network\nblockchain network ch·ªâ b·ªã delete khi m√† member cu·ªëi c√πng t·ª± x√≥a kh·ªèi m·∫°ng\nkhi 1 member join network, vi·ªác ƒë·∫ßu ti√™n ph·∫£i l√†m l√† t·∫°o peer node (m·ªói peer node l∆∞u 1 local copy c·ªßa s·ªï c√°i ledger)\n Q. Khi 1 ec2 t·ª´ Running -\u0026gt; hibernate (stop) -\u0026gt; Running th√¨ c√°c lo·∫°i IP thay ƒë·ªïi nh∆∞ n√†o? Private IPv4 \u0026amp; IPv6, Public IPv4?\n Private IPv4 \u0026amp; IPv6 s·∫Ω retain, c√≤n Public IPv4 th√¨ release r·ªìi renew\n Q. Khi 1 EC2 v√†o tr·∫°ng th√°i hibernate (stop theo ki·ªÉu hibernate ch·ªâ c√≥ tr√™n c√°c d√≤ng EC2 M3, M4, M5, C3, C4, C5, R3, R4, R5, v·ªõi OS Amazon Linux 1, v√† type On demand v√† Reserved) th√¨ s·∫Ω t√≠nh ti·ªÅn nh·ªØng c√°i g√¨?\n T√≠nh ti·ªÅn EIP (N·∫øu c√≥) v√† storage space of EBS volume\nko t√≠nh ti·ªÅn compute capacity (c√¥ng su·∫•t t√≠nh to√°n)\n\u0026mdash;P7\u0026mdash;\n Q. Mu·ªën EC2 khi m√¨nh terminate v·∫´n gi·ªØ l·∫°i EBS volume th√¨ l√†m n√†o?\n ch·ªçn c√°i DeleteOnTermination for EBS = False\n Q. B·∫°n d√πng RDS MySQL, C·∫ßn ch·∫Øc ch·∫Øn l√† ho·∫°t ƒë·ªông backup DB s·∫Ω ko g√¢y ƒë·ªô tr·ªÖ cho nh·ªØng I/O operation b√¨nh th∆∞·ªùng l√™n DB. N√™n l√†m sao?\n ensure ƒë√£ enable Multi-AZ cho RDS\nQu√° tr√¨nh automate backup c·ªßa DB s·∫Ω l√†m I/O activity treo 1 th·ªùi gian ng·∫Øn kho·∫£ng v√†i gi√¢y. V·ªõi MariaDB, MySQL, Oracle, and PostgreSQL th√¨ ko suspend primary db v√¨ n√≥ backup tr√™n DB standby, not primary.\nV·ªõi SQL server th√¨ s·∫Ω b·ªã suspended 1 time ng·∫Øn\nAutomate Backup ch·ªâ th·ª±c hi·ªán v·ªõi DB instance ·ªü tr·∫°ng th√°i ACTIVE\nKhi x√≥a DB instance n·∫øu b·∫°n ch·ªçn \u0026ldquo;Retain automated backups\u0026rdquo; th√¨ n√≥ s·∫Ω gi·ªØ l·∫°i backup, c√≤n n·∫øu ko ch·ªçn th√¨ n√≥ x√≥a h·∫øt v√† ko th·ªÉ recover\n Q. 1 app dc x√¢y d·ª±ng nh∆∞ sau: 1 nh√≥m ec2 accept video upload from users, 1 nh√≥m ec2 process videos ƒë√≥. L√†m g√¨ ƒë·ªÉ ki·∫øn tr√∫c ƒë√≥ tr·ªü th√†nh excellent?\n t·∫°o SQS queue ƒë·ªÉ l∆∞u th√¥ng tin video uploads. C√°c ec2 process video g·∫Øn v√†o ASG. Vi·ªác scale ASG d·ª±a tr√™n size c·ªßa queue.\nDoc c·ªßa AWS vi·∫øt:\n1 ASG qu·∫£n l√Ω c√°c EC2 process message from sqs\n1 custom metric g·ª≠i ƒë·∫øn Cloudwatch s·ªë l∆∞·ª£ng msg trong queue per ec2\n1 policy c·ªßa ASG, scale d·ª±a tr√™n custom metric, cloudwatch alarm invoke scaling policy ƒë√≥\nc√°i policy c·∫ßn c√≥ 1 value ƒë·ªÉ d·ª±a tr√™n value ƒë√≥ scale ASG, th√¨ c√°ch t√≠nh nh∆∞ sau:\nGi·∫£ s·ª≠ hi·ªán t·∫°i SQS c√≥ ApproximateNumberOfMessages = 1500 msg, capacity (s·ªë l∆∞·ª£ng ec2 process) = 10 instances.\nN·∫øu th·ªùi gian trung b√¨nh x·ª≠ l√Ω 1 msg = 0.1s, v√† ƒë·ªô tr·ªÖ l·ªõn nh·∫•t c√≥ th·ªÉ ch·∫•p nh·∫≠n (longest acceptable latency) = 10s, th√¨ acceptable backlog per instance = 10/0.1 = 100.\n100 s·∫Ω l√† target value cho c√°i policy c·ªßa b·∫°n.\nNh∆∞ng hi·ªán t·∫°i backlog per instance = 1500/10 = 150.\nSuy ra c·∫ßn gi·∫£m backlog per instance xu·ªëng 100, mu·ªën v·∫≠y th√¨ ph·∫£i scale out th√†nh 1500/100 = 15 instances. 15-10 = 5 instances s·∫Ω dc ASG t·∫°o th√™m.\n Q. 1 cty s·∫Øp host 1 app tr√™n aws. H·ªç mu·ªën load balance traffic d·ª±a tr√™n which route user choose. 2 route c·ªßa app l√† /customer v√† /orders. N√™n d√πng lo·∫°i ELB n√†o?\n D√πng lo·∫°i c√≥ path-based routing, ƒë√≥ l√† ALB\n Q. App c·ªßa b·∫°n t∆∞∆°ng t√°c v·ªõi DynamoDB, ƒëang c√≥ Read capacity l√† 10. G·∫ßn ƒë√¢y ph√°t hi·ªán hay b·ªã throttling ·ªü DynamoDB, l√†m sao?\n Enable Autoscaling cho DynamoDB\nƒêi·ªÅu n√†y s·∫Ω khi·∫øn khi traffic tƒÉng l√™n, n√≥ s·∫Ω provision read/write capacity ƒë·ªÉ handle traffic m√† ko b·ªã throttling.\nN·∫øu b·∫°n ch·ªâ ƒë∆°n gi·∫£n tƒÉng Read Capacity c·ªßa table l√™n 20 th√¨ ch·ªâ l√† gi·∫£i ph√°p t·∫°m th·ªùi.\n Q. B·∫°n l∆∞u data documents tr√™n S3, ko truy c·∫≠p th∆∞·ªùng xuy√™n, nh∆∞ng khi truy c·∫≠p th√¨ mu·ªën access ƒëc trong kho·∫£ng 20p, c·∫ßn d√πng lo·∫°i g√¨ cho ti·∫øt ki·ªám?\n S3 IA\nko d√πng Glacier Bulk retrieval (5-12h), Glacier Standard retrieval (3-5h)\n Q. B·∫°n c√≥ 1 app ch·∫°y tr√™n c√°c ec2 trong private subnet, gi·ªù mu·ªën access v√†o Kinessis stream m√† ko ƒëi qua internet, l√†m n√†o?\n T·∫°o VPC Interface Endpoint cho ph√©p Kinesis Streams\nko d√πng VPC Gateway Endpoint v√¨ c√°i ƒë√≥ d√πng cho S3 v√† DynamoDB\nVPC Interface Endpoint dc cung c·∫•p b·ªüi AWS PrivateLink, n√≥ ko y√™u c·∫ßu IGW, NAT device, VPN, Direct Connect, n√≥ cung c·∫•p kh·∫£ nƒÉng comunicate gi·ªØa c√°c AWS service\n Q. C·∫ßn monitor API activity cho m·ª•c ƒë√≠ch audit th√¨ c·∫ßn d√πng service n√†o? Cloudtrail? AWS Config? AWS Inspector?\n Cloudtrail th√¥i\n Q. 1 architecture nh∆∞ sau:\n1 set EC2 l√† web layer ƒë·ª©ng sau 1 CLB\n1 set EC2 l√† proxy server\n1 set EC2 l√† backend server\n c·∫ßn l√†m g√¨ ƒë·ªÉ c√≥ kh·∫£ nƒÉng scalability?\nASG cho proxy server\nASG cho backend server\nko c·∫ßn thay CLB b·∫±ng ALB (Application LB) v√¨ Clasic Load Balancer v·ªën ƒë√£ c√≥ kh·∫£ nƒÉng scalability r·ªìi\n Q. 1 cty c·∫ßn host 1 app tr√™n ec2. C√≥ y√™u c·∫ßu l√† b·∫°n c·∫ßn c√≥ quy·ªÅn control s·ªë l∆∞·ª£ng core d√†nh cho application. D√πng lo·∫°i EC2 n√†o?\n D√πng EC2 - Dedicated Host\nAWS cho b·∫°n to√†n quy·ªÅn tr√™n 1 con server v·∫≠t l√Ω lu√¥n, ko chia s·∫ª v·ªõi ai, cho b·∫°n to√†n quy·ªÅn control s·ªë l∆∞·ª£ng core, socket‚Ä¶, cho b·∫°n quy·ªÅn s·ª≠ d·ª•ng licence c√≥ s·∫µn, ko m·∫•t ti·ªÅn mua license c·ªßa AWS n·ªØa. B·∫°n c√≥ th·ªÉ run nhi·ªÅu instances tr√™n c√°i host ƒë√≥\nnh·ªõ ph√¢n bi·ªát v·ªõi Dedicated Instance:\nEC2 - Dedicated Instance:\nT·∫°o Ec2 tr√™n 1 con server v·∫≠t l√Ω, ƒë·∫£m b·∫£o duy nh·∫•t cho b·∫°n (tu√¢n th·ªß v·ªÅ m·∫∑t compliance)\n Q. Cty b·∫°n l∆∞u docs tr√™n s3, mu·ªën ensure l√† object s3 dc encrypt at rest, l√†m n√†o? c√≥ th·ªÉ l√†m trong ph·∫ßn bucket policy v√† bucket ACL ko?\n ko.\nC√≥ 3 c√°ch encrypt data s3 l√†:\nSSE with S3 managed key\nSSE with KMS managed key\nSSE with Customer-provided key\n Q. 1 cty host 1 active-active site. 1 site tr√™n AWS, 1 site ·ªü Onpremise. C·∫ßn c√°c traffic dc distributed ph√π h·ª£p tr√™n c·∫£ 2 site. D√πng c√°i g√¨ c·ªßa Route53? Simple / Failover / Latency / Weighted Routing ?\n ·ªü ƒë√¢y b·∫°n c√≥ 2 resource.\nch·ªçn Weighted Routing - cho ph√©p b·∫°n associate multiple resource v·ªõi 1 single domain name, ch·ªçn bao nhi√™u traffic s·∫Ω dc route ƒë·∫øn m·ªói resource\nko ch·ªçn m·∫•y c√°i c√≤n l·∫°i:\nV√¨ Simple ch·ªâ d√πng khi b·∫°n mu·ªën configure ƒë∆°n gi·∫£n, 1 resource cho 1 domain, standard DNS record\nFailover d√πng khi b·∫°n mu·ªën route traffic ƒë·∫øn 1 resource khi resource c√≤n l·∫°i unhealthy\nLatency d√πng khi b·∫°n mu·ªën improve performance c·ªßa user theo region ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ\n Q. Gi·ªØa c√°c VPC ·ªü c√°c regions kh√°c nhau, mu·ªën connect v√† ko pass ra internet? l√†m n√†o?\n VPC peering v√† AWS Direct Connect\n Q. App c·ªßa b·∫°n c√≥ web tier v√† db tier tr√™n ec2. ƒêang d√πng General purpose SSD cho volume type. ƒêang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ read write to DB. L√†m n√†o?\n v·∫•n ƒë·ªÅ v·ªÅ read write to DB -\u0026gt; li√™n quan ƒë·∫øn volume type, ko ph·∫£i instance type nh∆∞ t2, m5 etc\u0026hellip;\nc·∫ßn change to Provisioned IOPS SSD\n Q. App c·ªßa cty b·∫°n c√≥ ec2 v·ªõi ALB. Cty mu·ªën b·∫£o v·ªá app kh·ªèi application level attack. D√ông g√¨? Cloudfront/ WAF?\n WAF\nCh·ª© Cloudfront ch·ªâ l√† service ph·ª•c v·ª• CDN Content delivery\n Q. Cty ƒëang setup app nh∆∞ sau: 1 set EC2 host web app, ELB, User access v√†o app qua ELB, app connect to backend db, NAT gateway. N√™n setup c√°c server v√† ELB ·ªü c√°c subnet nh∆∞ n√†o ƒë·ªÉ HA v√† ƒë√∫ng architect?\n 2 public subnet cho ELB, 2 private subnet cho web server, 2 private subnet cho db\nch√∫ √Ω web server ko c·∫ßn ph·∫£i ·ªü ngo√†i public subnet, nh∆∞ v·∫≠y secure h∆°n\n Q. 1 app g·ªìm c√°c componnents, 2 primary component c·∫ßn ch·∫°y 3h m·ªói ng√†y. C√°c components kh√°c ch·∫°y 6-8h m·ªói ng√†y. N√™n d√πng c√°i type instance n√†o cho ti·∫øt ki·ªám?\n On-demand instance cho primary components, Reserved Instance cho c√≤n l·∫°i\nPrimary component ch·ªâ run 3h/day th√¨ ko c·∫ßn d√πng Reserved\nKo d√πng Spot instance v√¨ ko bi·∫øt lo·∫°i workload (data analysis, batch jobs, background processing, and optional tasks?) ƒë·ªÉ quy·∫øt ƒë·ªãnh s·ª≠ d·ª•ng spot\nReserved s·∫Ω dc discount v√† r·∫ª h∆°n On-demand\nC√°c c√¢u h·ªèi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh thi (ch∆∞a add c√¢u tr·∫£ l·ªùi)  Q. 1 cty mu·ªën encrypt data at rest, nh∆∞ng mu·ªën encryption key dc managed ·ªü on-premise. D√πng c√°i n√†o? S3 SSE-C (Server side encryption with customer provided key)? Hay CloudHSM?\n  Q. 1 app c√≥ private subnet v√† public subnet, gi·ªù private subnet c·∫ßn ra ngo√†i internet, th√¨ c·∫ßn l√†m g√¨?\nG·∫Øn IGW v√† update private subnet route table?\nHay g·∫Øn NatGW v√†o public subnet v√† update private route table?\n  Q. NAT Gatway s·∫Ω add v√†o public subnet hay private subnet?\n  Q. Gi·∫£i ph√°p cho web static content, ch·ªçn S3 static website hay cloudfront + s3 bucket as origin?\n  Q. 1 App ghi v√†o s3, user ph·∫£n √°nh hi·ªÉn th·ªã data c≈© v√¨ sao?\nV√¨ app ƒëang read parallel request?\nHay v√¨ app ƒëang read theo range of header?\nHay v√¨ app ƒëang write ki·ªÉu ghi ƒë√® l√™n object c√≥ s·∫µn?\n  Q. App tr√™n Ec2 memory intensive, c·∫ßn config policy base tr√™n cloudwatch metric nh∆∞ n√†o?\nT·∫°o custom metric from app?\nHay t·∫°o high resolution alarm?\n  Q. App d√πng Cloudfront, nh∆∞ng user ph·∫£n √°nh l√† h·ªç ƒëang th·∫•y c√°c data c≈©, c·∫ßn l√†m g√¨?\nPerform cloudfront refresh?\nHay change TTL c·ªßa cache tr√™n cloudfront?\n  Q. App ghi v√†o db b·ªã throttle th√¨ n√™n l√†m g√¨?\nD√πng elastic cache v√† lazy loading?\nHay elasticsearch v√† write-through db?\nHay g·∫Øn ASG cho db?\n  Q. App c√≥ 10 micro service m·ªói c√°i 1 Clasic Load Balancer, Mu·ªën gi·∫£m cost th√¨ n√™n l√†m j?\nThay h·∫øt CLBs b·∫±ng 1 single ALB?\nHay change ASG desired?\n  Q. ƒêang migrate on-premise proxy server l√™n aws, quy·∫øt ƒë·ªãnh d√πng ALB, c·∫ßn secure communication, th√¨ d√πng c√°i g√¨?\nD√πng Third party ƒë·ªÉ qu·∫£n l√Ω certificate manager?\nHay t·∫°o 1 SNI certificate v√† up l√™n ALB?\nHay t·∫°o 1 wildcard certificate v√† up l√™n ALB?\nHay t·∫°o 1 second proxy server ƒë·ªÉ terminate SSL traffic?\n  Q. SG c·∫ßn allow trafic t·ª´ ALB, th√¨ l√†m n√†o?\nG·∫Øn ALB ip v√†o sg?, Hay g·∫Øn subnet ip v√†o sg?, Hay g·∫Øn vpc cidr v√†o sg?\n  Q. 1 app d·ª± ƒëo√°n tu·∫ßn sau s·∫Ω ph·∫£i x2 c√¥ng su·∫•t th√¨ asg config nh∆∞ n√†o?\nD√πng Scheduled scaling policy?\nHay Dynamic Scaling policy?\n  Q. 1 app d√πng EC2 24/7 trong 1 nƒÉm, n√™n ch·ªçn type EC2 n√†o?\nConvertible Reserved EC2?\nStandard Reserved EC2?\nScheduled Reserved EC2?\n  Q. Kh√°i ni·ªám RTO RPO trong Disaster Recovery?\n ","href":"/posts/encrypt-aws-certified-solution-architect-associate-note-saa/","title":"Aws Certified Solution Architect Associate Note (SAA)"},{"content":"B√†i n√†y s·∫Ω gi·∫£i th√≠ch c√°ch Secure API Gateway b·∫±ng Lambda Authorizer ƒë∆°n gi·∫£n.\nN·∫øu API kh√¥ng ƒë∆∞·ª£c secure th√¨ b·∫•t c·ª© ai c√≥ API URL c≈©ng c√≥ th·ªÉ g·ª≠i request ƒë·∫øn server v√† l·∫•y ƒë∆∞·ª£c data trong DynamoDB c·ªßa m√¨nh.\nN√™n c·∫ßn ph·∫£i setup Authorization cho API Gateway.\n1. T·∫°o table trong DynamoDB Table name:¬†authors\nPrimary key:¬†id\nT·∫°o v√†i item trong table authors\n{ \u0026#34;firstName\u0026#34;: \u0026#34;Hoang\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;le-hoang\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Le\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;linoel-messi\u0026#34;, \u0026#34;firstName\u0026#34;: \u0026#34;lionel\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;messi\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;jonesta-smaldini\u0026#34;, \u0026#34;firstName\u0026#34;: \u0026#34;jonesta\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;smaldini\u0026#34; }, { \u0026#34;firstName\u0026#34;: \u0026#34;Fred\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;fred\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;the-red\u0026#34; } 2. T·∫°o role cho Lambda function ƒêo·∫°n code b√™n d∆∞·ªõi b·∫°n c·∫ßn s·ª≠a region v√† accountId ph√π h·ª£p v·ªõi m√¥i tr∆∞·ªùng b·∫°n ƒëang test: v√≠ d·ª• region = us-east-1, accountId = 923423492348\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:region:accountId:table/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } 3. T·∫°o Lambda function \u0026ldquo;get-all-authors-py\u0026rdquo; ƒë·ªÉ get all th√¥ng tin user t·ª´ DynamoDB G·∫Øn Role v·ª´a t·∫°o ·ªü Step 2 v√†o.\nN·ªôi dung function vi·∫øt b·∫±ng python 3.7:\nimport boto3 import json print(\u0026#39;Loading function\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;authors\u0026#39;) def respond(err, res=None): return { \u0026#39;statusCode\u0026#39;: \u0026#39;400\u0026#39; if err else \u0026#39;200\u0026#39;, \u0026#39;body\u0026#39;: err.message if err else json.dumps(res), \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, } def lambda_handler(event, context): scan_response = table.scan() print(scan_response) # return respond(None, scan_response[\u0026#39;Items\u0026#39;]) return scan_response[\u0026#39;Items\u0026#39;] 4. Setup API Gateway T·∫°o resource: /authors\nTrong resource /authors, t·∫°o method GET\nPh·∫ßn Integration Request ch·ªçn Lambda function v·ª´a t·∫°o get-all-authors-py\nBan ƒë·∫ßu ch∆∞a c·∫ßn setup Authorization cho method n√†y n√™n h√£y ƒë·ªÉ NONE (·∫£nh) Test th·ª≠ xem l·∫•y ƒë∆∞·ª£c data l√† OK (·∫£nh) Deploy API\n5. Secure API Gateway b·∫±ng Lambda Authorizer (C√°ch 1, ƒê∆°n gi·∫£n) B·∫£n ch·∫•t l√† m√¨nh s·∫Ω t·∫°o th√™m 1 function Lambda n·ªØa ƒë·ªÉ n√≥ handler c√°c request g·ª≠i t·ªõi API Gateway, nhi·ªám v·ª• l√† x√°c ƒë·ªãnh request ƒë√≥ c√≥ h·ª£p l·ªá kh√¥ng, n·∫øu h·ª£p l·ªá th√¨ m·ªõi t·ª´ API Gateway chuy·ªÉn request ƒë·∫øn Lambda function get-all-authors-py.\n5.1. T·∫°o 1 Lambda function ƒë·ªÉ th·ª±c hi·ªán nhi·ªám v·ª• Authorizer T√™n function: \u0026ldquo;tokenBasedLambdaAuthorizerBasic\u0026rdquo;. G·∫Øn Role v·ª´a t·∫°o ·ªü Step 2 v√†o.\nSource n√†y m√¨nh l·∫•y t·ª´ Blueprint c·ªßa AWS (blueprint l√† nh·ªØng template AWS recommend m√¨nh s·ª≠ d·ª•ng), th√™m 1 ƒëo·∫°n code nh·ªè:\nN·∫øu Header c·ªßa request m√† c√≥ authorizationToken=\u0026ldquo;what-the-fuck\u0026rdquo; th√¨ dc allowAllMethods, ng∆∞·ª£c l·∫°i th√¨ denyAllMethods\n\u0026#34;\u0026#34;\u0026#34; Copyright 2015-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \u0026#34;license\u0026#34; file accompanying this file. This file is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \u0026#34;\u0026#34;\u0026#34; from __future__ import print_function import re import time, datetime import pprint import json def lambda_handler(event, context): print(\u0026#34;Client token: \u0026#34; + event[\u0026#39;authorizationToken\u0026#39;]) print(\u0026#34;Method ARN: \u0026#34; + event[\u0026#39;methodArn\u0026#39;]) \u0026#34;\u0026#34;\u0026#34;validate the incoming token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and produce the principal user identifier associated with the token\u0026#34;\u0026#34;\u0026#34; # Hoang start token=event[\u0026#34;authorizationToken\u0026#34;] principalId = token print(\u0026#34;principalId: \u0026#34; + principalId) # Hoang end \u0026#34;\u0026#34;\u0026#34;this could be accomplished in a number of ways:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;1. Call out to OAuth provider\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;2. Decode a JWT token inline\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;3. Lookup in a self-managed DB\u0026#34;\u0026#34;\u0026#34; # principalId = \u0026#34;user|a1b2c3d4\u0026#34; \u0026#34;\u0026#34;\u0026#34;you can send a 401 Unauthorized response to the client by failing like so:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;raise Exception(\u0026#39;Unauthorized\u0026#39;)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if the token is valid, a policy must be generated which will allow or deny access to the client\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is denied, the client will recieve a 403 Access Denied response\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;this function must generate a policy that is associated with the recognized principal user identifier.\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;depending on your use case, you might store policies in a DB, or generate them on the fly\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and will apply to subsequent calls to any method/resource in the RestApi\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;made with the same token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;the example policy below denies access to all resources in the RestApi\u0026#34;\u0026#34;\u0026#34; tmp = event[\u0026#39;methodArn\u0026#39;].split(\u0026#39;:\u0026#39;) apiGatewayArnTmp = tmp[5].split(\u0026#39;/\u0026#39;) awsAccountId = tmp[4] policy = AuthPolicy(principalId, awsAccountId) policy.restApiId = apiGatewayArnTmp[0] policy.region = tmp[3] policy.stage = apiGatewayArnTmp[1] # Hoang start if (principalId == \u0026#34;what-the-fuck\u0026#34;): policy.allowAllMethods() else: policy.denyAllMethods() # Hoang end \u0026#34;\u0026#34;\u0026#34;policy.allowMethod(HttpVerb.GET, \u0026#34;/pets/*\u0026#34;)\u0026#34;\u0026#34;\u0026#34; # Finally, build the policy authResponse = policy.build() # new! -- add additional key-value pairs associated with the authenticated principal # these are made available by APIGW like so: $context.authorizer.\u0026lt;key\u0026gt; # additional context is cached context = { \u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;, # $context.authorizer.key -\u0026gt; value \u0026#39;number\u0026#39; : 1, \u0026#39;bool\u0026#39; : True } # context[\u0026#39;arr\u0026#39;] = [\u0026#39;foo\u0026#39;] \u0026lt;- this is invalid, APIGW will not accept it # context[\u0026#39;obj\u0026#39;] = {\u0026#39;foo\u0026#39;:\u0026#39;bar\u0026#39;} \u0026lt;- also invalid authResponse[\u0026#39;context\u0026#39;] = context return authResponse class HttpVerb: GET = \u0026#34;GET\u0026#34; POST = \u0026#34;POST\u0026#34; PUT = \u0026#34;PUT\u0026#34; PATCH = \u0026#34;PATCH\u0026#34; HEAD = \u0026#34;HEAD\u0026#34; DELETE = \u0026#34;DELETE\u0026#34; OPTIONS = \u0026#34;OPTIONS\u0026#34; ALL = \u0026#34;*\u0026#34; class AuthPolicy(object): awsAccountId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The AWS account id the policy will be generated for. This is used to create the method ARNs.\u0026#34;\u0026#34;\u0026#34; principalId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The principal used for the policy, this should be a unique identifier for the end user.\u0026#34;\u0026#34;\u0026#34; version = \u0026#34;2012-10-17\u0026#34; \u0026#34;\u0026#34;\u0026#34;The policy version used for the evaluation. This should always be \u0026#39;2012-10-17\u0026#39;\u0026#34;\u0026#34;\u0026#34; pathRegex = \u0026#34;^[/.a-zA-Z0-9-\\*]+$\u0026#34; \u0026#34;\u0026#34;\u0026#34;The regular expression used to validate resource paths for the policy\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;these are the internal lists of allowed and denied methods. These are lists of objects and each object has 2 properties: A resource ARN and a nullable conditions statement. the build method processes these lists and generates the approriate statements for the final policy\u0026#34;\u0026#34;\u0026#34; allowMethods = [] denyMethods = [] restApiId = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The API Gateway API id. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; region = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The region where the API is deployed. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; stage = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The name of the stage used in the policy. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; def __init__(self, principal, awsAccountId): self.awsAccountId = awsAccountId self.principalId = principal self.allowMethods = [] self.denyMethods = [] def _addMethod(self, effect, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds a method to the internal lists of allowed or denied methods. Each object in the internal list contains a resource ARN and a condition statement. The condition statement can be null.\u0026#34;\u0026#34;\u0026#34; if verb != \u0026#34;*\u0026#34; and not hasattr(HttpVerb, verb): raise NameError(\u0026#34;Invalid HTTP verb \u0026#34; + verb + \u0026#34;. Allowed verbs in HttpVerb class\u0026#34;) resourcePattern = re.compile(self.pathRegex) if not resourcePattern.match(resource): raise NameError(\u0026#34;Invalid resource path: \u0026#34; + resource + \u0026#34;. Path should match \u0026#34; + self.pathRegex) if resource[:1] == \u0026#34;/\u0026#34;: resource = resource[1:] resourceArn = (\u0026#34;arn:aws:execute-api:\u0026#34; + self.region + \u0026#34;:\u0026#34; + self.awsAccountId + \u0026#34;:\u0026#34; + self.restApiId + \u0026#34;/\u0026#34; + self.stage + \u0026#34;/\u0026#34; + verb + \u0026#34;/\u0026#34; + resource) if effect.lower() == \u0026#34;allow\u0026#34;: self.allowMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) elif effect.lower() == \u0026#34;deny\u0026#34;: self.denyMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) def _getEmptyStatement(self, effect): \u0026#34;\u0026#34;\u0026#34;Returns an empty statement object prepopulated with the correct action and the desired effect.\u0026#34;\u0026#34;\u0026#34; statement = { \u0026#39;Action\u0026#39;: \u0026#39;execute-api:Invoke\u0026#39;, \u0026#39;Effect\u0026#39;: effect[:1].upper() + effect[1:].lower(), \u0026#39;Resource\u0026#39;: [] } return statement def _getStatementForEffect(self, effect, methods): \u0026#34;\u0026#34;\u0026#34;This function loops over an array of objects containing a resourceArn and conditions statement and generates the array of statements for the policy.\u0026#34;\u0026#34;\u0026#34; statements = [] if len(methods) \u0026gt; 0: statement = self._getEmptyStatement(effect) for curMethod in methods: if curMethod[\u0026#39;conditions\u0026#39;] is None or len(curMethod[\u0026#39;conditions\u0026#39;]) == 0: statement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) else: conditionalStatement = self._getEmptyStatement(effect) conditionalStatement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) conditionalStatement[\u0026#39;Condition\u0026#39;] = curMethod[\u0026#39;conditions\u0026#39;] statements.append(conditionalStatement) statements.append(statement) return statements def allowAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to authorize access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def denyAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to deny access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def allowMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, []) def denyMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, []) def allowMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, conditions) def denyMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, conditions) def build(self): \u0026#34;\u0026#34;\u0026#34;Generates the policy document based on the internal lists of allowed and denied conditions. This will generate a policy with two main statements for the effect: one statement for Allow and one statement for Deny. Methods that includes conditions will have their own statement in the policy.\u0026#34;\u0026#34;\u0026#34; if ((self.allowMethods is None or len(self.allowMethods) == 0) and (self.denyMethods is None or len(self.denyMethods) == 0)): raise NameError(\u0026#34;No statements defined for the policy\u0026#34;) policy = { \u0026#39;principalId\u0026#39; : self.principalId, \u0026#39;policyDocument\u0026#39; : { \u0026#39;Version\u0026#39; : self.version, \u0026#39;Statement\u0026#39; : [] } } policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Allow\u0026#34;, self.allowMethods)) policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Deny\u0026#34;, self.denyMethods)) return policy 5.2. Setup Authorizer trong API Gateway Trong API Gateway, v√†o ph·∫ßn Authorizer, t·∫°o 1 Authorizer t√™n coursesLambdaAuthorizer nh∆∞ sau (·∫£nh): V√†o ph·∫ßn Resource /authors, s·ª≠a Method Request GET nh∆∞ sau (·∫£nh): Enable CORS, s·ª≠a c√°i Access-Control-Allow-Headers l·∫°i b·∫±ng gi√° tr·ªã m√¨nh ƒë√£ set ·ªü Authorizer nh∆∞ h√¨nh sau (·∫£nh): Deploy l·∫°i API.\n5.3. Test method GET tr√™n Postman N·∫øu test tr√™n console th√¨ m·∫∑c ƒë·ªãnh AWS s·∫Ω bypass c√°i Authorizer lu√¥n, n√™n ph·∫£i test tr√™n Postman\nN·∫øu ko truy·ªÅn g√¨ v√†o Header authorizationToken th√¨ message n·ªôi dung s·∫Ω l√† unauthorized (·∫£nh) N·∫øu truy·ªÅn \u0026ldquo;what-the-fuck\u0026rdquo; v√†o s·∫Ω l·∫•y ƒë∆∞·ª£c data (·∫£nh) N·∫øu truy·ªÅn linh tinh th√¨ message n·ªôi dung deny (·∫£nh) 6. Secure API Gateway b·∫±ng Lambda Authorizer (C√°ch 2, d√πng encode v√† decode JWT) 6.1. T·∫°o 1 Lambda function ƒë·ªÉ th·ª±c hi·ªán nhi·ªám v·ª• Authorizer C√°ch t·∫°o nh∆∞ sau, v√¨ c·∫ßn import th∆∞ vi·ªán b√™n ngo√†i (jwt), ko c√≥ s·∫µn c·ªßa AWS Lambda n√™n ph·∫£i l√†m c√°ch n√†y:\nV√†o Lambda t·∫°o 1 function tr·ªëng t√™n \u0026ldquo;tokenBasedLambdaAuthorizer\u0026rdquo;, G·∫Øn role dc t·∫°o t·ª´ Step 2, r·ªìi ƒë·ªÉ ƒë√≥.\nD·ª±ng 1 Ec2 t·ª´ ami n√†y \u0026ldquo;ami-0080e4c5bc078760e\u0026rdquo;\nSSH v√†o Ec2, t·∫°o lambda function file:\nnano lambda_function.py ƒê√¢y l√† fuction ƒë∆∞·ª£c l·∫•y t·ª´ blueprint c·ªßa AWS, r·ªìi m√¨nh modifed.\nN·ªôi dung add th√™m l√†:\nƒê·∫ßu ti√™n decode token nh·∫≠n dc, t·ª´ token ƒë√≥ l·∫•y ra \u0026ldquo;userRole\u0026rdquo;, n·∫øu \u0026ldquo;userRole==admin\u0026rdquo; th√¨ allowAllMethods, c√≤n ko th√¨ denyAllMethods.\n\u0026#34;\u0026#34;\u0026#34; Copyright 2015-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \u0026#34;license\u0026#34; file accompanying this file. This file is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \u0026#34;\u0026#34;\u0026#34; from __future__ import print_function import re import time, datetime import pprint import json from jose import jwk, jwt from jose.utils import base64url_decode def lambda_handler(event, context): print(\u0026#34;Client token: \u0026#34; + event[\u0026#39;authorizationToken\u0026#39;]) print(\u0026#34;Method ARN: \u0026#34; + event[\u0026#39;methodArn\u0026#39;]) \u0026#34;\u0026#34;\u0026#34;validate the incoming token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and produce the principal user identifier associated with the token\u0026#34;\u0026#34;\u0026#34; # Hoang start secret=\u0026#34;my-secret\u0026#34; token=event[\u0026#34;authorizationToken\u0026#34;] decoded=jwt.decode(token, secret,algorithms=[\u0026#39;HS256\u0026#39;]) principalId = decoded[\u0026#34;userRole\u0026#34;] print(\u0026#34;principalId: \u0026#34; + principalId) # Hoang end \u0026#34;\u0026#34;\u0026#34;this could be accomplished in a number of ways:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;1. Call out to OAuth provider\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;2. Decode a JWT token inline\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;3. Lookup in a self-managed DB\u0026#34;\u0026#34;\u0026#34; # principalId = \u0026#34;user|a1b2c3d4\u0026#34; \u0026#34;\u0026#34;\u0026#34;you can send a 401 Unauthorized response to the client by failing like so:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;raise Exception(\u0026#39;Unauthorized\u0026#39;)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if the token is valid, a policy must be generated which will allow or deny access to the client\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is denied, the client will recieve a 403 Access Denied response\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;this function must generate a policy that is associated with the recognized principal user identifier.\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;depending on your use case, you might store policies in a DB, or generate them on the fly\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and will apply to subsequent calls to any method/resource in the RestApi\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;made with the same token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;the example policy below denies access to all resources in the RestApi\u0026#34;\u0026#34;\u0026#34; tmp = event[\u0026#39;methodArn\u0026#39;].split(\u0026#39;:\u0026#39;) apiGatewayArnTmp = tmp[5].split(\u0026#39;/\u0026#39;) awsAccountId = tmp[4] policy = AuthPolicy(principalId, awsAccountId) policy.restApiId = apiGatewayArnTmp[0] policy.region = tmp[3] policy.stage = apiGatewayArnTmp[1] # Hoang start if (principalId == \u0026#34;admin\u0026#34;): policy.allowAllMethods() else: policy.denyAllMethods() # Hoang end \u0026#34;\u0026#34;\u0026#34;policy.allowMethod(HttpVerb.GET, \u0026#34;/pets/*\u0026#34;)\u0026#34;\u0026#34;\u0026#34; # Finally, build the policy authResponse = policy.build() # new! -- add additional key-value pairs associated with the authenticated principal # these are made available by APIGW like so: $context.authorizer.\u0026lt;key\u0026gt; # additional context is cached context = { \u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;, # $context.authorizer.key -\u0026gt; value \u0026#39;number\u0026#39; : 1, \u0026#39;bool\u0026#39; : True } # context[\u0026#39;arr\u0026#39;] = [\u0026#39;foo\u0026#39;] \u0026lt;- this is invalid, APIGW will not accept it # context[\u0026#39;obj\u0026#39;] = {\u0026#39;foo\u0026#39;:\u0026#39;bar\u0026#39;} \u0026lt;- also invalid authResponse[\u0026#39;context\u0026#39;] = context return authResponse class HttpVerb: GET = \u0026#34;GET\u0026#34; POST = \u0026#34;POST\u0026#34; PUT = \u0026#34;PUT\u0026#34; PATCH = \u0026#34;PATCH\u0026#34; HEAD = \u0026#34;HEAD\u0026#34; DELETE = \u0026#34;DELETE\u0026#34; OPTIONS = \u0026#34;OPTIONS\u0026#34; ALL = \u0026#34;*\u0026#34; class AuthPolicy(object): awsAccountId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The AWS account id the policy will be generated for. This is used to create the method ARNs.\u0026#34;\u0026#34;\u0026#34; principalId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The principal used for the policy, this should be a unique identifier for the end user.\u0026#34;\u0026#34;\u0026#34; version = \u0026#34;2012-10-17\u0026#34; \u0026#34;\u0026#34;\u0026#34;The policy version used for the evaluation. This should always be \u0026#39;2012-10-17\u0026#39;\u0026#34;\u0026#34;\u0026#34; pathRegex = \u0026#34;^[/.a-zA-Z0-9-\\*]+$\u0026#34; \u0026#34;\u0026#34;\u0026#34;The regular expression used to validate resource paths for the policy\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;these are the internal lists of allowed and denied methods. These are lists of objects and each object has 2 properties: A resource ARN and a nullable conditions statement. the build method processes these lists and generates the approriate statements for the final policy\u0026#34;\u0026#34;\u0026#34; allowMethods = [] denyMethods = [] restApiId = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The API Gateway API id. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; region = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The region where the API is deployed. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; stage = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The name of the stage used in the policy. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; def __init__(self, principal, awsAccountId): self.awsAccountId = awsAccountId self.principalId = principal self.allowMethods = [] self.denyMethods = [] def _addMethod(self, effect, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds a method to the internal lists of allowed or denied methods. Each object in the internal list contains a resource ARN and a condition statement. The condition statement can be null.\u0026#34;\u0026#34;\u0026#34; if verb != \u0026#34;*\u0026#34; and not hasattr(HttpVerb, verb): raise NameError(\u0026#34;Invalid HTTP verb \u0026#34; + verb + \u0026#34;. Allowed verbs in HttpVerb class\u0026#34;) resourcePattern = re.compile(self.pathRegex) if not resourcePattern.match(resource): raise NameError(\u0026#34;Invalid resource path: \u0026#34; + resource + \u0026#34;. Path should match \u0026#34; + self.pathRegex) if resource[:1] == \u0026#34;/\u0026#34;: resource = resource[1:] resourceArn = (\u0026#34;arn:aws:execute-api:\u0026#34; + self.region + \u0026#34;:\u0026#34; + self.awsAccountId + \u0026#34;:\u0026#34; + self.restApiId + \u0026#34;/\u0026#34; + self.stage + \u0026#34;/\u0026#34; + verb + \u0026#34;/\u0026#34; + resource) if effect.lower() == \u0026#34;allow\u0026#34;: self.allowMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) elif effect.lower() == \u0026#34;deny\u0026#34;: self.denyMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) def _getEmptyStatement(self, effect): \u0026#34;\u0026#34;\u0026#34;Returns an empty statement object prepopulated with the correct action and the desired effect.\u0026#34;\u0026#34;\u0026#34; statement = { \u0026#39;Action\u0026#39;: \u0026#39;execute-api:Invoke\u0026#39;, \u0026#39;Effect\u0026#39;: effect[:1].upper() + effect[1:].lower(), \u0026#39;Resource\u0026#39;: [] } return statement def _getStatementForEffect(self, effect, methods): \u0026#34;\u0026#34;\u0026#34;This function loops over an array of objects containing a resourceArn and conditions statement and generates the array of statements for the policy.\u0026#34;\u0026#34;\u0026#34; statements = [] if len(methods) \u0026gt; 0: statement = self._getEmptyStatement(effect) for curMethod in methods: if curMethod[\u0026#39;conditions\u0026#39;] is None or len(curMethod[\u0026#39;conditions\u0026#39;]) == 0: statement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) else: conditionalStatement = self._getEmptyStatement(effect) conditionalStatement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) conditionalStatement[\u0026#39;Condition\u0026#39;] = curMethod[\u0026#39;conditions\u0026#39;] statements.append(conditionalStatement) statements.append(statement) return statements def allowAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to authorize access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def denyAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to deny access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def allowMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, []) def denyMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, []) def allowMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, conditions) def denyMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, conditions) def build(self): \u0026#34;\u0026#34;\u0026#34;Generates the policy document based on the internal lists of allowed and denied conditions. This will generate a policy with two main statements for the effect: one statement for Allow and one statement for Deny. Methods that includes conditions will have their own statement in the policy.\u0026#34;\u0026#34;\u0026#34; if ((self.allowMethods is None or len(self.allowMethods) == 0) and (self.denyMethods is None or len(self.denyMethods) == 0)): raise NameError(\u0026#34;No statements defined for the policy\u0026#34;) policy = { \u0026#39;principalId\u0026#39; : self.principalId, \u0026#39;policyDocument\u0026#39; : { \u0026#39;Version\u0026#39; : self.version, \u0026#39;Statement\u0026#39; : [] } } policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Allow\u0026#34;, self.allowMethods)) policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Deny\u0026#34;, self.denyMethods)) return policy 6.2. R·ªìi run c√°c command sau ƒë·ªÉ ƒë√≥ng package lambda v·ªõi th∆∞ vi·ªán jwt b√™n ngo√†i l·∫°i pip install --target ./package python-jose cd package/ zip -r9 ${OLDPWD}/lambda_function.zip . cd .. zip -g lambda_function.zip lambda_function.py 6.3. R·ªìi gi·ªù up file zip n√†y l√™n AWS Lambda aws lambda update-function-code --function-name tokenBasedLambdaAuthorizer --zip-file fileb://lambda_function.zip --region us-east-1 V√†o nh√¨n n√≥ s·∫Ω c√≥ 1 ƒë·ªëng th∆∞ vi·ªán ·ªü b√™n c·∫°nh nh∆∞ n√†y: 6.5. Setup Authorizer trong API Gateway Trong API Gateway t·∫°o 1 Authorizer nh∆∞ sau: ƒê·ªëi v·ªõi Method GET c·ªßa resource /authors, add c√°i authorizer v·ª´a t·∫°o v√†o ph·∫ßn Authorization Enable CORS, s·ª≠a c√°i Access-Control-Allow-Origin l·∫°i b·∫±ng gi√° tr·ªã m√¨nh ƒë√£ set ·ªü Authorizer Deploy API.\n6.6. Test method GET tr√™n Postman N·∫øu test tr√™n console th√¨ m·∫∑c ƒë·ªãnh AWS s·∫Ω bypass c√°i Authorizer lu√¥n, n√™n ph·∫£i test tr√™n Postman.\nTr∆∞·ªõc ti√™n c·∫ßn t·∫°o token ƒë·ªÉ Lambda function c√≥ th·ªÉ decode.\nV√†o jwt.io ƒë·ªÉ t·∫°o token, s·ª≠a code b√™n Decoded ƒë·ªÉ t·∫°o token ·ªü c·ªôt Encoded. T·∫°o dc token r·ªìi th√¨ v√†o Postman test\nG·ª≠i ƒë√∫ng token c·ªßa admin th√¨ s·∫Ω l·∫•y dc data: Token m√† sai th√¨ nh·∫≠n dc th√¥ng b√°o: Kh√¥ng g·ª≠i token m√† g·ª≠i linh tinh th√¨ nh·∫≠n dc c√°i sau: Done!\nNgo√†i ra, 1 s·ªë web c√≥ √≠ch, demo ho√†n ch·ªânh v·ªÅ d√πng Google-sigin button v·ªõi API Gateway: https://blog.codecentric.de/en/2018/04/aws-lambda-authorizer/\nSource tham kh·∫£o Step 5:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\nhttps://medium.com/@checko/how-to-create-an-aws-lambda-authorizer-for-api-gateway-45df4745a0e\nhttps://github.com/awslabs/aws-apigateway-lambda-authorizer-blueprints/blob/master/blueprints/python/api-gateway-authorizer-python.py\nStep 6:\nhttp://www.awsomeblog.com/api-gateway-custom-authorization/\nhttps://github.com/awslabs/aws-apigateway-lambda-authorizer-blueprints/blob/master/blueprints/python/api-gateway-authorizer-python.py\nhttps://blog.codecentric.de/en/2018/04/aws-lambda-authorizer/\nhttps://github.com/awslabs/aws-support-tools/issues/34\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html\n","href":"/bk/secure-api-gateway-by-lambda-authorizer/","title":"Secure API Gateway by Lambda Authorizer"},{"content":"B√†i n√†y s·∫Ω gi·∫£i th√≠ch c√°ch Secure API Gateway b·∫±ng Lambda Authorizer ƒë∆°n gi·∫£n.\nN·∫øu API kh√¥ng ƒë∆∞·ª£c secure th√¨ b·∫•t c·ª© ai c√≥ API URL c≈©ng c√≥ th·ªÉ g·ª≠i request ƒë·∫øn server v√† l·∫•y ƒë∆∞·ª£c data trong DynamoDB c·ªßa m√¨nh.\nN√™n c·∫ßn ph·∫£i setup Authorization cho API Gateway.\n1. T·∫°o table trong DynamoDB Table name:¬†authors\nPrimary key:¬†id\nT·∫°o v√†i item trong table authors\n{ \u0026#34;firstName\u0026#34;: \u0026#34;Hoang\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;le-hoang\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Le\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;linoel-messi\u0026#34;, \u0026#34;firstName\u0026#34;: \u0026#34;lionel\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;messi\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;jonesta-smaldini\u0026#34;, \u0026#34;firstName\u0026#34;: \u0026#34;jonesta\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;smaldini\u0026#34; }, { \u0026#34;firstName\u0026#34;: \u0026#34;Fred\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;fred\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;the-red\u0026#34; } 2. T·∫°o role cho Lambda function ƒêo·∫°n code b√™n d∆∞·ªõi b·∫°n c·∫ßn s·ª≠a region v√† accountId ph√π h·ª£p v·ªõi m√¥i tr∆∞·ªùng b·∫°n ƒëang test: v√≠ d·ª• region = us-east-1, accountId = 923423492348\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:region:accountId:table/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } 3. T·∫°o Lambda function \u0026ldquo;get-all-authors-py\u0026rdquo; ƒë·ªÉ get all th√¥ng tin user t·ª´ DynamoDB G·∫Øn Role v·ª´a t·∫°o ·ªü Step 2 v√†o.\nN·ªôi dung function vi·∫øt b·∫±ng python 3.7:\nimport boto3 import json print(\u0026#39;Loading function\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;authors\u0026#39;) def respond(err, res=None): return { \u0026#39;statusCode\u0026#39;: \u0026#39;400\u0026#39; if err else \u0026#39;200\u0026#39;, \u0026#39;body\u0026#39;: err.message if err else json.dumps(res), \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, } def lambda_handler(event, context): scan_response = table.scan() print(scan_response) # return respond(None, scan_response[\u0026#39;Items\u0026#39;]) return scan_response[\u0026#39;Items\u0026#39;] 4. Setup API Gateway T·∫°o resource: /authors\nTrong resource /authors, t·∫°o method GET\nPh·∫ßn Integration Request ch·ªçn Lambda function v·ª´a t·∫°o get-all-authors-py\nBan ƒë·∫ßu ch∆∞a c·∫ßn setup Authorization cho method n√†y n√™n h√£y ƒë·ªÉ NONE (·∫£nh) Test th·ª≠ xem l·∫•y ƒë∆∞·ª£c data l√† OK (·∫£nh) Deploy API\n5. Secure API Gateway b·∫±ng Lambda Authorizer (C√°ch 1, ƒê∆°n gi·∫£n) B·∫£n ch·∫•t l√† m√¨nh s·∫Ω t·∫°o th√™m 1 function Lambda n·ªØa ƒë·ªÉ n√≥ handler c√°c request g·ª≠i t·ªõi API Gateway, nhi·ªám v·ª• l√† x√°c ƒë·ªãnh request ƒë√≥ c√≥ h·ª£p l·ªá kh√¥ng, n·∫øu h·ª£p l·ªá th√¨ m·ªõi t·ª´ API Gateway chuy·ªÉn request ƒë·∫øn Lambda function get-all-authors-py.\n5.1. T·∫°o 1 Lambda function ƒë·ªÉ th·ª±c hi·ªán nhi·ªám v·ª• Authorizer T√™n function: \u0026ldquo;tokenBasedLambdaAuthorizerBasic\u0026rdquo;. G·∫Øn Role v·ª´a t·∫°o ·ªü Step 2 v√†o.\nSource n√†y m√¨nh l·∫•y t·ª´ Blueprint c·ªßa AWS (blueprint l√† nh·ªØng template AWS recommend m√¨nh s·ª≠ d·ª•ng), th√™m 1 ƒëo·∫°n code nh·ªè:\nN·∫øu Header c·ªßa request m√† c√≥ authorizationToken=\u0026ldquo;what-the-fuck\u0026rdquo; th√¨ dc allowAllMethods, ng∆∞·ª£c l·∫°i th√¨ denyAllMethods\n\u0026#34;\u0026#34;\u0026#34; Copyright 2015-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \u0026#34;license\u0026#34; file accompanying this file. This file is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \u0026#34;\u0026#34;\u0026#34; from __future__ import print_function import re import time, datetime import pprint import json def lambda_handler(event, context): print(\u0026#34;Client token: \u0026#34; + event[\u0026#39;authorizationToken\u0026#39;]) print(\u0026#34;Method ARN: \u0026#34; + event[\u0026#39;methodArn\u0026#39;]) \u0026#34;\u0026#34;\u0026#34;validate the incoming token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and produce the principal user identifier associated with the token\u0026#34;\u0026#34;\u0026#34; # Hoang start token=event[\u0026#34;authorizationToken\u0026#34;] principalId = token print(\u0026#34;principalId: \u0026#34; + principalId) # Hoang end \u0026#34;\u0026#34;\u0026#34;this could be accomplished in a number of ways:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;1. Call out to OAuth provider\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;2. Decode a JWT token inline\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;3. Lookup in a self-managed DB\u0026#34;\u0026#34;\u0026#34; # principalId = \u0026#34;user|a1b2c3d4\u0026#34; \u0026#34;\u0026#34;\u0026#34;you can send a 401 Unauthorized response to the client by failing like so:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;raise Exception(\u0026#39;Unauthorized\u0026#39;)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if the token is valid, a policy must be generated which will allow or deny access to the client\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is denied, the client will recieve a 403 Access Denied response\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;this function must generate a policy that is associated with the recognized principal user identifier.\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;depending on your use case, you might store policies in a DB, or generate them on the fly\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and will apply to subsequent calls to any method/resource in the RestApi\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;made with the same token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;the example policy below denies access to all resources in the RestApi\u0026#34;\u0026#34;\u0026#34; tmp = event[\u0026#39;methodArn\u0026#39;].split(\u0026#39;:\u0026#39;) apiGatewayArnTmp = tmp[5].split(\u0026#39;/\u0026#39;) awsAccountId = tmp[4] policy = AuthPolicy(principalId, awsAccountId) policy.restApiId = apiGatewayArnTmp[0] policy.region = tmp[3] policy.stage = apiGatewayArnTmp[1] # Hoang start if (principalId == \u0026#34;what-the-fuck\u0026#34;): policy.allowAllMethods() else: policy.denyAllMethods() # Hoang end \u0026#34;\u0026#34;\u0026#34;policy.allowMethod(HttpVerb.GET, \u0026#34;/pets/*\u0026#34;)\u0026#34;\u0026#34;\u0026#34; # Finally, build the policy authResponse = policy.build() # new! -- add additional key-value pairs associated with the authenticated principal # these are made available by APIGW like so: $context.authorizer.\u0026lt;key\u0026gt; # additional context is cached context = { \u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;, # $context.authorizer.key -\u0026gt; value \u0026#39;number\u0026#39; : 1, \u0026#39;bool\u0026#39; : True } # context[\u0026#39;arr\u0026#39;] = [\u0026#39;foo\u0026#39;] \u0026lt;- this is invalid, APIGW will not accept it # context[\u0026#39;obj\u0026#39;] = {\u0026#39;foo\u0026#39;:\u0026#39;bar\u0026#39;} \u0026lt;- also invalid authResponse[\u0026#39;context\u0026#39;] = context return authResponse class HttpVerb: GET = \u0026#34;GET\u0026#34; POST = \u0026#34;POST\u0026#34; PUT = \u0026#34;PUT\u0026#34; PATCH = \u0026#34;PATCH\u0026#34; HEAD = \u0026#34;HEAD\u0026#34; DELETE = \u0026#34;DELETE\u0026#34; OPTIONS = \u0026#34;OPTIONS\u0026#34; ALL = \u0026#34;*\u0026#34; class AuthPolicy(object): awsAccountId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The AWS account id the policy will be generated for. This is used to create the method ARNs.\u0026#34;\u0026#34;\u0026#34; principalId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The principal used for the policy, this should be a unique identifier for the end user.\u0026#34;\u0026#34;\u0026#34; version = \u0026#34;2012-10-17\u0026#34; \u0026#34;\u0026#34;\u0026#34;The policy version used for the evaluation. This should always be \u0026#39;2012-10-17\u0026#39;\u0026#34;\u0026#34;\u0026#34; pathRegex = \u0026#34;^[/.a-zA-Z0-9-\\*]+$\u0026#34; \u0026#34;\u0026#34;\u0026#34;The regular expression used to validate resource paths for the policy\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;these are the internal lists of allowed and denied methods. These are lists of objects and each object has 2 properties: A resource ARN and a nullable conditions statement. the build method processes these lists and generates the approriate statements for the final policy\u0026#34;\u0026#34;\u0026#34; allowMethods = [] denyMethods = [] restApiId = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The API Gateway API id. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; region = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The region where the API is deployed. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; stage = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The name of the stage used in the policy. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; def __init__(self, principal, awsAccountId): self.awsAccountId = awsAccountId self.principalId = principal self.allowMethods = [] self.denyMethods = [] def _addMethod(self, effect, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds a method to the internal lists of allowed or denied methods. Each object in the internal list contains a resource ARN and a condition statement. The condition statement can be null.\u0026#34;\u0026#34;\u0026#34; if verb != \u0026#34;*\u0026#34; and not hasattr(HttpVerb, verb): raise NameError(\u0026#34;Invalid HTTP verb \u0026#34; + verb + \u0026#34;. Allowed verbs in HttpVerb class\u0026#34;) resourcePattern = re.compile(self.pathRegex) if not resourcePattern.match(resource): raise NameError(\u0026#34;Invalid resource path: \u0026#34; + resource + \u0026#34;. Path should match \u0026#34; + self.pathRegex) if resource[:1] == \u0026#34;/\u0026#34;: resource = resource[1:] resourceArn = (\u0026#34;arn:aws:execute-api:\u0026#34; + self.region + \u0026#34;:\u0026#34; + self.awsAccountId + \u0026#34;:\u0026#34; + self.restApiId + \u0026#34;/\u0026#34; + self.stage + \u0026#34;/\u0026#34; + verb + \u0026#34;/\u0026#34; + resource) if effect.lower() == \u0026#34;allow\u0026#34;: self.allowMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) elif effect.lower() == \u0026#34;deny\u0026#34;: self.denyMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) def _getEmptyStatement(self, effect): \u0026#34;\u0026#34;\u0026#34;Returns an empty statement object prepopulated with the correct action and the desired effect.\u0026#34;\u0026#34;\u0026#34; statement = { \u0026#39;Action\u0026#39;: \u0026#39;execute-api:Invoke\u0026#39;, \u0026#39;Effect\u0026#39;: effect[:1].upper() + effect[1:].lower(), \u0026#39;Resource\u0026#39;: [] } return statement def _getStatementForEffect(self, effect, methods): \u0026#34;\u0026#34;\u0026#34;This function loops over an array of objects containing a resourceArn and conditions statement and generates the array of statements for the policy.\u0026#34;\u0026#34;\u0026#34; statements = [] if len(methods) \u0026gt; 0: statement = self._getEmptyStatement(effect) for curMethod in methods: if curMethod[\u0026#39;conditions\u0026#39;] is None or len(curMethod[\u0026#39;conditions\u0026#39;]) == 0: statement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) else: conditionalStatement = self._getEmptyStatement(effect) conditionalStatement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) conditionalStatement[\u0026#39;Condition\u0026#39;] = curMethod[\u0026#39;conditions\u0026#39;] statements.append(conditionalStatement) statements.append(statement) return statements def allowAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to authorize access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def denyAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to deny access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def allowMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, []) def denyMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, []) def allowMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, conditions) def denyMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, conditions) def build(self): \u0026#34;\u0026#34;\u0026#34;Generates the policy document based on the internal lists of allowed and denied conditions. This will generate a policy with two main statements for the effect: one statement for Allow and one statement for Deny. Methods that includes conditions will have their own statement in the policy.\u0026#34;\u0026#34;\u0026#34; if ((self.allowMethods is None or len(self.allowMethods) == 0) and (self.denyMethods is None or len(self.denyMethods) == 0)): raise NameError(\u0026#34;No statements defined for the policy\u0026#34;) policy = { \u0026#39;principalId\u0026#39; : self.principalId, \u0026#39;policyDocument\u0026#39; : { \u0026#39;Version\u0026#39; : self.version, \u0026#39;Statement\u0026#39; : [] } } policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Allow\u0026#34;, self.allowMethods)) policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Deny\u0026#34;, self.denyMethods)) return policy 5.2. Setup Authorizer trong API Gateway Trong API Gateway, v√†o ph·∫ßn Authorizer, t·∫°o 1 Authorizer t√™n coursesLambdaAuthorizer nh∆∞ sau (·∫£nh): V√†o ph·∫ßn Resource /authors, s·ª≠a Method Request GET nh∆∞ sau (·∫£nh): Enable CORS, s·ª≠a c√°i Access-Control-Allow-Headers l·∫°i b·∫±ng gi√° tr·ªã m√¨nh ƒë√£ set ·ªü Authorizer nh∆∞ h√¨nh sau (·∫£nh): Deploy l·∫°i API.\n5.3. Test method GET tr√™n Postman N·∫øu test tr√™n console th√¨ m·∫∑c ƒë·ªãnh AWS s·∫Ω bypass c√°i Authorizer lu√¥n, n√™n ph·∫£i test tr√™n Postman\nN·∫øu ko truy·ªÅn g√¨ v√†o Header authorizationToken th√¨ message n·ªôi dung s·∫Ω l√† unauthorized (·∫£nh) N·∫øu truy·ªÅn \u0026ldquo;what-the-fuck\u0026rdquo; v√†o s·∫Ω l·∫•y ƒë∆∞·ª£c data (·∫£nh) N·∫øu truy·ªÅn linh tinh th√¨ message n·ªôi dung deny (·∫£nh) 6. Secure API Gateway b·∫±ng Lambda Authorizer (C√°ch 2, d√πng encode v√† decode JWT) 6.1. T·∫°o 1 Lambda function ƒë·ªÉ th·ª±c hi·ªán nhi·ªám v·ª• Authorizer C√°ch t·∫°o nh∆∞ sau, v√¨ c·∫ßn import th∆∞ vi·ªán b√™n ngo√†i (jwt), ko c√≥ s·∫µn c·ªßa AWS Lambda n√™n ph·∫£i l√†m c√°ch n√†y:\nV√†o Lambda t·∫°o 1 function tr·ªëng t√™n \u0026ldquo;tokenBasedLambdaAuthorizer\u0026rdquo;, G·∫Øn role dc t·∫°o t·ª´ Step 2, r·ªìi ƒë·ªÉ ƒë√≥.\nD·ª±ng 1 Ec2 t·ª´ ami n√†y \u0026ldquo;ami-0080e4c5bc078760e\u0026rdquo;\nSSH v√†o Ec2, t·∫°o lambda function file:\nnano lambda_function.py ƒê√¢y l√† fuction ƒë∆∞·ª£c l·∫•y t·ª´ blueprint c·ªßa AWS, r·ªìi m√¨nh modifed.\nN·ªôi dung add th√™m l√†:\nƒê·∫ßu ti√™n decode token nh·∫≠n dc, t·ª´ token ƒë√≥ l·∫•y ra \u0026ldquo;userRole\u0026rdquo;, n·∫øu \u0026ldquo;userRole==admin\u0026rdquo; th√¨ allowAllMethods, c√≤n ko th√¨ denyAllMethods.\n\u0026#34;\u0026#34;\u0026#34; Copyright 2015-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \u0026#34;license\u0026#34; file accompanying this file. This file is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \u0026#34;\u0026#34;\u0026#34; from __future__ import print_function import re import time, datetime import pprint import json from jose import jwk, jwt from jose.utils import base64url_decode def lambda_handler(event, context): print(\u0026#34;Client token: \u0026#34; + event[\u0026#39;authorizationToken\u0026#39;]) print(\u0026#34;Method ARN: \u0026#34; + event[\u0026#39;methodArn\u0026#39;]) \u0026#34;\u0026#34;\u0026#34;validate the incoming token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and produce the principal user identifier associated with the token\u0026#34;\u0026#34;\u0026#34; # Hoang start secret=\u0026#34;my-secret\u0026#34; token=event[\u0026#34;authorizationToken\u0026#34;] decoded=jwt.decode(token, secret,algorithms=[\u0026#39;HS256\u0026#39;]) principalId = decoded[\u0026#34;userRole\u0026#34;] print(\u0026#34;principalId: \u0026#34; + principalId) # Hoang end \u0026#34;\u0026#34;\u0026#34;this could be accomplished in a number of ways:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;1. Call out to OAuth provider\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;2. Decode a JWT token inline\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;3. Lookup in a self-managed DB\u0026#34;\u0026#34;\u0026#34; # principalId = \u0026#34;user|a1b2c3d4\u0026#34; \u0026#34;\u0026#34;\u0026#34;you can send a 401 Unauthorized response to the client by failing like so:\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;raise Exception(\u0026#39;Unauthorized\u0026#39;)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if the token is valid, a policy must be generated which will allow or deny access to the client\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is denied, the client will recieve a 403 Access Denied response\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;this function must generate a policy that is associated with the recognized principal user identifier.\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;depending on your use case, you might store policies in a DB, or generate them on the fly\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;and will apply to subsequent calls to any method/resource in the RestApi\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;made with the same token\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;the example policy below denies access to all resources in the RestApi\u0026#34;\u0026#34;\u0026#34; tmp = event[\u0026#39;methodArn\u0026#39;].split(\u0026#39;:\u0026#39;) apiGatewayArnTmp = tmp[5].split(\u0026#39;/\u0026#39;) awsAccountId = tmp[4] policy = AuthPolicy(principalId, awsAccountId) policy.restApiId = apiGatewayArnTmp[0] policy.region = tmp[3] policy.stage = apiGatewayArnTmp[1] # Hoang start if (principalId == \u0026#34;admin\u0026#34;): policy.allowAllMethods() else: policy.denyAllMethods() # Hoang end \u0026#34;\u0026#34;\u0026#34;policy.allowMethod(HttpVerb.GET, \u0026#34;/pets/*\u0026#34;)\u0026#34;\u0026#34;\u0026#34; # Finally, build the policy authResponse = policy.build() # new! -- add additional key-value pairs associated with the authenticated principal # these are made available by APIGW like so: $context.authorizer.\u0026lt;key\u0026gt; # additional context is cached context = { \u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;, # $context.authorizer.key -\u0026gt; value \u0026#39;number\u0026#39; : 1, \u0026#39;bool\u0026#39; : True } # context[\u0026#39;arr\u0026#39;] = [\u0026#39;foo\u0026#39;] \u0026lt;- this is invalid, APIGW will not accept it # context[\u0026#39;obj\u0026#39;] = {\u0026#39;foo\u0026#39;:\u0026#39;bar\u0026#39;} \u0026lt;- also invalid authResponse[\u0026#39;context\u0026#39;] = context return authResponse class HttpVerb: GET = \u0026#34;GET\u0026#34; POST = \u0026#34;POST\u0026#34; PUT = \u0026#34;PUT\u0026#34; PATCH = \u0026#34;PATCH\u0026#34; HEAD = \u0026#34;HEAD\u0026#34; DELETE = \u0026#34;DELETE\u0026#34; OPTIONS = \u0026#34;OPTIONS\u0026#34; ALL = \u0026#34;*\u0026#34; class AuthPolicy(object): awsAccountId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The AWS account id the policy will be generated for. This is used to create the method ARNs.\u0026#34;\u0026#34;\u0026#34; principalId = \u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;The principal used for the policy, this should be a unique identifier for the end user.\u0026#34;\u0026#34;\u0026#34; version = \u0026#34;2012-10-17\u0026#34; \u0026#34;\u0026#34;\u0026#34;The policy version used for the evaluation. This should always be \u0026#39;2012-10-17\u0026#39;\u0026#34;\u0026#34;\u0026#34; pathRegex = \u0026#34;^[/.a-zA-Z0-9-\\*]+$\u0026#34; \u0026#34;\u0026#34;\u0026#34;The regular expression used to validate resource paths for the policy\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;these are the internal lists of allowed and denied methods. These are lists of objects and each object has 2 properties: A resource ARN and a nullable conditions statement. the build method processes these lists and generates the approriate statements for the final policy\u0026#34;\u0026#34;\u0026#34; allowMethods = [] denyMethods = [] restApiId = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The API Gateway API id. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; region = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The region where the API is deployed. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; stage = \u0026#34;*\u0026#34; \u0026#34;\u0026#34;\u0026#34;The name of the stage used in the policy. By default this is set to \u0026#39;*\u0026#39;\u0026#34;\u0026#34;\u0026#34; def __init__(self, principal, awsAccountId): self.awsAccountId = awsAccountId self.principalId = principal self.allowMethods = [] self.denyMethods = [] def _addMethod(self, effect, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds a method to the internal lists of allowed or denied methods. Each object in the internal list contains a resource ARN and a condition statement. The condition statement can be null.\u0026#34;\u0026#34;\u0026#34; if verb != \u0026#34;*\u0026#34; and not hasattr(HttpVerb, verb): raise NameError(\u0026#34;Invalid HTTP verb \u0026#34; + verb + \u0026#34;. Allowed verbs in HttpVerb class\u0026#34;) resourcePattern = re.compile(self.pathRegex) if not resourcePattern.match(resource): raise NameError(\u0026#34;Invalid resource path: \u0026#34; + resource + \u0026#34;. Path should match \u0026#34; + self.pathRegex) if resource[:1] == \u0026#34;/\u0026#34;: resource = resource[1:] resourceArn = (\u0026#34;arn:aws:execute-api:\u0026#34; + self.region + \u0026#34;:\u0026#34; + self.awsAccountId + \u0026#34;:\u0026#34; + self.restApiId + \u0026#34;/\u0026#34; + self.stage + \u0026#34;/\u0026#34; + verb + \u0026#34;/\u0026#34; + resource) if effect.lower() == \u0026#34;allow\u0026#34;: self.allowMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) elif effect.lower() == \u0026#34;deny\u0026#34;: self.denyMethods.append({ \u0026#39;resourceArn\u0026#39; : resourceArn, \u0026#39;conditions\u0026#39; : conditions }) def _getEmptyStatement(self, effect): \u0026#34;\u0026#34;\u0026#34;Returns an empty statement object prepopulated with the correct action and the desired effect.\u0026#34;\u0026#34;\u0026#34; statement = { \u0026#39;Action\u0026#39;: \u0026#39;execute-api:Invoke\u0026#39;, \u0026#39;Effect\u0026#39;: effect[:1].upper() + effect[1:].lower(), \u0026#39;Resource\u0026#39;: [] } return statement def _getStatementForEffect(self, effect, methods): \u0026#34;\u0026#34;\u0026#34;This function loops over an array of objects containing a resourceArn and conditions statement and generates the array of statements for the policy.\u0026#34;\u0026#34;\u0026#34; statements = [] if len(methods) \u0026gt; 0: statement = self._getEmptyStatement(effect) for curMethod in methods: if curMethod[\u0026#39;conditions\u0026#39;] is None or len(curMethod[\u0026#39;conditions\u0026#39;]) == 0: statement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) else: conditionalStatement = self._getEmptyStatement(effect) conditionalStatement[\u0026#39;Resource\u0026#39;].append(curMethod[\u0026#39;resourceArn\u0026#39;]) conditionalStatement[\u0026#39;Condition\u0026#39;] = curMethod[\u0026#39;conditions\u0026#39;] statements.append(conditionalStatement) statements.append(statement) return statements def allowAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to authorize access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def denyAllMethods(self): \u0026#34;\u0026#34;\u0026#34;Adds a \u0026#39;*\u0026#39; allow to the policy to deny access to all methods of an API\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, HttpVerb.ALL, \u0026#34;*\u0026#34;, []) def allowMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, []) def denyMethod(self, verb, resource): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods for the policy\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, []) def allowMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of allowed methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Allow\u0026#34;, verb, resource, conditions) def denyMethodWithConditions(self, verb, resource, conditions): \u0026#34;\u0026#34;\u0026#34;Adds an API Gateway method (Http verb + Resource path) to the list of denied methods and includes a condition for the policy statement. More on AWS policy conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\u0026#34;\u0026#34;\u0026#34; self._addMethod(\u0026#34;Deny\u0026#34;, verb, resource, conditions) def build(self): \u0026#34;\u0026#34;\u0026#34;Generates the policy document based on the internal lists of allowed and denied conditions. This will generate a policy with two main statements for the effect: one statement for Allow and one statement for Deny. Methods that includes conditions will have their own statement in the policy.\u0026#34;\u0026#34;\u0026#34; if ((self.allowMethods is None or len(self.allowMethods) == 0) and (self.denyMethods is None or len(self.denyMethods) == 0)): raise NameError(\u0026#34;No statements defined for the policy\u0026#34;) policy = { \u0026#39;principalId\u0026#39; : self.principalId, \u0026#39;policyDocument\u0026#39; : { \u0026#39;Version\u0026#39; : self.version, \u0026#39;Statement\u0026#39; : [] } } policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Allow\u0026#34;, self.allowMethods)) policy[\u0026#39;policyDocument\u0026#39;][\u0026#39;Statement\u0026#39;].extend(self._getStatementForEffect(\u0026#34;Deny\u0026#34;, self.denyMethods)) return policy 6.2. R·ªìi run c√°c command sau ƒë·ªÉ ƒë√≥ng package lambda v·ªõi th∆∞ vi·ªán jwt b√™n ngo√†i l·∫°i pip install --target ./package python-jose cd package/ zip -r9 ${OLDPWD}/lambda_function.zip . cd .. zip -g lambda_function.zip lambda_function.py 6.3. R·ªìi gi·ªù up file zip n√†y l√™n AWS Lambda aws lambda update-function-code --function-name tokenBasedLambdaAuthorizer --zip-file fileb://lambda_function.zip --region us-east-1 V√†o nh√¨n n√≥ s·∫Ω c√≥ 1 ƒë·ªëng th∆∞ vi·ªán ·ªü b√™n c·∫°nh nh∆∞ n√†y: 6.5. Setup Authorizer trong API Gateway Trong API Gateway t·∫°o 1 Authorizer nh∆∞ sau: ƒê·ªëi v·ªõi Method GET c·ªßa resource /authors, add c√°i authorizer v·ª´a t·∫°o v√†o ph·∫ßn Authorization Enable CORS, s·ª≠a c√°i Access-Control-Allow-Origin l·∫°i b·∫±ng gi√° tr·ªã m√¨nh ƒë√£ set ·ªü Authorizer Deploy API.\n6.6. Test method GET tr√™n Postman N·∫øu test tr√™n console th√¨ m·∫∑c ƒë·ªãnh AWS s·∫Ω bypass c√°i Authorizer lu√¥n, n√™n ph·∫£i test tr√™n Postman.\nTr∆∞·ªõc ti√™n c·∫ßn t·∫°o token ƒë·ªÉ Lambda function c√≥ th·ªÉ decode.\nV√†o jwt.io ƒë·ªÉ t·∫°o token, s·ª≠a code b√™n Decoded ƒë·ªÉ t·∫°o token ·ªü c·ªôt Encoded. T·∫°o dc token r·ªìi th√¨ v√†o Postman test\nG·ª≠i ƒë√∫ng token c·ªßa admin th√¨ s·∫Ω l·∫•y dc data: Token m√† sai th√¨ nh·∫≠n dc th√¥ng b√°o: Kh√¥ng g·ª≠i token m√† g·ª≠i linh tinh th√¨ nh·∫≠n dc c√°i sau: Done!\nNgo√†i ra, 1 s·ªë web c√≥ √≠ch, demo ho√†n ch·ªânh v·ªÅ d√πng Google-sigin button v·ªõi API Gateway: https://blog.codecentric.de/en/2018/04/aws-lambda-authorizer/\nSource tham kh·∫£o Step 5:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\nhttps://medium.com/@checko/how-to-create-an-aws-lambda-authorizer-for-api-gateway-45df4745a0e\nhttps://github.com/awslabs/aws-apigateway-lambda-authorizer-blueprints/blob/master/blueprints/python/api-gateway-authorizer-python.py\nStep 6:\nhttp://www.awsomeblog.com/api-gateway-custom-authorization/\nhttps://github.com/awslabs/aws-apigateway-lambda-authorizer-blueprints/blob/master/blueprints/python/api-gateway-authorizer-python.py\nhttps://blog.codecentric.de/en/2018/04/aws-lambda-authorizer/\nhttps://github.com/awslabs/aws-support-tools/issues/34\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html\n","href":"/posts/secure-api-gateway-by-lambda-authorizer/","title":"Secure API Gateway by Lambda Authorizer"},{"content":"C√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh h·ªçc \u0026mdash;P1\u0026mdash;\n Q. 1 app ƒëang stores session state in memory. N√™n l∆∞u n√≥ ·ªü ƒë√¢u?\n Store session state in an ElastiCache cluster\n Q. 1 app c·∫ßn ph·∫£i monitor c√°c event. c·∫ßn capture s·ªë l∆∞·ª£ng user login, th·ªùi gian ƒë·ªânh ƒëi·ªÉm, period 10s. l√†m n√†o?\n Create a high-resolution custom Amazon CloudWatch metric\nhigh-resolution metric: can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, regular alarm with a period of any multiple of 60 seconds\n Q. dev ko c√≥ quy·ªÅn edit the code build nh∆∞ng c√≥ quy√™n run build/ l√†m n√†o ƒë·ªÉ override the build command?\n Run the start-build command with buildspecOverride property\n Q. cty ƒëang d√πng ElastiCache cluster. y√™u c·∫ßu l√† logic trong code sao cho cluster ch·ªâ request data t·ª´ RDS khi cache miss.D√πng strategy n√†o?\n c√≥ 3 lo·∫°i strategy c·ªßa elastiCache: Lazy Loading (ch·ªâ request data ƒëc cache, nh∆∞ng data cache c√≥ th·ªÉ c≈©), data trong cache s·∫Ω ch·ªâ dc update n·∫øu request b·ªã cache miss, c√≤n n·∫øu data DB dc update ri√™ng th√¨ s·∫Ω ko update cache.\nWrite Through (t·ªën resource, nh∆∞ng data lu√¥n m·ªõi), data trong cache s·∫Ω lu√¥n dc update m·ªói khi data trong DB dc update.\nAdding TTL (ch·ªâ ƒë·ªãnh th·ªùi gian h·∫øt h·∫°n cho key, h·∫°n ch·∫ø vi·ªác data qu√° c≈©)\n Q. 1 dev ƒëang build app access s3. 1 IAM role ƒë√£ dc t·∫°o ƒë·ªÉ acecess v√†o s3, API n√†o m√† dev c·∫ßn d√πng ƒë·ªÉ code access dc v√†o s3?\n STS:AssumeRole\n Q. 1 lambda function t√≠nh to√°n s·ªë Fibonacci d√πng v√≤ng l·∫∑p invoke lambda v√†i ng√†y sau th√¨ the Lambda function b·ªã throttled. Best practice?\n Theo best practice th√¨ n√™n: Avoid the use of recursion, n·∫øu v√¥ t√¨nh l√†m th√¨ n√™n set the function concurrent execution limit to \u0026lsquo;0\u0026rsquo; (Zero) ngay, R·ªìi update code.\n1 s·ªë best practice c·ªßa d√πng lambda:\nt√°ch c√°c Lambda handler,\nt·∫≠n d·ª•ng m√¥i tr∆∞·ªùng execute context,\nd√πng AWS Lambda Environment Variables ƒë·ªÉ truy·ªÅn param,\ngi·∫£m thi·∫øu s·ªë l∆∞·ª£ng deployment package xu·ªëng,\nƒë·ª´ng up c·∫£ sdk l√™n tr√°nh d√πng v√≤ng l·∫∑p trong code test performance,\ntest load,\ndelete function c≈© ko d√πng,\nt·∫≠n d·ª•ng AWS Lambda Metrics and CloudWatch Alarms,\nko cho function v√†o VPC n·∫øu ko c·∫ßn thi·∫øt,\nch·∫Øc ch·∫Øn r·∫±ng c√≥ ƒë·ªß elastic network interfaces (ENIs),\nn·∫øu ko th√¨ ph·∫£i t·∫°o c√°i subnet l·ªõn h∆°n,\nt·∫°o dedicated Lambda subnets in your VPC.\n Q. 1 function lambda s·∫Ω run in multiple stages, such a dev, test and production. v√† n√≥ ph·∫£i call c√°c endpoints kh√°c nhau t√πy theo stage. l√†m n√†o ƒë·ªÉ dev ch·∫Øc ch·∫Øn s·∫Ω call ƒë√∫ng endpoint khi ch·∫°y trong m·ªói stage?\n d√πng Environment variables\n Q. D√πng AWS SAM ƒë·ªÉ define Lambda function. Vi·∫øt 1 new function v√† c√°ch n√†o ƒë·ªÉ shift traffic t·ª´ function c≈© sang function m·ªõi nhanh nh·∫•t Canary10Percent5Minutes, Linear10PercentEvery10Minutes.\n Canary10Percent5Minutes: traffic dc shift 2 v√≤ng. v√≤ng 1 l√† 10% traffic, v√† t·∫•t c·∫£ 90% c√≤n l·∫°i s·∫Ω dc shift sau 5p Linear10PercentEvery10Minutes: traffic dc shift 10% m·ªói 10p, sau 100p th√¨ m·ªõi dc 100%\n Q. 1 dev ƒëang migrate to cloud 1 app d√πng MS SQL, data dc encrypt bang Transparent Data Encryption n√™n d√πng c√°i g√¨ ƒë·ªÉ ph·∫£i code √≠t nh·∫•t?\n Amazon RDS, v√¨ n√≥ support Transparent Data Encryption (TDE), MS SQL\n Q. 1 dev ƒëang vi·∫øt v√†i lambda function access v√†o RDS. h·ªç c·∫ßn share c√°c connection string that contains the database credentials, which are a secret. Cty c√≥ policy y√™u c·∫ßu t·∫•t c·∫£ secret ph·∫£i encrypted.n√™n d√πng c√°i g√¨ ƒë·ªÉ ph·∫£i code √≠t nh·∫•t?\n Use Systems Manager Parameter Store secure strings\n Q. A developer is using Amazon API Gateway as an HTTP proxy to a backend endpoint. c√≥ 3 moi tr∆∞·ªùng: Development, Testing, Production and three corresponding stages in the API gateway. L√†m n√†o ƒë·ªÉ direct traffic ƒë√∫ng enpoint c·ªßa stages m√† ko c·∫ßn t·∫°o ri√™ng API cho m·ªói stage\n Use stage variables\n Q. 1 cty ƒëang deploy static website tr√™n s3. dev c·∫ßn ph·∫£i l√†m th√†nh dynamic s·ª≠ d√πng serverless solution (code m√† ko c·∫ßn c√†i ƒë·∫∑t server). 2 c√°i n√†o ƒë·ªÉ l√†m?\n API Gateway and AWS Lambda are the best two choices to build this serverless application\n Q. 1 cty ƒëang c√≥ custom CloudWatch metric cho m·ªói khi c√≥ l·ªói HTTP 504 xu·∫•t hi·ªán trong logs. v√† c√≥ 1 CloudWatch Alarm cho c√°i metric kia. N·∫øu mu·ªën alarm dc trigger ch·ªâ khi \u0026gt;= 2 evaluation periods. th√¨ s·ª≠a c√°i g√¨?\n Data Points to Alarm should be set to 2 while creating this alarm\n Q. 1 dev ƒëang d√πng AWS Elastic Beanstalk env cho 1 web app. Hi·ªán t·∫°i ƒëang d√πng t1.micro. Sao ƒë·ªÉ change th√†nh m4.large??\n Create a new configuration file with the instance type as m4.large\n Q. 1 cty d√πng Aurora RDS. vi·ªác t·∫°o report m·ªói gi·ªù l√†m ch·∫≠m h·ªá th·ªëng/. C·∫ßn l√†m j?\n t·∫°o read replica DB v√† point c√°i reporting application ƒë·∫øn read replica ƒë√≥\n Q. 1 cty using AWS Elastic Beanstalk for a web app, Dev c·∫ßn config sao cho s·∫Ω create new instances and deploy code to those instances method n√†o s·∫Ω deploy code ONLY l√™n instance m·ªõi?\n Immutable, Blue/green\nPh√¢n bi·ªát c√°c lo·∫°i deploy method c·ªßa Beanstalk:\nAll in One: deploy l√™n exist ec2, th·ªùi gian deploy nhanh nh∆∞ng h·ªá th·ªëng s·∫Ω c√≥ downtime trong l√∫c deploy, n·∫øu fail th√¨ user ph·∫£i ch·ªãu downtime v√¨ system b·ªã treo, dev ph·∫£i manual redeploy\nRolling: deploy theo batch l√™n exist ec2, b·∫°n ph·∫£i define s·ªë l∆∞·ª£ng ec2 trong 1 batch. V√≠ d·ª• c√≥ 3 ec2, m·ªói ec2 l√† 1 batch.\ndeploy l√™n 1 batch tr∆∞·ªõc, n·∫øu batch fail th√¨ qu√° tr√¨nh deploy d·ª´ng l·∫°i, h·ªá th·ªëng v·∫´n ch·∫°y b√¨nh th∆∞·ªùng v√¨ c√≤n 2 ec2 kh√°c ƒëang ch·∫°y ko fail. Tuy nhi√™n trong qu√° tr√¨nh deploy to√†n b·ªô h·ªá th·ªëng s·∫Ω ko full capacity (nƒÉng su·∫•t) b·ªüi v√¨ ec2 m√† ƒëang deploy s·∫Ω ko serve traffic. N·∫øu fail th√¨ user ph·∫£i manual redeploy.\nRolling with additional batch: deploy theo batch, t·∫°o th√™m ec2 ƒë·ªÉ deploy version m·ªõi l√™n ƒë√≥.\nV√≠ d·ª• c√≥ 3 ec2, m·ªói ec2 l√† 1 batch. t·∫°o th√™m 1 ec2 deploy l√™n ƒë√≥. N·∫øu success h·∫øt th√¨ s·∫Ω terminate 1 ec2 ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng ch·ªâ c√≥ 3 ec2. ƒê·∫£m b·∫£o l√† l√∫c n√†o h·ªá th·ªëng c≈©ng full capacity. N·∫øu fail th√¨ c√°i batch fail s·∫Ω b·ªã terminate\nImmutable: deploy to√†n b·ªô l√™n c√°c ec2 m·ªõi. N·∫øu fail th√¨ s·∫Ω terminate c√°c ec2 m·ªõi ƒë√≥. Deploy time c≈©ng l√¢u h∆°n. Ch·ªâ duplicate c√°c ec2 th√¥i, ko duplicate Load Balancer n√™n ko thay ƒë·ªïi v·ªÅ DNS.\nBlue/green: deploy to√†n b·ªô l√™n c√°c ec2 m·ªõi. Duplicate c·∫£ ec2 v√† Load Balancer. Deploy time l√¢u ngang Immutable. Sau khi success th√¨ b·∫°n c·∫ßn swap URL ƒë·ªÉ route traffic ƒë·∫øn 1 DNS green. N·∫øu fail th√¨ swap URL l·∫°i m√¥i tr∆∞·ªùng blue c≈© th√¥i. Ch√∫ √Ω l√† d√πng c√°i n√†y ko n√™n ƒë·ªÉ DB ·ªü trong Beanstalk env. V√¨ DB b·ªã duplicate s·∫Ω ko dc sync.\ntham kh·∫£o: https://blog.shikisoft.com/which_elastic_beanstalk_deployment_should_you_use/\n Q. 1 dev ƒëang vi·∫øt 1 app store data v√†o Dynamo DB, t·ª∑ l·ªá ƒë·ªçc vi·∫øt v√†o db l√† 1000 reads:1 writes. data dc access li√™n t·ª•c Dev c·∫ßn enable c√°i g√¨ tr√™n DynamoDB ƒë·ªÉ optimize cost v√† performance?\n Amazon DynamoDB Accelerator (DAX)\n Q. 1 API dev ƒëang dc y√™u c·∫ßu d√πng deploy bang API gateway, n·∫øu l√† API frontend th√¨ config c√°i j? n·∫øu API backen th√¨ c·∫ßn config j?\n API frontend th√¨ config medthod respone v√† method request n·∫øu API backend th√¨ c·∫ßn config integration request v√† intergration response\nQ.b·∫°n ƒëang d√πng lambda t∆∞∆°ng t√°c v·ªõi DynamoDB, sau 1 time th√¨ k·∫øt qu·∫£ b·ªã delay. c·∫ßn DEBUG th√¨ d√πng c√°i j?\nAWS X-Ray\n Q. 1 cty ƒëang d√πng AWS Code Pipeline for CICD, code l·∫•y t·ª´ S3, cty y√™u c·∫ßu all data ph·∫£i encrypted v√† key dc customer qu·∫£n l√Ω. C·∫ßn enable nh·ªØng g√¨?\n Serverside encryption ·ªü S3 bucket, d√πng AWS KMS for s3 encryption\n Q. 1 App ƒëang d√πng 1 DynamoDB table ƒë·ªÉ store 1 s·ªë items. nh·ªØng items n√†y ch·ªâ dc access v√†i l·∫ßn sau ƒë√≥ th√¨ c√≥ th·ªÉ x√≥a. Qu·∫£n l√Ω vi·ªác x√≥a item ƒë√≥ bang c√°ch n√†o?\n enable TTL cho DynamoDB, cho ph√©p set timestamp ƒë·ªÉ t·ª± ƒë·ªông x√≥a items. vi·ªác versioning v·ªõi DynamoDB l√† ko th·ªÉ\n Q. You are using AWS Envelope Encryption for encrypting all sensitive data. Data dc encrypt nh∆∞ th·∫ø n√†o v·ªõi data key v√† master key\n data dc encrypt b·∫±ng plain text data key, v√† data key ƒë√≥ th√¨ l·∫°i dc encrpypt bang plain text Master key n·ªØa.\n Q. team b·∫°n ƒëang deploy 1 micro service v√† 1 app l√™n aws. Y√™u c·∫ßu l√† ph·∫£i qu·∫£n l√Ω app ki·ªÉu orchestration. d√πng service n√†o ƒë·ªÉ gi·∫£m thi·∫øu effort admin nh·∫•t?\n Use the Elastic Container Service\n Q. b·∫°n ƒëang deploy 1 app v·ªõi ki·∫øn tr√∫c nh∆∞ sau; ec2 ƒë·ªÉ process video, ASG, SQS, c√≥ 2 m·ª©c gi√° free v√† premium. SAo ƒë·ªÉ user l√† premium lu√¥n dc ∆∞u ti√™n?\n t·∫°o 2 queue trong SQS. user c√≥ premium th√¨ ·ªü trong queue dc ∆∞u ti√™n x·ª≠ l√Ω trc\n Q. 1 ki·∫øn tr√∫c dc v·∫Ω ra cho mobile app. User c·∫ßn sign in bang google, facebook. User c·∫ßn c√≥ ch·ª©c nƒÉng qu·∫£n l√Ω profile. N√™n d√πng c√°i g√¨?\n using User pools in AWS Cognito User pool cung c·∫•p: Sign-up and sign-in services, Social sign-in with external identity, qu·∫£n l√Ω and user profiles, MFA, protect, phone,mail verfy user migration.\n Q. Cong ty b·∫°n l√†m 1 app l√†m vi·ªác v·ªõi DynamoDB. y√™u c·∫ßu all data c·∫ßn encrypt at rest. ph·∫£i l√†m n√†o?\n enable ch·ª©c nƒÉng encryption khi t·∫°o dynamoDB table, ch·ª© t·∫°o r·ªìi th√¨ ko encrypt dc nha\n Q. b·∫°n ƒëang deploy 1 app l√™n AWS Elastic Beanstalk. ƒë√¢y ch·ªâ l√† m√¥i tr∆∞·ªùng dev. C·∫ßn ph·∫£i t·ªën √≠t time nh·∫•t cho m·ªói l·∫ßn deploy. C·∫ßn d√πng ph∆∞∆°ng th·ª©c deploy n√†o?\n All at once, t·ªën √≠t time deploy nh·∫•t.nh∆∞ng n·∫øu deploy fail th√¨ app s·∫Ω b·ªã downtime, nh∆∞ng v√¨ m√¥i tr∆∞·ªùng dev n√™n ko ·∫£nh h∆∞·ªüng l·∫Øm.\n Q. b·∫°n ƒëang dev 1 app tr√™n aws theo ki·ªÉu micro service, c√°c services ƒëc t·∫°o base tr√™n lambda, v√¨ s·ª± ph·ª©c t·∫°p c·ªßa flow c√°c component n√™n c·∫ßn 1 c√°ch ƒë·ªÉ qu·∫£n l√Ω flow c·ªßa nhi·ªÅu fcn lambda? d√πng c√°i j?\n AWS Step function\n Q. b·∫°n c√≥ 1 app c·∫ßn inject data t·ª´ nhi·ªÅu thi·∫øt b·ªã. B·∫°n c·∫ßn ƒë·∫£m b·∫£o data ph·∫£i dc pre process tr∆∞·ªõc khi ƒë∆∞a v√†o ph√¢n t√≠ch. d√πng c√°i j?\n d√πng kinesis + lambda ƒë·ªÉ define data c·∫•u tr√∫c nh∆∞ n√†o tr∆∞·ªõc khi dc query.\n Q. b·∫°n c·∫ßn deploy 1 app l√™n ec2. data c·ªßa app dc store ·ªü 1 volume ri√™ng. v√† c·∫ßn dc encrypt at rest. C·∫ßn l√†m g√¨?\n khi t·∫°o volume c·∫ßn enable encrpytion, v√† s·ª≠ d·ª•ng KMS service\n Q. b·∫°n dc y√™u c·∫ßu customize content c·ªßa app. Content n√†y dc distribute to user th√¥ng qua Cloudfront. content origin ·ªü S3 l√†m n√†o?\n d√πng lambda@edge, n√≥ cho ph√©p customize content m√† Cloudfront deliver. V√≠ d·ª•: 1 web b√°n qu·∫ßn √°o. b·∫°n d√πng cookie ƒë·ªÉ ƒëi·ªÅu h∆∞·ªõng m√†u n√†o user ƒë√£ ch·ªçn cho √°o, b·∫°n d√πng lambda ƒë·ªÉ change c√°i request ch·ªçn m√†u ƒë√≥ ƒë·ªÉ cloudfront tr·∫£ v·ªÅ h√¨nh ·∫£nh c·ªßa c√°i √°o v·ªõi m√†u ƒë√£ ch·ªçn gi·∫£m ƒë·ªô tr·ªÖ, tƒÉng tr·∫£i nghi·ªám ng d√πng.\n Q. 1 tool ƒë·ªÉ qu·∫£n to√†n b·ªô life cycle c·ªßa d·ª± √°n, project?\n CodeStar\n Q. b·∫°n ƒëang d√πng Route53 ƒë·ªÉ point DNS name c·ªßa cty ƒë·∫øn web app. B·∫°n mu·ªën deliver app l√™n 1 ph·∫ßn nh·ªè ng∆∞·ªùi d√πng ƒë·ªÉ test. L√†m n√†o?\n D√πng Route53 weighted policy ƒë·ªÉ ch·ªâ ƒë·ªãnh 1 ph·∫ßn nh·ªè ng d√πng\n Q. khi d√πng Lambda v·ªõi S3, best practice ƒë·ªÉ pass parameter v√†o lambda l√† g√¨?\n D√πng Lambda environment variables ƒë·ªÉ truy·ªÅn s3 bucket name\n Q. b·∫°n c√≥ v√†i lambda function c·∫ßn deploy bang Codeploy, l√†m n√†o ƒë·ªÉ ch·∫Øc ch·∫Øn s·∫Ω deploy ƒë√∫ng version c·ªßa function\n ch·ªâ ƒë·ªãnh r√µ version s·∫Ω dc deploy trong AppSpec file\n Q. b·∫°n ƒëang dev 1 app game d√πng aws. l√†m n√†o ƒë·ªÉ ch·∫Øc ch·∫Øn c√°i bang x·∫øp hang c·ªßa game lu√¥n ch√≠nh x√°c?\n D√πng elastic cache Redis\n Q. b·∫°n ƒëang c√≥ 1 app dev tr√™n ec2, app c·∫ßn l·∫•y private IP c·ªßa instance 1 c√°ch t·ª± ƒë·ªông trong code. L√†m n√†o?\n Query trong instance metadata. curl http://169.254.169.254/latest/meta-data\n Q. trong Lambda code, function dc call ƒë·ªÉ lambda execute code l√† g√¨?\n Handler\n Q. l√†m sao ƒë·ªÉ S3 ko cho upload nh·ªØng file ch∆∞a dc encrypted\n update policy cho S3, trong policy ph·∫£i defined condition\n Q. b·∫°n c·∫ßn t√¨m ch·ªó l∆∞u data. nh∆∞ng data ko ph√π h·ª£p schema n√†o c·∫£. data c√≥ th·ªÉ ƒë√°nh index. d√πng lo·∫°i n√†o?\n DynamoDB\n Q. Eventual Consistency v√† Strong Consistency\n Eventual Consistency=Weak Consistency, nh·∫•t qu√°n y·∫øu, sau khi 1 update v√†o DB, c√°c read request sau ƒë√≥ ko ƒë·∫£m b·∫£o data latest, c√≥ th·ªÉ c≈©, nh∆∞ng sau 1 time sync gi·ªØa c√°c DB th√¨ cu·ªëi c√πng read request s·∫Ω c√≥ dc data latest.\nStrong Consistency: sau khi 1 update v√†o DB, c√°c read request sau ƒë√≥ ƒë·∫£m b√°o ch·∫Øc ch·∫Øn s·∫Ω c√≥ dc data latest, nh∆∞ng m√† n·∫øu v·∫´n ƒëang trong time sync gi·ªØa c√°c DB th√¨ read request ƒë√≥ s·∫Ω b·ªã delay (tr·∫°ng th√°i b·∫≠n) cho ƒë·∫øn khi sync OK h·∫øt m·ªõi tr·∫£ v·ªÅ KQ.\n Q. DynamoDB, item size 4KB c·∫ßn 1 read request (Strongly Consistency): 1 read request ~ 1 read capacity UNIT (RCU)\n (Eventually consistency): 2 read request ~ 1 read capacity UNIT (RCU) c√°i read request n√†o ƒë·∫Øt h∆°n?\n-\u0026gt; 1 read request (Strongly Consistency) c√≥ gi√° g·∫•p ƒë√¥i 1 read request (Eventually consistency) (v√¨ ph·∫£i 2 read request (Eventually consistency) m·ªõi ƒë·ªïi ngang ra 1 RCU) v·∫≠y n√™n n·∫øu file size = 8KB\n8KB -\u0026gt; b·∫°n c·∫ßn 2 read request -\u0026gt; b·∫°n c·∫ßn 2 RCU (Strongly Consistency)\n8KB -\u0026gt; b·∫°n c·∫ßn 2 read request -\u0026gt; b·∫°n c·∫ßn 2 chia 2 = 1 RCU (Eventually consistency)\n Q. DynamoDB, eventually read, 300 items/30s 10 items/s, 1 item = 6KB th√¨ c·∫ßn set table bao nhi√™u RC (read capacity 1 gi√¢y)?\n 6KB -\u0026gt; c·∫ßn 2 read request -\u0026gt; c·∫ßn 20 request trong 1s v√¨ l√† eventually read -\u0026gt; 20/2 = 10 RC trong 1s\n Q. c√≥ 1 app tr√™n ec2, sau 1 ELB, b·∫°n c·∫ßn monitor c√°c connection ƒë·∫øn ELB, d√πng c√°i j?\n enable access log tr√™n LB\n Q. B·∫°n c·∫ßn dev 1 app m√† y√™u c·∫ßu l√† ph·∫£i decoupled system. N√™n d√πng c√°i g√¨ ƒë·ªÉ x·ª≠ l√Ω message cho h·ªá th·ªëng decoupled (t√°ch r·ªùi)?\n SQS\n Q. 1 static web dc host tr√™n s3 bucket. c√≥ 1 java script section mu·ªën access v√†o data tr√™n 1 s3 bucket kh√°c. gi·ªù c√°i web ƒë√≥ ko load dc trong browser. v√¨ sao?\n ch∆∞a enable CORS for bucket\n Q. app c·ªßa b·∫°n gen report c·ªßa 10 user m·∫•t 4h. b·∫°n c·∫ßn 1 app c√≥ th·ªÉ report at real time, always up to date, gi·∫£i quy·∫øt dc s·ªë l∆∞·ª£ng request tang cao. d√πng c√°i j?\n post log data l√™n kinesis ƒë·ªÉ n√≥ analyze.\n Q. B·∫°n ƒëang l√†m mobile app. N·∫øu c·∫ßn ch·ªó l∆∞u session data of user th√¨ ch·ªçn db n√†o?\n DynamoDB\n Q. App c·ªßa b·∫°n d√πng DunamoDB, l√†m sao ƒë·ªÉ ch·∫Øc ch·∫Øn rang m·ªói khi 1 item dc update v√†o primary table th√¨ 1 record kh√°c c≈©ng dc insert v√†o secondary table\n d√πng DynamoDB stream\n Q. 1 app d√πng DynamoDB cho data backend. table size 20GB. Lhi scan table hay b·ªã l·ªói throttling. L√†m sao ƒë·ªÉ tr√°nh l·ªói.\n Reduced Page size\n Q. Cty c·ªßa b·∫°n mu·ªën t·∫°o 1 m√¥i tr∆∞·ªùng m·ªõi tr√™n aws. mu·ªën s·ª≠ d√πng Chef ƒë√£ c√≥ s·∫µn tr√™n aws. L√†m n√†o?\n D√πng AWS Opswork. n√≥ cung c·∫•p c√°c instance c√≥ chef, Puppet, tool ƒë·ªÉ automate configure server\n Q. B·∫°n build 1 app l√™n ec2. r·ªìi th·ª±c hi·ªán test. b·∫°n mu·ªën capture log t·ª´ web server ƒë·ªÉ ph√°t hi·ªán issue n·∫øu c√≥. L√†m n√†o?\n install cloudwatch agent l√™n ec2. configure ƒë·ªÉ agent ƒë√≥ send log c·ªßa server l√™n cloudwatch\n Q. App b·∫°n ƒëang d√πng Cognito ƒë·ªÉ qu·∫£n l√Ω user identity. Gi·ªù mu·ªën analyze data c·ªßa user tr√™n cognito. D√πng c√°i j?\n Cognito Stream\n Q. N·∫øu Elastic Beanstalk ko c√≥ m√¥i tr∆∞·ªùng th·ªèa m√£n nhu c·∫ßu?\n T·∫°o custom platform tr√™n Beanstalk, d√πng packer.\n Q. App d√πng DynamoDB, write 10 item trong 1s. m·ªói item 15.5 KB. V·∫≠y c·∫ßn setting Write throughput bao nhi√™u?\n 1KB = 1 write request-\u0026gt; 15.5KB = 16 write request.\n10 items -\u0026gt; 160 write request.\n1 read request = 1 WCU -\u0026gt; 160 WCU/s\n\u0026mdash;P2\u0026mdash;\n Q. Elastic Beanstalk ƒëang ch·∫°y 1 app. mu·ªën config m√¥i tr∆∞·ªùng th√¨ c·∫ßn ƒë·∫∑t file config trong folder n√†o?\n .ebextensions\n Q. Lambda default time out l√† m·∫•y gi√¢y?\n 3s\n Q. B·∫°n c·∫ßn setup restful api service tr√™n aws, n√™n d√πng k·∫øt h·ª£p nh·ªØng service n√†o?\n Host code api tr√™n lambda, d√πng API gateway ƒë·ªÉ access v√†o api.\nHost code api tr√™n ec2 d√πng ELB ƒë·ªÉ path routing to api\n Q. b·∫°n dev 1 app c·∫ßn d√πng service authen. nh∆∞ng c√≥ 1 s·ªë video c·∫ßn cho ph√©p access t·ª´ nh·ªØng unauthenticated identity. best practice?\n AWS cognito v√† enable unauthenticated identity\n Q. 1 app c·∫ßn process c√°c message theo order ko b·ªã dublicate, SQS c√≥ 2 lo·∫°i g√¨?\n FIFO v√† standard\n Q. CodeDeploy c√≥ process hook l√† g√¨?\n App stop -before install - after install - app start\n Q. L√†m sao ƒë·ªÉ debug Lambda?\n them code debug v√†o r·ªìi lambda s·∫Ω ƒë·∫©y all log l√™n cloudwatch log\n Q. D√πng Lambda nh∆∞ng c·∫ßn include c√°c th∆∞ vi·ªán ngo√†i th√¨ c·∫ßn ch√∫ √Ω g√¨?\n Ch·ªâ them c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt, tr√°nh upload to√†n b·ªô sdk, ch·ªâ up nh·ªØng module m√¨nh c·∫ßn cho lambda\n Q. team b·∫°n c√≥ 1 Code Commit repository trong accout b·∫°n. l√†m n√†o cho c√°c account kh√°c truy c·∫≠p repo c·ªßa b·∫°n?\n t·∫°o cross account role, cho h·ªç role arn ƒë·ªÉ h·ªç v√†o dc.\n Q. B·∫°n c√≥ 1 lambda ch·∫°y async (dc retry v√†i l·∫ßn tr∆∞·ªõc khi b·ªã discard trong TH l·ªói) L√†m sao ƒë·ªÉ debug issue n·∫øu function fail?\n D√πng DLQ Dead letter queue\n Q. b·∫°n ƒëang d√πng kinesis stream cho app ·ªü cty. Y√™u c·∫ßu data at rest ph·∫£i dc encrypt. D√πng c√°i j? server-side encrypt hay client-side encrypt?\n enable Server-side encrypt cho kinesis stream, v√¨ n√≥ l√† feature c·ªßa Kinesis, ko c√≥ feature Client side trong kinesis. ko n√™n d√πng client side encrypt v√¨ t·ªën nhi·ªÅu th·ªùi gian\n Q. b·∫°n ƒëang dev 1 app d√πng kinesis, ƒë·ªÉ tang throughput, b·∫°n d√πng nhi·ªÅu shard. Nh∆∞ng nh∆∞·ª£c ƒëi·ªÉm l√† j?\n ko th·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± order c·ªßa data khi multi shard, b·∫°n ch·ªâ c√≥ th·ªÉ order trong single shard th√¥i.\n Q. DynamoDB c√≥ m·∫•y ki·ªÉu primary key?\n ki·ªÉu 1: ch·ªâ c√≥ partion key, l√† key unique\nki·ªÉu 2: partion key v√† sort key k·∫øt h·ª£p th√†nh b·ªô key unique n√™n ch·ªçn nh·ªØng key dc generate 1 c√°ch auto ra key unique\n Q. S3 ƒëang d√πng. sau khi b·∫≠t c√°i encryption data at rest b·∫±ng KMS th√¨ performance gi·∫£m xu·ªëng v√¨ sao?\n m·ªói l·∫ßn b·∫°n download or upload S3 nh∆∞ng object ƒë√£ dc encrypt b·∫±ng KMS, S3 s·∫Ω g·ª≠i request ƒë·∫øn KMS ƒë·ªÉ encrypt ho·∫∑c decrypt, N·∫øu v∆∞·ª£t qu√° 5500 or 10000 th√¨ KMS s·∫Ω b·ªã throttle. v√† gi·∫£m performance\n Q. Read replica c√≥ d√πng cho DynamoDB v√† RDS ko?\n d√πng cho c·∫£ 2. (nh∆∞ng n·∫øu DynamoDB th√¨ ph·∫£i n√≥i r√µ l√† DynamoDB Accelerator (DAX) th√¨ c√≥ th·ªÉ c√≥ ƒë·∫øn 9 read replicas)\n Q. N·∫øu lambda ko push log l√™n cloudwatch log?\n c√≥ th·ªÉ do role ch∆∞a c√≥ quy·ªÅn push log to cloudwatch\n Q. v·ªõi 1 app ƒë∆∞·ª£c setting 5 RCU th√¨ m·ªói gi√¢y read dc bao nhi√™u KB data\n TH strongly consistent: 5x4=20KB read m·ªói gi√¢y TH enventually consistent: 5x2x4=40 KB read m·ªói gi√¢y\n Q. B·∫°n ƒëang d√πng DynamoDB, data c·∫ßn ƒëi qua v√†i region tr√™n to√†n TG. l√†m sao ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ?\n enable global tables\n Q. B·∫°n ƒëang d√πng DynamoDB, app dc design ƒë·ªÉ scan to√†n b·ªô table. L√†m sao ƒë·ªÉ tang performance?\n d√πng scan song song (parallel scan) ho·∫∑c design app ƒë·ªÉ d√πng query thay v√¨ d√πng scan\n Q. b·∫°n c√≥ 1 l∆∞·ª£ng l·ªõn data c·∫ßn dc stream tr·ª±c ti·∫øp l√™n S3. N√™n d√πng c√°i j?\n Kinesis Firehose v√¨ c√°i n√†y l√™n S3 tr·ª±c ti·∫øp\n Q. Cty b·∫°n ƒëang d√πng Kinesis Firehose ƒë·ªÉ stream data to S3. Nh∆∞ng h·ªç c·∫ßn transform data tr∆∞·ªõc khi l√™n S3 th√¨ d√πng c√°i j?\n kinesis Firehose invoke lambda ƒë·ªÉ transform data\n Q. b·∫°n host static web tr√™n S3. Code g·∫ßn ƒë√¢y dc change ·ªü ch·ªó java script ƒë·ªÉ call ƒë·∫øn 1 web page ·ªü 1 bucket t∆∞∆°ng t·ª±, nh∆∞ng browse block request. L√†m sao?\n enable CORS on bucket\n Q. S3 bucket m√† ƒëang b·∫≠t encrypt data at rest th√¨ khi request t·ªõi, S3 s·∫Ω deny nh·ªØng request nh∆∞ n√†o?\n nh·ªØng request ko c√≥ attribute sau trong header: x-amz-server-side-encryption\n Q. b·∫°n ƒëang enable server logging tr√™n s3. b·∫°n host 1 static web kho·∫£ng 1MB tr√™n s3 bucket. sau 2 tu·∫ßn th√¨ bucket 50MB. v√¨ sao?\n server logging ƒë√£ push log v√†o same bucket. c·∫ßn c√≥ lifecle ƒë·ªÉ delete log c≈©\n Q. 1 cty ƒëang c√≥ api vi·∫øt tr√™n lambda, h·ªç cho ph√©p KH access API c·ªßa h·ªç qua API gateway. hi·ªán t·∫°i c√≥ kho·∫£ng 6 th√°ng ƒë·ªÉ move t·ª´ api c≈© sang api m·ªõi. c√°ch n√†o?\n t·∫°o them 1 stage trong api gateway t√™n l√† v2, ƒë·ªÉ KH c√≥ th·ªÉ d√πng c·∫£ 2 version api.\n Q. 1 cty ƒëang dev app mobile .app c·∫ßn cho user dƒÉng nh√¢p bang facebook. c√°i g√¨ ƒë·ªÉ qu·∫©n l√Ω user?\n cognito\n Q. c√°i template CF c·ªßa b·∫°n c√≥ nhi·ªÅu resource th√¨ c√≥ n√™n tack th√†nh c√°c stack ri√™ng ko?\n c√≥, t·∫°o nested stack\n Q. B·∫°n dc y√™u c·∫ßu t·∫°o 1 API gateway stage m√† t∆∞∆°ng t√°c tr·ª±c ti·∫øp v·ªõi DynamoDB. c√°i g√¨ c·∫ßn d√πng?\n c·∫ßn t·∫°o 1 Intergration request. ƒë·ªÉ forward c√°c incoming method request ƒëen backend, n√≥ s·∫Ω b·∫Øt ch·ªç DynamoDB action ph√π h·ª£p nh·ªõ l√† DAX ch·ªâ ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ cho DynamoDB th√¥i\n Q. b·∫°n c√≥ 1 lambda function l√† backend cho API gateway ec2. b·∫°n c·∫ßn ƒë∆∞a api gateway URL ƒë·ªÉ user test. th√¨ c·∫ßn l√†m g√¨ tr∆∞·ªõc?\n t·∫°o 1 deployment trong api gateway, r·ªìi associate 1 stage cho deployment ƒë√≥. khi b·∫°n update API, b·∫°n c√≥ th·ªÉ redeploy bang c√°ch associate 1 stage m·ªõi v·ªõi c√°i deployment c≈©.\n Q. b·∫°n c√≥ 1 app host tr√™n ec2. app l√† 1 ph·∫ßn c·ªßa web www.demo.com, app dc update sang call API gateway, Tuy nhi√™n browser ko render respone v√† Javascript b·ªã l·ªói, C·∫ßn l√†m j?\n b·∫≠t CORS cho API gateway\n Q. 1 app ƒëang call RDS. l√∫c high load th√¨ do to√†n query DB n√™n th·ªùi gian repsone l·ªõn, c·∫ßn l√†m j?\n b·∫≠t read replica cho DB, ho·∫∑c ƒë·∫∑t elastiCache tr∆∞·ªõc DB ko th·ªÉ d√πng cloudfront tr∆∞·ªõc DB\n Q. 1 Cty d√πng cache cho app c·ªßa h·ªç. Nh∆∞ng vi·ªác recover data lost trong cache r·∫•t t·ªën ti·ªÅn n√™n ko mu·ªën ph·∫£i recover data lost in cache. L√†m n√†o?\n D√πng elasticCache Redis v√¨ n√≥ c√≥ HA ch·ª© Elasticache memcached ko c√≥ HA\n Q. B·∫°n d√πng ECS ƒë·ªÉ deploy c√°c containers. Nh∆∞ng gi·ªØa c√°c container ko dc access v√†o nhau, v√¨ m·ªói container l√† 1 kh√°ch h√†ng ph·∫£i l√†m th·∫ø n√†o?\n config SG c·ªßa c√°c ec2 ch·ªâ allow c√°c request nh·∫•t ƒë·ªãnh.\nN·∫øu mu·ªën d√πng Role th√¨ c√°i ƒë√≥ d√πng cho level Task c·ªßa ECS ch·ª© ko ph·∫£i level container.\nKo n√™n d√πng access key v√¨ s·∫Ω ko secure.\nC√°c level: 1 cluster g·ªìm nhi·ªÅu task ho·∫∑c service. 1 task g·ªìm nhi·ªÅu container\n Q. khi call API tr√™n ec2 g·∫∑p l·ªói: You are not authorized to perform this operation. Encoded authorization failure message: oGsbAaIV7wl\u0026hellip; th√¨ l√†m n√†o ƒë·ªÉ ƒë·ªçc dc message ƒë√£ b·ªã encode ƒë√≥\n d√πng command aws sts decode-authorization-message\n Q. B·∫°n v·ª´a define v√†i custom policy tr√™n aws, b·∫°n c·∫ßn test permission c·ªßa c√°c policy ƒë√≥. l√†m n√†o b·∫±ng CLI?\n ƒë·∫ßu ti√™n get c√°c context key sau ƒë√≥ d√πng command: aws iam simulate-custom-policy\n Q. 1 cty d√πng Codepipeline ƒë·ªÉ l√†m CICD. v√¨ l√Ω do b·∫£o m·∫≠t, c√°c resource ƒë·ªÉ deploy n·∫±m ·ªü c√°c account kh√°c nhau. Ph·∫£i l√†m n√†o?\n define custom master key KMS add a across acount role\n Q. B·∫°n deploy app len ec2, D√πng DynamoDB. B·∫°n d√πng X-Ray ƒë·ªÉ debug nh∆∞ng ko xem log dc. v√¨ sao?\n X-ray daemon ch∆∞a install tr√™n ec2 IAM role attach v√†o ec2 ko c√≥ permission upload to X-ray\n Q. App c·ªßa b·∫°n ·ªü tr√™n cloud. Gi·ªù c√≥ y√™u c·∫ßu all state c·ªßa all request v√†o STS. l√†m n√†o?\n xem log ƒë√≥ trong cloudtrail ch·ª© STS ko c√≥ ch·ª©c nƒÉng logging\n Q. b·∫°n d√πng CodeDeploy ƒë·ªÉ deploy 1 app to aws. app n√†y d√πng AWS system manager parameter ƒë·ªÉ store secure param, tr∆∞·ªõc khi deploy c·∫ßn l√†m j?\n c·∫•p permission cho CodeDeploy b·∫±ng IAM ROLE, d√πng command aws ssm get-parameters \u0026ndash;with-decrpytion ƒë·ªÉ n√≥ decrypt password r·ªìi d√πng trong app\n Q. DynamoDB c·ªßa b·∫°n ƒëang ch·∫°y t·ªët. sau khi update th√¨ performace c·ªßa DynamoDB gi·∫£m xu·ªëng. v√¨ c√°c query ƒëang ko d√πng partition key? l√†m sao gi·ªù?\n add an index cho dynamodb Table s·ª≠a l·∫°i c√°c query ƒë·ªÉ n√≥ query ƒë√∫ng partition key c≈©ng dc nh∆∞ng t·ªën effort\n Q. b·∫°n deploy 1 app l√™n ec2. ƒë·ªÉ test app th√¨ b·∫°n dc c·∫•p access key ƒë·ªÉ c√≥ quy·ªÉn write v√†o S3. sau khi test xong th√¨ 1 IAM role dc attach v√†o ec2 Role n√†y ch·ªâ c√≥ quy·ªÅn read s3. nh∆∞ng ph√°t hi·ªán ra ec2 v·∫´n c√≥ quy·ªÅn write s3. V√¨ sao?\n Bi·∫øn m√¥i tr∆∞·ªùng trong CLI ƒëang dc set cho accesskey v√† n√≥ s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n h∆°n IAM Role\n Q. B·∫°n ƒëang deploy 1 app l√™n Beanstalk worrker, worker n√†y ch·∫°y ƒë·ªãnh k·ª≥. C·∫ßn ph·∫£i config ƒë·ªãnh k·ª≥ trong file n√†o?\n cron.yaml\n Q. 1 app ƒëang d√πng KMS ƒë·ªÉ encrypt data. qu√° tr√¨nh encrpyt v√† decrypt di·ªÖn ra nh∆∞ n√†o?\n d√πng master key ƒë·∫øn generate ra data key d√πng data key ƒë·ªÉ decrpyt data\n Q. N·∫øu 1 web ƒëang enable CORS th√¨ khi call API backend c·∫ßn nh·ªØng header n√†o?\n access-control-allow-headers access-control-allow-origin access-control-allow-method\n Q. b·∫°n ƒëang l√†m vi·ªác v·ªõi 1 app l√†m v·ªõi XML message. b·∫°n c·∫ßn ƒë·∫∑t app sau API gateway ƒë·ªÉ KH call api. C·∫ßn config g√¨?\n c·∫ßn mapping request v√† respone\n Q. b·∫°n d√πng Serverless Application Model ƒë·ªÉ deploy app, th√¨ c·∫ßn d√πng command g√¨?\n SAM package command v√† SAM deploy command\n Q. app c·ªßa b·∫°n ƒëang tr·ªè ƒë·∫øn v√†i lambda fucntion, khi 1 thay ƒë·ªïi update lambda, b·∫°n c·∫ßn l√†m c√°c step n√†o ƒë·ªÉ traffic dc shift sang version function m·ªõi 1 c√°ch t·ª´ t·ª´?\n t·∫°o alias v·ªõi routing-config param, update alias v·ªõi routing-config param b·∫°n c√≥ th·ªÉ setting cho c√°c traffic ƒë·∫øn version m·ªõi l√† 2% c√≤n 98% l√† ƒë·∫øn version c≈©, n√≥i chung b·∫±ng c√°ch setting atrribute AdditionalVersionWeights.\n Q. 1 app ƒëang s·ª≠ d·ª•ng DynamoDB, m·ªói gi√¢y c√≥ h√†ng ng√†n request. 1 app kh√°c l·∫•y nh·ªØng thay ƒë·ªïi c·ªßa DynamoDB ƒë√≥, v·ªõi m·ª•c ƒë√≠ch ph√¢n t√≠ch. VD: a new customer add data v√†o DynamoDB, event n√†y invoke 1 app kh√°c send welcome email cho customer D√πng service n√†o?\n enable t√≠nh nƒÉng dynamoDB Stream c√°c VD kh√°c: 1 app modify DynamoDB, 1 app kh√°c capture nh·ªØng modify ƒë√≥ ƒë·ªÉ cung c·∫•p 1 usage metric cho mobile app 1 app ƒë·ªÉ t·ª± ƒë·ªông send noti cho all friend in group m·ªói khi c√≥ 1 ai ƒë√≥ upload image l√™n group.\n Q. B·∫°n ƒëang dev 1 app upload ·∫£nh t·ª´ user. n√™n l∆∞u ·∫£nh ·ªü ƒë√¢u, th√¥ng tin user upload ·ªü ƒë√¢u?\n ·∫£nh s3, th√¥ng tin user ·ªü dynamodb (object identifier)\n Q. l·ªói HEALTH_CONSTRAINTS_INVALID c·ªßa CodeDeploy l√† j?\n c·∫ßn gi·∫£m s·ªë l∆∞·ª£ng instance nh·ªè nh·∫•t m√† b y√™u c·∫ßu xu·ªëng, ho·∫∑c tƒÉng s·ªë l∆∞∆°ng instance trong deployment group l√™n\n Q. App c·ªßa b·∫°n c·∫ßn write to SQS. c√≥ policy l√† credential ph·∫£i lu√¥n encrypt v√† rote 1 l·∫ßn /1 tu·∫ßn. l√†m sao ?\n d√πng IAM Role attach v√†o c√°i ec2 ch·ª©a app\n\u0026mdash;P3\u0026mdash;\n Q. L√†m sao ƒë·ªÉ ch·∫Øc ch·∫Øn c√°c custom software dc install khi lauch t·ª´ elastic beanstalk. ph·∫£i define ·ªü ƒë√¢u?\n vi·∫øt th√†nh file YAML or JSON v√† ƒë·ªÉ v√†o folder .ebextensions\n Q. C√≥ th·ªÉ b·∫≠t MFA cho resource trong s3 ko?\n C√≥. setting trong policy\n Q. Codedeploy dc configure ƒë·ªÉ t·ª± ƒë·ªông rollback khi build fail, tuy nhi√™n khi rollback n√≥ ko retrieve dc nh·ªØng file c·∫ßn cho previous version, l√†m sao gi·ªù?\n manually add file c·∫ßn thi·∫øt v√†o instance ho·∫∑c t·∫°o 1 new revision.\n Q. CORS c√≥ th·ªÉ set cho Lambda function ko?\n ko, S3, v√† API gateway\n Q. b·∫°n c√≥ 1 v√†i lambda fucntion host tr√™n API gateway. b·∫°n c·∫ßn control xem ai access v√†o APi c·ªßa b·∫°n ƒëang host tr√™n api gateway th√¨ d√πng c√°ch n√†o?\n Cognito user pool, ho·∫∑c lambda authorizer\n Q. b·∫°n d√πng kinesis Firehose ƒë·ªÉ stream data to S3. nh∆∞ng cty y√™u c·∫ßu data c·∫ßn dc encrypt at rest, l√†m n√†o?\n enable Encrpytion cho kinesis Firehose, ch·∫Øc ch·∫Øn r·∫±ng \u0026lsquo;Kinesis data stream\u0026rsquo; dc d√πng ƒë·ªÉ transfer from producer n·∫øu b·∫°n config Kinesic data stream l√† data source c·ªßa Kinesis data Firehose delivery stream, th√¨ \u0026lsquo;Kinesis data Firehose\u0026rsquo; s·∫Ω ko store data at rest. m√† n√≥ store ·ªü Kinesic data stream VD: b·∫°n send data t·ª´ producer ƒë·∫øn data stream, \u0026lsquo;Kinesic data stream\u0026rsquo; s·∫Ω encrypt data b·∫±ng KMS, r·ªìi store data at rest. Khi \u0026lsquo;Kinesis data Firehose\u0026rsquo; v√†o ƒë·ªçc data, \u0026lsquo;Kinesic data stream\u0026rsquo; s·∫Ω decrypt data v√† g·ª≠i cho \u0026lsquo;Kinesis data Firehose\u0026rsquo; \u0026lsquo;Kinesis data Firehose\u0026rsquo; s·∫Ω store data ƒë√£ decrypt ƒë√≥ trong buffer memory r·ªìi g·ª≠i ƒë·∫øn destination lu√¥n (ch·ª© ko l∆∞u data at rest)\n Q. b·∫°n d√πng kinesis Firehose ƒë·ªÉ stream data ,nh∆∞ng cty y√™u c·∫ßu data c·∫ßn dc encrypt in transit, l√†m n√†o?\n install SSL certificate trong Kinesis Firehose ch·∫Øc ch·∫Øn r·∫±ng all record ƒëang dc transfer via SSL\n Q. best practice ƒë·ªÉ gi·∫£m cost khi d√πng SQS?\n d√πng long polling, ƒë·ªÉ send, receive, delete message ch·ªâ v·ªõi 1 action, d√πng SQS batch api actions\n Q. app c·ªßa b·∫°n send request ƒë·∫øn dynamoDB. nh∆∞ng v√¨ request tƒÉng l√™n n√™n app c·ªßa b·∫°n b·ªã l·ªói throttled. l√†m n√†o?\n tƒÉng throughout c·ªßa dynamoDB table d√πng exponential backoff ·ªü trong request c·ªßa app ƒë·ªÉ n√≥ retry request theo l≈©y k·∫ø delay time. b√¨nh th∆∞·ªùng m·ªçi ng hay d√πng c√°ch ch·ªù 1 time nh·∫•t ƒë·ªãnh (1s) ƒë·ªÉ retry request. n·∫øu 1000 request g·ª≠i ƒë·∫øn b·ªã error, sau 1s s·∫Ω c√≥ 1000 request nh∆∞ th·∫ø g·ª≠i ƒë·∫øn-\u0026gt; l·∫°i l·ªói n·∫øu d√πng exponential backoff, th·ªùi gian retry c·ªßa m·ªói request l√† kh√°c nhau -\u0026gt; kh·∫£ nƒÉng ko b·ªã l·ªói cao h∆°n.\n Q. b·∫°n ph·∫£i deploy 1 app d√πng cloudformation. instance n√†y c·∫ßn install nginx tr∆∞·ªõc. l√†m n√†o ƒë·ªÉ l√†m v·ªõi CF? best practice l√† g√¨?\n d√πng cfn-init helper script v√† AWS::CloudFormation::Init ƒë·ªÉ define c√°c config h∆°n l√† ch·∫°y t·ª´ng command trong user-data\n Q. App c·∫£u b·∫°n host tr√™n beanstalk. C√≥ 1 s·ª± thay ƒë·ªïi, nh∆∞ng b·∫°n c·∫ßn minimized downtime c·ªßa app v√† current env ko dc thay ƒë·ªïi. d√πng c√°ch deploy n√†o? All at Once? Rolling? Immutable? Rolling with Additional Batch?\n immutable, v√¨ Rolling with Additional Batch th√¨ current env v·∫´n c√≥ thay ƒë·ªïi\n Q. B·∫°n ƒëang deploy 1 app d√πng lambda v√† APi gateway. b·∫°n c·∫ßn deploy version m·ªõi nh∆∞ng version m·ªõi s·∫Ω cho 1 ph·∫ßn nh·ªè user th√¥i, l√†m n√†o?\n t·∫°o canary release ·ªü trong API gateway\n Q. b·∫°n c√≥ 1 dynamoDB table d√πng global secondary index. C√°ch n√†o ƒë·ªÉ l·∫•y result latest m√† ko ·∫£nh h∆∞·ªüng ƒë·∫øn RCU?\n query b·∫±ng evetual read. dynamoDB with global secondary index th√¨ ko support consistent read, ch·ªâ support eventual read th√¥i. v·ªõi l·∫°i d√πng Scan th√¨ s·∫Ω ·∫£nh h∆∞·ªüng performance to√†n b·ªô table\n Q. b·∫°n c·∫ßn setup 1 restFUL API in aws m√† s·∫Ω dc g·ªçi qua URL https://democompany.com/customers?ID=1 KH s·∫Ω nh·∫≠n dc th√¥ng tin detail khi c·∫•p ID cho API l√†m n√†o?\n t·∫°o API gateway v√† 1 lambda fucntion ƒë·ªÉ get th√¥ng tin detail c·ªßa KH expose GET method ra cho API Gateway\n Q. c√°c bc ƒë·ªÉ encrypt/decrypt data locally (hay c√≤n g·ªçi l√† client side) in your app thoe best practice n·∫øu b·∫°n ƒë√£ c√≥ customer master key?\n step encrypt:\ncustomer master key (CMK) -\u0026gt; (generate) plain text data key + encrypted data key\nplain text data key -\u0026gt; (encrypt) data r·ªìi th√¨ x√≥a ƒëi\nencrypted data key -\u0026gt; gi·ªØ l·∫°i\nstep decrypt:\nencrypted data key -\u0026gt; (decrypt) plain text data key -\u0026gt; (decrypt) data r·ªìi th√¨ x√≥a ƒëi\nchi ti·∫øt:\nD√πng operation sau (GenerateDataKey) ƒë·ªÉ generate t·ª´ \u0026lsquo;customer master key\u0026rsquo; CMK ra \u0026lsquo;data encryption key\u0026rsquo; (\u0026lt;\u0026mdash;\u0026ndash;ƒë√¢y l√† respone)\nl·∫•y c√°i \u0026lsquo;plaintext data key\u0026rsquo; (·ªü trong field \u0026lsquo;Plaintext\u0026rsquo; c·ªßa respone) ƒë·ªÉ encrypt data locally, r·ªìi x√≥a lu√¥n c√°i \u0026lsquo;plaintext data key\u0026rsquo; ƒë√≥\nl∆∞u l·∫°i c√°i \u0026lsquo;encrypted data key\u0026rsquo; (·ªü trong field \u0026lsquo;CiphertextBlob\u0026rsquo; c·ªßa respone) ·ªü b√™n c·∫°nh data (locally/client side) ƒë√£ encrypt ƒë·ªÉ sau c√≤n decrypt\nD√πng operation sau (Decrypt) ƒë·ªÉ decrypt c√°i \u0026lsquo;encrypted data key\u0026rsquo; m√† m√¨nh ƒë√£ l∆∞u, decrypt xong dc \u0026lsquo;plaintext data key\u0026rsquo;\nD√πng \u0026lsquo;plaintext data key\u0026rsquo; ƒë·ªÉ decrypt data locally, r·ªìi x√≥a lu√¥n c√°i \u0026lsquo;plaintext data key\u0026rsquo; ƒë√≥\n Q. mu·ªën migrate source code t·ª´ SVN l√™n Code Commit? l√†m n√†o?\n tr∆∞·ªõc ti√™n ph·∫£i t·ª´ SVN migrate l√™n Git tr∆∞·ªõc r·ªìi ti·∫øp l√™n Code Commit\n Q. ƒë·ªÉ s·ª≠ d·ª•ng hi·ªáu qu·∫£ indexing ƒë·ªÉ tƒÉng performance cho dynamoDB l√†m n√†o?\n s·ªë l∆∞·ª£ng index n√™n keep minimum, nh·ªØng c√°i index hi·∫øm khi d√πng ch·ªâ l√†m tƒÉng storage th√¥i\ntr√°nh vi·ªác ƒë√°nh index cho c√°c table ph·∫£i Write data nhi·ªÅu.\n Q. trong dynamoDB th√¨ Projection expression, Condition Expressions, Update Expressions l√† g√¨?\n Projection expression d√πng ƒë·ªÉ query table\nCondition Expressions ƒë·ªÉ ch·ªâ ƒë·ªãnh ƒëi·ªÅu ki·ªán modify item\nUpdate Expressions ƒë·ªÉ update item\n Q. DAX v√† dynamoDB x·ª≠ l√Ω v·ªõi c√°c request strongly read v√† eventually read nh∆∞ n√†o?\n n·∫øu 1 eventually read request ƒë·∫øn, check xem DAX c√≥ cache ko, n·∫øu ko th√¨ pass t·ªõi dynamoDB, c√≥ kq th√¨ cache l·∫°i trong DAX r·ªìi m·ªõi pass cho user\nn·∫øu 1 strongly read request ƒë·∫øn, pass lu√¥n t·ªõi dynamoDB, c√≥ kq th√¨ pass cho user lu√¥n\n Q. b·∫°n ƒëang l√†m vi·ªác v·ªõi dynamoDB ƒë·ªÉ save c√°c string v√†o ƒë√≥. nh∆∞ng n·∫øu \u0026gt; 400KB th√¨ s·∫Ω b·ªã l·ªói, l√†m n√†o?\n l∆∞u string l·ªõn v√†o s3, dynamoDB ch·ªâ l∆∞u object identifier point to s3 th√¥i\ncon s·ªë 400 l√† fix c·ª©ng r·ªìi ko th·ªÉ thay ƒë·ªïi\nc√≥ th·ªÉ n√©n string ƒë·ªÉ gi·∫£m size nh∆∞ng ch·ªâ l√† gi·∫£i ph√°p tam th·ªùi\n Q. b·∫°n ƒëang d√πng lambda ƒë·ªÉ access DB. Y√™u c·∫ßu l√† all DB connection string c·∫ßn encrpyt at rest? l√†m n√†o?\n d√πng environment variable ƒë·ªÉ l∆∞u db connection string\nencrypt environment variable ƒë√≥\n Q. Trong dynamoDB, ph√¢n bi·ªát 1-Read Side Cache, 2-Read through Cache, 3-Write Through Cache, 4-Write Around Cache, 5-Write back/behind Cache?\n Read Side Cache:\napp request read to cache (redis) n·∫øu ko c√≥ trong cache th√¨ m·ªõi read t·ª´ dynamoDB, r·ªìi return to app, r·ªìi write v√†o cache\nuseful for read-heavy workload, nh∆∞ng vi·ªác ghi v√†o cache l√† data ko consistence, ko durable\nRead through Cache:\napp request read to cache DAX, n·∫øu ko c√≥ trong cache th√¨ read trong DynamoDB, result dc cache v√†o DAX, return to app\nuseful for read-heavy workload\nWrite Through Cache:\napp request write to DAX, DAX write v√†o dynamoDB, n·∫øu th√†nh c√¥ng m·ªõi write v√†o DAX, r·ªìi return to app\nuseful for read heavy workload, data consistence nh∆∞ng ko gi√∫p g√¨ n·∫øu write-heavy workload\nWrite Around Cache:\napp request write tr·ª±c ti·∫øp dynamoDB, r·ªìi m·ªõi write v√†o DAX, r·ªìi return to app\nuseful for write l∆∞·ª£ng data l·ªõn considerable\nWrite back/behind Cache:\napp request write to DAX, DAX return status lu√¥n cho app, in background data write to DynamoDB, r·ªìi write v√†o cache DAX\nuseful cho write-heavy workload nh∆∞ng c√≥ th·ªÉ m·∫•t data\n Q. khi t·∫°o table trong dynamoDB, n·∫øu mu·ªën encrypt data b·∫°n c√≥ th·ªÉ ch·ªçn 2 ki·ªÉu customer master key l√† j?\n AWS owned CMK, key c·ªßa dynamodb, ko m·∫•t ph√≠ th√™m\nAWS managed CMK, key c·ªßa KMS, m·∫•t ph√≠ KMS\n Q. b·∫°n dev 1 app d√πng elastic beanstalk. l√†m c√°ch n√†o ƒë·ªÉ trace issue, debug d·ªÖ nh·∫•t khi c√≥ issue?\n d√πng AWS X-Ray\n Q. B·∫°n ƒëang dev 1 app m√† n√≥ c·∫ßn ch·ª©c nƒÉng sign-up sign-in, b·∫°n ko mu·ªën code fucntion sign up in ƒë√≥, mu·ªën ƒë·∫£m b·∫£o code s·∫Ω dc excute t·ª± ƒë·ªông sau khi user sign in. l√†m n√†o?\n D√πng cognito, t·∫°o lambda fucntion v√† trigger function ƒë√≥ m·ªói khi user-sign in\n Q. b·∫°n dev 1 app tr√™n on premise network c·ªßa b·∫°n. app c·∫ßn t∆∞∆°ng t√°c v·ªõi aws services. l√†m n√†o?\n t·∫°o iam user, generate ra Access key, s·ª≠ d·ª•ng access key ƒë√≥ ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi aws service\n Q. B·∫°n c·∫ßn migrate 1 m√¥i tr∆∞·ªùng development c√≥ s·∫µn g·ªìm c√°c Docker container l√™n Aws. l√†m n√†o ƒë·ª° t·ªën effort?\n t·∫°o app v√† environment cho docker container ·ªü trong Beanstalk v√¨ n√≥ h·ªó tr·ª£ nhi·ªÅu platform v√† language\n Q. B·∫°n c·∫ßn qu·∫£n l√Ω blue/green deployment cho app. d√πng c√°ch n√†o gi·ªù?\n trong Beanstalk d√πng swap URL\nho·∫∑c d√πng Route53 v·ªõi weighted routing policy, ƒë·ªÉ ki·ªÉm so√°t traffic cho subdomain v√† domain\n Q. B·∫°n c·∫ßn cho developers quy·ªÅn l√†m vi·ªác v·ªõi beanstalk enviroment nh∆∞ng h·ªç ko dc v√†o console? l√†m n√†o?\n cho h·ªç l√†m vi·ªác qua EBeanstalk CLI\n Q. b·∫°n deploy 1 lambda function dc invoke qua APi gateway. b·∫°n mu·ªën bi·∫øt n·∫øu c√≥ l·ªói khi lambda ƒë∆∞·ª£c invoke th√¨ l√†m n√†o?\n d√πng Cloudwatch Metrics for AWS Lambda\n\u0026mdash;P4\u0026mdash;\n Q. B·∫°n c√≥ v√†i function lambda, c·∫ßn g·ªçi ƒë√∫ng version c·ªßa function ƒë√≥, l√†m n√†o?\n t·∫°o alias\n Q. B·∫°n ƒëang c·∫ßn test cac services b·∫±ng c√°ch query b·∫±ng REST API, c·∫ßn c√°i g√¨?\n Access key\n Q. app ƒëc x√¢y d·ª±ng b·ªõi nhi·ªÅu component ph√¢n t√°n. nh∆∞ng c√°c th√¥ng tin quan tr·ªçng trao ƒë·ªïi gi·ªØa c√°c component b·ªã m·∫•t khi 1 c√°i component b·ªã down. l√†m sao?\n d√πng SQS ƒë·ªÉ qu·∫£n l√Ω message gi·ªØa c√°c component\n Q. 1 cty c√≥ c√°c aws ec2 v√† on-premise, mu·ªën d√πng aws codeDeploy th√¨ c·∫ßn nh·ªØng g√¨?\n CodeDeploy agent ph·∫£i dc install tr√™n c·∫£ 2 server,\nc·∫£ 2 server ƒë·ªÅu dc tagged,\non-premise instance ko c·∫ßn IAM instance profile\n Q. ƒëang d√πng SQS, mu·ªën sau khi message v√†o queue n√≥ s·∫Ω invisble trong v√≤ng 5p l√†m n√†o?\n setting delay queues (0 ƒë·∫øn 15p)\ns·ª≠ d·ª•ng message timer cho individual message\n Q. parameter trong CF cho ph√©p nh·ªØng ki·ªÉu d·ªØ li·ªáu n√†o?\n String, number, list, commadelimited list, SSM param, aws type\n Q. b·∫°n t·∫°o nhi·ªÅu s3 bucket, khi mu·ªën x√≥a s3 bucket b·∫±ng DeletionPolicy trong Cloudformation th√¨ c·∫ßn ch√∫ √Ω g√¨?\n C√°c object trong s3 bucket c·∫ßn ph·∫£i b·ªã x√≥a h·∫øt tr∆∞·ªõc\n Q. c√≥ th·ªÉ k·∫øt h·ª£p DynamoDB v·ªõi lambda ƒë·ªÉ m·ªói khi c√≥ thay ƒë·ªïi tr√™n dynamnoDB th√¨ trigger lambda ko?∆∞\n c√≥. Enable dynamoDB stream ƒë·ªÉ invoke lambda\n Q. b·∫°n l√†m vi·ªác v·ªõi dynamoDB, b·∫°n mu·ªën bi·∫øt consumed capacity cho m·ªói query l√† bao nhi√™u th√¨ l√†m n√†o?\n set ReturnConsumedCapacity in query to TOTAL\ndefault n√≥ set l√† NONE\n Q. b·∫°n c√≥ 1 s3 bucket ƒë√£ dc versioning v√† encrypted.nh·∫≠n h√†ng ng√†n PUT request m·ªói ng√†y. sau 6 th√°ng th√¨ c√≥ l·ªói HTTP 503, l√†m n√†o?\n d√πng S3 inventory tool, kh·∫£ nƒÉng l√† c√≥ nh·ªØng object l√™n h√†ng tri·ªáu version, d√πng inventory ƒë·ªÉ x√°c ƒë·ªãnh object n√†o\n Q. b·∫°n l√†m vi·ªác v·ªõi aws Step function, c·∫ßn ch√∫ √Ω c√°c recommend practice g√¨?\n set timeout khi define c√°c state, th∆∞·ªùng th√¨ step functin ch·ªâ d·ª±a v√†o ph·∫£n h·ªìi c·ªßa worker, n√™n n·∫øu c√≥ l·ªói m√† ko set timeout s·∫Ω b·ªã stuck\nv·ªõi l∆∞·ª£ng payload l·ªõn qu√° 32KB truy·ªÅn gi·ªØa c√°c state th√¨ s·∫Ω b·ªã terminate, n√™n l∆∞u tr√™n s3 r·ªìi truy·ªÅn c√°i resource name thay v√¨ raw data\n Q. B·∫°n t·∫°o 1 lambda fucntion d√πng Cloudformation. sao ƒë·ªÉ route 5% c√°c traffic to version m·ªõi\n set AdditionalVersionWeights = 0.05\n Q. B·∫°n d√πng aws CodeCommit, t·∫°o c√°c repository. b·∫°n c·∫ßn share c√°c repo ƒë√≥ cho developer team, l√†m n√†o?\n v√†o IAM console, ch·ªçn user, ch·ªçn \u0026lsquo;HTTPS Git credentials for AWS CodeCommit\u0026rsquo; ƒë·ªÉ generate user/password\ncho ph√©p dev connect qua htttps r·ªìi gitclone t·ª´ codeCommit v·ªÅ\n Q. Move t·ª´ jenkins l√™n codePipeline th√¨ c·∫ßn d√πng g√¨ detect c√°c change c·ªßa pipeline?\n Cloudwatch Events\n Q. d√πng dynamoDB m√† b·ªã l·ªói ProvisionedThroughputExceededException trong log th√¨ c·∫ßn l√†m j?\n b·ªüi v√¨ dynamoDB t·ª± ƒë·ªông retry c√°c request, n·∫øu request qu√° l·ªõn th√¨ s·∫Ω b·ªã l·ªói ProvisionedThroughputExceededException\nconsider vi·ªác d√πng exponential backoff trong code\n Q. b·∫°n d√πng SQS ƒë·ªÉ send message gi·ªØa c√°c component, b·∫°n c·∫ßn g·ª≠i c·∫£ metadata c√πng message th√¨ l√†m n√†o?\n d√πng message attribute ƒë·ªÉ g·ª≠i metadata c√πng l√∫c v·ªõi message body.\n Q. b·∫°n c·∫ßn d√πng cloudformation ƒë·ªÉ t·∫°o ra stack ·ªü 1 account aws kh√°c, l√†m n√†o?\n d√πng ch·ª©c nƒÉng Create StackSets\n Q. Aws Opswork support officially nh·ªØng tool n√†o?\n Chef, Puppet\n Q. B·∫°n ƒëang mu·ªën deploy app l√™n ECS. mu·ªën ch·∫Øc ch·∫Øn c√≥ th·ªÉ d√πng X-ray ƒë·ªÉ trace deploy. l√†m n√†o?\n t·∫°o docker image v·ªõi X-ray, r·ªìi deploy container to ECS\n Q. b·∫°n d√πng CodeBuild, code dc l∆∞u ·ªü S3, khi run build b·ªã l·ªói addressed using the specified endpoint, v√¨ sao?\n Bucket ko ·ªü c√πng region v·ªõi CodeBuild project\n Q. b·∫°n deploy 1 app l√™n Beanstalk, app c·∫ßn RDS l√† 1 ph·∫ßn trong Beanstalk, nh∆∞ng mu·ªën DB dc preserve ngay c·∫£ khi Environment down. l√†m n√†o?\n ch·∫Øc ch·∫Øn r·∫±ng ƒë√£ setting c√°i field Retention khi t·∫°o DB = create snapshot\n Q. M·ªói khi upload ho·∫∑c deploy source l√™n Beanstalk, n√≥ c√≥ gi·ªØ l·∫°i version app ko?\n C√≥\n Q. d√πng DynamoDB, Read capacity l√† 5 units, mu·ªën ƒë·ªçc 5 item 1 gi√¢y, m·ªói item 2KB, default consistency=eventually consistency total size KB dc ƒë·ªçc 1s?\n 2KB -\u0026gt; l√†m tr√≤n 4KB -\u0026gt; c·∫ßn 1 read request\neventually consistency -\u0026gt; 1/2 = 0.5 RCU\ns·ªë KB/s = 5 x 2 = 10 KB/s\neventually -\u0026gt; s·ªë KB/s = 10 x 2 = 20 KB/s\nv√¨ RCU = 0.5 -\u0026gt; 20 x 0.5 = 10 KB\n Q. cty b·∫°n ƒëang qu·∫£n l√Ω deployment env b·∫±ng CodeDeploy. H·ªç mu·ªën t·ª± ƒë·ªông lu√¥n c·∫£ vi·ªác deploy c√°i deployment env. l√†m n√†o?\n d√πng Cloudformation\n Q. B·∫°n qu·∫£n l√Ω nhi·ªÅu DB v·ªõi nhi√™u passwword? l√†m sao ƒë·ªÉ l∆∞u pass cho DB secure?\n d√πng aws secret manager\nb·∫°n t·∫°o db credential, l∆∞u n√≥ v√†o Aws secret manager\nkhi app c·∫ßn access db, n√≥ s·∫Ω request Aws secret manager\nAws secret manager query, decrypt v√† tr·∫£ v·ªÅ cho app qua HTTPS, TLS\napp d√πng credential ƒë√≥ access db\n Q. B·∫°n ƒëang dev 1 mobile app. User ƒë√£ authen b·∫±ng facebook. App c·∫ßn get temporary access credential ƒë·ªÉ l√†m vi·ªác v·ªõi aws resource d√πng c√°i j?\n AssumeRoleWithWebIdentity\n Q. b·∫°n c√≥ 1 s3 bucket, bucket ƒë√≥ c·∫ßn ph·∫£i dc access t·ª´ nh·ªØng aws account kh√°c. l√†m th·ªÉ n√†o ƒë·ªÉ secure c√°c account kh√°c c√≥ th·ªÉ access v√†o resource c·ªßa b·∫°n?\n t·∫°o cho h·ªç 1 role c√≥ quy·ªÅn assume role v√† cung c·∫•p cho h·ªç ARN role ƒë√≥\nh·ªç cung c·∫•p cho b·∫°n externalID unique ƒë·ªÉ b·∫°n update trust policy trong role c·ªßa h·ªç.\n Q. B·∫°n ƒëang c√≥ Beanstalk d√πng java7 tomcat7. B·∫°n mu·ªën d√πng java 8 v√† tomcat 8.5 l√†m n√†o?\n t·∫°o m√¥i tr∆∞·ªùng m·ªõi\nupload code, deploy, fix bug\nSwap Environment Url (CNAME)\nok th√¨ x√≥a m√¥i tr∆∞·ªùng c≈©\n Q. b·∫°n ƒëang d√πng APi gateway, sao ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ c·ªßa c√°c request to API gateway?\n d√πng API caching\n Q. d√πng DynamoDB m√† b·ªã l·ªói HTTP 400, l√† sao?\n v∆∞·ª£t qu√° throughput c·ªßa table, ho·∫∑c thi·∫øu required parameters\n Q. app c·ªßa b·∫°n d√πng SQS, message s·∫Ω xu·∫•t hi·ªán trong queue trong kho·∫£ng 20-60s, n√™n d√πng lo·∫°i SQS n√†o?\n long polling, SQS s·∫Ω ch·ªù cho ƒë·∫øn khi message xu·∫•t hi·ªán\n Q. AWS Athena ƒë·ªÉ l√†m g√¨?\n ƒë·ªÉ perform SQL query in data S3\n Q. 1 app ƒëang connect ƒë·∫øn RDS. b·∫°n c√≥ nhi·ªám v·ª• debug performance issue li√™n quan ƒë·∫øn query, l√†m n√†o?\n l·∫•y c√°i log Slow query log c·ªßa RDS\nError log: cung c·∫•p log l·ªói,\nGeneral query log: cung c·∫•p log query normal v√† client connect time,\nSlow query log: cung c·∫•p log m√† query ch·∫°y ch·∫≠m h∆°n b√¨nh th∆∞·ªùng\n Q. c√≥ th·ªÉ bi·∫øn 1 ec2 th√†nh ECS ko?\n C√†i install ecs agent tr√™n Amazon ec2\n Q. b·∫°n c√≥ 1 app host b·ªüi s3, sao ƒë·ªÉ t√≠nh s·ªë get request to s3?\n d√πng cloudwatch\n\u0026mdash;P5\u0026mdash;\n Q. b·∫°n c·∫ßn ph∆∞∆°ng th·ª©c deploy lambda function d√πng SAM, c·∫ßn l√†m g√¨?\n 1, sam init app: t·∫°o ra 1 file yaml define Lambda function v√† API gateway\n2, test app tr√™n local\n3, sam package app: upload app l√™n s3, output ra 1 file YAML ch·ª©a s3 uri\n4, sam deploy app b·∫±ng file YAML m·ªõi ƒë√≥\n Q. trong Step Function b·∫°n t·∫°o lambda fucntion b·ªã l·ªói ServiceException, best practice l√† g√¨?\n Retry. D√πng Retry Code v·ªõi ch·ªâ ƒë·ªãnh ErrorEquals\nko th·ªÉ d√πng Catch code v√¨ n√≥ dc √°p d·ª•ng khi retry 1 s·ªë l·∫ßn\n Q. b·∫°n ƒëang d√πng CodeBuild cho 1 app, app ƒë√≥ c·∫ßn access resource trong private subnet, l√†m n√†o?\n Default l√† Codebuild ko access dc v√†o resource trong VPC. C·∫ßn ph·∫£i s·ª≠a configure c·ªßa CodeBuild\nth√™m th√¥ng tin v·ªÅ VPC, subnet, SG m√† CodeBuild c·∫ßn access\n Q. S3 bucket cho ph√©p bao nhi√™u PUT GET request m·ªói gi√¢y v√† n·∫øu b·∫°n t·∫°o 3 prefix trong s3 n·ªØa th√¨ tƒÉng l√™n bao nhi√™u request?\n 3500 PUT POST DELETE/s, 5500 GET /s\nn·∫øu c√≥ 3 prefix th√¨ x3\n Q. B·∫°n ƒëang d√πng X-ray, n√≥ cung c·∫•p nh·ªØng t√≠nh nƒÉng g√¨, c·∫ßn trace all incoming http request to app c·ªßa b·∫°n d√πng c√°i j?\n interceptor ƒë·ªÉ trace all incoming http request\nclient handler ƒë·ªÉ trace sdk m√† m√¨nh d√πng ƒë·ªÉ call ƒë·∫øn aws resource\nhttp client ƒë·ªÉ trace internal v√† external web service\n Q. Data pipeline v√† c√°c kh√°i ni·ªám, c√°i g√¨ define source data v√† destination data?\n Data Node: define location c·ªßa input v√† output data dc store ·ªü ƒë√¢u\nActivities: define work c·ªßa pipeline\nResources: l√† nh·ªØng c√°i th·ª±c hi·ªán vi·ªác m√† activity ch·ªâ ƒë·ªãnh\nAction: dc trigger khi activity th·ªèa m√£n ƒëi·ªÅu ki·ªán\n Q. Load data s3 sang Readshift d√πng command g√¨?\n Copy\n Q. b·∫°n d√πng Cognito, y√™u c·∫ßu l√† n·∫øu user c√≥ compromised credential (v√≠ d·ª• password d·ªÖ ƒëo√°n) th√¨ ph·∫£i b·∫Øt user t·∫°o m·ªõi passw, l√†m n√†o?\n tick v√†o ph·∫ßn block-use trong Advance security section ƒë·ªëi v·ªõi nh·ªØng compromised credential\nt·∫°o user pool trong Cognito\n Q. B·∫°n c√≥ nhi·ªÅu data tr√™n aws, c·∫ßn 1 gi·∫£i ph√°p nhanh ƒë·ªÉ d·ª±ng visualization screens cho ƒë·ªëng data ƒë√≥, d√πng j?\n Quicksight\n Q. AWS Glue l√† g√¨?\n ƒë·ªÉ qu·∫£n l√Ω service ETL (extract, transform, load)\n Q. b·∫°n c√≥ S3 bucket v√† l∆∞u object size tr√™n DynamoDB, y√™u c·∫ßu l√† m·ªói khi s3 c√≥ object dc upload th√¨ s·∫Ω trigger 1 record update trong DynamoDB, l√†m n√†o?\n S3 event trigger tr·ª±c ti·∫øp Lambda , ko c·∫ßn SQS\n Q. kinesis ƒë·ªÉ t√≠nh s·ªë l∆∞·ª£ng shards c·∫ßn c√¥ng th·ª©c g√¨?\n Incoming write bandwith, Outgoing read bandwidth\nnumber_of_shards = max(incoming_write_bandwidth_in_KiB/1024, outgoing_read_bandwidth_in_KiB/2048)\n Q. b·∫°n dev app d√πng ECS, docker, th√¨ s·∫Ω l∆∞u docker base image ·ªü ƒë√¢u?\n ECR, Dockerhub\n Q. b·∫°n t·∫°o policy cho user access v√†o resource, sau ƒë√≥ b·∫°n update policy th√¨ user report ko v√†o dc resource n·ªØa, c√≥ th·ªÉ revert l·∫°i version policy c≈© ko?\n c√≥, set version policy c≈© th√†nh default\n Q. b·∫°n d√πng CodeCommit, y√™u c·∫ßu l√† m·ªói khi c√≥ change l√™n repo th√¨ ph·∫£i notification cho b·∫°n, l√†m n√†o? co d√πng AWS Config ko?\n Lambda ho·∫∑c SNS\nko d√πng AWS Config dc v√¨ n√≥ l√† ƒë·ªÉ monitor nh·ªØng thay ƒë·ªïi v·ªÅ m·∫∑t configuration ch·ª© ko ph·∫£i thay ƒë·ªïi tr√™n repository\n Q. b·∫°n ƒë·ªãnh d√πng AWS Batch service ƒë·ªÉ ch·∫°y c√°c job compute, tool n√†o ƒë·ªÉ monitor progress c·ªßa job?\n Cloudwatch event\n Q. best practice khi define secondary index cho dynamodb l√† g√¨?\n keep minimum index\ntr√°nh ƒë√°nh index cho table ph·∫£i ghi nhi·ªÅu\n Q. b·∫°n ƒëang d√πng RDS cho app, y√™u c·∫ßu l√† c√°c connection t·ª´ app t·ªõi DB c·∫ßn encrypt th√¨ d√πng g√¨? SSL v√† KMS ch·ªçn n√†o?\n SSL, chuy√™n d√πng ƒë·ªÉ encrypt connect to DB, data in transit\nKMS ch·ªâ ƒë·ªÉ d√πng encrpyt data at rest ho·∫∑c before transit th√¥i\n Q. b·∫°n ƒëang d√πng lambda, v√† lambda c·ªßa b·∫°n c·∫ßn nhi·ªÅu th∆∞ vi·ªán thirdparty nh∆∞ x·ª≠ l√Ω ·∫£nh, hay ƒë·ªì h·ªça, l√†m n√†o?\n t·∫°o lambda deployment package ch·ª©a c·∫£ lambda v√† th∆∞ vi·ªán thirdparty, upl√™n s3, r·ªìi d√πng cli\n Q. b·∫°n ƒëang d√πng cloudfront v·ªõi origin l√† s3 cho web distribution. b·∫°n mu·ªën control th·ªùi gian object dc l∆∞u trong cache l√†m n√†o?\n config c√°i origin ph·∫£i c√≥ Expires header field m·ªói object, ho·∫∑c add Cache control\nconfig Cloudfront s·ª≠ d·ª•ng ch·ªâ ƒë·ªãnh time trong TTL\n Q. b·∫°n d√πng 1 tool POSTMAN ƒë·ªÉ send API request t·ªõi resource trong aws. b·∫°n c·∫ßn sign request ƒë·ªÉ aws bi·∫øt b·∫°n l√† ai? th√¨ d√πng gi? user+pass, private key file, KMS?\n D√πng access key + secret key.\nCh·ª© KMS ƒë·ªÉ encrpyt data, user/pass d√πng v√†o console, private key d√πng ƒë·ªÉ login ec2\n Q. ƒëang d√πng SNS, mu·ªën ch·ªâ nh·∫≠n c√°c lo·∫°i message nh·∫•t ƒë·ªãnh, ko mu·ªën nh·∫≠n h·∫øt message dc publish v√†o topic dc ko?\n filter policy c·ªßa topic\n Q. b·∫°n ƒëang c√≥ s·∫µn 1 redshift cluster, c√≥ nhi·ªÅu data quan tr·ªçng, c·∫ßn encrypt at rest l√†m n√†o? c√≥ d√πng SSL cert dc ko?\n C·ª© th·∫ø enable KMS encryptor th√¥i (c√°i n√†y l√† feature m·ªõi, tr∆∞·ªõc ƒë√¢y ko l√†m th·∫ø dc)\ntr∆∞·ªõc ƒë√¢y ph·∫£i t·∫°o 1 cluster m·ªõi v√† enable KMS encrypt l√™n, unload kh·ªèi cluster c≈©, reload l·∫°i t·ªõi cluster m·ªõi\nSSL cert ch·ªâ ƒë·ªÉ encrpyt data in transit\n Q. b·∫°n c√≥ app tr√™n ec2, gi·ªù mu·ªën n·∫øu primary app t·∫°ch th√¨ s·∫Ω route traffic ƒë·∫øn 1 static web th·ª© 2, l√†m n√†o? R53 hay LB?\n d√πng route53, LB ch·ªâ d√πng ƒë·ªÉ distribute traffic (ph√¢n ph·ªëi), ko ƒëi·ªÅu h∆∞·ªõng traffic nh∆∞ R53 dc\n Q. B·∫°n ƒëang d√πng AWS Cognito Sync, n√≥ cung c·∫•p nh·ªØng feature g√¨?\n sync app user data gi·ªØa nhi·ªÅu device v√† web app,\nweb app c√≥ th·ªÉ read write data l√™n cache ko c·∫ßn quan t√¢m k·∫øt n·ªëi c·ªßa mobile device, sau khi device k·∫øt n·ªëi th√¨ s·∫Ω sync data,\nv√† c√≥ th·ªÉ push notify c√°c thi·∫øt b·ªã kh√°c khi c√≥ update\n Q. app c·ªßa b·∫°n host tr√™n ec2, d√πng cloudfront ƒë·ªÉ distribute content, sao ƒë·ªÉ encrpyt traffic gi·ªØa orgin v·ªõi cloudfront, v√† gi·ªØa cloudfront v·ªõi viewer?\n gi·ªØa origin v·ªõi cloudfront: tr√™n console, setting Origin Protocol policy,\ngi·ªØa cloudfront v·ªõi viewer: tr√™n console, setting Viewer Protocol policy\n Q. App b·∫°n t∆∞∆°ng t√°c v·ªõi s3, m√† gi·ªù s3 t·ª± nhi√™n ko t·ªìn t·∫°i, sao ƒë·ªÉ bi·∫øt n√≥ b·ªã x√≥a nh∆∞ n√†o? c√≥ d√πng Inspector dc ko?\n d√πng cloudtrail log ƒë·ªÉ xem bucket delete api request,\nInspector ch·ªâ ƒë·ªÉ check server c√≥ b·ªã vulnerable ko th√¥i\n Q. Best practice c·ªßa lambda, l√†m sao t·∫≠n d·ª•ng Execution Context l√† g√¨?\n Execution Context l√† m√¥i tr∆∞·ªùng runtime t·∫°m th·ªùi lambda t·∫°o cho function ch·∫°y\nl·∫ßn ƒë·∫ßu ho·∫∑c update function th√¨ n√≥ s·∫Ω m·∫•t time ƒë·ªÉ t·∫°o m√¥i tr∆∞·ªùng Execution Context n√†y\nc√°c l·∫ßn sau n√≥ d√πng l·∫°i n√™n nhanh h∆°n\nƒë·ªÉ l√¢u 20-45p th√¨ n√≥ t·ª± x√≥a m√¥i tr∆∞·ªùng n√†y\nExecution Context l∆∞u kho·∫£ng 500Mb cache c√°c data required\nkhi ch·∫°y n√™n check xem n·∫øu data required exist th√¨ ko c·∫ßn reuquest ƒë·ªÉ ch·∫°y function cho nhanh\n Q. b·∫°n ƒëang x√¢y d·ª±ng web stateful, th√¨ c·∫ßn c√°i g√¨ l∆∞u session data/ c√°i g√¨ distribute traffic? ALB hay APIGateway?\n DynamoDB l∆∞u session data, ALB ƒë·ªÉ distribute traffic\nAPI Gateway ko ph·∫£i ƒë·ªÉ distribute traffic m√† l√† qu·∫£n l√Ω API\n Q. app c·ªßa b·∫°n c√≥ ASG, c·∫ßn d·ª±a tr√™n s·ªë l∆∞·ª£ng user concurrent ƒë·ªÉ scale th√¨ l√†m n√†o?\n t·∫°o 1 cloudwatch metric cho s·ªë l∆∞·ª£ng user concurrent ƒë·ªÉ trigger ASG policy\n Q. B·∫°n ƒëang d√πng S3, app r·∫•t ph·ªï bi·∫øn n√™n c√≥ nh·ªØng object s3 ƒëc access th∆∞·ªùng xuy√™n, sao ƒë·ªÉ enhance c√°c GET request/ PUT request?\n Enhance c·ªßa GET request l√† d√πng cloudfront (v·ª´a gi·∫£m ƒë·ªô tr·ªÖ v·ª´a tƒÉng performance), ho·∫∑c tƒÉng s·ªë l∆∞·ª£ng prefix trong s3 (nh∆∞ng c√°i n√†y ko gi·∫£m ƒë·ªô tr·ªÖ dc) v√≠ du: bucket/folder1/sub1/file\nbucket/folder1/sub2/file bucket/1/file\nbucket/2/file\nPrefixes of the object \u0026lsquo;file\u0026rsquo; would be: \u0026lsquo;/folder1/sub1/\u0026rsquo; , \u0026lsquo;/folder1/sub2/\u0026rsquo;, \u0026lsquo;/1/\u0026rsquo;, \u0026lsquo;/2/\u0026rsquo;.\nN·∫øu c√≥ 4 prefix nh∆∞ hi·ªán t·∫°i th√¨ c√≥ th·ªÉ tƒÉng GET reuqest l√™n 4 l·∫ßn\nEnhance c·ªßa PUT request l√† th√™m random prefix (nh·ªØng chu·ªói ng·∫´u nhi√™n) v√†o key name\n Q. B·∫°n d√πng SQS, app c·ªßa b·∫°n m·∫•t 60s ƒë·ªÉ process message, SQS ƒëang setting default th√¨ c·∫ßn setting g√¨ n·ªØa?\n Change visible timeout tƒÉng l√™n 60s, v√¨ default visible timeout l√† 30s th√¥i,\nvisible timeout l√† th·ªùi gian message bi·∫øn m·∫•t kh·ªèi queue, sau ƒë√≥ xu·∫•t hi·ªán l·∫°i,\napp c·∫ßn ph·∫£i v√†o m√† x√≥a message ƒëi sau khi process xong\n Q. khi deploy code l√™n Beanstalk th√¨ c·∫ßn ch√∫ √Ω g√¨?\n up file zip ho·∫∑c war,\nko qu√° 512MB,\nko ·ªü trong folder,\nn·∫øu th·ª±c c√°c task theo l·ªãch th√¨ c·∫ßn c√≥ file cron.yaml\nC√°c c√¢u h·ªèi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh thi (ch∆∞a add c√¢u tr·∫£ l·ªùi)  Code pipepilne, CodeBuild, CodeCommit c√°i n√†o support parrallel branch, versionning, batch? Global secondary index th√¨ d√πng cho eventually read hay srongly read request? Xray m√† mu·ªën tr·∫£ v·ªÅ th√™m information th√¨ c·∫ßn ph·∫£i th√™m g√¨? annotation hay meta data? Deploy Serverless application model b·∫±ng Cloudformation th√¨ c·∫ßn define g√¨ trong template? Api ƒëc protect b·ªüi Multifactor Authentication th√¨ dev l√†m sao ƒë·ªÉ call api ƒë√≥? d√πng c√°i g√¨ trong nh·ªØng c√°i sau GetSessionToken hay GetFederationToken hay GetCallerIdentity? Api gateway b·ªã timeout m·∫∑c d√π lambda ƒë√£ finish v√¨ sao? D√πng cli command b·ªã timeout v√¨ nhi·ªÅu resource th√¨ c·∫ßn l√†m g√¨? Quoting, shorthand syntax.. Deploy lambda vs 1 custom environment ch·ª©a library ri√™ng l√†m th·∫ø n√†o? D√πng Codedeploy, khi mu·ªën update version source m·ªõi l√†m n√†o? Upldate file buildspec.js hay appspec.js? ƒê·ªÉ application c√≥ t√≠nh elastic v√† horizontal scale th√¨ d√πng elb + asg hay cloudfront vs asg? Cloudwatch c√≥ nh·ªØng metric n√†o d√πng cho Apigateway vs lambda? CacheMissHit, IntergrationLatency? Log app ƒëang ·ªü tr√™n cloudwatch log, nh∆∞ng t·∫°o metric ƒë·ªÉ filter log th√¨ ko c√≥ k·∫øt qu·∫£? V√¨ sao?  ","href":"/bk/encrypt-aws-certified-developer-associate-note-cda/","title":"Aws Certified Developer Associate Note (CDA)"},{"content":"C√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh h·ªçc \u0026mdash;P1\u0026mdash;\n Q. 1 app ƒëang stores session state in memory. N√™n l∆∞u n√≥ ·ªü ƒë√¢u?\n Store session state in an ElastiCache cluster\n Q. 1 app c·∫ßn ph·∫£i monitor c√°c event. c·∫ßn capture s·ªë l∆∞·ª£ng user login, th·ªùi gian ƒë·ªânh ƒëi·ªÉm, period 10s. l√†m n√†o?\n Create a high-resolution custom Amazon CloudWatch metric\nhigh-resolution metric: can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, regular alarm with a period of any multiple of 60 seconds\n Q. dev ko c√≥ quy·ªÅn edit the code build nh∆∞ng c√≥ quy√™n run build/ l√†m n√†o ƒë·ªÉ override the build command?\n Run the start-build command with buildspecOverride property\n Q. cty ƒëang d√πng ElastiCache cluster. y√™u c·∫ßu l√† logic trong code sao cho cluster ch·ªâ request data t·ª´ RDS khi cache miss.D√πng strategy n√†o?\n c√≥ 3 lo·∫°i strategy c·ªßa elastiCache: Lazy Loading (ch·ªâ request data ƒëc cache, nh∆∞ng data cache c√≥ th·ªÉ c≈©), data trong cache s·∫Ω ch·ªâ dc update n·∫øu request b·ªã cache miss, c√≤n n·∫øu data DB dc update ri√™ng th√¨ s·∫Ω ko update cache.\nWrite Through (t·ªën resource, nh∆∞ng data lu√¥n m·ªõi), data trong cache s·∫Ω lu√¥n dc update m·ªói khi data trong DB dc update.\nAdding TTL (ch·ªâ ƒë·ªãnh th·ªùi gian h·∫øt h·∫°n cho key, h·∫°n ch·∫ø vi·ªác data qu√° c≈©)\n Q. 1 dev ƒëang build app access s3. 1 IAM role ƒë√£ dc t·∫°o ƒë·ªÉ acecess v√†o s3, API n√†o m√† dev c·∫ßn d√πng ƒë·ªÉ code access dc v√†o s3?\n STS:AssumeRole\n Q. 1 lambda function t√≠nh to√°n s·ªë Fibonacci d√πng v√≤ng l·∫∑p invoke lambda v√†i ng√†y sau th√¨ the Lambda function b·ªã throttled. Best practice?\n Theo best practice th√¨ n√™n: Avoid the use of recursion, n·∫øu v√¥ t√¨nh l√†m th√¨ n√™n set the function concurrent execution limit to \u0026lsquo;0\u0026rsquo; (Zero) ngay, R·ªìi update code.\n1 s·ªë best practice c·ªßa d√πng lambda:\nt√°ch c√°c Lambda handler,\nt·∫≠n d·ª•ng m√¥i tr∆∞·ªùng execute context,\nd√πng AWS Lambda Environment Variables ƒë·ªÉ truy·ªÅn param,\ngi·∫£m thi·∫øu s·ªë l∆∞·ª£ng deployment package xu·ªëng,\nƒë·ª´ng up c·∫£ sdk l√™n tr√°nh d√πng v√≤ng l·∫∑p trong code test performance,\ntest load,\ndelete function c≈© ko d√πng,\nt·∫≠n d·ª•ng AWS Lambda Metrics and CloudWatch Alarms,\nko cho function v√†o VPC n·∫øu ko c·∫ßn thi·∫øt,\nch·∫Øc ch·∫Øn r·∫±ng c√≥ ƒë·ªß elastic network interfaces (ENIs),\nn·∫øu ko th√¨ ph·∫£i t·∫°o c√°i subnet l·ªõn h∆°n,\nt·∫°o dedicated Lambda subnets in your VPC.\n Q. 1 function lambda s·∫Ω run in multiple stages, such a dev, test and production. v√† n√≥ ph·∫£i call c√°c endpoints kh√°c nhau t√πy theo stage. l√†m n√†o ƒë·ªÉ dev ch·∫Øc ch·∫Øn s·∫Ω call ƒë√∫ng endpoint khi ch·∫°y trong m·ªói stage?\n d√πng Environment variables\n Q. D√πng AWS SAM ƒë·ªÉ define Lambda function. Vi·∫øt 1 new function v√† c√°ch n√†o ƒë·ªÉ shift traffic t·ª´ function c≈© sang function m·ªõi nhanh nh·∫•t Canary10Percent5Minutes, Linear10PercentEvery10Minutes.\n Canary10Percent5Minutes: traffic dc shift 2 v√≤ng. v√≤ng 1 l√† 10% traffic, v√† t·∫•t c·∫£ 90% c√≤n l·∫°i s·∫Ω dc shift sau 5p Linear10PercentEvery10Minutes: traffic dc shift 10% m·ªói 10p, sau 100p th√¨ m·ªõi dc 100%\n Q. 1 dev ƒëang migrate to cloud 1 app d√πng MS SQL, data dc encrypt bang Transparent Data Encryption n√™n d√πng c√°i g√¨ ƒë·ªÉ ph·∫£i code √≠t nh·∫•t?\n Amazon RDS, v√¨ n√≥ support Transparent Data Encryption (TDE), MS SQL\n Q. 1 dev ƒëang vi·∫øt v√†i lambda function access v√†o RDS. h·ªç c·∫ßn share c√°c connection string that contains the database credentials, which are a secret. Cty c√≥ policy y√™u c·∫ßu t·∫•t c·∫£ secret ph·∫£i encrypted.n√™n d√πng c√°i g√¨ ƒë·ªÉ ph·∫£i code √≠t nh·∫•t?\n Use Systems Manager Parameter Store secure strings\n Q. A developer is using Amazon API Gateway as an HTTP proxy to a backend endpoint. c√≥ 3 moi tr∆∞·ªùng: Development, Testing, Production and three corresponding stages in the API gateway. L√†m n√†o ƒë·ªÉ direct traffic ƒë√∫ng enpoint c·ªßa stages m√† ko c·∫ßn t·∫°o ri√™ng API cho m·ªói stage\n Use stage variables\n Q. 1 cty ƒëang deploy static website tr√™n s3. dev c·∫ßn ph·∫£i l√†m th√†nh dynamic s·ª≠ d√πng serverless solution (code m√† ko c·∫ßn c√†i ƒë·∫∑t server). 2 c√°i n√†o ƒë·ªÉ l√†m?\n API Gateway and AWS Lambda are the best two choices to build this serverless application\n Q. 1 cty ƒëang c√≥ custom CloudWatch metric cho m·ªói khi c√≥ l·ªói HTTP 504 xu·∫•t hi·ªán trong logs. v√† c√≥ 1 CloudWatch Alarm cho c√°i metric kia. N·∫øu mu·ªën alarm dc trigger ch·ªâ khi \u0026gt;= 2 evaluation periods. th√¨ s·ª≠a c√°i g√¨?\n Data Points to Alarm should be set to 2 while creating this alarm\n Q. 1 dev ƒëang d√πng AWS Elastic Beanstalk env cho 1 web app. Hi·ªán t·∫°i ƒëang d√πng t1.micro. Sao ƒë·ªÉ change th√†nh m4.large??\n Create a new configuration file with the instance type as m4.large\n Q. 1 cty d√πng Aurora RDS. vi·ªác t·∫°o report m·ªói gi·ªù l√†m ch·∫≠m h·ªá th·ªëng/. C·∫ßn l√†m j?\n t·∫°o read replica DB v√† point c√°i reporting application ƒë·∫øn read replica ƒë√≥\n Q. 1 cty using AWS Elastic Beanstalk for a web app, Dev c·∫ßn config sao cho s·∫Ω create new instances and deploy code to those instances method n√†o s·∫Ω deploy code ONLY l√™n instance m·ªõi?\n Immutable, Blue/green\nPh√¢n bi·ªát c√°c lo·∫°i deploy method c·ªßa Beanstalk:\nAll in One: deploy l√™n exist ec2, th·ªùi gian deploy nhanh nh∆∞ng h·ªá th·ªëng s·∫Ω c√≥ downtime trong l√∫c deploy, n·∫øu fail th√¨ user ph·∫£i ch·ªãu downtime v√¨ system b·ªã treo, dev ph·∫£i manual redeploy\nRolling: deploy theo batch l√™n exist ec2, b·∫°n ph·∫£i define s·ªë l∆∞·ª£ng ec2 trong 1 batch. V√≠ d·ª• c√≥ 3 ec2, m·ªói ec2 l√† 1 batch.\ndeploy l√™n 1 batch tr∆∞·ªõc, n·∫øu batch fail th√¨ qu√° tr√¨nh deploy d·ª´ng l·∫°i, h·ªá th·ªëng v·∫´n ch·∫°y b√¨nh th∆∞·ªùng v√¨ c√≤n 2 ec2 kh√°c ƒëang ch·∫°y ko fail. Tuy nhi√™n trong qu√° tr√¨nh deploy to√†n b·ªô h·ªá th·ªëng s·∫Ω ko full capacity (nƒÉng su·∫•t) b·ªüi v√¨ ec2 m√† ƒëang deploy s·∫Ω ko serve traffic. N·∫øu fail th√¨ user ph·∫£i manual redeploy.\nRolling with additional batch: deploy theo batch, t·∫°o th√™m ec2 ƒë·ªÉ deploy version m·ªõi l√™n ƒë√≥.\nV√≠ d·ª• c√≥ 3 ec2, m·ªói ec2 l√† 1 batch. t·∫°o th√™m 1 ec2 deploy l√™n ƒë√≥. N·∫øu success h·∫øt th√¨ s·∫Ω terminate 1 ec2 ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng ch·ªâ c√≥ 3 ec2. ƒê·∫£m b·∫£o l√† l√∫c n√†o h·ªá th·ªëng c≈©ng full capacity. N·∫øu fail th√¨ c√°i batch fail s·∫Ω b·ªã terminate\nImmutable: deploy to√†n b·ªô l√™n c√°c ec2 m·ªõi. N·∫øu fail th√¨ s·∫Ω terminate c√°c ec2 m·ªõi ƒë√≥. Deploy time c≈©ng l√¢u h∆°n. Ch·ªâ duplicate c√°c ec2 th√¥i, ko duplicate Load Balancer n√™n ko thay ƒë·ªïi v·ªÅ DNS.\nBlue/green: deploy to√†n b·ªô l√™n c√°c ec2 m·ªõi. Duplicate c·∫£ ec2 v√† Load Balancer. Deploy time l√¢u ngang Immutable. Sau khi success th√¨ b·∫°n c·∫ßn swap URL ƒë·ªÉ route traffic ƒë·∫øn 1 DNS green. N·∫øu fail th√¨ swap URL l·∫°i m√¥i tr∆∞·ªùng blue c≈© th√¥i. Ch√∫ √Ω l√† d√πng c√°i n√†y ko n√™n ƒë·ªÉ DB ·ªü trong Beanstalk env. V√¨ DB b·ªã duplicate s·∫Ω ko dc sync.\ntham kh·∫£o: https://blog.shikisoft.com/which_elastic_beanstalk_deployment_should_you_use/\n Q. 1 dev ƒëang vi·∫øt 1 app store data v√†o Dynamo DB, t·ª∑ l·ªá ƒë·ªçc vi·∫øt v√†o db l√† 1000 reads:1 writes. data dc access li√™n t·ª•c Dev c·∫ßn enable c√°i g√¨ tr√™n DynamoDB ƒë·ªÉ optimize cost v√† performance?\n Amazon DynamoDB Accelerator (DAX)\n Q. 1 API dev ƒëang dc y√™u c·∫ßu d√πng deploy bang API gateway, n·∫øu l√† API frontend th√¨ config c√°i j? n·∫øu API backen th√¨ c·∫ßn config j?\n API frontend th√¨ config medthod respone v√† method request n·∫øu API backend th√¨ c·∫ßn config integration request v√† intergration response\nQ.b·∫°n ƒëang d√πng lambda t∆∞∆°ng t√°c v·ªõi DynamoDB, sau 1 time th√¨ k·∫øt qu·∫£ b·ªã delay. c·∫ßn DEBUG th√¨ d√πng c√°i j?\nAWS X-Ray\n Q. 1 cty ƒëang d√πng AWS Code Pipeline for CICD, code l·∫•y t·ª´ S3, cty y√™u c·∫ßu all data ph·∫£i encrypted v√† key dc customer qu·∫£n l√Ω. C·∫ßn enable nh·ªØng g√¨?\n Serverside encryption ·ªü S3 bucket, d√πng AWS KMS for s3 encryption\n Q. 1 App ƒëang d√πng 1 DynamoDB table ƒë·ªÉ store 1 s·ªë items. nh·ªØng items n√†y ch·ªâ dc access v√†i l·∫ßn sau ƒë√≥ th√¨ c√≥ th·ªÉ x√≥a. Qu·∫£n l√Ω vi·ªác x√≥a item ƒë√≥ bang c√°ch n√†o?\n enable TTL cho DynamoDB, cho ph√©p set timestamp ƒë·ªÉ t·ª± ƒë·ªông x√≥a items. vi·ªác versioning v·ªõi DynamoDB l√† ko th·ªÉ\n Q. You are using AWS Envelope Encryption for encrypting all sensitive data. Data dc encrypt nh∆∞ th·∫ø n√†o v·ªõi data key v√† master key\n data dc encrypt b·∫±ng plain text data key, v√† data key ƒë√≥ th√¨ l·∫°i dc encrpypt bang plain text Master key n·ªØa.\n Q. team b·∫°n ƒëang deploy 1 micro service v√† 1 app l√™n aws. Y√™u c·∫ßu l√† ph·∫£i qu·∫£n l√Ω app ki·ªÉu orchestration. d√πng service n√†o ƒë·ªÉ gi·∫£m thi·∫øu effort admin nh·∫•t?\n Use the Elastic Container Service\n Q. b·∫°n ƒëang deploy 1 app v·ªõi ki·∫øn tr√∫c nh∆∞ sau; ec2 ƒë·ªÉ process video, ASG, SQS, c√≥ 2 m·ª©c gi√° free v√† premium. SAo ƒë·ªÉ user l√† premium lu√¥n dc ∆∞u ti√™n?\n t·∫°o 2 queue trong SQS. user c√≥ premium th√¨ ·ªü trong queue dc ∆∞u ti√™n x·ª≠ l√Ω trc\n Q. 1 ki·∫øn tr√∫c dc v·∫Ω ra cho mobile app. User c·∫ßn sign in bang google, facebook. User c·∫ßn c√≥ ch·ª©c nƒÉng qu·∫£n l√Ω profile. N√™n d√πng c√°i g√¨?\n using User pools in AWS Cognito User pool cung c·∫•p: Sign-up and sign-in services, Social sign-in with external identity, qu·∫£n l√Ω and user profiles, MFA, protect, phone,mail verfy user migration.\n Q. Cong ty b·∫°n l√†m 1 app l√†m vi·ªác v·ªõi DynamoDB. y√™u c·∫ßu all data c·∫ßn encrypt at rest. ph·∫£i l√†m n√†o?\n enable ch·ª©c nƒÉng encryption khi t·∫°o dynamoDB table, ch·ª© t·∫°o r·ªìi th√¨ ko encrypt dc nha\n Q. b·∫°n ƒëang deploy 1 app l√™n AWS Elastic Beanstalk. ƒë√¢y ch·ªâ l√† m√¥i tr∆∞·ªùng dev. C·∫ßn ph·∫£i t·ªën √≠t time nh·∫•t cho m·ªói l·∫ßn deploy. C·∫ßn d√πng ph∆∞∆°ng th·ª©c deploy n√†o?\n All at once, t·ªën √≠t time deploy nh·∫•t.nh∆∞ng n·∫øu deploy fail th√¨ app s·∫Ω b·ªã downtime, nh∆∞ng v√¨ m√¥i tr∆∞·ªùng dev n√™n ko ·∫£nh h∆∞·ªüng l·∫Øm.\n Q. b·∫°n ƒëang dev 1 app tr√™n aws theo ki·ªÉu micro service, c√°c services ƒëc t·∫°o base tr√™n lambda, v√¨ s·ª± ph·ª©c t·∫°p c·ªßa flow c√°c component n√™n c·∫ßn 1 c√°ch ƒë·ªÉ qu·∫£n l√Ω flow c·ªßa nhi·ªÅu fcn lambda? d√πng c√°i j?\n AWS Step function\n Q. b·∫°n c√≥ 1 app c·∫ßn inject data t·ª´ nhi·ªÅu thi·∫øt b·ªã. B·∫°n c·∫ßn ƒë·∫£m b·∫£o data ph·∫£i dc pre process tr∆∞·ªõc khi ƒë∆∞a v√†o ph√¢n t√≠ch. d√πng c√°i j?\n d√πng kinesis + lambda ƒë·ªÉ define data c·∫•u tr√∫c nh∆∞ n√†o tr∆∞·ªõc khi dc query.\n Q. b·∫°n c·∫ßn deploy 1 app l√™n ec2. data c·ªßa app dc store ·ªü 1 volume ri√™ng. v√† c·∫ßn dc encrypt at rest. C·∫ßn l√†m g√¨?\n khi t·∫°o volume c·∫ßn enable encrpytion, v√† s·ª≠ d·ª•ng KMS service\n Q. b·∫°n dc y√™u c·∫ßu customize content c·ªßa app. Content n√†y dc distribute to user th√¥ng qua Cloudfront. content origin ·ªü S3 l√†m n√†o?\n d√πng lambda@edge, n√≥ cho ph√©p customize content m√† Cloudfront deliver. V√≠ d·ª•: 1 web b√°n qu·∫ßn √°o. b·∫°n d√πng cookie ƒë·ªÉ ƒëi·ªÅu h∆∞·ªõng m√†u n√†o user ƒë√£ ch·ªçn cho √°o, b·∫°n d√πng lambda ƒë·ªÉ change c√°i request ch·ªçn m√†u ƒë√≥ ƒë·ªÉ cloudfront tr·∫£ v·ªÅ h√¨nh ·∫£nh c·ªßa c√°i √°o v·ªõi m√†u ƒë√£ ch·ªçn gi·∫£m ƒë·ªô tr·ªÖ, tƒÉng tr·∫£i nghi·ªám ng d√πng.\n Q. 1 tool ƒë·ªÉ qu·∫£n to√†n b·ªô life cycle c·ªßa d·ª± √°n, project?\n CodeStar\n Q. b·∫°n ƒëang d√πng Route53 ƒë·ªÉ point DNS name c·ªßa cty ƒë·∫øn web app. B·∫°n mu·ªën deliver app l√™n 1 ph·∫ßn nh·ªè ng∆∞·ªùi d√πng ƒë·ªÉ test. L√†m n√†o?\n D√πng Route53 weighted policy ƒë·ªÉ ch·ªâ ƒë·ªãnh 1 ph·∫ßn nh·ªè ng d√πng\n Q. khi d√πng Lambda v·ªõi S3, best practice ƒë·ªÉ pass parameter v√†o lambda l√† g√¨?\n D√πng Lambda environment variables ƒë·ªÉ truy·ªÅn s3 bucket name\n Q. b·∫°n c√≥ v√†i lambda function c·∫ßn deploy bang Codeploy, l√†m n√†o ƒë·ªÉ ch·∫Øc ch·∫Øn s·∫Ω deploy ƒë√∫ng version c·ªßa function\n ch·ªâ ƒë·ªãnh r√µ version s·∫Ω dc deploy trong AppSpec file\n Q. b·∫°n ƒëang dev 1 app game d√πng aws. l√†m n√†o ƒë·ªÉ ch·∫Øc ch·∫Øn c√°i bang x·∫øp hang c·ªßa game lu√¥n ch√≠nh x√°c?\n D√πng elastic cache Redis\n Q. b·∫°n ƒëang c√≥ 1 app dev tr√™n ec2, app c·∫ßn l·∫•y private IP c·ªßa instance 1 c√°ch t·ª± ƒë·ªông trong code. L√†m n√†o?\n Query trong instance metadata. curl http://169.254.169.254/latest/meta-data\n Q. trong Lambda code, function dc call ƒë·ªÉ lambda execute code l√† g√¨?\n Handler\n Q. l√†m sao ƒë·ªÉ S3 ko cho upload nh·ªØng file ch∆∞a dc encrypted\n update policy cho S3, trong policy ph·∫£i defined condition\n Q. b·∫°n c·∫ßn t√¨m ch·ªó l∆∞u data. nh∆∞ng data ko ph√π h·ª£p schema n√†o c·∫£. data c√≥ th·ªÉ ƒë√°nh index. d√πng lo·∫°i n√†o?\n DynamoDB\n Q. Eventual Consistency v√† Strong Consistency\n Eventual Consistency=Weak Consistency, nh·∫•t qu√°n y·∫øu, sau khi 1 update v√†o DB, c√°c read request sau ƒë√≥ ko ƒë·∫£m b·∫£o data latest, c√≥ th·ªÉ c≈©, nh∆∞ng sau 1 time sync gi·ªØa c√°c DB th√¨ cu·ªëi c√πng read request s·∫Ω c√≥ dc data latest.\nStrong Consistency: sau khi 1 update v√†o DB, c√°c read request sau ƒë√≥ ƒë·∫£m b√°o ch·∫Øc ch·∫Øn s·∫Ω c√≥ dc data latest, nh∆∞ng m√† n·∫øu v·∫´n ƒëang trong time sync gi·ªØa c√°c DB th√¨ read request ƒë√≥ s·∫Ω b·ªã delay (tr·∫°ng th√°i b·∫≠n) cho ƒë·∫øn khi sync OK h·∫øt m·ªõi tr·∫£ v·ªÅ KQ.\n Q. DynamoDB, item size 4KB c·∫ßn 1 read request (Strongly Consistency): 1 read request ~ 1 read capacity UNIT (RCU)\n (Eventually consistency): 2 read request ~ 1 read capacity UNIT (RCU) c√°i read request n√†o ƒë·∫Øt h∆°n?\n-\u0026gt; 1 read request (Strongly Consistency) c√≥ gi√° g·∫•p ƒë√¥i 1 read request (Eventually consistency) (v√¨ ph·∫£i 2 read request (Eventually consistency) m·ªõi ƒë·ªïi ngang ra 1 RCU) v·∫≠y n√™n n·∫øu file size = 8KB\n8KB -\u0026gt; b·∫°n c·∫ßn 2 read request -\u0026gt; b·∫°n c·∫ßn 2 RCU (Strongly Consistency)\n8KB -\u0026gt; b·∫°n c·∫ßn 2 read request -\u0026gt; b·∫°n c·∫ßn 2 chia 2 = 1 RCU (Eventually consistency)\n Q. DynamoDB, eventually read, 300 items/30s 10 items/s, 1 item = 6KB th√¨ c·∫ßn set table bao nhi√™u RC (read capacity 1 gi√¢y)?\n 6KB -\u0026gt; c·∫ßn 2 read request -\u0026gt; c·∫ßn 20 request trong 1s v√¨ l√† eventually read -\u0026gt; 20/2 = 10 RC trong 1s\n Q. c√≥ 1 app tr√™n ec2, sau 1 ELB, b·∫°n c·∫ßn monitor c√°c connection ƒë·∫øn ELB, d√πng c√°i j?\n enable access log tr√™n LB\n Q. B·∫°n c·∫ßn dev 1 app m√† y√™u c·∫ßu l√† ph·∫£i decoupled system. N√™n d√πng c√°i g√¨ ƒë·ªÉ x·ª≠ l√Ω message cho h·ªá th·ªëng decoupled (t√°ch r·ªùi)?\n SQS\n Q. 1 static web dc host tr√™n s3 bucket. c√≥ 1 java script section mu·ªën access v√†o data tr√™n 1 s3 bucket kh√°c. gi·ªù c√°i web ƒë√≥ ko load dc trong browser. v√¨ sao?\n ch∆∞a enable CORS for bucket\n Q. app c·ªßa b·∫°n gen report c·ªßa 10 user m·∫•t 4h. b·∫°n c·∫ßn 1 app c√≥ th·ªÉ report at real time, always up to date, gi·∫£i quy·∫øt dc s·ªë l∆∞·ª£ng request tang cao. d√πng c√°i j?\n post log data l√™n kinesis ƒë·ªÉ n√≥ analyze.\n Q. B·∫°n ƒëang l√†m mobile app. N·∫øu c·∫ßn ch·ªó l∆∞u session data of user th√¨ ch·ªçn db n√†o?\n DynamoDB\n Q. App c·ªßa b·∫°n d√πng DunamoDB, l√†m sao ƒë·ªÉ ch·∫Øc ch·∫Øn rang m·ªói khi 1 item dc update v√†o primary table th√¨ 1 record kh√°c c≈©ng dc insert v√†o secondary table\n d√πng DynamoDB stream\n Q. 1 app d√πng DynamoDB cho data backend. table size 20GB. Lhi scan table hay b·ªã l·ªói throttling. L√†m sao ƒë·ªÉ tr√°nh l·ªói.\n Reduced Page size\n Q. Cty c·ªßa b·∫°n mu·ªën t·∫°o 1 m√¥i tr∆∞·ªùng m·ªõi tr√™n aws. mu·ªën s·ª≠ d√πng Chef ƒë√£ c√≥ s·∫µn tr√™n aws. L√†m n√†o?\n D√πng AWS Opswork. n√≥ cung c·∫•p c√°c instance c√≥ chef, Puppet, tool ƒë·ªÉ automate configure server\n Q. B·∫°n build 1 app l√™n ec2. r·ªìi th·ª±c hi·ªán test. b·∫°n mu·ªën capture log t·ª´ web server ƒë·ªÉ ph√°t hi·ªán issue n·∫øu c√≥. L√†m n√†o?\n install cloudwatch agent l√™n ec2. configure ƒë·ªÉ agent ƒë√≥ send log c·ªßa server l√™n cloudwatch\n Q. App b·∫°n ƒëang d√πng Cognito ƒë·ªÉ qu·∫£n l√Ω user identity. Gi·ªù mu·ªën analyze data c·ªßa user tr√™n cognito. D√πng c√°i j?\n Cognito Stream\n Q. N·∫øu Elastic Beanstalk ko c√≥ m√¥i tr∆∞·ªùng th·ªèa m√£n nhu c·∫ßu?\n T·∫°o custom platform tr√™n Beanstalk, d√πng packer.\n Q. App d√πng DynamoDB, write 10 item trong 1s. m·ªói item 15.5 KB. V·∫≠y c·∫ßn setting Write throughput bao nhi√™u?\n 1KB = 1 write request-\u0026gt; 15.5KB = 16 write request.\n10 items -\u0026gt; 160 write request.\n1 read request = 1 WCU -\u0026gt; 160 WCU/s\n\u0026mdash;P2\u0026mdash;\n Q. Elastic Beanstalk ƒëang ch·∫°y 1 app. mu·ªën config m√¥i tr∆∞·ªùng th√¨ c·∫ßn ƒë·∫∑t file config trong folder n√†o?\n .ebextensions\n Q. Lambda default time out l√† m·∫•y gi√¢y?\n 3s\n Q. B·∫°n c·∫ßn setup restful api service tr√™n aws, n√™n d√πng k·∫øt h·ª£p nh·ªØng service n√†o?\n Host code api tr√™n lambda, d√πng API gateway ƒë·ªÉ access v√†o api.\nHost code api tr√™n ec2 d√πng ELB ƒë·ªÉ path routing to api\n Q. b·∫°n dev 1 app c·∫ßn d√πng service authen. nh∆∞ng c√≥ 1 s·ªë video c·∫ßn cho ph√©p access t·ª´ nh·ªØng unauthenticated identity. best practice?\n AWS cognito v√† enable unauthenticated identity\n Q. 1 app c·∫ßn process c√°c message theo order ko b·ªã dublicate, SQS c√≥ 2 lo·∫°i g√¨?\n FIFO v√† standard\n Q. CodeDeploy c√≥ process hook l√† g√¨?\n App stop -before install - after install - app start\n Q. L√†m sao ƒë·ªÉ debug Lambda?\n them code debug v√†o r·ªìi lambda s·∫Ω ƒë·∫©y all log l√™n cloudwatch log\n Q. D√πng Lambda nh∆∞ng c·∫ßn include c√°c th∆∞ vi·ªán ngo√†i th√¨ c·∫ßn ch√∫ √Ω g√¨?\n Ch·ªâ them c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt, tr√°nh upload to√†n b·ªô sdk, ch·ªâ up nh·ªØng module m√¨nh c·∫ßn cho lambda\n Q. team b·∫°n c√≥ 1 Code Commit repository trong accout b·∫°n. l√†m n√†o cho c√°c account kh√°c truy c·∫≠p repo c·ªßa b·∫°n?\n t·∫°o cross account role, cho h·ªç role arn ƒë·ªÉ h·ªç v√†o dc.\n Q. B·∫°n c√≥ 1 lambda ch·∫°y async (dc retry v√†i l·∫ßn tr∆∞·ªõc khi b·ªã discard trong TH l·ªói) L√†m sao ƒë·ªÉ debug issue n·∫øu function fail?\n D√πng DLQ Dead letter queue\n Q. b·∫°n ƒëang d√πng kinesis stream cho app ·ªü cty. Y√™u c·∫ßu data at rest ph·∫£i dc encrypt. D√πng c√°i j? server-side encrypt hay client-side encrypt?\n enable Server-side encrypt cho kinesis stream, v√¨ n√≥ l√† feature c·ªßa Kinesis, ko c√≥ feature Client side trong kinesis. ko n√™n d√πng client side encrypt v√¨ t·ªën nhi·ªÅu th·ªùi gian\n Q. b·∫°n ƒëang dev 1 app d√πng kinesis, ƒë·ªÉ tang throughput, b·∫°n d√πng nhi·ªÅu shard. Nh∆∞ng nh∆∞·ª£c ƒëi·ªÉm l√† j?\n ko th·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± order c·ªßa data khi multi shard, b·∫°n ch·ªâ c√≥ th·ªÉ order trong single shard th√¥i.\n Q. DynamoDB c√≥ m·∫•y ki·ªÉu primary key?\n ki·ªÉu 1: ch·ªâ c√≥ partion key, l√† key unique\nki·ªÉu 2: partion key v√† sort key k·∫øt h·ª£p th√†nh b·ªô key unique n√™n ch·ªçn nh·ªØng key dc generate 1 c√°ch auto ra key unique\n Q. S3 ƒëang d√πng. sau khi b·∫≠t c√°i encryption data at rest b·∫±ng KMS th√¨ performance gi·∫£m xu·ªëng v√¨ sao?\n m·ªói l·∫ßn b·∫°n download or upload S3 nh∆∞ng object ƒë√£ dc encrypt b·∫±ng KMS, S3 s·∫Ω g·ª≠i request ƒë·∫øn KMS ƒë·ªÉ encrypt ho·∫∑c decrypt, N·∫øu v∆∞·ª£t qu√° 5500 or 10000 th√¨ KMS s·∫Ω b·ªã throttle. v√† gi·∫£m performance\n Q. Read replica c√≥ d√πng cho DynamoDB v√† RDS ko?\n d√πng cho c·∫£ 2. (nh∆∞ng n·∫øu DynamoDB th√¨ ph·∫£i n√≥i r√µ l√† DynamoDB Accelerator (DAX) th√¨ c√≥ th·ªÉ c√≥ ƒë·∫øn 9 read replicas)\n Q. N·∫øu lambda ko push log l√™n cloudwatch log?\n c√≥ th·ªÉ do role ch∆∞a c√≥ quy·ªÅn push log to cloudwatch\n Q. v·ªõi 1 app ƒë∆∞·ª£c setting 5 RCU th√¨ m·ªói gi√¢y read dc bao nhi√™u KB data\n TH strongly consistent: 5x4=20KB read m·ªói gi√¢y TH enventually consistent: 5x2x4=40 KB read m·ªói gi√¢y\n Q. B·∫°n ƒëang d√πng DynamoDB, data c·∫ßn ƒëi qua v√†i region tr√™n to√†n TG. l√†m sao ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ?\n enable global tables\n Q. B·∫°n ƒëang d√πng DynamoDB, app dc design ƒë·ªÉ scan to√†n b·ªô table. L√†m sao ƒë·ªÉ tang performance?\n d√πng scan song song (parallel scan) ho·∫∑c design app ƒë·ªÉ d√πng query thay v√¨ d√πng scan\n Q. b·∫°n c√≥ 1 l∆∞·ª£ng l·ªõn data c·∫ßn dc stream tr·ª±c ti·∫øp l√™n S3. N√™n d√πng c√°i j?\n Kinesis Firehose v√¨ c√°i n√†y l√™n S3 tr·ª±c ti·∫øp\n Q. Cty b·∫°n ƒëang d√πng Kinesis Firehose ƒë·ªÉ stream data to S3. Nh∆∞ng h·ªç c·∫ßn transform data tr∆∞·ªõc khi l√™n S3 th√¨ d√πng c√°i j?\n kinesis Firehose invoke lambda ƒë·ªÉ transform data\n Q. b·∫°n host static web tr√™n S3. Code g·∫ßn ƒë√¢y dc change ·ªü ch·ªó java script ƒë·ªÉ call ƒë·∫øn 1 web page ·ªü 1 bucket t∆∞∆°ng t·ª±, nh∆∞ng browse block request. L√†m sao?\n enable CORS on bucket\n Q. S3 bucket m√† ƒëang b·∫≠t encrypt data at rest th√¨ khi request t·ªõi, S3 s·∫Ω deny nh·ªØng request nh∆∞ n√†o?\n nh·ªØng request ko c√≥ attribute sau trong header: x-amz-server-side-encryption\n Q. b·∫°n ƒëang enable server logging tr√™n s3. b·∫°n host 1 static web kho·∫£ng 1MB tr√™n s3 bucket. sau 2 tu·∫ßn th√¨ bucket 50MB. v√¨ sao?\n server logging ƒë√£ push log v√†o same bucket. c·∫ßn c√≥ lifecle ƒë·ªÉ delete log c≈©\n Q. 1 cty ƒëang c√≥ api vi·∫øt tr√™n lambda, h·ªç cho ph√©p KH access API c·ªßa h·ªç qua API gateway. hi·ªán t·∫°i c√≥ kho·∫£ng 6 th√°ng ƒë·ªÉ move t·ª´ api c≈© sang api m·ªõi. c√°ch n√†o?\n t·∫°o them 1 stage trong api gateway t√™n l√† v2, ƒë·ªÉ KH c√≥ th·ªÉ d√πng c·∫£ 2 version api.\n Q. 1 cty ƒëang dev app mobile .app c·∫ßn cho user dƒÉng nh√¢p bang facebook. c√°i g√¨ ƒë·ªÉ qu·∫©n l√Ω user?\n cognito\n Q. c√°i template CF c·ªßa b·∫°n c√≥ nhi·ªÅu resource th√¨ c√≥ n√™n tack th√†nh c√°c stack ri√™ng ko?\n c√≥, t·∫°o nested stack\n Q. B·∫°n dc y√™u c·∫ßu t·∫°o 1 API gateway stage m√† t∆∞∆°ng t√°c tr·ª±c ti·∫øp v·ªõi DynamoDB. c√°i g√¨ c·∫ßn d√πng?\n c·∫ßn t·∫°o 1 Intergration request. ƒë·ªÉ forward c√°c incoming method request ƒëen backend, n√≥ s·∫Ω b·∫Øt ch·ªç DynamoDB action ph√π h·ª£p nh·ªõ l√† DAX ch·ªâ ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ cho DynamoDB th√¥i\n Q. b·∫°n c√≥ 1 lambda function l√† backend cho API gateway ec2. b·∫°n c·∫ßn ƒë∆∞a api gateway URL ƒë·ªÉ user test. th√¨ c·∫ßn l√†m g√¨ tr∆∞·ªõc?\n t·∫°o 1 deployment trong api gateway, r·ªìi associate 1 stage cho deployment ƒë√≥. khi b·∫°n update API, b·∫°n c√≥ th·ªÉ redeploy bang c√°ch associate 1 stage m·ªõi v·ªõi c√°i deployment c≈©.\n Q. b·∫°n c√≥ 1 app host tr√™n ec2. app l√† 1 ph·∫ßn c·ªßa web www.demo.com, app dc update sang call API gateway, Tuy nhi√™n browser ko render respone v√† Javascript b·ªã l·ªói, C·∫ßn l√†m j?\n b·∫≠t CORS cho API gateway\n Q. 1 app ƒëang call RDS. l√∫c high load th√¨ do to√†n query DB n√™n th·ªùi gian repsone l·ªõn, c·∫ßn l√†m j?\n b·∫≠t read replica cho DB, ho·∫∑c ƒë·∫∑t elastiCache tr∆∞·ªõc DB ko th·ªÉ d√πng cloudfront tr∆∞·ªõc DB\n Q. 1 Cty d√πng cache cho app c·ªßa h·ªç. Nh∆∞ng vi·ªác recover data lost trong cache r·∫•t t·ªën ti·ªÅn n√™n ko mu·ªën ph·∫£i recover data lost in cache. L√†m n√†o?\n D√πng elasticCache Redis v√¨ n√≥ c√≥ HA ch·ª© Elasticache memcached ko c√≥ HA\n Q. B·∫°n d√πng ECS ƒë·ªÉ deploy c√°c containers. Nh∆∞ng gi·ªØa c√°c container ko dc access v√†o nhau, v√¨ m·ªói container l√† 1 kh√°ch h√†ng ph·∫£i l√†m th·∫ø n√†o?\n config SG c·ªßa c√°c ec2 ch·ªâ allow c√°c request nh·∫•t ƒë·ªãnh.\nN·∫øu mu·ªën d√πng Role th√¨ c√°i ƒë√≥ d√πng cho level Task c·ªßa ECS ch·ª© ko ph·∫£i level container.\nKo n√™n d√πng access key v√¨ s·∫Ω ko secure.\nC√°c level: 1 cluster g·ªìm nhi·ªÅu task ho·∫∑c service. 1 task g·ªìm nhi·ªÅu container\n Q. khi call API tr√™n ec2 g·∫∑p l·ªói: You are not authorized to perform this operation. Encoded authorization failure message: oGsbAaIV7wl\u0026hellip; th√¨ l√†m n√†o ƒë·ªÉ ƒë·ªçc dc message ƒë√£ b·ªã encode ƒë√≥\n d√πng command aws sts decode-authorization-message\n Q. B·∫°n v·ª´a define v√†i custom policy tr√™n aws, b·∫°n c·∫ßn test permission c·ªßa c√°c policy ƒë√≥. l√†m n√†o b·∫±ng CLI?\n ƒë·∫ßu ti√™n get c√°c context key sau ƒë√≥ d√πng command: aws iam simulate-custom-policy\n Q. 1 cty d√πng Codepipeline ƒë·ªÉ l√†m CICD. v√¨ l√Ω do b·∫£o m·∫≠t, c√°c resource ƒë·ªÉ deploy n·∫±m ·ªü c√°c account kh√°c nhau. Ph·∫£i l√†m n√†o?\n define custom master key KMS add a across acount role\n Q. B·∫°n deploy app len ec2, D√πng DynamoDB. B·∫°n d√πng X-Ray ƒë·ªÉ debug nh∆∞ng ko xem log dc. v√¨ sao?\n X-ray daemon ch∆∞a install tr√™n ec2 IAM role attach v√†o ec2 ko c√≥ permission upload to X-ray\n Q. App c·ªßa b·∫°n ·ªü tr√™n cloud. Gi·ªù c√≥ y√™u c·∫ßu all state c·ªßa all request v√†o STS. l√†m n√†o?\n xem log ƒë√≥ trong cloudtrail ch·ª© STS ko c√≥ ch·ª©c nƒÉng logging\n Q. b·∫°n d√πng CodeDeploy ƒë·ªÉ deploy 1 app to aws. app n√†y d√πng AWS system manager parameter ƒë·ªÉ store secure param, tr∆∞·ªõc khi deploy c·∫ßn l√†m j?\n c·∫•p permission cho CodeDeploy b·∫±ng IAM ROLE, d√πng command aws ssm get-parameters \u0026ndash;with-decrpytion ƒë·ªÉ n√≥ decrypt password r·ªìi d√πng trong app\n Q. DynamoDB c·ªßa b·∫°n ƒëang ch·∫°y t·ªët. sau khi update th√¨ performace c·ªßa DynamoDB gi·∫£m xu·ªëng. v√¨ c√°c query ƒëang ko d√πng partition key? l√†m sao gi·ªù?\n add an index cho dynamodb Table s·ª≠a l·∫°i c√°c query ƒë·ªÉ n√≥ query ƒë√∫ng partition key c≈©ng dc nh∆∞ng t·ªën effort\n Q. b·∫°n deploy 1 app l√™n ec2. ƒë·ªÉ test app th√¨ b·∫°n dc c·∫•p access key ƒë·ªÉ c√≥ quy·ªÉn write v√†o S3. sau khi test xong th√¨ 1 IAM role dc attach v√†o ec2 Role n√†y ch·ªâ c√≥ quy·ªÅn read s3. nh∆∞ng ph√°t hi·ªán ra ec2 v·∫´n c√≥ quy·ªÅn write s3. V√¨ sao?\n Bi·∫øn m√¥i tr∆∞·ªùng trong CLI ƒëang dc set cho accesskey v√† n√≥ s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n h∆°n IAM Role\n Q. B·∫°n ƒëang deploy 1 app l√™n Beanstalk worrker, worker n√†y ch·∫°y ƒë·ªãnh k·ª≥. C·∫ßn ph·∫£i config ƒë·ªãnh k·ª≥ trong file n√†o?\n cron.yaml\n Q. 1 app ƒëang d√πng KMS ƒë·ªÉ encrypt data. qu√° tr√¨nh encrpyt v√† decrypt di·ªÖn ra nh∆∞ n√†o?\n d√πng master key ƒë·∫øn generate ra data key d√πng data key ƒë·ªÉ decrpyt data\n Q. N·∫øu 1 web ƒëang enable CORS th√¨ khi call API backend c·∫ßn nh·ªØng header n√†o?\n access-control-allow-headers access-control-allow-origin access-control-allow-method\n Q. b·∫°n ƒëang l√†m vi·ªác v·ªõi 1 app l√†m v·ªõi XML message. b·∫°n c·∫ßn ƒë·∫∑t app sau API gateway ƒë·ªÉ KH call api. C·∫ßn config g√¨?\n c·∫ßn mapping request v√† respone\n Q. b·∫°n d√πng Serverless Application Model ƒë·ªÉ deploy app, th√¨ c·∫ßn d√πng command g√¨?\n SAM package command v√† SAM deploy command\n Q. app c·ªßa b·∫°n ƒëang tr·ªè ƒë·∫øn v√†i lambda fucntion, khi 1 thay ƒë·ªïi update lambda, b·∫°n c·∫ßn l√†m c√°c step n√†o ƒë·ªÉ traffic dc shift sang version function m·ªõi 1 c√°ch t·ª´ t·ª´?\n t·∫°o alias v·ªõi routing-config param, update alias v·ªõi routing-config param b·∫°n c√≥ th·ªÉ setting cho c√°c traffic ƒë·∫øn version m·ªõi l√† 2% c√≤n 98% l√† ƒë·∫øn version c≈©, n√≥i chung b·∫±ng c√°ch setting atrribute AdditionalVersionWeights.\n Q. 1 app ƒëang s·ª≠ d·ª•ng DynamoDB, m·ªói gi√¢y c√≥ h√†ng ng√†n request. 1 app kh√°c l·∫•y nh·ªØng thay ƒë·ªïi c·ªßa DynamoDB ƒë√≥, v·ªõi m·ª•c ƒë√≠ch ph√¢n t√≠ch. VD: a new customer add data v√†o DynamoDB, event n√†y invoke 1 app kh√°c send welcome email cho customer D√πng service n√†o?\n enable t√≠nh nƒÉng dynamoDB Stream c√°c VD kh√°c: 1 app modify DynamoDB, 1 app kh√°c capture nh·ªØng modify ƒë√≥ ƒë·ªÉ cung c·∫•p 1 usage metric cho mobile app 1 app ƒë·ªÉ t·ª± ƒë·ªông send noti cho all friend in group m·ªói khi c√≥ 1 ai ƒë√≥ upload image l√™n group.\n Q. B·∫°n ƒëang dev 1 app upload ·∫£nh t·ª´ user. n√™n l∆∞u ·∫£nh ·ªü ƒë√¢u, th√¥ng tin user upload ·ªü ƒë√¢u?\n ·∫£nh s3, th√¥ng tin user ·ªü dynamodb (object identifier)\n Q. l·ªói HEALTH_CONSTRAINTS_INVALID c·ªßa CodeDeploy l√† j?\n c·∫ßn gi·∫£m s·ªë l∆∞·ª£ng instance nh·ªè nh·∫•t m√† b y√™u c·∫ßu xu·ªëng, ho·∫∑c tƒÉng s·ªë l∆∞∆°ng instance trong deployment group l√™n\n Q. App c·ªßa b·∫°n c·∫ßn write to SQS. c√≥ policy l√† credential ph·∫£i lu√¥n encrypt v√† rote 1 l·∫ßn /1 tu·∫ßn. l√†m sao ?\n d√πng IAM Role attach v√†o c√°i ec2 ch·ª©a app\n\u0026mdash;P3\u0026mdash;\n Q. L√†m sao ƒë·ªÉ ch·∫Øc ch·∫Øn c√°c custom software dc install khi lauch t·ª´ elastic beanstalk. ph·∫£i define ·ªü ƒë√¢u?\n vi·∫øt th√†nh file YAML or JSON v√† ƒë·ªÉ v√†o folder .ebextensions\n Q. C√≥ th·ªÉ b·∫≠t MFA cho resource trong s3 ko?\n C√≥. setting trong policy\n Q. Codedeploy dc configure ƒë·ªÉ t·ª± ƒë·ªông rollback khi build fail, tuy nhi√™n khi rollback n√≥ ko retrieve dc nh·ªØng file c·∫ßn cho previous version, l√†m sao gi·ªù?\n manually add file c·∫ßn thi·∫øt v√†o instance ho·∫∑c t·∫°o 1 new revision.\n Q. CORS c√≥ th·ªÉ set cho Lambda function ko?\n ko, S3, v√† API gateway\n Q. b·∫°n c√≥ 1 v√†i lambda fucntion host tr√™n API gateway. b·∫°n c·∫ßn control xem ai access v√†o APi c·ªßa b·∫°n ƒëang host tr√™n api gateway th√¨ d√πng c√°ch n√†o?\n Cognito user pool, ho·∫∑c lambda authorizer\n Q. b·∫°n d√πng kinesis Firehose ƒë·ªÉ stream data to S3. nh∆∞ng cty y√™u c·∫ßu data c·∫ßn dc encrypt at rest, l√†m n√†o?\n enable Encrpytion cho kinesis Firehose, ch·∫Øc ch·∫Øn r·∫±ng \u0026lsquo;Kinesis data stream\u0026rsquo; dc d√πng ƒë·ªÉ transfer from producer n·∫øu b·∫°n config Kinesic data stream l√† data source c·ªßa Kinesis data Firehose delivery stream, th√¨ \u0026lsquo;Kinesis data Firehose\u0026rsquo; s·∫Ω ko store data at rest. m√† n√≥ store ·ªü Kinesic data stream VD: b·∫°n send data t·ª´ producer ƒë·∫øn data stream, \u0026lsquo;Kinesic data stream\u0026rsquo; s·∫Ω encrypt data b·∫±ng KMS, r·ªìi store data at rest. Khi \u0026lsquo;Kinesis data Firehose\u0026rsquo; v√†o ƒë·ªçc data, \u0026lsquo;Kinesic data stream\u0026rsquo; s·∫Ω decrypt data v√† g·ª≠i cho \u0026lsquo;Kinesis data Firehose\u0026rsquo; \u0026lsquo;Kinesis data Firehose\u0026rsquo; s·∫Ω store data ƒë√£ decrypt ƒë√≥ trong buffer memory r·ªìi g·ª≠i ƒë·∫øn destination lu√¥n (ch·ª© ko l∆∞u data at rest)\n Q. b·∫°n d√πng kinesis Firehose ƒë·ªÉ stream data ,nh∆∞ng cty y√™u c·∫ßu data c·∫ßn dc encrypt in transit, l√†m n√†o?\n install SSL certificate trong Kinesis Firehose ch·∫Øc ch·∫Øn r·∫±ng all record ƒëang dc transfer via SSL\n Q. best practice ƒë·ªÉ gi·∫£m cost khi d√πng SQS?\n d√πng long polling, ƒë·ªÉ send, receive, delete message ch·ªâ v·ªõi 1 action, d√πng SQS batch api actions\n Q. app c·ªßa b·∫°n send request ƒë·∫øn dynamoDB. nh∆∞ng v√¨ request tƒÉng l√™n n√™n app c·ªßa b·∫°n b·ªã l·ªói throttled. l√†m n√†o?\n tƒÉng throughout c·ªßa dynamoDB table d√πng exponential backoff ·ªü trong request c·ªßa app ƒë·ªÉ n√≥ retry request theo l≈©y k·∫ø delay time. b√¨nh th∆∞·ªùng m·ªçi ng hay d√πng c√°ch ch·ªù 1 time nh·∫•t ƒë·ªãnh (1s) ƒë·ªÉ retry request. n·∫øu 1000 request g·ª≠i ƒë·∫øn b·ªã error, sau 1s s·∫Ω c√≥ 1000 request nh∆∞ th·∫ø g·ª≠i ƒë·∫øn-\u0026gt; l·∫°i l·ªói n·∫øu d√πng exponential backoff, th·ªùi gian retry c·ªßa m·ªói request l√† kh√°c nhau -\u0026gt; kh·∫£ nƒÉng ko b·ªã l·ªói cao h∆°n.\n Q. b·∫°n ph·∫£i deploy 1 app d√πng cloudformation. instance n√†y c·∫ßn install nginx tr∆∞·ªõc. l√†m n√†o ƒë·ªÉ l√†m v·ªõi CF? best practice l√† g√¨?\n d√πng cfn-init helper script v√† AWS::CloudFormation::Init ƒë·ªÉ define c√°c config h∆°n l√† ch·∫°y t·ª´ng command trong user-data\n Q. App c·∫£u b·∫°n host tr√™n beanstalk. C√≥ 1 s·ª± thay ƒë·ªïi, nh∆∞ng b·∫°n c·∫ßn minimized downtime c·ªßa app v√† current env ko dc thay ƒë·ªïi. d√πng c√°ch deploy n√†o? All at Once? Rolling? Immutable? Rolling with Additional Batch?\n immutable, v√¨ Rolling with Additional Batch th√¨ current env v·∫´n c√≥ thay ƒë·ªïi\n Q. B·∫°n ƒëang deploy 1 app d√πng lambda v√† APi gateway. b·∫°n c·∫ßn deploy version m·ªõi nh∆∞ng version m·ªõi s·∫Ω cho 1 ph·∫ßn nh·ªè user th√¥i, l√†m n√†o?\n t·∫°o canary release ·ªü trong API gateway\n Q. b·∫°n c√≥ 1 dynamoDB table d√πng global secondary index. C√°ch n√†o ƒë·ªÉ l·∫•y result latest m√† ko ·∫£nh h∆∞·ªüng ƒë·∫øn RCU?\n query b·∫±ng evetual read. dynamoDB with global secondary index th√¨ ko support consistent read, ch·ªâ support eventual read th√¥i. v·ªõi l·∫°i d√πng Scan th√¨ s·∫Ω ·∫£nh h∆∞·ªüng performance to√†n b·ªô table\n Q. b·∫°n c·∫ßn setup 1 restFUL API in aws m√† s·∫Ω dc g·ªçi qua URL https://democompany.com/customers?ID=1 KH s·∫Ω nh·∫≠n dc th√¥ng tin detail khi c·∫•p ID cho API l√†m n√†o?\n t·∫°o API gateway v√† 1 lambda fucntion ƒë·ªÉ get th√¥ng tin detail c·ªßa KH expose GET method ra cho API Gateway\n Q. c√°c bc ƒë·ªÉ encrypt/decrypt data locally (hay c√≤n g·ªçi l√† client side) in your app thoe best practice n·∫øu b·∫°n ƒë√£ c√≥ customer master key?\n step encrypt:\ncustomer master key (CMK) -\u0026gt; (generate) plain text data key + encrypted data key\nplain text data key -\u0026gt; (encrypt) data r·ªìi th√¨ x√≥a ƒëi\nencrypted data key -\u0026gt; gi·ªØ l·∫°i\nstep decrypt:\nencrypted data key -\u0026gt; (decrypt) plain text data key -\u0026gt; (decrypt) data r·ªìi th√¨ x√≥a ƒëi\nchi ti·∫øt:\nD√πng operation sau (GenerateDataKey) ƒë·ªÉ generate t·ª´ \u0026lsquo;customer master key\u0026rsquo; CMK ra \u0026lsquo;data encryption key\u0026rsquo; (\u0026lt;\u0026mdash;\u0026ndash;ƒë√¢y l√† respone)\nl·∫•y c√°i \u0026lsquo;plaintext data key\u0026rsquo; (·ªü trong field \u0026lsquo;Plaintext\u0026rsquo; c·ªßa respone) ƒë·ªÉ encrypt data locally, r·ªìi x√≥a lu√¥n c√°i \u0026lsquo;plaintext data key\u0026rsquo; ƒë√≥\nl∆∞u l·∫°i c√°i \u0026lsquo;encrypted data key\u0026rsquo; (·ªü trong field \u0026lsquo;CiphertextBlob\u0026rsquo; c·ªßa respone) ·ªü b√™n c·∫°nh data (locally/client side) ƒë√£ encrypt ƒë·ªÉ sau c√≤n decrypt\nD√πng operation sau (Decrypt) ƒë·ªÉ decrypt c√°i \u0026lsquo;encrypted data key\u0026rsquo; m√† m√¨nh ƒë√£ l∆∞u, decrypt xong dc \u0026lsquo;plaintext data key\u0026rsquo;\nD√πng \u0026lsquo;plaintext data key\u0026rsquo; ƒë·ªÉ decrypt data locally, r·ªìi x√≥a lu√¥n c√°i \u0026lsquo;plaintext data key\u0026rsquo; ƒë√≥\n Q. mu·ªën migrate source code t·ª´ SVN l√™n Code Commit? l√†m n√†o?\n tr∆∞·ªõc ti√™n ph·∫£i t·ª´ SVN migrate l√™n Git tr∆∞·ªõc r·ªìi ti·∫øp l√™n Code Commit\n Q. ƒë·ªÉ s·ª≠ d·ª•ng hi·ªáu qu·∫£ indexing ƒë·ªÉ tƒÉng performance cho dynamoDB l√†m n√†o?\n s·ªë l∆∞·ª£ng index n√™n keep minimum, nh·ªØng c√°i index hi·∫øm khi d√πng ch·ªâ l√†m tƒÉng storage th√¥i\ntr√°nh vi·ªác ƒë√°nh index cho c√°c table ph·∫£i Write data nhi·ªÅu.\n Q. trong dynamoDB th√¨ Projection expression, Condition Expressions, Update Expressions l√† g√¨?\n Projection expression d√πng ƒë·ªÉ query table\nCondition Expressions ƒë·ªÉ ch·ªâ ƒë·ªãnh ƒëi·ªÅu ki·ªán modify item\nUpdate Expressions ƒë·ªÉ update item\n Q. DAX v√† dynamoDB x·ª≠ l√Ω v·ªõi c√°c request strongly read v√† eventually read nh∆∞ n√†o?\n n·∫øu 1 eventually read request ƒë·∫øn, check xem DAX c√≥ cache ko, n·∫øu ko th√¨ pass t·ªõi dynamoDB, c√≥ kq th√¨ cache l·∫°i trong DAX r·ªìi m·ªõi pass cho user\nn·∫øu 1 strongly read request ƒë·∫øn, pass lu√¥n t·ªõi dynamoDB, c√≥ kq th√¨ pass cho user lu√¥n\n Q. b·∫°n ƒëang l√†m vi·ªác v·ªõi dynamoDB ƒë·ªÉ save c√°c string v√†o ƒë√≥. nh∆∞ng n·∫øu \u0026gt; 400KB th√¨ s·∫Ω b·ªã l·ªói, l√†m n√†o?\n l∆∞u string l·ªõn v√†o s3, dynamoDB ch·ªâ l∆∞u object identifier point to s3 th√¥i\ncon s·ªë 400 l√† fix c·ª©ng r·ªìi ko th·ªÉ thay ƒë·ªïi\nc√≥ th·ªÉ n√©n string ƒë·ªÉ gi·∫£m size nh∆∞ng ch·ªâ l√† gi·∫£i ph√°p tam th·ªùi\n Q. b·∫°n ƒëang d√πng lambda ƒë·ªÉ access DB. Y√™u c·∫ßu l√† all DB connection string c·∫ßn encrpyt at rest? l√†m n√†o?\n d√πng environment variable ƒë·ªÉ l∆∞u db connection string\nencrypt environment variable ƒë√≥\n Q. Trong dynamoDB, ph√¢n bi·ªát 1-Read Side Cache, 2-Read through Cache, 3-Write Through Cache, 4-Write Around Cache, 5-Write back/behind Cache?\n Read Side Cache:\napp request read to cache (redis) n·∫øu ko c√≥ trong cache th√¨ m·ªõi read t·ª´ dynamoDB, r·ªìi return to app, r·ªìi write v√†o cache\nuseful for read-heavy workload, nh∆∞ng vi·ªác ghi v√†o cache l√† data ko consistence, ko durable\nRead through Cache:\napp request read to cache DAX, n·∫øu ko c√≥ trong cache th√¨ read trong DynamoDB, result dc cache v√†o DAX, return to app\nuseful for read-heavy workload\nWrite Through Cache:\napp request write to DAX, DAX write v√†o dynamoDB, n·∫øu th√†nh c√¥ng m·ªõi write v√†o DAX, r·ªìi return to app\nuseful for read heavy workload, data consistence nh∆∞ng ko gi√∫p g√¨ n·∫øu write-heavy workload\nWrite Around Cache:\napp request write tr·ª±c ti·∫øp dynamoDB, r·ªìi m·ªõi write v√†o DAX, r·ªìi return to app\nuseful for write l∆∞·ª£ng data l·ªõn considerable\nWrite back/behind Cache:\napp request write to DAX, DAX return status lu√¥n cho app, in background data write to DynamoDB, r·ªìi write v√†o cache DAX\nuseful cho write-heavy workload nh∆∞ng c√≥ th·ªÉ m·∫•t data\n Q. khi t·∫°o table trong dynamoDB, n·∫øu mu·ªën encrypt data b·∫°n c√≥ th·ªÉ ch·ªçn 2 ki·ªÉu customer master key l√† j?\n AWS owned CMK, key c·ªßa dynamodb, ko m·∫•t ph√≠ th√™m\nAWS managed CMK, key c·ªßa KMS, m·∫•t ph√≠ KMS\n Q. b·∫°n dev 1 app d√πng elastic beanstalk. l√†m c√°ch n√†o ƒë·ªÉ trace issue, debug d·ªÖ nh·∫•t khi c√≥ issue?\n d√πng AWS X-Ray\n Q. B·∫°n ƒëang dev 1 app m√† n√≥ c·∫ßn ch·ª©c nƒÉng sign-up sign-in, b·∫°n ko mu·ªën code fucntion sign up in ƒë√≥, mu·ªën ƒë·∫£m b·∫£o code s·∫Ω dc excute t·ª± ƒë·ªông sau khi user sign in. l√†m n√†o?\n D√πng cognito, t·∫°o lambda fucntion v√† trigger function ƒë√≥ m·ªói khi user-sign in\n Q. b·∫°n dev 1 app tr√™n on premise network c·ªßa b·∫°n. app c·∫ßn t∆∞∆°ng t√°c v·ªõi aws services. l√†m n√†o?\n t·∫°o iam user, generate ra Access key, s·ª≠ d·ª•ng access key ƒë√≥ ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi aws service\n Q. B·∫°n c·∫ßn migrate 1 m√¥i tr∆∞·ªùng development c√≥ s·∫µn g·ªìm c√°c Docker container l√™n Aws. l√†m n√†o ƒë·ª° t·ªën effort?\n t·∫°o app v√† environment cho docker container ·ªü trong Beanstalk v√¨ n√≥ h·ªó tr·ª£ nhi·ªÅu platform v√† language\n Q. B·∫°n c·∫ßn qu·∫£n l√Ω blue/green deployment cho app. d√πng c√°ch n√†o gi·ªù?\n trong Beanstalk d√πng swap URL\nho·∫∑c d√πng Route53 v·ªõi weighted routing policy, ƒë·ªÉ ki·ªÉm so√°t traffic cho subdomain v√† domain\n Q. B·∫°n c·∫ßn cho developers quy·ªÅn l√†m vi·ªác v·ªõi beanstalk enviroment nh∆∞ng h·ªç ko dc v√†o console? l√†m n√†o?\n cho h·ªç l√†m vi·ªác qua EBeanstalk CLI\n Q. b·∫°n deploy 1 lambda function dc invoke qua APi gateway. b·∫°n mu·ªën bi·∫øt n·∫øu c√≥ l·ªói khi lambda ƒë∆∞·ª£c invoke th√¨ l√†m n√†o?\n d√πng Cloudwatch Metrics for AWS Lambda\n\u0026mdash;P4\u0026mdash;\n Q. B·∫°n c√≥ v√†i function lambda, c·∫ßn g·ªçi ƒë√∫ng version c·ªßa function ƒë√≥, l√†m n√†o?\n t·∫°o alias\n Q. B·∫°n ƒëang c·∫ßn test cac services b·∫±ng c√°ch query b·∫±ng REST API, c·∫ßn c√°i g√¨?\n Access key\n Q. app ƒëc x√¢y d·ª±ng b·ªõi nhi·ªÅu component ph√¢n t√°n. nh∆∞ng c√°c th√¥ng tin quan tr·ªçng trao ƒë·ªïi gi·ªØa c√°c component b·ªã m·∫•t khi 1 c√°i component b·ªã down. l√†m sao?\n d√πng SQS ƒë·ªÉ qu·∫£n l√Ω message gi·ªØa c√°c component\n Q. 1 cty c√≥ c√°c aws ec2 v√† on-premise, mu·ªën d√πng aws codeDeploy th√¨ c·∫ßn nh·ªØng g√¨?\n CodeDeploy agent ph·∫£i dc install tr√™n c·∫£ 2 server,\nc·∫£ 2 server ƒë·ªÅu dc tagged,\non-premise instance ko c·∫ßn IAM instance profile\n Q. ƒëang d√πng SQS, mu·ªën sau khi message v√†o queue n√≥ s·∫Ω invisble trong v√≤ng 5p l√†m n√†o?\n setting delay queues (0 ƒë·∫øn 15p)\ns·ª≠ d·ª•ng message timer cho individual message\n Q. parameter trong CF cho ph√©p nh·ªØng ki·ªÉu d·ªØ li·ªáu n√†o?\n String, number, list, commadelimited list, SSM param, aws type\n Q. b·∫°n t·∫°o nhi·ªÅu s3 bucket, khi mu·ªën x√≥a s3 bucket b·∫±ng DeletionPolicy trong Cloudformation th√¨ c·∫ßn ch√∫ √Ω g√¨?\n C√°c object trong s3 bucket c·∫ßn ph·∫£i b·ªã x√≥a h·∫øt tr∆∞·ªõc\n Q. c√≥ th·ªÉ k·∫øt h·ª£p DynamoDB v·ªõi lambda ƒë·ªÉ m·ªói khi c√≥ thay ƒë·ªïi tr√™n dynamnoDB th√¨ trigger lambda ko?∆∞\n c√≥. Enable dynamoDB stream ƒë·ªÉ invoke lambda\n Q. b·∫°n l√†m vi·ªác v·ªõi dynamoDB, b·∫°n mu·ªën bi·∫øt consumed capacity cho m·ªói query l√† bao nhi√™u th√¨ l√†m n√†o?\n set ReturnConsumedCapacity in query to TOTAL\ndefault n√≥ set l√† NONE\n Q. b·∫°n c√≥ 1 s3 bucket ƒë√£ dc versioning v√† encrypted.nh·∫≠n h√†ng ng√†n PUT request m·ªói ng√†y. sau 6 th√°ng th√¨ c√≥ l·ªói HTTP 503, l√†m n√†o?\n d√πng S3 inventory tool, kh·∫£ nƒÉng l√† c√≥ nh·ªØng object l√™n h√†ng tri·ªáu version, d√πng inventory ƒë·ªÉ x√°c ƒë·ªãnh object n√†o\n Q. b·∫°n l√†m vi·ªác v·ªõi aws Step function, c·∫ßn ch√∫ √Ω c√°c recommend practice g√¨?\n set timeout khi define c√°c state, th∆∞·ªùng th√¨ step functin ch·ªâ d·ª±a v√†o ph·∫£n h·ªìi c·ªßa worker, n√™n n·∫øu c√≥ l·ªói m√† ko set timeout s·∫Ω b·ªã stuck\nv·ªõi l∆∞·ª£ng payload l·ªõn qu√° 32KB truy·ªÅn gi·ªØa c√°c state th√¨ s·∫Ω b·ªã terminate, n√™n l∆∞u tr√™n s3 r·ªìi truy·ªÅn c√°i resource name thay v√¨ raw data\n Q. B·∫°n t·∫°o 1 lambda fucntion d√πng Cloudformation. sao ƒë·ªÉ route 5% c√°c traffic to version m·ªõi\n set AdditionalVersionWeights = 0.05\n Q. B·∫°n d√πng aws CodeCommit, t·∫°o c√°c repository. b·∫°n c·∫ßn share c√°c repo ƒë√≥ cho developer team, l√†m n√†o?\n v√†o IAM console, ch·ªçn user, ch·ªçn \u0026lsquo;HTTPS Git credentials for AWS CodeCommit\u0026rsquo; ƒë·ªÉ generate user/password\ncho ph√©p dev connect qua htttps r·ªìi gitclone t·ª´ codeCommit v·ªÅ\n Q. Move t·ª´ jenkins l√™n codePipeline th√¨ c·∫ßn d√πng g√¨ detect c√°c change c·ªßa pipeline?\n Cloudwatch Events\n Q. d√πng dynamoDB m√† b·ªã l·ªói ProvisionedThroughputExceededException trong log th√¨ c·∫ßn l√†m j?\n b·ªüi v√¨ dynamoDB t·ª± ƒë·ªông retry c√°c request, n·∫øu request qu√° l·ªõn th√¨ s·∫Ω b·ªã l·ªói ProvisionedThroughputExceededException\nconsider vi·ªác d√πng exponential backoff trong code\n Q. b·∫°n d√πng SQS ƒë·ªÉ send message gi·ªØa c√°c component, b·∫°n c·∫ßn g·ª≠i c·∫£ metadata c√πng message th√¨ l√†m n√†o?\n d√πng message attribute ƒë·ªÉ g·ª≠i metadata c√πng l√∫c v·ªõi message body.\n Q. b·∫°n c·∫ßn d√πng cloudformation ƒë·ªÉ t·∫°o ra stack ·ªü 1 account aws kh√°c, l√†m n√†o?\n d√πng ch·ª©c nƒÉng Create StackSets\n Q. Aws Opswork support officially nh·ªØng tool n√†o?\n Chef, Puppet\n Q. B·∫°n ƒëang mu·ªën deploy app l√™n ECS. mu·ªën ch·∫Øc ch·∫Øn c√≥ th·ªÉ d√πng X-ray ƒë·ªÉ trace deploy. l√†m n√†o?\n t·∫°o docker image v·ªõi X-ray, r·ªìi deploy container to ECS\n Q. b·∫°n d√πng CodeBuild, code dc l∆∞u ·ªü S3, khi run build b·ªã l·ªói addressed using the specified endpoint, v√¨ sao?\n Bucket ko ·ªü c√πng region v·ªõi CodeBuild project\n Q. b·∫°n deploy 1 app l√™n Beanstalk, app c·∫ßn RDS l√† 1 ph·∫ßn trong Beanstalk, nh∆∞ng mu·ªën DB dc preserve ngay c·∫£ khi Environment down. l√†m n√†o?\n ch·∫Øc ch·∫Øn r·∫±ng ƒë√£ setting c√°i field Retention khi t·∫°o DB = create snapshot\n Q. M·ªói khi upload ho·∫∑c deploy source l√™n Beanstalk, n√≥ c√≥ gi·ªØ l·∫°i version app ko?\n C√≥\n Q. d√πng DynamoDB, Read capacity l√† 5 units, mu·ªën ƒë·ªçc 5 item 1 gi√¢y, m·ªói item 2KB, default consistency=eventually consistency total size KB dc ƒë·ªçc 1s?\n 2KB -\u0026gt; l√†m tr√≤n 4KB -\u0026gt; c·∫ßn 1 read request\neventually consistency -\u0026gt; 1/2 = 0.5 RCU\ns·ªë KB/s = 5 x 2 = 10 KB/s\neventually -\u0026gt; s·ªë KB/s = 10 x 2 = 20 KB/s\nv√¨ RCU = 0.5 -\u0026gt; 20 x 0.5 = 10 KB\n Q. cty b·∫°n ƒëang qu·∫£n l√Ω deployment env b·∫±ng CodeDeploy. H·ªç mu·ªën t·ª± ƒë·ªông lu√¥n c·∫£ vi·ªác deploy c√°i deployment env. l√†m n√†o?\n d√πng Cloudformation\n Q. B·∫°n qu·∫£n l√Ω nhi·ªÅu DB v·ªõi nhi√™u passwword? l√†m sao ƒë·ªÉ l∆∞u pass cho DB secure?\n d√πng aws secret manager\nb·∫°n t·∫°o db credential, l∆∞u n√≥ v√†o Aws secret manager\nkhi app c·∫ßn access db, n√≥ s·∫Ω request Aws secret manager\nAws secret manager query, decrypt v√† tr·∫£ v·ªÅ cho app qua HTTPS, TLS\napp d√πng credential ƒë√≥ access db\n Q. B·∫°n ƒëang dev 1 mobile app. User ƒë√£ authen b·∫±ng facebook. App c·∫ßn get temporary access credential ƒë·ªÉ l√†m vi·ªác v·ªõi aws resource d√πng c√°i j?\n AssumeRoleWithWebIdentity\n Q. b·∫°n c√≥ 1 s3 bucket, bucket ƒë√≥ c·∫ßn ph·∫£i dc access t·ª´ nh·ªØng aws account kh√°c. l√†m th·ªÉ n√†o ƒë·ªÉ secure c√°c account kh√°c c√≥ th·ªÉ access v√†o resource c·ªßa b·∫°n?\n t·∫°o cho h·ªç 1 role c√≥ quy·ªÅn assume role v√† cung c·∫•p cho h·ªç ARN role ƒë√≥\nh·ªç cung c·∫•p cho b·∫°n externalID unique ƒë·ªÉ b·∫°n update trust policy trong role c·ªßa h·ªç.\n Q. B·∫°n ƒëang c√≥ Beanstalk d√πng java7 tomcat7. B·∫°n mu·ªën d√πng java 8 v√† tomcat 8.5 l√†m n√†o?\n t·∫°o m√¥i tr∆∞·ªùng m·ªõi\nupload code, deploy, fix bug\nSwap Environment Url (CNAME)\nok th√¨ x√≥a m√¥i tr∆∞·ªùng c≈©\n Q. b·∫°n ƒëang d√πng APi gateway, sao ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ c·ªßa c√°c request to API gateway?\n d√πng API caching\n Q. d√πng DynamoDB m√† b·ªã l·ªói HTTP 400, l√† sao?\n v∆∞·ª£t qu√° throughput c·ªßa table, ho·∫∑c thi·∫øu required parameters\n Q. app c·ªßa b·∫°n d√πng SQS, message s·∫Ω xu·∫•t hi·ªán trong queue trong kho·∫£ng 20-60s, n√™n d√πng lo·∫°i SQS n√†o?\n long polling, SQS s·∫Ω ch·ªù cho ƒë·∫øn khi message xu·∫•t hi·ªán\n Q. AWS Athena ƒë·ªÉ l√†m g√¨?\n ƒë·ªÉ perform SQL query in data S3\n Q. 1 app ƒëang connect ƒë·∫øn RDS. b·∫°n c√≥ nhi·ªám v·ª• debug performance issue li√™n quan ƒë·∫øn query, l√†m n√†o?\n l·∫•y c√°i log Slow query log c·ªßa RDS\nError log: cung c·∫•p log l·ªói,\nGeneral query log: cung c·∫•p log query normal v√† client connect time,\nSlow query log: cung c·∫•p log m√† query ch·∫°y ch·∫≠m h∆°n b√¨nh th∆∞·ªùng\n Q. c√≥ th·ªÉ bi·∫øn 1 ec2 th√†nh ECS ko?\n C√†i install ecs agent tr√™n Amazon ec2\n Q. b·∫°n c√≥ 1 app host b·ªüi s3, sao ƒë·ªÉ t√≠nh s·ªë get request to s3?\n d√πng cloudwatch\n\u0026mdash;P5\u0026mdash;\n Q. b·∫°n c·∫ßn ph∆∞∆°ng th·ª©c deploy lambda function d√πng SAM, c·∫ßn l√†m g√¨?\n 1, sam init app: t·∫°o ra 1 file yaml define Lambda function v√† API gateway\n2, test app tr√™n local\n3, sam package app: upload app l√™n s3, output ra 1 file YAML ch·ª©a s3 uri\n4, sam deploy app b·∫±ng file YAML m·ªõi ƒë√≥\n Q. trong Step Function b·∫°n t·∫°o lambda fucntion b·ªã l·ªói ServiceException, best practice l√† g√¨?\n Retry. D√πng Retry Code v·ªõi ch·ªâ ƒë·ªãnh ErrorEquals\nko th·ªÉ d√πng Catch code v√¨ n√≥ dc √°p d·ª•ng khi retry 1 s·ªë l·∫ßn\n Q. b·∫°n ƒëang d√πng CodeBuild cho 1 app, app ƒë√≥ c·∫ßn access resource trong private subnet, l√†m n√†o?\n Default l√† Codebuild ko access dc v√†o resource trong VPC. C·∫ßn ph·∫£i s·ª≠a configure c·ªßa CodeBuild\nth√™m th√¥ng tin v·ªÅ VPC, subnet, SG m√† CodeBuild c·∫ßn access\n Q. S3 bucket cho ph√©p bao nhi√™u PUT GET request m·ªói gi√¢y v√† n·∫øu b·∫°n t·∫°o 3 prefix trong s3 n·ªØa th√¨ tƒÉng l√™n bao nhi√™u request?\n 3500 PUT POST DELETE/s, 5500 GET /s\nn·∫øu c√≥ 3 prefix th√¨ x3\n Q. B·∫°n ƒëang d√πng X-ray, n√≥ cung c·∫•p nh·ªØng t√≠nh nƒÉng g√¨, c·∫ßn trace all incoming http request to app c·ªßa b·∫°n d√πng c√°i j?\n interceptor ƒë·ªÉ trace all incoming http request\nclient handler ƒë·ªÉ trace sdk m√† m√¨nh d√πng ƒë·ªÉ call ƒë·∫øn aws resource\nhttp client ƒë·ªÉ trace internal v√† external web service\n Q. Data pipeline v√† c√°c kh√°i ni·ªám, c√°i g√¨ define source data v√† destination data?\n Data Node: define location c·ªßa input v√† output data dc store ·ªü ƒë√¢u\nActivities: define work c·ªßa pipeline\nResources: l√† nh·ªØng c√°i th·ª±c hi·ªán vi·ªác m√† activity ch·ªâ ƒë·ªãnh\nAction: dc trigger khi activity th·ªèa m√£n ƒëi·ªÅu ki·ªán\n Q. Load data s3 sang Readshift d√πng command g√¨?\n Copy\n Q. b·∫°n d√πng Cognito, y√™u c·∫ßu l√† n·∫øu user c√≥ compromised credential (v√≠ d·ª• password d·ªÖ ƒëo√°n) th√¨ ph·∫£i b·∫Øt user t·∫°o m·ªõi passw, l√†m n√†o?\n tick v√†o ph·∫ßn block-use trong Advance security section ƒë·ªëi v·ªõi nh·ªØng compromised credential\nt·∫°o user pool trong Cognito\n Q. B·∫°n c√≥ nhi·ªÅu data tr√™n aws, c·∫ßn 1 gi·∫£i ph√°p nhanh ƒë·ªÉ d·ª±ng visualization screens cho ƒë·ªëng data ƒë√≥, d√πng j?\n Quicksight\n Q. AWS Glue l√† g√¨?\n ƒë·ªÉ qu·∫£n l√Ω service ETL (extract, transform, load)\n Q. b·∫°n c√≥ S3 bucket v√† l∆∞u object size tr√™n DynamoDB, y√™u c·∫ßu l√† m·ªói khi s3 c√≥ object dc upload th√¨ s·∫Ω trigger 1 record update trong DynamoDB, l√†m n√†o?\n S3 event trigger tr·ª±c ti·∫øp Lambda , ko c·∫ßn SQS\n Q. kinesis ƒë·ªÉ t√≠nh s·ªë l∆∞·ª£ng shards c·∫ßn c√¥ng th·ª©c g√¨?\n Incoming write bandwith, Outgoing read bandwidth\nnumber_of_shards = max(incoming_write_bandwidth_in_KiB/1024, outgoing_read_bandwidth_in_KiB/2048)\n Q. b·∫°n dev app d√πng ECS, docker, th√¨ s·∫Ω l∆∞u docker base image ·ªü ƒë√¢u?\n ECR, Dockerhub\n Q. b·∫°n t·∫°o policy cho user access v√†o resource, sau ƒë√≥ b·∫°n update policy th√¨ user report ko v√†o dc resource n·ªØa, c√≥ th·ªÉ revert l·∫°i version policy c≈© ko?\n c√≥, set version policy c≈© th√†nh default\n Q. b·∫°n d√πng CodeCommit, y√™u c·∫ßu l√† m·ªói khi c√≥ change l√™n repo th√¨ ph·∫£i notification cho b·∫°n, l√†m n√†o? co d√πng AWS Config ko?\n Lambda ho·∫∑c SNS\nko d√πng AWS Config dc v√¨ n√≥ l√† ƒë·ªÉ monitor nh·ªØng thay ƒë·ªïi v·ªÅ m·∫∑t configuration ch·ª© ko ph·∫£i thay ƒë·ªïi tr√™n repository\n Q. b·∫°n ƒë·ªãnh d√πng AWS Batch service ƒë·ªÉ ch·∫°y c√°c job compute, tool n√†o ƒë·ªÉ monitor progress c·ªßa job?\n Cloudwatch event\n Q. best practice khi define secondary index cho dynamodb l√† g√¨?\n keep minimum index\ntr√°nh ƒë√°nh index cho table ph·∫£i ghi nhi·ªÅu\n Q. b·∫°n ƒëang d√πng RDS cho app, y√™u c·∫ßu l√† c√°c connection t·ª´ app t·ªõi DB c·∫ßn encrypt th√¨ d√πng g√¨? SSL v√† KMS ch·ªçn n√†o?\n SSL, chuy√™n d√πng ƒë·ªÉ encrypt connect to DB, data in transit\nKMS ch·ªâ ƒë·ªÉ d√πng encrpyt data at rest ho·∫∑c before transit th√¥i\n Q. b·∫°n ƒëang d√πng lambda, v√† lambda c·ªßa b·∫°n c·∫ßn nhi·ªÅu th∆∞ vi·ªán thirdparty nh∆∞ x·ª≠ l√Ω ·∫£nh, hay ƒë·ªì h·ªça, l√†m n√†o?\n t·∫°o lambda deployment package ch·ª©a c·∫£ lambda v√† th∆∞ vi·ªán thirdparty, upl√™n s3, r·ªìi d√πng cli\n Q. b·∫°n ƒëang d√πng cloudfront v·ªõi origin l√† s3 cho web distribution. b·∫°n mu·ªën control th·ªùi gian object dc l∆∞u trong cache l√†m n√†o?\n config c√°i origin ph·∫£i c√≥ Expires header field m·ªói object, ho·∫∑c add Cache control\nconfig Cloudfront s·ª≠ d·ª•ng ch·ªâ ƒë·ªãnh time trong TTL\n Q. b·∫°n d√πng 1 tool POSTMAN ƒë·ªÉ send API request t·ªõi resource trong aws. b·∫°n c·∫ßn sign request ƒë·ªÉ aws bi·∫øt b·∫°n l√† ai? th√¨ d√πng gi? user+pass, private key file, KMS?\n D√πng access key + secret key.\nCh·ª© KMS ƒë·ªÉ encrpyt data, user/pass d√πng v√†o console, private key d√πng ƒë·ªÉ login ec2\n Q. ƒëang d√πng SNS, mu·ªën ch·ªâ nh·∫≠n c√°c lo·∫°i message nh·∫•t ƒë·ªãnh, ko mu·ªën nh·∫≠n h·∫øt message dc publish v√†o topic dc ko?\n filter policy c·ªßa topic\n Q. b·∫°n ƒëang c√≥ s·∫µn 1 redshift cluster, c√≥ nhi·ªÅu data quan tr·ªçng, c·∫ßn encrypt at rest l√†m n√†o? c√≥ d√πng SSL cert dc ko?\n C·ª© th·∫ø enable KMS encryptor th√¥i (c√°i n√†y l√† feature m·ªõi, tr∆∞·ªõc ƒë√¢y ko l√†m th·∫ø dc)\ntr∆∞·ªõc ƒë√¢y ph·∫£i t·∫°o 1 cluster m·ªõi v√† enable KMS encrypt l√™n, unload kh·ªèi cluster c≈©, reload l·∫°i t·ªõi cluster m·ªõi\nSSL cert ch·ªâ ƒë·ªÉ encrpyt data in transit\n Q. b·∫°n c√≥ app tr√™n ec2, gi·ªù mu·ªën n·∫øu primary app t·∫°ch th√¨ s·∫Ω route traffic ƒë·∫øn 1 static web th·ª© 2, l√†m n√†o? R53 hay LB?\n d√πng route53, LB ch·ªâ d√πng ƒë·ªÉ distribute traffic (ph√¢n ph·ªëi), ko ƒëi·ªÅu h∆∞·ªõng traffic nh∆∞ R53 dc\n Q. B·∫°n ƒëang d√πng AWS Cognito Sync, n√≥ cung c·∫•p nh·ªØng feature g√¨?\n sync app user data gi·ªØa nhi·ªÅu device v√† web app,\nweb app c√≥ th·ªÉ read write data l√™n cache ko c·∫ßn quan t√¢m k·∫øt n·ªëi c·ªßa mobile device, sau khi device k·∫øt n·ªëi th√¨ s·∫Ω sync data,\nv√† c√≥ th·ªÉ push notify c√°c thi·∫øt b·ªã kh√°c khi c√≥ update\n Q. app c·ªßa b·∫°n host tr√™n ec2, d√πng cloudfront ƒë·ªÉ distribute content, sao ƒë·ªÉ encrpyt traffic gi·ªØa orgin v·ªõi cloudfront, v√† gi·ªØa cloudfront v·ªõi viewer?\n gi·ªØa origin v·ªõi cloudfront: tr√™n console, setting Origin Protocol policy,\ngi·ªØa cloudfront v·ªõi viewer: tr√™n console, setting Viewer Protocol policy\n Q. App b·∫°n t∆∞∆°ng t√°c v·ªõi s3, m√† gi·ªù s3 t·ª± nhi√™n ko t·ªìn t·∫°i, sao ƒë·ªÉ bi·∫øt n√≥ b·ªã x√≥a nh∆∞ n√†o? c√≥ d√πng Inspector dc ko?\n d√πng cloudtrail log ƒë·ªÉ xem bucket delete api request,\nInspector ch·ªâ ƒë·ªÉ check server c√≥ b·ªã vulnerable ko th√¥i\n Q. Best practice c·ªßa lambda, l√†m sao t·∫≠n d·ª•ng Execution Context l√† g√¨?\n Execution Context l√† m√¥i tr∆∞·ªùng runtime t·∫°m th·ªùi lambda t·∫°o cho function ch·∫°y\nl·∫ßn ƒë·∫ßu ho·∫∑c update function th√¨ n√≥ s·∫Ω m·∫•t time ƒë·ªÉ t·∫°o m√¥i tr∆∞·ªùng Execution Context n√†y\nc√°c l·∫ßn sau n√≥ d√πng l·∫°i n√™n nhanh h∆°n\nƒë·ªÉ l√¢u 20-45p th√¨ n√≥ t·ª± x√≥a m√¥i tr∆∞·ªùng n√†y\nExecution Context l∆∞u kho·∫£ng 500Mb cache c√°c data required\nkhi ch·∫°y n√™n check xem n·∫øu data required exist th√¨ ko c·∫ßn reuquest ƒë·ªÉ ch·∫°y function cho nhanh\n Q. b·∫°n ƒëang x√¢y d·ª±ng web stateful, th√¨ c·∫ßn c√°i g√¨ l∆∞u session data/ c√°i g√¨ distribute traffic? ALB hay APIGateway?\n DynamoDB l∆∞u session data, ALB ƒë·ªÉ distribute traffic\nAPI Gateway ko ph·∫£i ƒë·ªÉ distribute traffic m√† l√† qu·∫£n l√Ω API\n Q. app c·ªßa b·∫°n c√≥ ASG, c·∫ßn d·ª±a tr√™n s·ªë l∆∞·ª£ng user concurrent ƒë·ªÉ scale th√¨ l√†m n√†o?\n t·∫°o 1 cloudwatch metric cho s·ªë l∆∞·ª£ng user concurrent ƒë·ªÉ trigger ASG policy\n Q. B·∫°n ƒëang d√πng S3, app r·∫•t ph·ªï bi·∫øn n√™n c√≥ nh·ªØng object s3 ƒëc access th∆∞·ªùng xuy√™n, sao ƒë·ªÉ enhance c√°c GET request/ PUT request?\n Enhance c·ªßa GET request l√† d√πng cloudfront (v·ª´a gi·∫£m ƒë·ªô tr·ªÖ v·ª´a tƒÉng performance), ho·∫∑c tƒÉng s·ªë l∆∞·ª£ng prefix trong s3 (nh∆∞ng c√°i n√†y ko gi·∫£m ƒë·ªô tr·ªÖ dc) v√≠ du: bucket/folder1/sub1/file\nbucket/folder1/sub2/file bucket/1/file\nbucket/2/file\nPrefixes of the object \u0026lsquo;file\u0026rsquo; would be: \u0026lsquo;/folder1/sub1/\u0026rsquo; , \u0026lsquo;/folder1/sub2/\u0026rsquo;, \u0026lsquo;/1/\u0026rsquo;, \u0026lsquo;/2/\u0026rsquo;.\nN·∫øu c√≥ 4 prefix nh∆∞ hi·ªán t·∫°i th√¨ c√≥ th·ªÉ tƒÉng GET reuqest l√™n 4 l·∫ßn\nEnhance c·ªßa PUT request l√† th√™m random prefix (nh·ªØng chu·ªói ng·∫´u nhi√™n) v√†o key name\n Q. B·∫°n d√πng SQS, app c·ªßa b·∫°n m·∫•t 60s ƒë·ªÉ process message, SQS ƒëang setting default th√¨ c·∫ßn setting g√¨ n·ªØa?\n Change visible timeout tƒÉng l√™n 60s, v√¨ default visible timeout l√† 30s th√¥i,\nvisible timeout l√† th·ªùi gian message bi·∫øn m·∫•t kh·ªèi queue, sau ƒë√≥ xu·∫•t hi·ªán l·∫°i,\napp c·∫ßn ph·∫£i v√†o m√† x√≥a message ƒëi sau khi process xong\n Q. khi deploy code l√™n Beanstalk th√¨ c·∫ßn ch√∫ √Ω g√¨?\n up file zip ho·∫∑c war,\nko qu√° 512MB,\nko ·ªü trong folder,\nn·∫øu th·ª±c c√°c task theo l·ªãch th√¨ c·∫ßn c√≥ file cron.yaml\nC√°c c√¢u h·ªèi ƒë∆∞·ª£c note l·∫°i trong qu√° tr√¨nh thi (ch∆∞a add c√¢u tr·∫£ l·ªùi)  Code pipepilne, CodeBuild, CodeCommit c√°i n√†o support parrallel branch, versionning, batch? Global secondary index th√¨ d√πng cho eventually read hay srongly read request? Xray m√† mu·ªën tr·∫£ v·ªÅ th√™m information th√¨ c·∫ßn ph·∫£i th√™m g√¨? annotation hay meta data? Deploy Serverless application model b·∫±ng Cloudformation th√¨ c·∫ßn define g√¨ trong template? Api ƒëc protect b·ªüi Multifactor Authentication th√¨ dev l√†m sao ƒë·ªÉ call api ƒë√≥? d√πng c√°i g√¨ trong nh·ªØng c√°i sau GetSessionToken hay GetFederationToken hay GetCallerIdentity? Api gateway b·ªã timeout m·∫∑c d√π lambda ƒë√£ finish v√¨ sao? D√πng cli command b·ªã timeout v√¨ nhi·ªÅu resource th√¨ c·∫ßn l√†m g√¨? Quoting, shorthand syntax.. Deploy lambda vs 1 custom environment ch·ª©a library ri√™ng l√†m th·∫ø n√†o? D√πng Codedeploy, khi mu·ªën update version source m·ªõi l√†m n√†o? Upldate file buildspec.js hay appspec.js? ƒê·ªÉ application c√≥ t√≠nh elastic v√† horizontal scale th√¨ d√πng elb + asg hay cloudfront vs asg? Cloudwatch c√≥ nh·ªØng metric n√†o d√πng cho Apigateway vs lambda? CacheMissHit, IntergrationLatency? Log app ƒëang ·ªü tr√™n cloudwatch log, nh∆∞ng t·∫°o metric ƒë·ªÉ filter log th√¨ ko c√≥ k·∫øt qu·∫£? V√¨ sao?  ","href":"/posts/encrypt-aws-certified-developer-associate-note-cda/","title":"Aws Certified Developer Associate Note (CDA)"},{"content":"Demo v·ªÅ c√°ch s·ª≠ d·ª•ng Ansible\nY√™u c·∫ßu ƒê√£ setup Ansible ƒë·ªÉ master v√† client c√≥ th·ªÉ connect ƒë∆∞·ª£c v·ªõi nhau theo c√°ch Dynamic Inventory trong link sau: Setup Ansible Dynamic Inventory Aws\nC√°ch l√†m 1. Change owner of /etc/ansilbe cd /etc/ansible sudo chown -R ec2-user:ec2-user . 2. T·∫°o playbook define c√°c role /etc/ansible/lamp.yml cd /etc/ansible nano /etc/ansible/lamp.yml N·ªôi dung file /etc/ansible/lamp.yml nh∆∞ sau:\n--- - hosts: all become: yes roles: - apache - php Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n3. T·∫°o folder ch·ª©a c√°c roles cd /etc/ansible mkdir roles 4. T·∫°o role php b·∫±ng ansible-galaxy cd /etc/ansible/roles ansible-galaxy init php cd php ls -lsa 5. T·∫°o task cho role cd tasks ls -lsa nano main.yml N·ªôi dung file /etc/ansible/roles/php/tasks/main.yml nh∆∞ sau:\n--- - name: Install php 7 with the common packages yum: name: \u0026quot;{{ item }}\u0026quot; state: present lock_timeout: 60 with_items: - php70 - php70-gd - php70-imap - php70-mbstring notify: restart Apache - name: Upload index.php file to remote web dir copy: src: index.php dest: /var/www/html owner: ec2-user group: ec2-user Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n6. T·∫°o file index.php ƒë·ªÉ hi·ªÉn th·ªã cd /etc/ansible/roles/php/files/ nano index.php N·ªôi dung file /etc/ansible/roles/php/files/index.php:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to devops on AWS\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;?php echo \u0026quot;This text was generated by php7\u0026quot;?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n7. T·∫°o file trong folder handlers cd /etc/ansible/roles/php/handlers/ nano main.yml N·ªôi dung file /etc/ansible/roles/php/handlers/main.yml:\n--- - name: restart Apache service: name: httpd state: restarted Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n8. T·∫°o role apache b·∫±ng ansible-galaxy cd /etc/ansible/roles ansible-galaxy init apache cd /etc/ansible/roles/apache/tasks nano main.yml N·ªôi dung file /etc/ansible/roles/apache/tasks/main.yml\n--- - name: Ensure that the httpd package is not installed in system yum: name: httpd state: absent lock_timeout: 60 - name: Ensure that the httpd-tool package is not installed in system yum: name: httpd-tools state: absent lock_timeout: 60 Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n9. S·ª≠a denpendencies trong /etc/ansible/roles/php/meta cd /etc/ansible/roles/php/meta nano main.yml T√¨m ƒë·∫øn d√≤ng denpendencies v√† s·ª≠a nh∆∞ sau:\ndependencies: - apache Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n10. Run ansible-playbook cd /etc/ansible ansible-playbook lamp.yml Check n·∫øu k·∫øt qu·∫£ ok h·∫øt v√† failed = 0 l√† ƒë∆∞·ª£c.\nV√†o check public ip c·ªßa 2 con ec2 n·∫øu hi·ªÉn th·ªã l√† OK.\n11. Gi·ªõi thi·ªáu Ansible Pull Use case:\nTr∆∞·ªùng h·ª£p b·∫°n s·ª≠a 1 d√≤ng trong ansible-master v√† mu·ªën apply tr√™n h√†ng ch·ª•c server con kh√°c.\nTr∆∞·ªõc ti√™n c·∫ßn chu·∫©n b·ªã 1 github repository.\nT·∫°o 1 github repo (gi·∫£ s·ª≠ https://github.com/hoangmnsd/ansible-pull)\nSSH v√†o con ansible-master\n11.a. Commit c√°c file c·∫ßn thi·∫øt l√™n github cd /etc/ansible/ git init git add ec2.py ec2.ini git commit -m \u0026#34;Config ansible to work with dynamic inventory file\u0026#34; git add roles/ lamp.yml git commit -m \u0026#34;Add playbook and role for apache\u0026amp;php7\u0026#34; 11.b. Push file trong folder ansible l√™n github cd /etc/ansible/ git remote add origin https://github.com/hoangmnsd/ansible-pull.git git push origin master 11.c. Install git v√† ansible tr√™n t·∫•t c·∫£ c√°c client ansible --become -m yum -a \u0026#34;name=git enablerepo=epel state=installed\u0026#34; all ansible --become -m pip -a \u0026#34;name=ansible state=present\u0026#34; all 11.d. Add file localhost cd /etc/ansible nano localhost N·ªôi dung file /etc/ansible/localhost nh∆∞ sau\n[localhost] localhost ansible_connection=local Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n11.e. Push file /etc/ansible/localhost l√™n github git add localhost git commit -m \u0026#34;add localhost inventory file\u0026#34; git push origin master 11.f. T·∫°o cron job ƒë·ªÉ EC2 t·ª± run c√¢u l·ªánh ansible-pull 5 ph√∫t 1 l·∫ßn ansible -m cron -a \u0026#39;name=ansible-pull minute=\u0026#34;*/5\u0026#34; job=\u0026#34;/usr/local/bin/ansible-pull -U https://github.com/hoangmnsd/ansible-pull lamp.yml -i localhost\u0026#34;\u0026#39; all 11.g. Test cron job ch·∫°y ƒë√∫ng ch∆∞a Gi·ªù s·ª≠a n·ªôi dung file /etc/ansible/roles/php/files/index.phpnh∆∞ sau:\ncd /etc/ansible/roles/php/files/ nano index.php N·ªôi dung file nh∆∞ sau:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to devops on AWS\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;?php echo \u0026quot;This text was generated by php7\u0026quot;?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;?php echo \u0026quot;This change reflect Ansible pull cron job\u0026quot;?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Push thay ƒë·ªïi ƒë√≥ l√™n github:\ngit add . git commit -m \u0026#34;Test ansible pull\u0026#34; git push origin master Ch·ªù kho·∫£ng 5 ph√∫t sau v√†o public IP c·ªßa 2 con ansible-master v√† ansible-client xem ƒë√£ xu·∫•t hi·ªán d√≤ng This change reflect Ansible pull cron job th√¨ l√† OK\n*Note:\nT·∫°i th·ªùi ƒëi·ªÉm m√¨nh vi·∫øt b√†i n√†y th√¨ m√¨nh ƒëang s·ª≠ d·ª•ng ansible 2.8\nansible 2.8 ƒëang c√≥ 1 bug l√† \u0026ldquo;yum lockfile is held by another user\u0026rdquo; nh∆∞ link sau: https://unix.stackexchange.com/questions/522246/yum-lockfile-is-held-by-another-user\nC·∫ßn fix workaround l√† v·ªõi nh·ªØng task n√†o c√≥ l·ªánh yum th√¨ ph·∫£i th√™m lock_timeout: 180 v√†o\n-\u0026gt; Th·∫ø n√™n c·∫£ apache v√† php task m√¨nh ƒë·ªÅu ƒë√£ th√™m d√≤ng n√†y.\n","href":"/bk/ansible-demo-ansible-pull-galaxy-playbook/","title":"Ansible Demo Ansible Pull Galaxy Playbook"},{"content":"Demo v·ªÅ c√°ch s·ª≠ d·ª•ng Ansible\nY√™u c·∫ßu ƒê√£ setup Ansible ƒë·ªÉ master v√† client c√≥ th·ªÉ connect ƒë∆∞·ª£c v·ªõi nhau theo c√°ch Dynamic Inventory trong link sau: Setup Ansible Dynamic Inventory Aws\nC√°ch l√†m 1. Change owner of /etc/ansilbe cd /etc/ansible sudo chown -R ec2-user:ec2-user . 2. T·∫°o playbook define c√°c role /etc/ansible/lamp.yml cd /etc/ansible nano /etc/ansible/lamp.yml N·ªôi dung file /etc/ansible/lamp.yml nh∆∞ sau:\n--- - hosts: all become: yes roles: - apache - php Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n3. T·∫°o folder ch·ª©a c√°c roles cd /etc/ansible mkdir roles 4. T·∫°o role php b·∫±ng ansible-galaxy cd /etc/ansible/roles ansible-galaxy init php cd php ls -lsa 5. T·∫°o task cho role cd tasks ls -lsa nano main.yml N·ªôi dung file /etc/ansible/roles/php/tasks/main.yml nh∆∞ sau:\n--- - name: Install php 7 with the common packages yum: name: \u0026quot;{{ item }}\u0026quot; state: present lock_timeout: 60 with_items: - php70 - php70-gd - php70-imap - php70-mbstring notify: restart Apache - name: Upload index.php file to remote web dir copy: src: index.php dest: /var/www/html owner: ec2-user group: ec2-user Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n6. T·∫°o file index.php ƒë·ªÉ hi·ªÉn th·ªã cd /etc/ansible/roles/php/files/ nano index.php N·ªôi dung file /etc/ansible/roles/php/files/index.php:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to devops on AWS\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;?php echo \u0026quot;This text was generated by php7\u0026quot;?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n7. T·∫°o file trong folder handlers cd /etc/ansible/roles/php/handlers/ nano main.yml N·ªôi dung file /etc/ansible/roles/php/handlers/main.yml:\n--- - name: restart Apache service: name: httpd state: restarted Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n8. T·∫°o role apache b·∫±ng ansible-galaxy cd /etc/ansible/roles ansible-galaxy init apache cd /etc/ansible/roles/apache/tasks nano main.yml N·ªôi dung file /etc/ansible/roles/apache/tasks/main.yml\n--- - name: Ensure that the httpd package is not installed in system yum: name: httpd state: absent lock_timeout: 60 - name: Ensure that the httpd-tool package is not installed in system yum: name: httpd-tools state: absent lock_timeout: 60 Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n9. S·ª≠a denpendencies trong /etc/ansible/roles/php/meta cd /etc/ansible/roles/php/meta nano main.yml T√¨m ƒë·∫øn d√≤ng denpendencies v√† s·ª≠a nh∆∞ sau:\ndependencies: - apache Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n10. Run ansible-playbook cd /etc/ansible ansible-playbook lamp.yml Check n·∫øu k·∫øt qu·∫£ ok h·∫øt v√† failed = 0 l√† ƒë∆∞·ª£c.\nV√†o check public ip c·ªßa 2 con ec2 n·∫øu hi·ªÉn th·ªã l√† OK.\n11. Gi·ªõi thi·ªáu Ansible Pull Use case:\nTr∆∞·ªùng h·ª£p b·∫°n s·ª≠a 1 d√≤ng trong ansible-master v√† mu·ªën apply tr√™n h√†ng ch·ª•c server con kh√°c.\nTr∆∞·ªõc ti√™n c·∫ßn chu·∫©n b·ªã 1 github repository.\nT·∫°o 1 github repo (gi·∫£ s·ª≠ https://github.com/hoangmnsd/ansible-pull)\nSSH v√†o con ansible-master\n11.a. Commit c√°c file c·∫ßn thi·∫øt l√™n github cd /etc/ansible/ git init git add ec2.py ec2.ini git commit -m \u0026#34;Config ansible to work with dynamic inventory file\u0026#34; git add roles/ lamp.yml git commit -m \u0026#34;Add playbook and role for apache\u0026amp;php7\u0026#34; 11.b. Push file trong folder ansible l√™n github cd /etc/ansible/ git remote add origin https://github.com/hoangmnsd/ansible-pull.git git push origin master 11.c. Install git v√† ansible tr√™n t·∫•t c·∫£ c√°c client ansible --become -m yum -a \u0026#34;name=git enablerepo=epel state=installed\u0026#34; all ansible --become -m pip -a \u0026#34;name=ansible state=present\u0026#34; all 11.d. Add file localhost cd /etc/ansible nano localhost N·ªôi dung file /etc/ansible/localhost nh∆∞ sau\n[localhost] localhost ansible_connection=local Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n11.e. Push file /etc/ansible/localhost l√™n github git add localhost git commit -m \u0026#34;add localhost inventory file\u0026#34; git push origin master 11.f. T·∫°o cron job ƒë·ªÉ EC2 t·ª± run c√¢u l·ªánh ansible-pull 5 ph√∫t 1 l·∫ßn ansible -m cron -a \u0026#39;name=ansible-pull minute=\u0026#34;*/5\u0026#34; job=\u0026#34;/usr/local/bin/ansible-pull -U https://github.com/hoangmnsd/ansible-pull lamp.yml -i localhost\u0026#34;\u0026#39; all 11.g. Test cron job ch·∫°y ƒë√∫ng ch∆∞a Gi·ªù s·ª≠a n·ªôi dung file /etc/ansible/roles/php/files/index.phpnh∆∞ sau:\ncd /etc/ansible/roles/php/files/ nano index.php N·ªôi dung file nh∆∞ sau:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to devops on AWS\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;?php echo \u0026quot;This text was generated by php7\u0026quot;?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;?php echo \u0026quot;This change reflect Ansible pull cron job\u0026quot;?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Push thay ƒë·ªïi ƒë√≥ l√™n github:\ngit add . git commit -m \u0026#34;Test ansible pull\u0026#34; git push origin master Ch·ªù kho·∫£ng 5 ph√∫t sau v√†o public IP c·ªßa 2 con ansible-master v√† ansible-client xem ƒë√£ xu·∫•t hi·ªán d√≤ng This change reflect Ansible pull cron job th√¨ l√† OK\n*Note:\nT·∫°i th·ªùi ƒëi·ªÉm m√¨nh vi·∫øt b√†i n√†y th√¨ m√¨nh ƒëang s·ª≠ d·ª•ng ansible 2.8\nansible 2.8 ƒëang c√≥ 1 bug l√† \u0026ldquo;yum lockfile is held by another user\u0026rdquo; nh∆∞ link sau: https://unix.stackexchange.com/questions/522246/yum-lockfile-is-held-by-another-user\nC·∫ßn fix workaround l√† v·ªõi nh·ªØng task n√†o c√≥ l·ªánh yum th√¨ ph·∫£i th√™m lock_timeout: 180 v√†o\n-\u0026gt; Th·∫ø n√™n c·∫£ apache v√† php task m√¨nh ƒë·ªÅu ƒë√£ th√™m d√≤ng n√†y.\n","href":"/posts/ansible-demo-ansible-pull-galaxy-playbook/","title":"Ansible Demo Ansible Pull Galaxy Playbook"},{"content":"Khi setup Ansible th√¨ c√≥ 2 c√°ch, 1 l√† d√πng \u0026ldquo;Static inventory\u0026rdquo;, 2 l√† \u0026ldquo;Dynamic inventory\u0026rdquo;\nTr√™n AWS th√¨ n√™n s·ª≠ d·ª•ng c√°ch 2, ƒë√¢y l√† best practice c·ªßa Ansible.\nB√†i n√†y s·∫Ω t·∫≠p trung n√≥i v·ªÅ c√°ch 2, ph·∫ßn cu·ªëi s·∫Ω n√≥i v·ªÅ c√°ch 1 (C√°ch 1 setup s·∫Ω d·ªÖ h∆°n nhi·ªÅu)\nY√™u c·∫ßu Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒë√£ c√≥ 1 t√†i kho·∫£n AWS r·ªìi, c√≥ th·ªÉ t·∫°o ƒë∆∞·ª£c EC2\nC√≥ base ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ AWS, bi·∫øt c√°ch SSH v√†o EC2\nC√°ch l√†m 1. T·∫°o b·ªô key cho Ansible V√†o AWS IAM t·∫°o user \u0026ldquo;ansible\u0026rdquo; (ho·∫∑c b·∫•t c·ª© t√™n g√¨) c·∫•p policy ph√π h·ª£p (v√≠ d·ª• ch·ªçn AdministratorAccess).\nV√†o tab Security cedentials t·∫°o Key cho User ƒë√≥.\nCopy b·ªô access key id v√† sceret access key ra ƒë·ªÉ sau n√†y d√πng. Launch 2 Amazon Linux EC2 c√πng m·ªü port 22, 1 EC2 l√† ansible-master, 1 EC2 l√† ansible-client.\n2 con EC2 n√†y c√πng s·ª≠ d·ª•ng 1 file key pem (gi·∫£ s·ª≠ ƒë·∫∑t t√™n l√† key.pem).\nSSH v√†o con ansible-master l√†m t·∫•t c·∫£ c√°c step d∆∞·ªõi:\n2. Install Ansible sudo pip install ansible sudo pip install boto 3. Setup AWS Dynamic Inventory 3.a. Export AWS KEY S·ª≠ d·ª•ng 2 c√°i key ƒë√£ chu·∫©n b·ªã ·ªü step 1:\nexport AWS_ACCESS_KEY_ID=\u0026#39;AKIA3RRRRRRRVPGEZ6\u0026#39; export AWS_SECRET_ACCESS_KEY=\u0026#39;ICUZP9+++++++++++++++++1USvtY/FSJt9\u0026#39; 3.b. T·∫°o folder v√† download 2 file c·∫ßn thi·∫øt ec2.py v√† ec2.ini v·ªÅ: sudo mkdir /etc/ansible cd /etc/ansible sudo wget https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.py sudo wget https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.ini sudo chmod +x ec2.py 3.c. Edit file ec2.ini cho ph√π h·ª£p: ·ªû ƒë√¢y c·∫ßn s·ª≠a ph·∫ßn regions v√† regions_exclude:\nsudo nano ec2.ini T√¨m ƒë·∫øn 2 d√≤ng sau v√† s·ª≠a regions = us-east-1 v√† comment out c√°i d√≤ng regions_exclude:\nregions = us-east-1 #regions_exclude = us-gov-west-1 Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n3.d. Setup ansible.cfg: cd /etc/ansible sudo nano ansible.cfg Th√™m d√≤ng sau v√†o:\n[defaults] inventory = /etc/ansible/ec2.py Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n3.e. Copy file key pem v√†o con ansible-master: cd ~/.ssh/ nano key.pem Paste v√†o ƒë√¢y n·ªôi dung trong file key pem m√† m√¨nh d√πng ƒë·ªÉ SSH v√†o ch√≠nh con EC2 n√†y\nCtr+X r·ªìi Yes-Enter ƒë·ªÉ save file\nChange permission cho file key.pem:\ncd ~/.ssh/ chmod 600 key.pem 3.f. Test connection gi·ªØa 2 con EC2: T·ª´ con master SSH ƒë·∫øn con client xem ƒë√£ d√πng ƒë√∫ng file key pem ch∆∞a:\nssh -i key.pem ec2-user@`private-ip-c·ªßa-ansible-client` N·∫øu connect th√†nh c√¥ng nghƒ©a l√† ƒë√£ d√πng ƒë√∫ng file key.pem.\n3.g. Test ansible connection b·∫±ng file key pem ƒë√≥: exit kh·ªèi terminal, SSH v√†o ansible-master:\nansible --private-key ~/.ssh/key.pem --user=ec2-user -m ping all N·∫øu ping th√†nh c√¥ng s·∫Ω hi·ªán nh∆∞ sau: C√≥ th·ªÉ x·∫£y ra kh·∫£ nƒÉng Unreachable v·ªõi ch√≠nh n√≥, nh∆∞ng ko sao l√†m ti·∫øp.\n3.h. ƒê·ªÉ ko ph·∫£i l·∫ßn n√†o c≈©ng ch·ªâ ƒë·ªãnh private-key trong command? Th·∫ø th√¨ s·ª≠a file ~/.ssh/config nh∆∞ sau:\nnano ~/.ssh/config S·ª≠a n·ªôi dung file config nh∆∞ sau:\nIdentityFile ~/.ssh/key.pem User ec2-user StrictHostKeyChecking no PasswordAuthentication no Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\nChange permission cho file ~/.ssh/config:\nchmod 600 ~/.ssh/config 3.i. Test ansible connection l·∫°i l·∫ßn n·ªØa ko c·∫ßn ch·ªâ ƒë·ªãnh private-key: ansible -m ping all N·∫øu ping th√†nh c√¥ng s·∫Ω hi·ªán nh∆∞ sau: 3.j. Test th·ª≠ ch·∫°y l·ªánh df -h v√† check version c·ªßa bash tr√™n 2 server master v√† client: ansible all -a \u0026#34;df -h\u0026#34; ansible all -a \u0026#34;bash --version\u0026#34; Done c√°ch 2!\n4. Setup AWS Static Inventory C√°ch n√†y c·∫ßn y√™u c·∫ßu nh∆∞ sau:\nLaunch 2 Amazon Linux EC2 c√πng m·ªü port 22, 1 EC2 l√† ansible-master, 1 EC2 l√† ansible-client.\n2 con EC2 n√†y c√πng s·ª≠ d·ª•ng 1 file key pem (gi·∫£ s·ª≠ ƒë·∫∑t t√™n l√† key.pem).\nSSH v√†o con ansible-master l√†m c√°c step d∆∞·ªõi:\n4.a. Install Ansible sudo pip install ansible sudo pip install boto 4.b. T·∫°o folder /etc/ansible v√† S·ª≠a file hosts sudo mkdir /etc/ansible cd /etc/ansible sudo nano hosts ƒêi·ªÅn pirvate IP c·ªßa con ansible-client v√† ansible-master v√†o:\n[client] 172.31.27.78 [master] 172.31.29.91 Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file.\n·ªû tr√™n v√¨ mu·ªën ansible connect ƒë∆∞·ª£c ƒë·∫øn ch√≠nh n√≥ n√™n m√¨nh th√™m ip c·ªßa master v√†o file hosts.\nC≈©ng c√≥ th·ªÉ d√πng public ip nh∆∞ng n√™n d√πng private ip ƒë·ªÉ n√≥ ko b·ªã thay ƒë·ªïi.\n4.c. Generate key ssh-keygen -t rsa cd ~/.ssh cat id_rsa.pub Copy n·ªôi dung file id_rsa.pub ƒë·ªÉ chu·∫©n b·ªã cho b∆∞·ªõc sau.\n4.d. SSH v√†o ansible-client ƒë·ªÉ add key v·ª´a ƒë∆∞·ª£c generate SSH v√†o con EC2 ansible-client, s·ª≠a file ~/.ssh/authorized_keys:\ncd ~/.ssh ll -lsa nano authorized_keys Paste n·ªôi dung file id_rsa.pub c·ªßa ansible-master m√† m√¨nh ƒë√£ copy ·ªü b∆∞·ªõc 4.c.\nCh√∫ √Ω l√† t·∫°o th√™m 1 d√≤ng n·ªØa ch·ª© ƒë·ª´ng x√≥a c√°i key c≈© trong file authorized_keys ƒëi.\nCtr+X r·ªìi Yes-Enter ƒë·ªÉ save file.\n4.e. SSH v√†o ansible-masterƒë·ªÉ test Ansible connection Gi·ªù ƒë√£ c√≥ th·ªÉ ping client t·ª´ master r·ªìi SSH v√†o ansible master:\nansible -m ping all Hi·ªán m√†u xanh l√® kh√¥ng c√≥ l·ªói UNREACHABLE l√† OK r·ªìi\nDone c√°ch 1!\n","href":"/bk/setup-ansible-dynamic-inventory-aws/","title":"Setup Ansible Dynamic Inventory Aws"},{"content":"Khi setup Ansible th√¨ c√≥ 2 c√°ch, 1 l√† d√πng \u0026ldquo;Static inventory\u0026rdquo;, 2 l√† \u0026ldquo;Dynamic inventory\u0026rdquo;\nTr√™n AWS th√¨ n√™n s·ª≠ d·ª•ng c√°ch 2, ƒë√¢y l√† best practice c·ªßa Ansible.\nB√†i n√†y s·∫Ω t·∫≠p trung n√≥i v·ªÅ c√°ch 2, ph·∫ßn cu·ªëi s·∫Ω n√≥i v·ªÅ c√°ch 1 (C√°ch 1 setup s·∫Ω d·ªÖ h∆°n nhi·ªÅu)\nY√™u c·∫ßu Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒë√£ c√≥ 1 t√†i kho·∫£n AWS r·ªìi, c√≥ th·ªÉ t·∫°o ƒë∆∞·ª£c EC2\nC√≥ base ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ AWS, bi·∫øt c√°ch SSH v√†o EC2\nC√°ch l√†m 1. T·∫°o b·ªô key cho Ansible V√†o AWS IAM t·∫°o user \u0026ldquo;ansible\u0026rdquo; (ho·∫∑c b·∫•t c·ª© t√™n g√¨) c·∫•p policy ph√π h·ª£p (v√≠ d·ª• ch·ªçn AdministratorAccess).\nV√†o tab Security cedentials t·∫°o Key cho User ƒë√≥.\nCopy b·ªô access key id v√† sceret access key ra ƒë·ªÉ sau n√†y d√πng. Launch 2 Amazon Linux EC2 c√πng m·ªü port 22, 1 EC2 l√† ansible-master, 1 EC2 l√† ansible-client.\n2 con EC2 n√†y c√πng s·ª≠ d·ª•ng 1 file key pem (gi·∫£ s·ª≠ ƒë·∫∑t t√™n l√† key.pem).\nSSH v√†o con ansible-master l√†m t·∫•t c·∫£ c√°c step d∆∞·ªõi:\n2. Install Ansible sudo pip install ansible sudo pip install boto 3. Setup AWS Dynamic Inventory 3.a. Export AWS KEY S·ª≠ d·ª•ng 2 c√°i key ƒë√£ chu·∫©n b·ªã ·ªü step 1:\nexport AWS_ACCESS_KEY_ID=\u0026#39;AKIA3RRRRRRRVPGEZ6\u0026#39; export AWS_SECRET_ACCESS_KEY=\u0026#39;ICUZP9+++++++++++++++++1USvtY/FSJt9\u0026#39; 3.b. T·∫°o folder v√† download 2 file c·∫ßn thi·∫øt ec2.py v√† ec2.ini v·ªÅ: sudo mkdir /etc/ansible cd /etc/ansible sudo wget https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.py sudo wget https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.ini sudo chmod +x ec2.py 3.c. Edit file ec2.ini cho ph√π h·ª£p: ·ªû ƒë√¢y c·∫ßn s·ª≠a ph·∫ßn regions v√† regions_exclude:\nsudo nano ec2.ini T√¨m ƒë·∫øn 2 d√≤ng sau v√† s·ª≠a regions = us-east-1 v√† comment out c√°i d√≤ng regions_exclude:\nregions = us-east-1 #regions_exclude = us-gov-west-1 Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n3.d. Setup ansible.cfg: cd /etc/ansible sudo nano ansible.cfg Th√™m d√≤ng sau v√†o:\n[defaults] inventory = /etc/ansible/ec2.py Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\n3.e. Copy file key pem v√†o con ansible-master: cd ~/.ssh/ nano key.pem Paste v√†o ƒë√¢y n·ªôi dung trong file key pem m√† m√¨nh d√πng ƒë·ªÉ SSH v√†o ch√≠nh con EC2 n√†y\nCtr+X r·ªìi Yes-Enter ƒë·ªÉ save file\nChange permission cho file key.pem:\ncd ~/.ssh/ chmod 600 key.pem 3.f. Test connection gi·ªØa 2 con EC2: T·ª´ con master SSH ƒë·∫øn con client xem ƒë√£ d√πng ƒë√∫ng file key pem ch∆∞a:\nssh -i key.pem ec2-user@`private-ip-c·ªßa-ansible-client` N·∫øu connect th√†nh c√¥ng nghƒ©a l√† ƒë√£ d√πng ƒë√∫ng file key.pem.\n3.g. Test ansible connection b·∫±ng file key pem ƒë√≥: exit kh·ªèi terminal, SSH v√†o ansible-master:\nansible --private-key ~/.ssh/key.pem --user=ec2-user -m ping all N·∫øu ping th√†nh c√¥ng s·∫Ω hi·ªán nh∆∞ sau: C√≥ th·ªÉ x·∫£y ra kh·∫£ nƒÉng Unreachable v·ªõi ch√≠nh n√≥, nh∆∞ng ko sao l√†m ti·∫øp.\n3.h. ƒê·ªÉ ko ph·∫£i l·∫ßn n√†o c≈©ng ch·ªâ ƒë·ªãnh private-key trong command? Th·∫ø th√¨ s·ª≠a file ~/.ssh/config nh∆∞ sau:\nnano ~/.ssh/config S·ª≠a n·ªôi dung file config nh∆∞ sau:\nIdentityFile ~/.ssh/key.pem User ec2-user StrictHostKeyChecking no PasswordAuthentication no Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file\nChange permission cho file ~/.ssh/config:\nchmod 600 ~/.ssh/config 3.i. Test ansible connection l·∫°i l·∫ßn n·ªØa ko c·∫ßn ch·ªâ ƒë·ªãnh private-key: ansible -m ping all N·∫øu ping th√†nh c√¥ng s·∫Ω hi·ªán nh∆∞ sau: 3.j. Test th·ª≠ ch·∫°y l·ªánh df -h v√† check version c·ªßa bash tr√™n 2 server master v√† client: ansible all -a \u0026#34;df -h\u0026#34; ansible all -a \u0026#34;bash --version\u0026#34; Done c√°ch 2!\n4. Setup AWS Static Inventory C√°ch n√†y c·∫ßn y√™u c·∫ßu nh∆∞ sau:\nLaunch 2 Amazon Linux EC2 c√πng m·ªü port 22, 1 EC2 l√† ansible-master, 1 EC2 l√† ansible-client.\n2 con EC2 n√†y c√πng s·ª≠ d·ª•ng 1 file key pem (gi·∫£ s·ª≠ ƒë·∫∑t t√™n l√† key.pem).\nSSH v√†o con ansible-master l√†m c√°c step d∆∞·ªõi:\n4.a. Install Ansible sudo pip install ansible sudo pip install boto 4.b. T·∫°o folder /etc/ansible v√† S·ª≠a file hosts sudo mkdir /etc/ansible cd /etc/ansible sudo nano hosts ƒêi·ªÅn pirvate IP c·ªßa con ansible-client v√† ansible-master v√†o:\n[client] 172.31.27.78 [master] 172.31.29.91 Ctr+X r·ªìi Yes-Enter ƒë·ªÉ save file.\n·ªû tr√™n v√¨ mu·ªën ansible connect ƒë∆∞·ª£c ƒë·∫øn ch√≠nh n√≥ n√™n m√¨nh th√™m ip c·ªßa master v√†o file hosts.\nC≈©ng c√≥ th·ªÉ d√πng public ip nh∆∞ng n√™n d√πng private ip ƒë·ªÉ n√≥ ko b·ªã thay ƒë·ªïi.\n4.c. Generate key ssh-keygen -t rsa cd ~/.ssh cat id_rsa.pub Copy n·ªôi dung file id_rsa.pub ƒë·ªÉ chu·∫©n b·ªã cho b∆∞·ªõc sau.\n4.d. SSH v√†o ansible-client ƒë·ªÉ add key v·ª´a ƒë∆∞·ª£c generate SSH v√†o con EC2 ansible-client, s·ª≠a file ~/.ssh/authorized_keys:\ncd ~/.ssh ll -lsa nano authorized_keys Paste n·ªôi dung file id_rsa.pub c·ªßa ansible-master m√† m√¨nh ƒë√£ copy ·ªü b∆∞·ªõc 4.c.\nCh√∫ √Ω l√† t·∫°o th√™m 1 d√≤ng n·ªØa ch·ª© ƒë·ª´ng x√≥a c√°i key c≈© trong file authorized_keys ƒëi.\nCtr+X r·ªìi Yes-Enter ƒë·ªÉ save file.\n4.e. SSH v√†o ansible-masterƒë·ªÉ test Ansible connection Gi·ªù ƒë√£ c√≥ th·ªÉ ping client t·ª´ master r·ªìi SSH v√†o ansible master:\nansible -m ping all Hi·ªán m√†u xanh l√® kh√¥ng c√≥ l·ªói UNREACHABLE l√† OK r·ªìi\nDone c√°ch 1!\n","href":"/posts/setup-ansible-dynamic-inventory-aws/","title":"Setup Ansible Dynamic Inventory Aws"},{"content":"Linh sang ch∆°i v√†o m√πa h√®, th·ªùi ti·∫øt kh√° n√≥ng, c√¢y c·ªëi c≈©ng ko c√≥ hoa ho√©t g√¨ ƒë·∫πp, v√† c≈©ng ko ph·∫£i d·ªãp l·ªÖ h·ªôi g√¨ c·∫£.\nHai ƒë·ª©a g·∫∑p nhau l√† vui l·∫Øm r·ªìi n√™n c≈©ng kh√¥ng ƒë·∫∑t tour ƒëi ch∆°i xa ƒë√¢u c·∫£, loanh quanh Tokyo th√¥i :))\nTham quan Th√°p Tokyo, nh∆∞ng m√† ch·ª•p t·ªëi ko ƒë·∫πp l·∫Øm, n√™n ch·ª•p v·ªõi nhau dc m·∫•y t·∫•m m·ªù m·ªù thui\nTham quan ga Tokyo, 1 ga c√≥ ki·∫øn tr√∫c ƒë·∫πp, phong c√°ch Ch√¢u √Çu ·∫£nh n√†y ƒë·∫πp n√®, c∆∞·ªùi t·ª± nhi√™n, m·ªói t·ªôi b·ªã c·∫Øt ch√¢n thui :v n·∫Øng qu√° \u0026ldquo;B·∫°n ∆°i ƒë∆∞a tay ƒë√¢y n√†o\u0026rdquo;\n\u0026ldquo;M√£i b√™n nhau b·∫°n nhooooo√≥\u0026rdquo;\nch√¢n d√†i qu√° n√™n c·∫Øt b·ªõt üòÇ quen th√≥i ·ªü nh√†, ng·∫Øt ·ªü ƒë√¢u dc c√°i l√° üòí Ch√πa Sensoji ·ªü Asakusa m·ªèi l∆∞ng qu√° b√≠ch h·ªça ·ªü ga n√†o ƒë√≥ quanh Asakusa ra Odaiba - ƒë·∫£o nh√¢n t·∫°o ch∆°i cosplay kh√° t·ªët üòÜ ch·ª•p v·ªõi con robot kh·ªïng l·ªì ph√¢n th√¢n!! c√°i n√†y tui ƒë·∫°o di·ªÖn üòÑ c√°i n√†y Linh ƒë·∫°o di·ªÖn :v tr·∫©u! B√∫n s∆∞·ªùn, ngon qu√°, l√¢u l·∫Øm ch∆∞a ƒë∆∞·ª£c ƒÉn b√∫n üòç tr·∫ª tr√¢u :))   Bowling   Tr√™n ƒë∆∞·ªùng ra s√¢n bay ƒëi v√¨a VN üò• ch·ªù t√†u Ng iu v·ªÅ m·∫•t r·ªìi üò≠ Bonus :v Ngo√†i ra c√≤n 1 s·ªë ƒëi·ªÉm ƒëi n·ªØa, nh∆∞ng ch∆∞a up\n","href":"/love/encrypt-linh-and-tokyo-2019/","title":"Linh and Tokyo 6.2019"},{"content":"","href":"/tags/circleci/","title":"CircleCI"},{"content":"Y√™u c·∫ßu Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒë√£ c√≥ 1 Github Blog r·ªìi, c√≥ th·ªÉ t·∫°o theo link sau:\nCreate a Gihub Blog by Hugo\nV√† b·∫°n ƒë√£ t·∫°o tr√™n github 2 repo l√†: username v√† username.github.io\nTrong ƒë√≥ th√¨ username l√† repo m√† m√¨nh s·∫Ω l√†m vi·ªác (vi·∫øt post m·ªõi, s·ª≠a ch·ª©c nƒÉng v..v)\nC√≤n username.github.io l√† repo s·∫Ω dc t·ª± ƒë·ªông generate ra code v√† ph·∫£n √°nh n·ªôi dung trang web tr√™n ƒë∆∞·ªùng link https://username.github.io\nGi·ªù c·∫ßn setup CircleCI cho repo username. M·ª•c ti√™u l√†:\n M·ªói l·∫ßn push code m·ªõi l√™n nh√°nh master c·ªßa repo username th√¨ repo username.github.io c≈©ng s·∫Ω dc c·∫≠p nh·∫≠t t·ª± ƒë·ªông, t·ª´ ƒë√≥ n·ªôi dung trang web https://username.github.io c≈©ng s·∫Ω thay ƒë·ªïi theo √Ω m√¨nh\n Tr√™n m√°y t√≠nh windows c·ªßa b·∫°n th√¨ ƒë√£ c√≥ 2 folder cho m·ªói repo nh∆∞ sau:\nusername v√† username.github.io ·ªü trong 1 folder cha l√† Sites\nC√°ch l√†m 1. Ch·ªçn docker image ph√π h·ª£p cho CircleCI   CircleCI c·∫ßn 1 docker image ƒë·ªÉ n√≥ pull v·ªÅ r·ªìi ch·∫°y c√°c command m√† m√¨nh define tr∆∞·ªõc.\n  N√™n ch·ªçn nh·ªØng image ƒë√£ c√†i ƒë·∫∑t s·∫µn git bash v√† hugo, ·ªü ƒë√¢y m√¨nh s·∫Ω ch·ªçn docker image l√† https://hub.docker.com/r/andthensome/alpine-hugo-git-bash\n  ƒê·ªÉ ƒë√≥ ƒë√£.\n  2. ƒêƒÉng nh·∫≠p v√†o https://circleci.com b·∫±ng account github c·ªßa m√¨nh   CircleCI s·∫Ω t·ª± ƒë·ªông li·ªát k√™ ra c√°c repo tr√™n github c·ªßa m√¨nh.\n  Ch·ªçn repo username.\n  Ch·ªçn language other.\n  N√≥ h∆∞·ªõng d·∫´n c√°c step ƒë·ªÉ setup CircleCI. (C√≥ th·ªÉ c√°c b·∫°n s·∫Ω c·∫ßn t·∫°o Deploy key v√† add v√†o Github repository)\n  Sau ƒë√≥ ·∫•n button Start Building.\n  3. Setup CircleCI cho project c·ªßa b·∫°n nh∆∞ sau:   T·∫°o folder .circleci ·ªü trong project username\n  Trong folder .circleci th√¨ t·∫°o file config.yml\n  N·ªôi dung file config.yml l√†:\n  version: 2 jobs: build: branches: only: - master docker: - image: andthensome/alpine-hugo-git-bash steps: - checkout - run: name: Upgrade hugo command: | apk update \u0026amp;\u0026amp; apk add ca-certificates \u0026amp;\u0026amp; update-ca-certificates \u0026amp;\u0026amp; apk add openssl \u0026amp;\u0026amp; apk add openssh-client wget https://github.com/gohugoio/hugo/releases/download/v0.55.6/hugo_0.55.6_Linux-64bit.tar.gz tar xzf hugo_0.55.6_Linux-64bit.tar.gz -C /usr/local/bin/ - run: name: Config git command: | git config --global user.email \u0026#34;username@gmail.com\u0026#34; git config --global user.name \u0026#34;username\u0026#34; - run: name: Publish site to github command: sh publish.sh 3.a. Gi·∫£i th√≠ch n·ªôi dung file tr√™n ·ªû ƒë√¢y m√¨nh ƒëang define:\n  Ch·ªâ ch·∫°y job n√†y khi c√≥ s·ª± thay ƒë·ªïi ·ªü nh√°nh master.\n  Ch·ªçn image l√† andthensome/alpine-hugo-git-bash.\n  C√°c step l·∫ßn l∆∞·ª£t l√†:\n  Checkout src code nh√°nh master c·ªßa https://github.com/username/username.git v·ªÅ.\n  Upgrade hugo l√™n version 0.55.\n  Config git.\n  Ch·∫°y file publish.sh m√† m√¨nh t·ª± t·∫°o.\n    4. Quay l·∫°i folder Sites/username t·∫°o file publish.sh #!/bin/bash  pwd # Get environment from circleci, token is a env variable in circleci, also a personal access token from github # this token has granted access to push commit to repo export GITHUB_TOKEN=$token # Clone from repository which serve the blog git clone https://github.com/username/username.github.io.git # Generate the source code to new folder hugo -d username.github.io # Publish the blog by push all generated resource to reposiotry cd username.github.io git remote set-url origin https://$GITHUB_TOKEN:x-oauth-basic@github.com/username/username.github.io.git git add . git commit -m \u0026#34;push to username/username.github.io.git by [publish.sh]\u0026#34; git push origin master 5. Setup Personal access token (PAT) gi·ªØa CircleCI v√† Github ·ªû file publish.sh tr√™n ƒë·ªÉ √Ω th·∫•y bi·∫øn $token,\nƒê√¢y l√† bi·∫øn m√¨nh setup ƒë·ªÉ CircleCI s·ª≠ d·ª•ng push source code ƒë√£ generate l√™n nh√°nh master c·ªßa repo username.github.io\nC√°ch setup nh∆∞ sau:\n5.a. Tr√™n Github   v√†o link https://github.com/settings/tokens\n  ·∫§n button Generate new token\n  ·ªü ƒë√¢y m√¨nh ƒëang ch·ªçn Classic Token\n  ƒê·∫∑t t√™n cho n√≥ l√† circlecikey ch·∫≥ng h·∫°n\n  Ch·ªçn quy·ªÅn cho key ƒë√≥, m√¨nh ch·ªçn quy·ªÅn admin c√≥ quy·ªÅn write\n  Copy c√°i key ƒë√≥ ra, ch√∫ √Ω copy gi·ªØ c·∫©n th·∫≠n ƒë·ª´ng ƒë·ªÉ m·∫•t, v√¨ n√≥ ch·ªâ hi·ªÉn th·ªã 1 l·∫ßn th√¥i\n  S·ª± kh√°c nhau gi·ªØa deploy key v√† PAT th·ªÉ hi·ªán ·ªü ƒë√¢y:\nDeploy key l√† CircleCI d√πng ƒë·ªÉ pull code t·ª´ private repo v·ªÅ,\nc√≤n PAT l√† github d√πng ƒë·ªÉ push code t·ª´ trong pipeline l√™n public repo.\nC√≥ th·ªÉ b·∫°n s·∫Ω h·ªèi t·∫°i sao ko d√πng h·∫øt 1 lo·∫°i key/token th√¥i, c√°i n√†y m√¨nh c≈©ng ch∆∞a r√µ l·∫Øm.\nC√°i deploy key th√¨ ch·∫Øc ch·∫Øn s·∫Ω c·∫ßn v√¨ CircleCI ngay t·ª´ b∆∞·ªõc setup ƒë√£ y√™u c·∫ßu r·ªìi. Nh∆∞ng ƒë·ªÉ t·∫°o deploy key th√¨ b·∫°n ph·∫£i generate 1 c·∫∑p public/private key tr∆∞·ªõc, Sau ƒë√≥ ƒë∆∞a private key cho CircleCI gi·ªØ.\nC√≤n PAT th√¨ ch·ªâ c·∫ßn 1 token th√¥i c≈©ng c√≥ th·ªÉ v·ª´a pull/push code ƒë∆∞·ª£c, ti·ªán l·ª£i h∆°n deploy key.\nNh∆∞ng PAT Classic l·∫°i ko th·ªÉ gi·ªõi h·∫°n 1 repo nh∆∞ deploy key ƒë∆∞·ª£c. (PAT Fine-grained th√¨ c√≥ th·ªÉ)\n5.b. Tr√™n CircleCI   V√†o trang https://circleci.com, ƒëƒÉng nh·∫≠p b·∫±ng github account c·ªßa m√¨nh, v√†o job username\n  Trong m·ª•c setting c·ªßa m·ªói job s·∫Ω c√≥ ph·∫ßn setting environment variables,\n  Add variable m·ªõi t√™n l√† token\n  Value c·ªßa variable th√¨ paste c√°i key ·ªü b∆∞·ªõc 5.a l√∫c n√£y ra\n  R·ªìi test th·ª≠ commit l√™n private repo xem th·∫ø n√†o?\n6. Troubleshoot Trong qu√° tr√¨nh tr√™n l√†m t·ª´ng b·ªã l·ªói nh∆∞ sau: ƒê√¢y l√† do github ko push nh·ªØng folder empty l√™n, d·∫´n ƒë·∫øn vi·ªác kh√¥ng c√≥ layout ƒë·ªÉ hugo trong images generate ra ƒë·ªß s·ªë pages.\nV√¨ v·∫≠y trong nh·ªØng folder empty nh∆∞ layout, resources/_gen/assets v√† resources/_gen/images th√¨ n√™n t·∫°o 1 file .gitkeep ƒë·ªÉ n√≥ push folder ƒë√≥ l√™n github\nDone! gi·ªù th√¨ m·ªói khi push code l√™n nh√°nh master c·ªßa repo username\nCircleCI s·∫Ω nh·∫≠n nh·∫≠n ra c√≥ s·ª± thay ƒë·ªïi v√† ch·∫°y file .circleci/config.yml r·ªìi file publish.sh ƒë·ªÉ generate source cu·ªëi c√πng push l√™n repo username.github.io\nT·ª´ ƒë√≥ trang web https://username.github.io c≈©ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t\n","href":"/bk/using-circleci-in-github-blog/","title":"√Åp D·ª•ng CircleCI V√†o Github Blog"},{"content":"Y√™u c·∫ßu Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒë√£ c√≥ 1 Github Blog r·ªìi, c√≥ th·ªÉ t·∫°o theo link sau:\nCreate a Gihub Blog by Hugo\nV√† b·∫°n ƒë√£ t·∫°o tr√™n github 2 repo l√†: username v√† username.github.io\nTrong ƒë√≥ th√¨ username l√† repo m√† m√¨nh s·∫Ω l√†m vi·ªác (vi·∫øt post m·ªõi, s·ª≠a ch·ª©c nƒÉng v..v)\nC√≤n username.github.io l√† repo s·∫Ω dc t·ª± ƒë·ªông generate ra code v√† ph·∫£n √°nh n·ªôi dung trang web tr√™n ƒë∆∞·ªùng link https://username.github.io\nGi·ªù c·∫ßn setup CircleCI cho repo username. M·ª•c ti√™u l√†:\n M·ªói l·∫ßn push code m·ªõi l√™n nh√°nh master c·ªßa repo username th√¨ repo username.github.io c≈©ng s·∫Ω dc c·∫≠p nh·∫≠t t·ª± ƒë·ªông, t·ª´ ƒë√≥ n·ªôi dung trang web https://username.github.io c≈©ng s·∫Ω thay ƒë·ªïi theo √Ω m√¨nh\n Tr√™n m√°y t√≠nh windows c·ªßa b·∫°n th√¨ ƒë√£ c√≥ 2 folder cho m·ªói repo nh∆∞ sau:\nusername v√† username.github.io ·ªü trong 1 folder cha l√† Sites\nC√°ch l√†m 1. Ch·ªçn docker image ph√π h·ª£p cho CircleCI   CircleCI c·∫ßn 1 docker image ƒë·ªÉ n√≥ pull v·ªÅ r·ªìi ch·∫°y c√°c command m√† m√¨nh define tr∆∞·ªõc.\n  N√™n ch·ªçn nh·ªØng image ƒë√£ c√†i ƒë·∫∑t s·∫µn git bash v√† hugo, ·ªü ƒë√¢y m√¨nh s·∫Ω ch·ªçn docker image l√† https://hub.docker.com/r/andthensome/alpine-hugo-git-bash\n  ƒê·ªÉ ƒë√≥ ƒë√£.\n  2. ƒêƒÉng nh·∫≠p v√†o https://circleci.com b·∫±ng account github c·ªßa m√¨nh   CircleCI s·∫Ω t·ª± ƒë·ªông li·ªát k√™ ra c√°c repo tr√™n github c·ªßa m√¨nh.\n  Ch·ªçn repo username.\n  Ch·ªçn language other.\n  N√≥ h∆∞·ªõng d·∫´n c√°c step ƒë·ªÉ setup CircleCI. (C√≥ th·ªÉ c√°c b·∫°n s·∫Ω c·∫ßn t·∫°o Deploy key v√† add v√†o Github repository)\n  Sau ƒë√≥ ·∫•n button Start Building.\n  3. Setup CircleCI cho project c·ªßa b·∫°n nh∆∞ sau:   T·∫°o folder .circleci ·ªü trong project username\n  Trong folder .circleci th√¨ t·∫°o file config.yml\n  N·ªôi dung file config.yml l√†:\n  version: 2 jobs: build: branches: only: - master docker: - image: andthensome/alpine-hugo-git-bash steps: - checkout - run: name: Upgrade hugo command: | apk update \u0026amp;\u0026amp; apk add ca-certificates \u0026amp;\u0026amp; update-ca-certificates \u0026amp;\u0026amp; apk add openssl \u0026amp;\u0026amp; apk add openssh-client wget https://github.com/gohugoio/hugo/releases/download/v0.55.6/hugo_0.55.6_Linux-64bit.tar.gz tar xzf hugo_0.55.6_Linux-64bit.tar.gz -C /usr/local/bin/ - run: name: Config git command: | git config --global user.email \u0026#34;username@gmail.com\u0026#34; git config --global user.name \u0026#34;username\u0026#34; - run: name: Publish site to github command: sh publish.sh 3.a. Gi·∫£i th√≠ch n·ªôi dung file tr√™n ·ªû ƒë√¢y m√¨nh ƒëang define:\n  Ch·ªâ ch·∫°y job n√†y khi c√≥ s·ª± thay ƒë·ªïi ·ªü nh√°nh master.\n  Ch·ªçn image l√† andthensome/alpine-hugo-git-bash.\n  C√°c step l·∫ßn l∆∞·ª£t l√†:\n  Checkout src code nh√°nh master c·ªßa https://github.com/username/username.git v·ªÅ.\n  Upgrade hugo l√™n version 0.55.\n  Config git.\n  Ch·∫°y file publish.sh m√† m√¨nh t·ª± t·∫°o.\n    4. Quay l·∫°i folder Sites/username t·∫°o file publish.sh #!/bin/bash  pwd # Get environment from circleci, token is a env variable in circleci, also a personal access token from github # this token has granted access to push commit to repo export GITHUB_TOKEN=$token # Clone from repository which serve the blog git clone https://github.com/username/username.github.io.git # Generate the source code to new folder hugo -d username.github.io # Publish the blog by push all generated resource to reposiotry cd username.github.io git remote set-url origin https://$GITHUB_TOKEN:x-oauth-basic@github.com/username/username.github.io.git git add . git commit -m \u0026#34;push to username/username.github.io.git by [publish.sh]\u0026#34; git push origin master 5. Setup Personal access token (PAT) gi·ªØa CircleCI v√† Github ·ªû file publish.sh tr√™n ƒë·ªÉ √Ω th·∫•y bi·∫øn $token,\nƒê√¢y l√† bi·∫øn m√¨nh setup ƒë·ªÉ CircleCI s·ª≠ d·ª•ng push source code ƒë√£ generate l√™n nh√°nh master c·ªßa repo username.github.io\nC√°ch setup nh∆∞ sau:\n5.a. Tr√™n Github   v√†o link https://github.com/settings/tokens\n  ·∫§n button Generate new token\n  ·ªü ƒë√¢y m√¨nh ƒëang ch·ªçn Classic Token\n  ƒê·∫∑t t√™n cho n√≥ l√† circlecikey ch·∫≥ng h·∫°n\n  Ch·ªçn quy·ªÅn cho key ƒë√≥, m√¨nh ch·ªçn quy·ªÅn admin c√≥ quy·ªÅn write\n  Copy c√°i key ƒë√≥ ra, ch√∫ √Ω copy gi·ªØ c·∫©n th·∫≠n ƒë·ª´ng ƒë·ªÉ m·∫•t, v√¨ n√≥ ch·ªâ hi·ªÉn th·ªã 1 l·∫ßn th√¥i\n  S·ª± kh√°c nhau gi·ªØa deploy key v√† PAT th·ªÉ hi·ªán ·ªü ƒë√¢y:\nDeploy key l√† CircleCI d√πng ƒë·ªÉ pull code t·ª´ private repo v·ªÅ,\nc√≤n PAT l√† github d√πng ƒë·ªÉ push code t·ª´ trong pipeline l√™n public repo.\nC√≥ th·ªÉ b·∫°n s·∫Ω h·ªèi t·∫°i sao ko d√πng h·∫øt 1 lo·∫°i key/token th√¥i, c√°i n√†y m√¨nh c≈©ng ch∆∞a r√µ l·∫Øm.\nC√°i deploy key th√¨ ch·∫Øc ch·∫Øn s·∫Ω c·∫ßn v√¨ CircleCI ngay t·ª´ b∆∞·ªõc setup ƒë√£ y√™u c·∫ßu r·ªìi. Nh∆∞ng ƒë·ªÉ t·∫°o deploy key th√¨ b·∫°n ph·∫£i generate 1 c·∫∑p public/private key tr∆∞·ªõc, Sau ƒë√≥ ƒë∆∞a private key cho CircleCI gi·ªØ.\nC√≤n PAT th√¨ ch·ªâ c·∫ßn 1 token th√¥i c≈©ng c√≥ th·ªÉ v·ª´a pull/push code ƒë∆∞·ª£c, ti·ªán l·ª£i h∆°n deploy key.\nNh∆∞ng PAT Classic l·∫°i ko th·ªÉ gi·ªõi h·∫°n 1 repo nh∆∞ deploy key ƒë∆∞·ª£c. (PAT Fine-grained th√¨ c√≥ th·ªÉ)\n5.b. Tr√™n CircleCI   V√†o trang https://circleci.com, ƒëƒÉng nh·∫≠p b·∫±ng github account c·ªßa m√¨nh, v√†o job username\n  Trong m·ª•c setting c·ªßa m·ªói job s·∫Ω c√≥ ph·∫ßn setting environment variables,\n  Add variable m·ªõi t√™n l√† token\n  Value c·ªßa variable th√¨ paste c√°i key ·ªü b∆∞·ªõc 5.a l√∫c n√£y ra\n  R·ªìi test th·ª≠ commit l√™n private repo xem th·∫ø n√†o?\n6. Troubleshoot Trong qu√° tr√¨nh tr√™n l√†m t·ª´ng b·ªã l·ªói nh∆∞ sau: ƒê√¢y l√† do github ko push nh·ªØng folder empty l√™n, d·∫´n ƒë·∫øn vi·ªác kh√¥ng c√≥ layout ƒë·ªÉ hugo trong images generate ra ƒë·ªß s·ªë pages.\nV√¨ v·∫≠y trong nh·ªØng folder empty nh∆∞ layout, resources/_gen/assets v√† resources/_gen/images th√¨ n√™n t·∫°o 1 file .gitkeep ƒë·ªÉ n√≥ push folder ƒë√≥ l√™n github\nDone! gi·ªù th√¨ m·ªói khi push code l√™n nh√°nh master c·ªßa repo username\nCircleCI s·∫Ω nh·∫≠n nh·∫≠n ra c√≥ s·ª± thay ƒë·ªïi v√† ch·∫°y file .circleci/config.yml r·ªìi file publish.sh ƒë·ªÉ generate source cu·ªëi c√πng push l√™n repo username.github.io\nT·ª´ ƒë√≥ trang web https://username.github.io c≈©ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t\n","href":"/posts/using-circleci-in-github-blog/","title":"√Åp D·ª•ng CircleCI V√†o Github Blog"},{"content":"","href":"/tags/github/","title":"Github"},{"content":"","href":"/tags/hugo/","title":"Hugo"},{"content":"Blog s·∫Ω c√≥ d·∫°ng https://username.github.io/. Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒëang d√πng Windows, ƒë√£ c√†i ƒë·∫∑t Git Bash\n1. Install Hugo b·∫±ng file exe cho Windows V·ªÅ version c·ªßa Hugo th√¨ n√™n ch·ªçn version 0.55.6\nInstall tr√™n Windows n√™n l√†m nh∆∞ link sau: https://discourse.gohugo.io/t/howto-install-hugo-on-windows/741\nv√†o link sau: https://github.com/gohugoio/hugo/releases/tag/v0.55.6\ndownload file hugo_0.55.6_Windows-64bit.zip v·ªÅ th∆∞ m·ª•c t√πy √Ω, ch·∫≥ng h·∫°n \u0026ldquo;E:\\Downloads\u0026quot;\ngi·∫£i n√©n ra s·∫Ω c√≥ file hugo.exe\ngi·∫£ s·ª≠ file hugo.exe trong ƒë∆∞·ªùng d·∫´n sau: E:\\Downloads\\hugo_0.55.6_Windows-64bit\nV√†o Environment Variables c·ªßa Windows ƒë·ªÉ add ƒë∆∞·ªùng d·∫´n ƒë√≥ v√†o bi·∫øn Path nh∆∞ h√¨nh sau:\nxong check version ƒë√∫ng 0.55.6 l√† OK:\n$ hugo version Hugo Static Site Generator v0.55.6-A5D4C82D windows/amd64 BuildDate: 2019-05-18T07:57:00Z ƒê·ªÉ install Hugo th√¨ c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng Chocolatey ho·∫∑c Scoop, ch·ªçn c√°i n√†o th√¨ t√πy m·ªçi ng∆∞·ªùi.\nTuy nhi√™n 2 c√°ch n√†y m√¨nh ch∆∞a bi·∫øt ch·ªçn specific version nh∆∞ n√†o, n√≥ to√†n install latest version (d·ªÖ b·ªã l·ªói)\n2. Install Hugo b·∫±ng Choco ho·∫∑c Scoop 2.a. C√°ch 1, Chocolatey B·∫≠t powershell c·ªßa windows, nh·ªõ ch·ªçn Run as Administrator, paste command sau v√†o:\nSet-ExecutionPolicy Bypass -Scope Process -Force; ` iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) 2.b. C√°ch 2, Scoop B·∫≠t powershell c·ªßa windows, nh·ªõ ch·ªçn Run as Administrator, paste command sau v√†o:\nSet-ExecutionPolicy RemoteSigned -scope CurrentUser t·∫Øt powershell r·ªìi b·∫≠t l·∫°i, nh·ªõ ch·ªçn Run as Administrator, paste command sau v√†o:\niex (new-object net.webclient).downloadstring(\u0026#39;https://get.scoop.sh\u0026#39;) 2.c. Install Hugo Run Git Bash as Administrator, t·ª´ ƒë√¢y tr·ªü xu·ªëng c√°c command s·∫Ω s·ª≠ d·ª•ng Git Bash:\nN·∫øu d√πng Chocolatey:\nchoco install hugo -confirm N·∫øu d√πng Scoop:\nscoop install hugo Ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ c√†i ƒë·∫∑t hugo th√†nh c√¥ng:\nhugo version N·∫øu m√†n h√¨nh terminal hi·ªÉn th·ªã version c√≥ d·∫°ng t∆∞∆°ng t·ª± nh∆∞ sau l√† ƒë√£ install hugo th√†nh c√¥ng:\n$ hugo version Hugo Static Site Generator v0.55.6-A5D4C82D windows/amd64 BuildDate: 2019-05-18T07:57:00Z 3. T·∫°o v√† run blog c·ªßa b·∫°n tr√™n local 3.a. T·∫°o folder ch·ª©a site, ch√∫ng ta s·∫Ω ch·ªâ l√†m vi·ªác trong folder n√†y:\nmkdir Sites 3.b. T·∫°o site:\ncd Sites hugo new site username 3.c. Download themes, ·ªü ƒë√¢y m√¨nh ch·ªçn theme kiera.\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username/themes, clone theme v·ªÅ b·∫±ng command sau:\ncd Sites/username/themes git clone https://github.com/avianto/hugo-kiera kiera 3.d. Gi·ªù ch√∫ng ta s·∫Ω Copy to√†n b·ªô file trong exampleSite paste ƒë√® l√™n th∆∞ m·ª•c username,\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username,\ncopy b·∫±ng command sau:\ncd Sites/username cp themes/kiera/exampleSite/* . N·∫øu kh√¥ng l√†m dc b·∫±ng command th√¨ l√†m b·∫±ng tay tr√™n giao di·ªán c≈©ng ok.\n3.e. Gi·ªù s·∫Ω Edit file config.toml trong th∆∞ m·ª•c Sites/username.\nM·ªü file n√†y b·∫±ng Notepad ho·∫∑c b·∫•t k·ª≥ editor n√†o,\ns·ª≠a d√≤ng ƒë·∫ßu ti√™n nh∆∞ sau:\nbaseurl = \u0026quot;https://username.github.io/\u0026quot; 3.f. T·∫°o post ƒë·∫ßu ti√™n v√† t·∫°o file about.\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username,\nd√πng command sau:\ncd Sites/username mkdir content/posts hugo new posts/first-post.md hugo new about.md 3.g. File first-post.md s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông sinh ra trong Site/username/content/posts.\nEdit file first-post.md b·∫±ng Notepad:\n+++ title = \u0026quot;First Post\u0026quot; date = 2018-03-03T13:23:10+01:00 draft = false tags = [\u0026quot;Getting started\u0026quot;] categories = [] +++ Hello Hugo world! No more excuses for having no blog or documentation now! 3.h. File about.md s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông sinh ra trong Site/username/content/.\nEdit file about.md b·∫±ng Notepad:\n+++ title = \u0026quot;About\u0026quot; date = 2018-03-03T13:50:49+01:00 menu = \u0026quot;main\u0026quot; #Display this page on the nav menu weight = \u0026quot;30\u0026quot; #Right-most nav item meta = \u0026quot;false\u0026quot; #Do not display tags or categories +++ \u0026gt; Waves are the practice of the water. Shunryu Suzuki 3.i. Run tr√™n local.\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username,\nrun command sau:\ncd Sites/username hugo server m·ªü browser b·∫•t k·ª≥, check site c·ªßa b·∫°n tr√™n http://localhost:1313/\nGi·ªù c√≥ th·ªÉ t√πy √Ω ch·ªânh s·ª≠a n·ªôi dung trong post, ho·∫∑c t·∫°o post m·ªõi ..v.v\n4. Hosting tr√™n Github Login v√†o Github,\nt·∫°o 1 repo th·ª© nh·∫•t t√™n username,\nt·∫°o 1 repo th·ª© hai t√™n username.github.io\nadd repo th·ª© nh·∫•t v√†o remote origin c·ªßa folder username:\ncd Sites/username/ git init git remote add origin https://github.com/username/username.git clone repo th·ª© hai v·ªÅ:\ncd Sites git clone https://github.com/username/username.github.io.git d√πng hugo ƒë·ªÉ generate ra html v√†o th∆∞ m·ª•c username.github.io v·ª´a clone v·ªÅ\ncd Sites/username/ hugo -d ../username.github.io push l√™n Github\ncd Sites/username.github.io/ git add . git commit -m \u0026#34;your message\u0026#34; git push origin master push c·∫£ folder username c≈©ng l√™n github\ncd Sites/username/ git add . git commit -m \u0026#34;your message\u0026#34; git push origin master check th√†nh qu·∫£ c·ªßa b·∫°n tr√™n ƒë∆∞·ªùng link sau: https://username.github.io/\n5. Sau m·ªói l·∫ßn update  Sau n√†y, m·ªói khi t·∫°o 1 post m·ªõi, check xong xu√¥i tr√™n local r·ªìi th√¨ b·∫°n h√£y c·∫≠p nh·∫≠t l√™n github. ƒê·ªÉ c·∫≠p nh·∫≠t l√™n github th√¨ l√†m nh∆∞ sau: generate html b·∫±ng hugo v√†o folder username.github.io:  cd Sites/username/ hugo -d ../username.github.io push n·ªôi dung trong Sites/username.github.io/ l√™n Github repo username.github.io\ncd Sites/username.github.io/ git add . git commit -m \u0026#34;your message\u0026#34; git push origin master check nh·ªØng thay ƒë·ªïi ƒë√£ ƒë∆∞·ª£c apply ch∆∞a sau kho·∫£ng 1 ph√∫t. C√≥ 1 c√°ch n·ªØa l√† t·∫°o submodule nh∆∞ng m√¨nh ko ƒë·ªÅ c·∫≠p ·ªü ƒë√¢y.\nDone!\n","href":"/bk/create-a-github-blog-by-hugo/","title":"T·∫°o 1 Blog tr√™n Github b·∫±ng Hugo"},{"content":"Blog s·∫Ω c√≥ d·∫°ng https://username.github.io/. Gi·∫£ ƒë·ªãnh l√† b·∫°n ƒëang d√πng Windows, ƒë√£ c√†i ƒë·∫∑t Git Bash\n1. Install Hugo b·∫±ng file exe cho Windows V·ªÅ version c·ªßa Hugo th√¨ n√™n ch·ªçn version 0.55.6\nInstall tr√™n Windows n√™n l√†m nh∆∞ link sau: https://discourse.gohugo.io/t/howto-install-hugo-on-windows/741\nv√†o link sau: https://github.com/gohugoio/hugo/releases/tag/v0.55.6\ndownload file hugo_0.55.6_Windows-64bit.zip v·ªÅ th∆∞ m·ª•c t√πy √Ω, ch·∫≥ng h·∫°n \u0026ldquo;E:\\Downloads\u0026quot;\ngi·∫£i n√©n ra s·∫Ω c√≥ file hugo.exe\ngi·∫£ s·ª≠ file hugo.exe trong ƒë∆∞·ªùng d·∫´n sau: E:\\Downloads\\hugo_0.55.6_Windows-64bit\nV√†o Environment Variables c·ªßa Windows ƒë·ªÉ add ƒë∆∞·ªùng d·∫´n ƒë√≥ v√†o bi·∫øn Path nh∆∞ h√¨nh sau:\nxong check version ƒë√∫ng 0.55.6 l√† OK:\n$ hugo version Hugo Static Site Generator v0.55.6-A5D4C82D windows/amd64 BuildDate: 2019-05-18T07:57:00Z ƒê·ªÉ install Hugo th√¨ c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng Chocolatey ho·∫∑c Scoop, ch·ªçn c√°i n√†o th√¨ t√πy m·ªçi ng∆∞·ªùi.\nTuy nhi√™n 2 c√°ch n√†y m√¨nh ch∆∞a bi·∫øt ch·ªçn specific version nh∆∞ n√†o, n√≥ to√†n install latest version (d·ªÖ b·ªã l·ªói)\n2. Install Hugo b·∫±ng Choco ho·∫∑c Scoop 2.a. C√°ch 1, Chocolatey B·∫≠t powershell c·ªßa windows, nh·ªõ ch·ªçn Run as Administrator, paste command sau v√†o:\nSet-ExecutionPolicy Bypass -Scope Process -Force; ` iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) 2.b. C√°ch 2, Scoop B·∫≠t powershell c·ªßa windows, nh·ªõ ch·ªçn Run as Administrator, paste command sau v√†o:\nSet-ExecutionPolicy RemoteSigned -scope CurrentUser t·∫Øt powershell r·ªìi b·∫≠t l·∫°i, nh·ªõ ch·ªçn Run as Administrator, paste command sau v√†o:\niex (new-object net.webclient).downloadstring(\u0026#39;https://get.scoop.sh\u0026#39;) 2.c. Install Hugo Run Git Bash as Administrator, t·ª´ ƒë√¢y tr·ªü xu·ªëng c√°c command s·∫Ω s·ª≠ d·ª•ng Git Bash:\nN·∫øu d√πng Chocolatey:\nchoco install hugo -confirm N·∫øu d√πng Scoop:\nscoop install hugo Ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ c√†i ƒë·∫∑t hugo th√†nh c√¥ng:\nhugo version N·∫øu m√†n h√¨nh terminal hi·ªÉn th·ªã version c√≥ d·∫°ng t∆∞∆°ng t·ª± nh∆∞ sau l√† ƒë√£ install hugo th√†nh c√¥ng:\n$ hugo version Hugo Static Site Generator v0.55.6-A5D4C82D windows/amd64 BuildDate: 2019-05-18T07:57:00Z 3. T·∫°o v√† run blog c·ªßa b·∫°n tr√™n local 3.a. T·∫°o folder ch·ª©a site, ch√∫ng ta s·∫Ω ch·ªâ l√†m vi·ªác trong folder n√†y:\nmkdir Sites 3.b. T·∫°o site:\ncd Sites hugo new site username 3.c. Download themes, ·ªü ƒë√¢y m√¨nh ch·ªçn theme kiera.\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username/themes, clone theme v·ªÅ b·∫±ng command sau:\ncd Sites/username/themes git clone https://github.com/avianto/hugo-kiera kiera 3.d. Gi·ªù ch√∫ng ta s·∫Ω Copy to√†n b·ªô file trong exampleSite paste ƒë√® l√™n th∆∞ m·ª•c username,\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username,\ncopy b·∫±ng command sau:\ncd Sites/username cp themes/kiera/exampleSite/* . N·∫øu kh√¥ng l√†m dc b·∫±ng command th√¨ l√†m b·∫±ng tay tr√™n giao di·ªán c≈©ng ok.\n3.e. Gi·ªù s·∫Ω Edit file config.toml trong th∆∞ m·ª•c Sites/username.\nM·ªü file n√†y b·∫±ng Notepad ho·∫∑c b·∫•t k·ª≥ editor n√†o,\ns·ª≠a d√≤ng ƒë·∫ßu ti√™n nh∆∞ sau:\nbaseurl = \u0026quot;https://username.github.io/\u0026quot; 3.f. T·∫°o post ƒë·∫ßu ti√™n v√† t·∫°o file about.\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username,\nd√πng command sau:\ncd Sites/username mkdir content/posts hugo new posts/first-post.md hugo new about.md 3.g. File first-post.md s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông sinh ra trong Site/username/content/posts.\nEdit file first-post.md b·∫±ng Notepad:\n+++ title = \u0026quot;First Post\u0026quot; date = 2018-03-03T13:23:10+01:00 draft = false tags = [\u0026quot;Getting started\u0026quot;] categories = [] +++ Hello Hugo world! No more excuses for having no blog or documentation now! 3.h. File about.md s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông sinh ra trong Site/username/content/.\nEdit file about.md b·∫±ng Notepad:\n+++ title = \u0026quot;About\u0026quot; date = 2018-03-03T13:50:49+01:00 menu = \u0026quot;main\u0026quot; #Display this page on the nav menu weight = \u0026quot;30\u0026quot; #Right-most nav item meta = \u0026quot;false\u0026quot; #Do not display tags or categories +++ \u0026gt; Waves are the practice of the water. Shunryu Suzuki 3.i. Run tr√™n local.\nH√£y ch·∫Øc ch·∫Øn l√† b·∫°n ƒëang cd ƒë·∫øn directory n√†y Sites/username,\nrun command sau:\ncd Sites/username hugo server m·ªü browser b·∫•t k·ª≥, check site c·ªßa b·∫°n tr√™n http://localhost:1313/\nGi·ªù c√≥ th·ªÉ t√πy √Ω ch·ªânh s·ª≠a n·ªôi dung trong post, ho·∫∑c t·∫°o post m·ªõi ..v.v\n4. Hosting tr√™n Github Login v√†o Github,\nt·∫°o 1 repo th·ª© nh·∫•t t√™n username,\nt·∫°o 1 repo th·ª© hai t√™n username.github.io\nadd repo th·ª© nh·∫•t v√†o remote origin c·ªßa folder username:\ncd Sites/username/ git init git remote add origin https://github.com/username/username.git clone repo th·ª© hai v·ªÅ:\ncd Sites git clone https://github.com/username/username.github.io.git d√πng hugo ƒë·ªÉ generate ra html v√†o th∆∞ m·ª•c username.github.io v·ª´a clone v·ªÅ\ncd Sites/username/ hugo -d ../username.github.io push l√™n Github\ncd Sites/username.github.io/ git add . git commit -m \u0026#34;your message\u0026#34; git push origin master push c·∫£ folder username c≈©ng l√™n github\ncd Sites/username/ git add . git commit -m \u0026#34;your message\u0026#34; git push origin master check th√†nh qu·∫£ c·ªßa b·∫°n tr√™n ƒë∆∞·ªùng link sau: https://username.github.io/\n5. Sau m·ªói l·∫ßn update  Sau n√†y, m·ªói khi t·∫°o 1 post m·ªõi, check xong xu√¥i tr√™n local r·ªìi th√¨ b·∫°n h√£y c·∫≠p nh·∫≠t l√™n github. ƒê·ªÉ c·∫≠p nh·∫≠t l√™n github th√¨ l√†m nh∆∞ sau: generate html b·∫±ng hugo v√†o folder username.github.io:  cd Sites/username/ hugo -d ../username.github.io push n·ªôi dung trong Sites/username.github.io/ l√™n Github repo username.github.io\ncd Sites/username.github.io/ git add . git commit -m \u0026#34;your message\u0026#34; git push origin master check nh·ªØng thay ƒë·ªïi ƒë√£ ƒë∆∞·ª£c apply ch∆∞a sau kho·∫£ng 1 ph√∫t. C√≥ 1 c√°ch n·ªØa l√† t·∫°o submodule nh∆∞ng m√¨nh ko ƒë·ªÅ c·∫≠p ·ªü ƒë√¢y.\nDone!\n","href":"/posts/create-a-github-blog-by-hugo/","title":"T·∫°o 1 Blog tr√™n Github b·∫±ng Hugo"},{"content":"","href":"/cv/","title":"Cvs"},{"content":"https://bit.ly/lehoangcv\n","href":"/cv/encrypt-my-cv-link/","title":"My CV link"},{"content":"","href":"/family/","title":"Families"},{"content":"M·∫π v·ªõi ch·ªã sang ƒë√∫ng ƒë·ª£t b√£o Hagibis to nh·∫•t l·ªãch s·ª≠ 60 nƒÉm g·∫ßn ƒë√¢y c·ªßa Nh·∫≠t B·∫£n,\nth·∫ø l√† b·ªã delay m·∫•t 1 ng√†y,\ntuy nhi√™n may m·∫Øn l√† b√£o v√†o ch·ªß y·∫øu Tokyo v√† khu v·ª±c Kanto ch·ª© kh√¥ng v√†o Osaka/Kyoto (ch·ªó m√† 2 ng∆∞·ªùi bay ƒë·∫øn) .\nTh·∫ø l√† v·∫´n ƒëi ch∆°i Kyoto ƒë∆∞·ª£c b√¨nh th∆∞·ªùng, hehe ^^\nNg√†y 1 (11-12-13/10/2019) Chi·ªÅu h√¥m t6 v·ªÅ s·ªõm mua v√© Shinkansen nh∆∞ng v·∫´n ph·∫£i ƒë·ª©ng su·ªët 2 ti·∫øng v√¨ h·∫øt v√© ng·ªìi\nƒê·∫øn n∆°i ·ªü nh·ªù nh√† th·∫±ng L·ª±c, tr·ªùi m∆∞a do ·∫£nh h∆∞·ªüng c·ªßa b√£o, ƒë·ªè r·ª±c c·∫£ tr·ªùi, 2 ƒë·ª©a thi nhau ch·ª•p ·∫£nh L√¢u l·∫Øm m·ªõi ƒë∆∞·ª£c ng kh√°c n·∫•u c∆°m cho, c·∫£m ƒë·ªông vl, th·∫±ng n·∫•u c≈©ng ch·ª•p ·∫£nh khoe ng y√™u n√≥ :v\ng·∫∑p m·∫π vs ch·ªã v√†o s√°ng h√¥m sau v√¨ m∆∞a b√£o b·ªã delay\nM·ªçi ng∆∞·ªùi ƒë·∫øn t·ª´ 4h s√°ng ng·ªß ƒë·∫øn 9h s√°ng d·∫≠y ƒëi ch∆°i lu√¥n :v\nB·∫Øt ƒë·∫ßu ng√†y 1, ƒëi R·ª´ng Tr√∫c Arashiyama, Ch√πa V√†ng Kinkakuji, Nara Park, r·ªìi v·ªÅ tr∆∞·ªõc c·ªïng ga Kyoto ƒÉn c∆°m su·∫•t\nNg√†y 2 (14/10/2019) Thu√™ xe c·ªßa 1 anh l√†m ·ªü Toyota ƒë·ªÉ anh ·∫•y ch·ªü ƒëi ch∆°i, ƒë·ª° m·ªèi ch√¢n, m√† l·∫°i c√≥ h∆∞·ªõng d·∫´n vi√™n lu√¥n\nƒêi c√¥ng vi√™n Kasamatsu, Amanohashidate - ch·ªó b√£i bi·ªÉn r·∫•t ƒë·∫πp, L√†ng ch√†i Ine, t·ªëi v·ªÅ Kyoto ƒÉn Sushi Musashi kh√° n·ªïi ti·∫øng, ƒÉn c≈©ng ngon v√† t∆∞∆°i n·ªØa\nMua v√© c√°p treo l√™n Kasamatsu, bao g·ªìm c·∫£ v√© ƒëi t√†u th·ªßy ƒë·∫øn Amanohashidate n·ªØa\nNg√†y 3 (15/10/2019) ƒêi ch√πa Thanh Th·ªßy Kiyomizu, ch√πa ng√†n c·ªôt Funari taisha, r·ªìi v·ªÅ Tokyo b·∫±ng Shinkansen, tham quan Ga Tokyo, r·ªìi v·ªÅ ngh·ªâ ng∆°i. T·ªëi ƒëi tham quan Shibuya, ƒÉn b√≤ Ikinari steak (ch·ªó n√†y bi·∫øt th√™m c√°ch ch·ªçn ƒë·ªô ch√≠n cho th·ªãt ƒë·ª° dai)\nNg√†y 4 (16/10/2019) Mua Tour c·ªßa Hato Bus qua Samurai Travel tr√™n facebook, ƒëi Nikko, d·ªëc irohazaka, Th√°c Kegon, v√† 1 c√°i ch√πa to\n![](https://d32yh8fbac5ivo.cloudfront.net/static/images/family/2019-10-23 21.33.33.jpg)\n![](https://d32yh8fbac5ivo.cloudfront.net/static/images/family/2019-10-23 21.33.58.jpg)\nNg√†y 5 (17/10/2019) Ti·∫øp t·ª•c ƒëi 1 Tour c·ªßa Hato Bus, ƒëi Kawaguchiko, Ph√∫ S·ªπ Tr·∫°m 5, Fuji Panoramic Ropeway, L√†ng c·ªï Oshino Hakkai\nNg√†y 6 (18/10/2019) Ngh·ªâ ng∆°i 1 ch√∫t, ƒë·ªÉ c·∫£ nh√† x·∫£ h∆°i sau 1 chu·ªói d·∫≠y s·ªõm ƒëi \u0026hellip;ch∆°i :v\nƒêi m·∫•y c·ª≠a h√†ng ƒë·ªì ƒÉn u·ªëng quanh nh√†, Unqlo ·ªü ginza, v·ªÅ ga Tokyo g·∫∑p b·∫°n ch·ªã Hi·∫øu ·ªü Character Street\nNg√†y 7 (19/10/2019) T·ª± ƒëi Teamlab Borderless ·ªü Odaiba, t·∫Øm Onsen, t·ªëi v·ªÅ ch·ªã Hi·∫øu n·∫•u M√¨ t√¥m :v\nNg√†y 8 (20/10/2019) ƒêi tour c√¥ng vi√™n Hitachi ng·∫Øm hoa, h√°i h·ªìng v√† Ami outlet\nNg√†y 9 (21/10/2019) Ng√†y cu·ªëi, t·ª± ƒëi Hakone, ƒëi c√°nh ƒë·ªìng lau, h·ªì Ashi, c·ªïng Tori jinza tr√™n h·ªì, tr·ªùi kh√° l·∫°nh, nh∆∞ng ko m∆∞a, n√≥i chung l√† ƒë·∫πp :v\nNg√†y 10 (22/10/2019) T·∫°m bi·ªát m·ªçi ng∆∞·ªùi, 5h30 ra s√¢n bay, m∆∞a to do ·∫£nh h∆∞·ªüng c·ªßa b√£o, ∆∞·ªõt h·∫øt c·∫£ ng∆∞·ªùi may m√† v·∫´n ƒë·∫øn n∆°i :))\n","href":"/family/encrypt-me-vs-chi-hieu-japan-2019-10/","title":"M·∫π vs Ch·ªã Hi·∫øu, Japan 10/2019"},{"content":"Xin c·∫£m ∆°n t·∫•t c·∫£ m·ªçi ng∆∞·ªùi ^^\nD∆∞·ªõi ƒë√¢y l√† 1 s·ªë ·∫£nh\u0026hellip; üíè üíè A new chapper is coming \u0026hellip;\nƒÇn H·ªèi 20/03/2021 Ti·ªác 23/03/2021 ƒê√≥n d√¢u 27/03/2021 üíè\n","href":"/love/encrypt-linh-hoang-wedding/","title":"Linh + Ho√†ng [ƒÇn H·ªèi - Ti·ªác - ƒê√≥n d√¢u]"},{"content":"1 bu·ªïi s√°ng ƒë·∫πp tr·ªùi ng√†y 21-02-2021, tr·ªùi se se l·∫°nh, t√¥i ƒëi ch·ª•p ·∫£nh c∆∞·ªõi :)))\nv√† d∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£\u0026hellip;üíë\nthank you ch·ªã Hi·∫øu ƒë·∫øn h·ªó tr·ª£ :v\nnh·∫´n con h·∫°c m√¨nh l√†m :))\n","href":"/love/encrypt-linh-hoang-prewedding/","title":"Linh + Ho√†ng [Prewedding]"},{"content":"","href":"/tags/ci/cd/","title":"CI/CD"},{"content":"","href":"/tags/config/","title":"Configuration"},{"content":"","href":"/tags/discord/slack/","title":"Discord/Slack"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"}]
